{
    "exportedDoiLength": 101,
    "fileName": "acm",
    "style": "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<style xmlns=\"http://purl.org/net/xbiblio/csl\" class=\"in-text\" version=\"1.0\" demote-non-dropping-particle=\"sort-only\" default-locale=\"en-US\">\n    <!-- This style was edited with the Visual CSL Editor (http://editor.citationstyles.org/visualEditor/) -->\n    <info>\n        <title>BibTeX ACM citation style</title>\n        <id>http://www.zotero.org/styles/bibtex-acm-citation-style</id>\n        <link href=\"http://www.zotero.org/styles/bibtex-acm-citation-style\" rel=\"self\"/>\n        <link href=\"http://www.bibtex.org/\" rel=\"documentation\"/>\n        <author>\n            <name>Markus Schaffner</name>\n        </author>\n        <contributor>\n            <name>Richard Karnesky</name>\n            <email>karnesky+zotero@gmail.com</email>\n            <uri>http://arc.nucapt.northwestern.edu/Richard_Karnesky</uri>\n        </contributor>\n        <category citation-format=\"author-date\"/>\n        <category field=\"generic-base\"/>\n        <updated>2018-06-11T10:52:49+00:00</updated>\n        <rights license=\"http://creativecommons.org/licenses/by-sa/3.0/\">This work is licensed under a Creative Commons Attribution-ShareAlike 3.0 License</rights>\n    </info>\n    <macro name=\"zotero2bibtexType\">\n        <choose>\n            <if type=\"BILL BOOK GRAPHIC LEGAL_CASE LEGISLATION MOTION_PICTURE SONG\" match=\"any\">\n                <choose>\n                    <if genre=\"rfc\"  match=\"any\">\n                       <text value=\"rfc\"/>\n                    </if>\n                     <else-if  genre=\"bibliography\" match=\"any\">\n                        <text value=\"bibliography\"/>\n                    </else-if>\n                    <else-if  genre=\"play_drama\" match=\"any\">\n                        <text value=\"playdrama\"/>\n                    </else-if>\n                    <else-if  genre=\"proceeding\" match=\"any\">\n                        <text value=\"proceedings\"/>\n                    </else-if>\n                    <else-if  genre=\"tech_brief\" match=\"any\">\n                        <text value=\"tech-brief\"/>\n                    </else-if>\n                    <else>\n                       <text value=\"book\"/>\n                    </else>\n                </choose>\n            </if>\n            <else-if type=\"CHAPTER\" match=\"any\">\n                <text value=\"inbook\"/>\n            </else-if>\n            <else-if type=\"ARTICLE ARTICLE_JOURNAL ARTICLE_MAGAZINE ARTICLE_NEWSPAPER\" match=\"any\">\n                <text value=\"article\"/>\n            </else-if>\n            <else-if type=\"THESIS\" match=\"any\">\n                <choose>\n                    <if variable=\"genre\">\n                        <text variable=\"genre\" text-case=\"lowercase\" strip-periods=\"true\"/>\n                    </if>\n                </choose>\n                <text value=\"thesis\"/>\n            </else-if>\n            <else-if type=\"PAPER_CONFERENCE\" match=\"any\">\n                <text value=\"inproceedings\"/>\n            </else-if>\n            <else-if type=\"REPORT\" match=\"any\">\n                <text value=\"techreport\"/>\n            </else-if>\n            <else-if type=\"DATASET\" match=\"any\">\n                <choose>\n                    <if genre=\"software\"  match=\"any\">\n                       <text value=\"software\"/>\n                    </if>\n                    <else>\n                       <text value=\"dataset\"/>\n                    </else>\n                </choose>\n            </else-if>\n            <else>\n                <text value=\"misc\"/>\n            </else>\n        </choose>\n    </macro>\n    <macro name=\"citeKey\">\n           <text variable=\"call-number\"/>\n    </macro>\n    <macro name=\"editor-short\">\n        <names variable=\"editor\">\n            <name form=\"short\" delimiter=\":\" delimiter-precedes-last=\"always\"/>\n        </names>\n    </macro>\n    <macro name=\"author-short\">\n        <names variable=\"author\">\n            <name form=\"short\" delimiter=\":\" delimiter-precedes-last=\"always\"/>\n        </names>\n    </macro>\n    <macro name=\"issued-year\">\n        <date variable=\"issued\">\n            <date-part name=\"year\"/>\n        </date>\n    </macro>\n    <macro name=\"issued-month\">\n        <choose>\n            <if type=\"ARTICLE\" match=\"any\">\n                <date variable=\"issued\">\n                    <date-part name=\"month\" form=\"short\" strip-periods=\"true\" text-case=\"lowercase\"/>\n                </date>\n            </if>\n        </choose>\n    </macro>\n    <macro name=\"issue-date\">\n        <choose>\n            <if type=\"ARTICLE ARTICLE_JOURNAL\" match=\"any\">\n                <choose>\n                    <if variable=\"note\" match=\"none\">\n                        <choose>\n                            <if variable=\"source\">\n                                 <text variable=\"source\"/>\n                            </if>\n                               <else>\n                                 <date date-parts=\"year-month\" form=\"text\" variable=\"issued\"/>\n                               </else>\n                        </choose>\n                    </if>\n                </choose>\n            </if>\n        </choose>\n    </macro>\n    <macro name=\"author\">\n        <names variable=\"author\">\n            <name sort-separator=\", \" delimiter=\" and \" delimiter-precedes-last=\"always\" name-as-sort-order=\"all\"/>\n            <label form=\"long\" text-case=\"capitalize-first\"/>\n        </names>\n    </macro>\n    <macro name=\"editor-translator\">\n        <names variable=\"editor translator\" delimiter=\", \">\n            <name sort-separator=\", \" delimiter=\" and \" delimiter-precedes-last=\"always\" name-as-sort-order=\"all\"/>\n            <label form=\"long\" text-case=\"capitalize-first\"/>\n        </names>\n    </macro>\n    <macro name=\"title\">\n        <choose>\n            <if genre=\"proceeding\" match=\"any\">\n                <text variable=\"container-title-short\" suffix=\": \"/>\n                <text variable=\"title\" text-case=\"title\"/>\n            </if>\n            <else-if type=\"ARTICLE_JOURNAL\" match=\"none\">\n                <text variable=\"title\" text-case=\"title\"/>\n            </else-if>\n        </choose>\n    </macro>\n    <macro name=\"volume\">\n        <choose>\n            <if type=\"ARTICLE BOOK ARTICLE_JOURNAL\" match=\"any\">\n                <text variable=\"volume\" prefix=\"volume = {\" suffix=\"}\"/>\n            </if>\n        </choose>\n    </macro>\n    <macro name=\"DOI\">\n        <choose>\n            <if type=\"ARTICLE PAPER_CONFERENCE\" match=\"any\">\n                <text variable=\"DOI\" prefix=\"doi = {\" suffix=\"}\"/>\n            </if>\n        </choose>\n    </macro>\n    <macro name=\"abstract\">\n        <if match=\"any\" variable=\"abstract\">\n            <text variable=\"abstract\"/>\n        </if>\n    </macro>\n    <macro name=\"URL\">\n        <text variable=\"URL\" prefix=\"url = {\" suffix=\"}\"/>\n    </macro>\n    <macro name=\"container-title\">\n        <choose>\n            <if type=\"CHAPTER PAPER_CONFERENCE\" match=\"any\">\n                <text variable=\"container-title\" prefix=\"booktitle = {\" suffix=\"}\" text-case=\"title\"/>\n            </if>\n            <else-if type=\"ARTICLE ARTICLE_JOURNAL\" match=\"any\">\n                <text variable=\"container-title\" prefix=\"journal = {\" suffix=\"}\" text-case=\"title\"/>\n            </else-if>\n        </choose>\n    </macro>\n    <macro name=\"pages\">\n        <group delimiter=\",&#10;\">\n            <choose>\n                <if match=\"any\" variable=\"collection-number\">\n                    <text variable=\"collection-number\" prefix=\"articleno = {\" suffix=\"}\"/>\n                </if>\n                <else>\n                    <text variable=\"page\" prefix=\"pages = {\" suffix=\"}\"/>\n                </else>\n            </choose>\n            <text variable=\"number-of-pages\" prefix=\"numpages = {\" suffix=\"}\"/>\n        </group>\n    </macro>\n    <macro name=\"edition\">\n        <text variable=\"edition\"/>\n    </macro>\n    <macro name=\"editor\">\n        <choose>\n            <if match=\"any\" type=\"THESIS\">\n                <names variable=\"editor\" delimiter=\", \" prefix=\"advisor = {\" suffix=\"}\">\n                    <name delimiter=\" and \" delimiter-precedes-last=\"always\" name-as-sort-order=\"all\"/>\n                </names>\n            </if>\n            <else>\n               <names variable=\"editor\" delimiter=\", \" prefix=\"editor = {\" suffix=\"}\">\n                   <name delimiter=\" and \" delimiter-precedes-last=\"always\" name-as-sort-order=\"all\"/>\n               </names>\n            </else>\n        </choose>\n    </macro>\n    <macro name=\"keyword\">\n        <choose>\n            <if type=\"ARTICLE PAPER_CONFERENCE DATASET\" match=\"any\">\n                <text variable=\"keyword\" prefix=\"keywords = {\" suffix=\"}\"/>\n            </if>\n        </choose>\n    </macro>\n    <citation et-al-min=\"11\" et-al-use-first=\"10\" disambiguate-add-year-suffix=\"true\" disambiguate-add-names=\"false\" disambiguate-add-givenname=\"false\" collapse=\"year\">\n        <layout delimiter=\"_\">\n            <text macro=\"citeKey\"/>\n        </layout>\n    </citation>\n    <bibliography hanging-indent=\"false\">\n        <layout>\n            <group display=\"right-inline\">\n                <text macro=\"zotero2bibtexType\" prefix=\"@\"/>\n                <group prefix=\"{\" suffix=\"&#10;}\" delimiter=\",&#10;\">\n                    <text macro=\"citeKey\"/>\n                    <text macro=\"author\" prefix=\"author = {\" suffix=\"}\"/>\n                    <text macro=\"editor\"/>\n                    <text macro=\"title\" prefix=\"title = {\" suffix=\"}\"/>\n                    <text macro=\"issued-year\" prefix=\"year = {\" suffix=\"}\"/>\n                    <text macro=\"issue-date\" prefix=\"issue_date = {\" suffix=\"}\"/>\n                    <text variable=\"ISBN\" prefix=\"isbn = {\" suffix=\"}\"/>\n                    <text variable=\"publisher\" prefix=\"publisher = {\" suffix=\"}\"/>\n                    <text variable=\"publisher-place\" prefix=\"address = {\" suffix=\"}\"/>\n                    <text variable=\"chapter-number\" prefix=\"chapter = {\" suffix=\"}\"/>\n                    <text macro=\"edition\" prefix=\"edition = {\" suffix=\"}\"/>\n                    <text macro=\"volume\"/>\n                    <text variable=\"issue\" prefix=\"number = {\" suffix=\"}\"/>\n                    <text variable=\"ISSN\" prefix=\"issn = {\" suffix=\"}\"/>\n                    <text variable=\"archive_location\" prefix=\"archiveLocation = {\" suffix=\"}\"/>\n                    <text macro=\"URL\"/>\n                    <text macro=\"DOI\"/>\n                    <text macro=\"abstract\" prefix=\"abstract = {\" suffix=\"}\"/>\n                    <text variable=\"note\" prefix=\"note = {\" suffix=\"}\"/>\n                    <text macro=\"container-title\"/>\n                    <text macro=\"issued-month\" prefix=\"month = {\" suffix=\"}\"/>\n                    <text macro=\"pages\"/>\n                    <text macro=\"keyword\"/>\n                    <text variable=\"event-place\" prefix=\"location = {\" suffix=\"}\"/>\n                    <choose>\n                        <if type=\"PAPER_CONFERENCE\" match=\"any\">\n                            <text variable=\"collection-title\" prefix=\"series = {\" suffix=\"}\"/>\n                        </if>\n                        <else>\n                            <text variable=\"collection-title\" prefix=\"collection = {\" suffix=\"}\"/>\n                        </else>\n                    </choose>\n                </group>\n            </group>\n        </layout>\n    </bibliography>\n</style>\n",
    "suffix": "bib",
    "locale": "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<locale xmlns=\"http://purl.org/net/xbiblio/csl\" version=\"1.0\" xml:lang=\"en-US\">\n  <info>\n    <rights license=\"http://creativecommons.org/licenses/by-sa/3.0/\">This work is licensed under a Creative Commons Attribution-ShareAlike 3.0 License</rights>\n    <updated>2012-07-04T23:31:02+00:00</updated>\n  </info>\n  <style-options punctuation-in-quote=\"true\"\n                 leading-noise-words=\"a,an,the\"\n                 name-as-sort-order=\"ja zh kr my hu vi\"\n                 name-never-short=\"ja zh kr my hu vi\"/>\n  <date form=\"text\">\n    <date-part name=\"month\" suffix=\" \"/>\n    <date-part name=\"day\" suffix=\", \"/>\n    <date-part name=\"year\"/>\n  </date>\n  <date form=\"numeric\">\n    <date-part name=\"month\" form=\"numeric-leading-zeros\" suffix=\"/\"/>\n    <date-part name=\"day\" form=\"numeric-leading-zeros\" suffix=\"/\"/>\n    <date-part name=\"year\"/>\n  </date>\n  <terms>\n    <term name=\"radio-broadcast\">radio broadcast</term>\n    <term name=\"television-broadcast\">television broadcast</term>\n    <term name=\"podcast\">podcast</term>\n    <term name=\"instant-message\">instant message</term>\n    <term name=\"email\">email</term>\n    <term name=\"number-of-volumes\">\n      <single>volume</single>\n      <multiple>volumes</multiple>\n    </term>\n    <term name=\"accessed\">accessed</term>\n    <term name=\"and\">and</term>\n    <term name=\"and\" form=\"symbol\">&amp;</term>\n    <term name=\"and others\">and others</term>\n    <term name=\"anonymous\">anonymous</term>\n    <term name=\"anonymous\" form=\"short\">anon.</term>\n    <term name=\"at\">at</term>\n    <term name=\"available at\">available at</term>\n    <term name=\"by\">by</term>\n    <term name=\"circa\">circa</term>\n    <term name=\"circa\" form=\"short\">c.</term>\n    <term name=\"cited\">cited</term>\n    <term name=\"edition\">\n      <single>edition</single>\n      <multiple>editions</multiple>\n    </term>\n    <term name=\"edition\" form=\"short\">ed.</term>\n    <term name=\"et-al\">et al.</term>\n    <term name=\"forthcoming\">forthcoming</term>\n    <term name=\"from\">from</term>\n    <term name=\"ibid\">ibid.</term>\n    <term name=\"in\">in</term>\n    <term name=\"in press\">in press</term>\n    <term name=\"internet\">internet</term>\n    <term name=\"interview\">interview</term>\n    <term name=\"letter\">letter</term>\n    <term name=\"no date\">no date</term>\n    <term name=\"no date\" form=\"short\">n.d.</term>\n    <term name=\"online\">online</term>\n    <term name=\"presented at\">presented at the</term>\n    <term name=\"reference\">\n      <single>reference</single>\n      <multiple>references</multiple>\n    </term>\n    <term name=\"reference\" form=\"short\">\n      <single>ref.</single>\n      <multiple>refs.</multiple>\n    </term>\n    <term name=\"retrieved\">retrieved</term>\n    <term name=\"scale\">scale</term>\n    <term name=\"version\">version</term>\n\n    <!-- ANNO DOMINI; BEFORE CHRIST -->\n    <term name=\"ad\">AD</term>\n    <term name=\"bc\">BC</term>\n\n    <!-- PUNCTUATION -->\n    <term name=\"open-quote\">“</term>\n    <term name=\"close-quote\">”</term>\n    <term name=\"open-inner-quote\">‘</term>\n    <term name=\"close-inner-quote\">’</term>\n    <term name=\"page-range-delimiter\">–</term>\n\n    <!-- ORDINALS -->\n    <term name=\"ordinal\">th</term>\n    <term name=\"ordinal-01\">st</term>\n    <term name=\"ordinal-02\">nd</term>\n    <term name=\"ordinal-03\">rd</term>\n    <term name=\"ordinal-11\">th</term>\n    <term name=\"ordinal-12\">th</term>\n    <term name=\"ordinal-13\">th</term>\n\n    <!-- LONG ORDINALS -->\n    <term name=\"long-ordinal-01\">first</term>\n    <term name=\"long-ordinal-02\">second</term>\n    <term name=\"long-ordinal-03\">third</term>\n    <term name=\"long-ordinal-04\">fourth</term>\n    <term name=\"long-ordinal-05\">fifth</term>\n    <term name=\"long-ordinal-06\">sixth</term>\n    <term name=\"long-ordinal-07\">seventh</term>\n    <term name=\"long-ordinal-08\">eighth</term>\n    <term name=\"long-ordinal-09\">ninth</term>\n    <term name=\"long-ordinal-10\">tenth</term>\n\n    <!-- LONG LOCATOR FORMS -->\n    <term name=\"book\">\n      <single>book</single>\n      <multiple>books</multiple>\n    </term>\n    <term name=\"chapter\">\n      <single>chapter</single>\n      <multiple>chapters</multiple>\n    </term>\n    <term name=\"column\">\n      <single>column</single>\n      <multiple>columns</multiple>\n    </term>\n    <term name=\"figure\">\n      <single>figure</single>\n      <multiple>figures</multiple>\n    </term>\n    <term name=\"folio\">\n      <single>folio</single>\n      <multiple>folios</multiple>\n    </term>\n    <term name=\"issue\">\n      <single>number</single>\n      <multiple>numbers</multiple>\n    </term>\n    <term name=\"line\">\n      <single>line</single>\n      <multiple>lines</multiple>\n    </term>\n    <term name=\"note\">\n      <single>note</single>\n      <multiple>notes</multiple>\n    </term>\n    <term name=\"opus\">\n      <single>opus</single>\n      <multiple>opera</multiple>\n    </term>\n    <term name=\"page\">\n      <single>page</single>\n      <multiple>pages</multiple>\n    </term>\n    <term name=\"paragraph\">\n      <single>paragraph</single>\n      <multiple>paragraph</multiple>\n    </term>\n    <term name=\"part\">\n      <single>part</single>\n      <multiple>parts</multiple>\n    </term>\n    <term name=\"section\">\n      <single>section</single>\n      <multiple>sections</multiple>\n    </term>\n    <term name=\"sub verbo\">\n      <single>sub verbo</single>\n      <multiple>sub verbis</multiple>\n    </term>\n    <term name=\"verse\">\n      <single>verse</single>\n      <multiple>verses</multiple>\n    </term>\n    <term name=\"volume\">\n      <single>volume</single>\n      <multiple>volumes</multiple>\n    </term>\n\n    <!-- SHORT LOCATOR FORMS -->\n    <term name=\"book\" form=\"short\">bk.</term>\n    <term name=\"chapter\" form=\"short\">chap.</term>\n    <term name=\"column\" form=\"short\">col.</term>\n    <term name=\"figure\" form=\"short\">fig.</term>\n    <term name=\"folio\" form=\"short\">f.</term>\n    <term name=\"issue\" form=\"short\">no.</term>\n    <term name=\"line\" form=\"short\">l.</term>\n    <term name=\"note\" form=\"short\">n.</term>\n    <term name=\"opus\" form=\"short\">op.</term>\n    <term name=\"page\" form=\"short\">\n      <single>p.</single>\n      <multiple>pp.</multiple>\n    </term>\n    <term name=\"paragraph\" form=\"short\">para.</term>\n    <term name=\"part\" form=\"short\">pt.</term>\n    <term name=\"section\" form=\"short\">sec.</term>\n    <term name=\"sub verbo\" form=\"short\">\n      <single>s.v.</single>\n      <multiple>s.vv.</multiple>\n    </term>\n    <term name=\"verse\" form=\"short\">\n      <single>v.</single>\n      <multiple>vv.</multiple>\n    </term>\n    <term name=\"volume\" form=\"short\">\n      <single>vol.</single>\n      <multiple>vols.</multiple>\n    </term>\n\n    <!-- SYMBOL LOCATOR FORMS -->\n    <term name=\"paragraph\" form=\"symbol\">\n      <single>¶</single>\n      <multiple>¶¶</multiple>\n    </term>\n    <term name=\"section\" form=\"symbol\">\n      <single>§</single>\n      <multiple>§§</multiple>\n    </term>\n\n    <!-- LONG ROLE FORMS -->\n    <term name=\"director\">\n      <single>director</single>\n      <multiple>directors</multiple>\n    </term>\n    <term name=\"editor\">\n      <single>editor</single>\n      <multiple>editors</multiple>\n    </term>\n    <term name=\"editorial-director\">\n      <single>editor</single>\n      <multiple>editors</multiple>\n    </term>\n    <term name=\"illustrator\">\n      <single>illustrator</single>\n      <multiple>illustrators</multiple>\n    </term>\n    <term name=\"translator\">\n      <single>translator</single>\n      <multiple>translators</multiple>\n    </term>\n    <term name=\"editortranslator\">\n      <single>editor &amp; translator</single>\n      <multiple>editors &amp; translators</multiple>\n    </term>\n\n    <!-- SHORT ROLE FORMS -->\n    <term name=\"director\" form=\"short\">\n      <single>dir.</single>\n      <multiple>dirs.</multiple>\n    </term>\n    <term name=\"editor\" form=\"short\">\n      <single>ed.</single>\n      <multiple>eds.</multiple>\n    </term>\n    <term name=\"editorial-director\" form=\"short\">\n      <single>ed.</single>\n      <multiple>eds.</multiple>\n    </term>\n    <term name=\"illustrator\" form=\"short\">\n      <single>ill.</single>\n      <multiple>ills.</multiple>\n    </term>\n    <term name=\"translator\" form=\"short\">\n      <single>tran.</single>\n      <multiple>trans.</multiple>\n    </term>\n    <term name=\"editortranslator\" form=\"short\">\n      <single>ed. &amp; tran.</single>\n      <multiple>eds. &amp; trans.</multiple>\n    </term>\n\n    <!-- VERB ROLE FORMS -->\n    <term name=\"director\" form=\"verb\">directed by</term>\n    <term name=\"editor\" form=\"verb\">edited by</term>\n    <term name=\"editorial-director\" form=\"verb\">edited by</term>\n    <term name=\"illustrator\" form=\"verb\">illustrated by</term>\n    <term name=\"interviewer\" form=\"verb\">interview by</term>\n    <term name=\"recipient\" form=\"verb\">to</term>\n    <term name=\"reviewed-author\" form=\"verb\">by</term>\n    <term name=\"translator\" form=\"verb\">translated by</term>\n    <term name=\"editortranslator\" form=\"verb\">edited &amp; translated by</term>\n\n    <!-- SHORT VERB ROLE FORMS -->\n    <term name=\"container-author\" form=\"verb-short\">by</term>\n    <term name=\"director\" form=\"verb-short\">dir.</term>\n    <term name=\"editor\" form=\"verb-short\">ed.</term>\n    <term name=\"editorial-director\" form=\"verb-short\">ed.</term>\n    <term name=\"illustrator\" form=\"verb-short\">illus.</term>\n    <term name=\"translator\" form=\"verb-short\">trans.</term>\n    <term name=\"editortranslator\" form=\"verb-short\">ed. &amp; trans.</term>\n\n    <!-- LONG MONTH FORMS -->\n    <term name=\"month-01\">January</term>\n    <term name=\"month-02\">February</term>\n    <term name=\"month-03\">March</term>\n    <term name=\"month-04\">April</term>\n    <term name=\"month-05\">May</term>\n    <term name=\"month-06\">June</term>\n    <term name=\"month-07\">July</term>\n    <term name=\"month-08\">August</term>\n    <term name=\"month-09\">September</term>\n    <term name=\"month-10\">October</term>\n    <term name=\"month-11\">November</term>\n    <term name=\"month-12\">December</term>\n\n    <!-- SHORT MONTH FORMS -->\n    <term name=\"month-01\" form=\"short\">Jan.</term>\n    <term name=\"month-02\" form=\"short\">Feb.</term>\n    <term name=\"month-03\" form=\"short\">Mar.</term>\n    <term name=\"month-04\" form=\"short\">Apr.</term>\n    <term name=\"month-05\" form=\"short\">May</term>\n    <term name=\"month-06\" form=\"short\">Jun.</term>\n    <term name=\"month-07\" form=\"short\">Jul.</term>\n    <term name=\"month-08\" form=\"short\">Aug.</term>\n    <term name=\"month-09\" form=\"short\">Sep.</term>\n    <term name=\"month-10\" form=\"short\">Oct.</term>\n    <term name=\"month-11\" form=\"short\">Nov.</term>\n    <term name=\"month-12\" form=\"short\">Dec.</term>\n\n    <!-- SEASONS -->\n    <term name=\"season-01\">Spring</term>\n    <term name=\"season-02\">Summer</term>\n    <term name=\"season-03\">Autumn</term>\n    <term name=\"season-04\">Winter</term>\n  </terms>\n</locale>\n",
    "contentType": "Application/x-bibtex",
    "items": [
        {
            "10.14778/2733004.2733037": {
                "id": "10.14778/2733004.2733037",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Lei",
                        "given": "Chuan"
                    },
                    {
                        "family": "Zhuang",
                        "given": "Zhongfang"
                    },
                    {
                        "family": "Rundensteiner",
                        "given": "Elke A."
                    },
                    {
                        "family": "Eltabakh",
                        "given": "Mohamed Y."
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2014,
                            8,
                            1
                        ]
                    ]
                },
                "abstract": "This demonstration presents the Redoop infrastructure, the first full-fledged MapReduce framework with native support for recurring big data queries. Recurring queries, repeatedly being executed for long periods of time over evolving high-volume data, have become a bedrock component in most large-scale data analytic applications. Redoop is a comprehensive extension to Hadoop that pushes the support and optimization of recurring queries into Hadoop's core functionality. While backward compatible with regular MapReduce jobs, Redoop achieves an order of magnitude better performance than Hadoop for recurring workloads. Redoop employs innovative window-aware optimization techniques for such recurring workloads including adaptive window-aware data partitioning, cache-aware task scheduling, and inter-window caching mechanisms. We will demonstrate Redoop's capabilities on a compute cluster against real life workloads including click-stream and sensor data analysis.",
                "call-number": "10.14778/2733004.2733037",
                "container-title": "Proc. VLDB Endow.",
                "DOI": "10.14778/2733004.2733037",
                "ISSN": "2150-8097",
                "issue": "13",
                "number-of-pages": "4",
                "page": "1589–1592",
                "publisher": "VLDB Endowment",
                "source": "August 2014",
                "title": "Redoop infrastructure for recurring big data queries",
                "URL": "https://doi.org/10.14778/2733004.2733037",
                "volume": "7"
            }
        },
        {
            "10.14778/2733004.2733045": {
                "id": "10.14778/2733004.2733045",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Cao",
                        "given": "Lei"
                    },
                    {
                        "family": "Wang",
                        "given": "Qingyang"
                    },
                    {
                        "family": "Rundensteiner",
                        "given": "Elke A."
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2014,
                            8,
                            1
                        ]
                    ]
                },
                "abstract": "We demonstrate our VSOutlier system for supporting interactive exploration of outliers in big data streams. VSOutlier not only supports a rich variety of outlier types supported by innovative and efficient outlier detection strategies, but also provides a rich set of interactive interfaces to explore outliers in real time. Using the stock transactions dataset from the US stock market and the moving objects dataset from MITRE, we demonstrate that the VSOutlier system enables analysts to more efficiently identify, understand, and respond to phenomena of interest in near real-time even when applied to high volume streams.",
                "call-number": "10.14778/2733004.2733045",
                "container-title": "Proc. VLDB Endow.",
                "DOI": "10.14778/2733004.2733045",
                "ISSN": "2150-8097",
                "issue": "13",
                "number-of-pages": "4",
                "page": "1621–1624",
                "publisher": "VLDB Endowment",
                "source": "August 2014",
                "title": "Interactive outlier exploration in big data streams",
                "URL": "https://doi.org/10.14778/2733004.2733045",
                "volume": "7"
            }
        },
        {
            "10.1145/2674026.2674031": {
                "id": "10.1145/2674026.2674031",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Tran",
                        "given": "Dang-Hoan"
                    },
                    {
                        "family": "Gaber",
                        "given": "Mohamed Medhat"
                    },
                    {
                        "family": "Sattler",
                        "given": "Kai-Uwe"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2014,
                            9,
                            25
                        ]
                    ]
                },
                "abstract": "Big Data is identified by its three Vs, namely velocity, volume, and variety. The area of data stream processing has long dealt with the former two Vs velocity and volume. Over a decade of intensive research, the community has provided many important research discoveries in the area. The third V of Big Data has been the result of social media and the large unstructured data it generates. Streaming techniques have also been proposed recently addressing this emerging need. However, a hidden factor can represent an important fourth V, that is variability or change. Our world is changing rapidly, and accounting to variability is a crucial success factor. This paper provides a survey of change detection techniques as applied to streaming data. The review is timely with the rise of Big Data technologies, and the need to have this important aspect highlighted and its techniques categorized and detailed.",
                "call-number": "10.1145/2674026.2674031",
                "container-title": "SIGKDD Explor. Newsl.",
                "DOI": "10.1145/2674026.2674031",
                "ISSN": "1931-0145",
                "issue": "1",
                "number-of-pages": "9",
                "page": "30–38",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "June 2014",
                "title": "Change detection in streaming data in the era of big data: models and issues",
                "URL": "https://doi.org/10.1145/2674026.2674031",
                "volume": "16"
            }
        },
        {
            "10.1145/2342441.2342462": {
                "id": "10.1145/2342441.2342462",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Wang",
                        "given": "Guohui"
                    },
                    {
                        "family": "Ng",
                        "given": "T.S. Eugene"
                    },
                    {
                        "family": "Shaikh",
                        "given": "Anees"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2012,
                            8,
                            13
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2012,
                            8,
                            13
                        ]
                    ]
                },
                "abstract": "Recent advances of software defined networking and optical switching technology make it possible to program the network stack all the way from physical topology to flow level traffic control. In this paper, we leverage the combination of SDN controller with optical switching to explore the tight integration of application and network control. We particularly study the run-time network configuration for big data applications to jointly optimize application performance and network utilization. We use Hadoop as an example to discuss the integrated network control architecture, job scheduling, topology and routing configuration mechanisms for Hadoop jobs. Our analysis suggests that such an integrated control has great potential to improve application performance with relatively small configuration overhead. We believe our study shows early promise of achieving the long-term goal of tight network and application integration using SDN.",
                "call-number": "10.1145/2342441.2342462",
                "collection-title": "HotSDN '12",
                "container-title": "Proceedings of the first workshop on Hot topics in software defined networks",
                "DOI": "10.1145/2342441.2342462",
                "event-place": "Helsinki, Finland",
                "ISBN": "9781450314770",
                "keyword": "optical circuit switching, big data applications, software defined networking",
                "number-of-pages": "6",
                "page": "103–108",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Programming your network at run-time for big data applications",
                "URL": "https://doi.org/10.1145/2342441.2342462"
            }
        },
        {
            "10.1145/3265007.3265015": {
                "id": "10.1145/3265007.3265015",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Zhang",
                        "given": "Bin"
                    },
                    {
                        "family": "Zhu",
                        "given": "Guobin"
                    },
                    {
                        "family": "Yu",
                        "given": "Riji"
                    },
                    {
                        "family": "Wei",
                        "given": "Shaoyan"
                    },
                    {
                        "family": "Peng",
                        "given": "Ling"
                    },
                    {
                        "family": "Fei",
                        "given": "Dingzhou"
                    },
                    {
                        "family": "Yu",
                        "given": "Xuesong"
                    },
                    {
                        "family": "Pan",
                        "given": "Peiwen"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            6,
                            13
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            6,
                            13
                        ]
                    ]
                },
                "abstract": "With the development of modern society. The unprecedented prosperity of science & technology and finance. Objects formed a huge amount of track data in its movement. The large amount of track data contains rich spatio-temporal characteristics information, it exposes the privacy information such as the behavior characteristics, interests and social habits of mobile objects. Through trajectory data processing technology. It can excavate information such as human activity pattern and behavior characteristic, urban vehicle movement characteristic, atmospheric environment change law and so on. The large amount of track data also reveals the privacy information, such as the behavior characteristics, interests and social habits of mobile objects, which is rich in spatio-temporal characteristics information. This paper begins with the significance of the study of trajectory big data. Introducing track big data acquisition mode and social application in various fields, In the specific application With the development of modern society. The unprecedented prosperity of science & technology and finance. Objects formed a huge amount of track data in its movement. The large amount of track data contains rich spatio-temporal characteristics information, it exposes the privacy information such as the behavior characteristics, interests and social habits of mobile objects. Through trajectory data processing technology. It can excavate information such as human activity pattern and behavior characteristic, urban vehicle movement characteristic, atmospheric environment change law and so on. The large amount of track data also reveals the privacy information, such as the behavior characteristics, interests and social habits of mobile objects, which is rich in spatio-temporal characteristics information. This paper begins with the significance of the study of trajectory big data. Introducing track big data acquisition mode and social application in various fields, In the specific application, we pay more attention to the object's trajectory privacy protection. Applying the big data of trajectory to social governance; In addition, the application of big data in social governance is summarized and the future work prospect is discussed. We pay more attention to the object's trajectory privacy protection. Applying the big data of trajectory to social governance; In addition, the application of big data in social governance is summarized and the future work prospect is discussed.",
                "call-number": "10.1145/3265007.3265015",
                "collection-title": "ACIT 2018",
                "container-title": "Proceedings of the 6th ACM/ACIS International Conference on Applied Computing and Information Technology",
                "DOI": "10.1145/3265007.3265015",
                "event-place": "Kunming, China",
                "ISBN": "9781450365741",
                "keyword": "Social Governance, Trajectory Big Data, Social Computing, Privacy Protection",
                "number-of-pages": "5",
                "page": "38–42",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Research on the Innovation of Trajectory Big Data in Social Governance",
                "URL": "https://doi.org/10.1145/3265007.3265015"
            }
        },
        {
            "10.1145/2903220.2903255": {
                "id": "10.1145/2903220.2903255",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Tsapanos",
                        "given": "Nikolaos"
                    },
                    {
                        "family": "Tefas",
                        "given": "Anastasios"
                    },
                    {
                        "family": "Nikolaidis",
                        "given": "Nikolaos"
                    },
                    {
                        "family": "Pitas",
                        "given": "Ioannis"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            5,
                            18
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2016,
                            5,
                            18
                        ]
                    ]
                },
                "abstract": "Data clustering is an unsupervised learning task that has found many applications in various scientific fields. The goal is to find subgroups of closely related data samples (clusters) in a set of unlabeled data. A classic clustering algorithm is the so-called k-Means. It is very popular, however, it is also unable to handle cases in which the clusters are not linearly separable. Kernel k-Means is a state of the art clustering algorithm, which employs the kernel trick, in order to perform clustering on a higher dimensionality space, thus overcoming the limitations of classic k-Means regarding the non linear separability of the input data. It has recently received a distributed implementation, named Trimmed Kernel k-Means, following the MapReduce distributed computing model. In addition to performing the computations in a distributed manner, Trimmed Kernel k-Means also trims the kernel matrix, in order to reduce the memory requirements and improve performance. The trimming of each row of the kernel matrix is achieved by attempting to estimate the cardinality of the cluster that the corresponding sample belongs to, and removing the kernel matrix entries connecting the sample to samples that probably belong to another cluster. The Spark cluster computing framework was used for the distributed implementation. In this paper, we present a distributed clustering scheme that is based on Trimmed Kernel k-Means, which employs subsampling, in order to be able to efficiently perform clustering on an extremely large dataset. The results indicate that the proposed method run much faster than the original Trimmed Kernel k-Means, while still providing clustering performance competitive with other state of the art kernel approaches.",
                "call-number": "10.1145/2903220.2903255",
                "collection-number": "28",
                "collection-title": "SETN '16",
                "container-title": "Proceedings of the 9th Hellenic Conference on Artificial Intelligence",
                "DOI": "10.1145/2903220.2903255",
                "event-place": "Thessaloniki, Greece",
                "ISBN": "9781450337342",
                "keyword": "clustering, Big Data, distributed computing, MapReduce, Kernel k-Means",
                "number": "Article 28",
                "number-of-pages": "5",
                "page": "1–5",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Efficient MapReduce Kernel k-Means for Big Data Clustering",
                "URL": "https://doi.org/10.1145/2903220.2903255"
            }
        },
        {
            "10.1145/2968332": {
                "id": "10.1145/2968332",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Geisler",
                        "given": "Sandra"
                    },
                    {
                        "family": "Quix",
                        "given": "Christoph"
                    },
                    {
                        "family": "Weber",
                        "given": "Sven"
                    },
                    {
                        "family": "Jarke",
                        "given": "Matthias"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            10,
                            6
                        ]
                    ]
                },
                "abstract": "Data Stream Management Systems (DSMS) provide real-time data processing in an effective way, but there is always a tradeoff between data quality (DQ) and performance. We propose an ontology-based data quality framework for relational DSMS that includes DQ measurement and monitoring in a transparent, modular, and flexible way. We follow a threefold approach that takes the characteristics of relational data stream management for DQ metrics into account. While (1) Query Metrics respect changes in data quality due to query operations, (2) Content Metrics allow the semantic evaluation of data in the streams. Finally, (3) Application Metrics allow easy user-defined computation of data quality values to account for application specifics. Additionally, a quality monitor allows us to observe data quality values and take counteractions to balance data quality and performance. The framework has been designed along a DQ management methodology suited for data streams. It has been evaluated in the domains of transportation systems and health monitoring.",
                "call-number": "10.1145/2968332",
                "collection-number": "18",
                "container-title": "J. Data and Information Quality",
                "DOI": "10.1145/2968332",
                "ISSN": "1936-1955",
                "issue": "4",
                "keyword": "ontologies, data quality control, data quality assessment, Data streams",
                "number": "Article 18",
                "number-of-pages": "34",
                "page": "1–34",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "October 2016",
                "title": "Ontology-Based Data Quality Management for Data Streams",
                "URL": "https://doi.org/10.1145/2968332",
                "volume": "7"
            }
        },
        {
            "10.1145/3469213.3470409": {
                "id": "10.1145/3469213.3470409",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Cai",
                        "given": "Liya"
                    },
                    {
                        "family": "Yao",
                        "given": "Shuchun"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            5,
                            28
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2021,
                            5,
                            28
                        ]
                    ]
                },
                "abstract": "Under the information age, a large number of structurally complicated and unmanageable data generated by all walks of life is increasingly multiplied. As a newly emerging technology, compared with the traditional data management methods, blockchain is characterized in advantages such as decentralization, de-trustness, and data encryption, which enables it to solve data management problems existing in big data applications in a more efficient way. Since government big data involves huge economic and social values, it is of great significance these curity sharing of government big data for the transformation of the government and social demand patterns. Taking government big data as an example, this paper has analyzed the feasibility of big data security sharing that is based on blockchain, and proposed the demand model and support plan of big data security sharing. Last but not least, it expounded the characteristics of big data security sharing based on blockchain technology, so as to provide beneficial references for the e-government big data security sharing of the government.",
                "call-number": "10.1145/3469213.3470409",
                "collection-number": "202",
                "collection-title": "ICAIIS 2021",
                "container-title": "2021 2nd International Conference on Artificial Intelligence and Information Systems",
                "DOI": "10.1145/3469213.3470409",
                "event-place": "Chongqing, China",
                "ISBN": "9781450390200",
                "number": "Article 202",
                "number-of-pages": "3",
                "page": "1–3",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Application of big data technology in blockchain computing",
                "URL": "https://doi.org/10.1145/3469213.3470409"
            }
        },
        {
            "10.1145/3097983.3105810": {
                "id": "10.1145/3097983.3105810",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Karpatne",
                        "given": "Anuj"
                    },
                    {
                        "family": "Kumar",
                        "given": "Vipin"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            8,
                            4
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            8,
                            13
                        ]
                    ]
                },
                "abstract": "The climate and Earth sciences have recently undergone a rapid transformation from a data-poor to a data-rich environment. In particular, massive amount of data about Earth and its environment is now continuously being generated by a large number of Earth observing satellites as well as physics-based earth system models running on large-scale computational platforms. These massive and information-rich datasets offer huge potential for understanding how the Earth's climate and ecosystem have been changing and how they are being impacted by humans actions. We discuss the challenges involved in analyzing these massive data sets as well as opportunities they present for both advancing machine learning as well as the science of climate change.",
                "call-number": "10.1145/3097983.3105810",
                "collection-title": "KDD '17",
                "container-title": "Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
                "DOI": "10.1145/3097983.3105810",
                "event-place": "Halifax, NS, Canada",
                "ISBN": "9781450348874",
                "keyword": "machine learning, climate science, earth observation data",
                "number-of-pages": "2",
                "page": "21–22",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Big Data in Climate: Opportunities and Challenges for Machine Learning",
                "URL": "https://doi.org/10.1145/3097983.3105810"
            }
        },
        {
            "10.1145/3127479.3132685": {
                "id": "10.1145/3127479.3132685",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Yang",
                        "given": "Chen"
                    },
                    {
                        "family": "Guo",
                        "given": "Qi"
                    },
                    {
                        "family": "Meng",
                        "given": "Xiaofeng"
                    },
                    {
                        "family": "Xin",
                        "given": "Rihui"
                    },
                    {
                        "family": "Wang",
                        "given": "Chunkai"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            9,
                            24
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            9,
                            24
                        ]
                    ]
                },
                "abstract": "Big data systems for large-scale data processing are now in widespread use. To improve their performance, both academia and industry have expended a great deal of effort in the analysis of performance bottlenecks. Most big data systems, as Hadoop and Spark, allow distributed computing across clusters. As a result, the execution of systems always parallelizes the use of the CPU, memory, disk and network. If a given resource has the greatest limiting impact on performance, systems will be bottlenecked on it. For a system designer, it is effective for the improvement of performance to tune the bottleneck resource. The key point for the aforementioned scenario is how to determine the bottleneck resource. The nature clue is to quantify the impact of the four major components and identify one causing the greatest impact factor as the bottleneck resource.",
                "call-number": "10.1145/3127479.3132685",
                "collection-title": "SoCC '17",
                "container-title": "Proceedings of the 2017 Symposium on Cloud Computing",
                "DOI": "10.1145/3127479.3132685",
                "event-place": "Santa Clara, California",
                "ISBN": "9781450350280",
                "number-of-pages": "1",
                "page": "639",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Revisiting performance in big data systems: an resource decoupling approach",
                "URL": "https://doi.org/10.1145/3127479.3132685"
            }
        },
        {
            "10.1145/3491204.3527473": {
                "id": "10.1145/3491204.3527473",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Sarnayak",
                        "given": "Samyak S."
                    },
                    {
                        "family": "Ahuja",
                        "given": "Aditi"
                    },
                    {
                        "family": "Kesavarapu",
                        "given": "Pranav"
                    },
                    {
                        "family": "Naik",
                        "given": "Aayush"
                    },
                    {
                        "family": "Kumar V.",
                        "given": "Santhosh"
                    },
                    {
                        "family": "Kalambur",
                        "given": "Subramaniam"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2022,
                            7,
                            14
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2022,
                            7,
                            14
                        ]
                    ]
                },
                "abstract": "Java uses automatic memory allocation where the user does not have to explicitly free used memory. This is done by the garbage collector. Garbage Collection (GC) can take up a significant amount of time, especially in Big Data applications running large workloads where garbage collection can take up to 50 percent of the application's run time. Although benchmarks have been designed to trace garbage collection events, these are not specifically suited for Big Data workloads, due to their unique memory usage patterns. We have developed a free and open source pipeline to extract and analyze object-level details from any Java program including benchmarks and Big Data applications such as Hadoop. The data contains information such as lifetime, class and allocation site of every object allocated by the program. Through the analysis of this data, we propose a small set of benchmarks designed to emulate some of the patterns observed in Big Data applications. These benchmarks also allow us to experiment and compare some Java programming patterns.",
                "call-number": "10.1145/3491204.3527473",
                "collection-title": "ICPE '22",
                "container-title": "Companion of the 2022 ACM/SPEC International Conference on Performance Engineering",
                "DOI": "10.1145/3491204.3527473",
                "event-place": "Bejing, China",
                "ISBN": "9781450391597",
                "keyword": "java, hadoop, java virtual machine, big data, garbage collection",
                "number-of-pages": "8",
                "page": "121–128",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Analysis of Garbage Collection Patterns to Extend Microbenchmarks for Big Data Workloads",
                "URL": "https://doi.org/10.1145/3491204.3527473"
            }
        },
        {
            "10.1145/3093338.3093351": {
                "id": "10.1145/3093338.3093351",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Liu",
                        "given": "Fang Cherry"
                    },
                    {
                        "family": "Xu",
                        "given": "Weijia"
                    },
                    {
                        "family": "Belgin",
                        "given": "Mehmet"
                    },
                    {
                        "family": "Huang",
                        "given": "Ruizhu"
                    },
                    {
                        "family": "Fleischer",
                        "given": "Blake C."
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            7,
                            9
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            7,
                            9
                        ]
                    ]
                },
                "abstract": "Research computing centers provide a wide variety of services including large-scale computing resources, data storage, high-speed interconnect and scientific software repositories to facilitate continuous competitive research. Efficient management of these complex resources and services, as well as ensuring their fair use by a large number of researchers from different scientific domains are key to a center's success. Almost all research centers use monitoring services based on real time data gathered from systems and services, but often lack tools to perform a deeper analysis on large volumes of historical logs for identifying insightful trends from recurring events. The size of collected data can be massive, posing significant challenges for the use of conventional tools for this kind of analysis. This paper describes a big data pipeline based on Hadoop and Spark technologies, developed in close collaboration between TACC and Georgia Tech. This data pipeline is capable of processing large volumes of data collected from schedulers using PBSTools, making it possible to run a deep analysis in minutes as opposed to hours with conventional tools. Our component-based pipeline design adds the flexibility of plugging in different components, as well as promotes data reuse. Using this data pipeline, we demonstrate the process of formulating several critical operational questions around researcher behavior, systems health, operational aspects and software usage trends, all of which are critical factors in determining solutions and strategies for efficient management of research computing centers.",
                "call-number": "10.1145/3093338.3093351",
                "collection-number": "31",
                "collection-title": "PEARC17",
                "container-title": "Proceedings of the Practice and Experience in Advanced Research Computing 2017 on Sustainability, Success and Impact",
                "DOI": "10.1145/3093338.3093351",
                "event-place": "New Orleans, LA, USA",
                "ISBN": "9781450352727",
                "keyword": "big data, spark, log analysis, hadoop",
                "number": "Article 31",
                "number-of-pages": "8",
                "page": "1–8",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Insights into Research Computing Operations using Big Data-Powered Log Analysis",
                "URL": "https://doi.org/10.1145/3093338.3093351"
            }
        },
        {
            "10.1145/3264560.3264567": {
                "id": "10.1145/3264560.3264567",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Gaona-García",
                        "given": "Paulo Alonso"
                    },
                    {
                        "family": "Martín-Moncunill",
                        "given": "David"
                    },
                    {
                        "family": "Gaona-García",
                        "given": "Elvis Eduardo"
                    },
                    {
                        "family": "Gómez-Acosta",
                        "given": "Adriana"
                    },
                    {
                        "family": "Monenegro-Marin",
                        "given": "Carlos"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            8,
                            3
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            8,
                            3
                        ]
                    ]
                },
                "abstract": "Digital repositories allow storage and manage digital resources and collections of museums, libraries, archives in order to be use in educational context. Unfortunately, several deficiencies in user interfaces based on resource discovery, user-centered design, and strategy of search, among others, prevent the widespread use of the valuable services that data of repository offers. Having the intuition that some deficiencies are reflected in usability problems associated with interfaces, we conducted a research from the Human Computer Interaction (HCI) perspective in order to present a novel framework to evaluate usability of different types of visual interfaces based on visualization techniques and Knowledge Organization System (KOS). Our study analyzed the efficacy of a framework in order to allow repository creators the assessment and selection of appropriate user interfaces according to the needs and demands of the data collection of learning objects. The preliminary results show that framework could improve the select of appropriate visualization techniques after to development of them in a digital repository. Although, some problems associated with the limited computational capabilities for information visualization are difficult to overcome.",
                "call-number": "10.1145/3264560.3264567",
                "collection-title": "ICCBDC'18",
                "container-title": "Proceedings of the 2018 2nd International Conference on Cloud and Big Data Computing",
                "DOI": "10.1145/3264560.3264567",
                "event-place": "Barcelona, Spain",
                "ISBN": "9781450364744",
                "keyword": "user interfaces, Digital repositories, human computer interface, visual search interfaces, information visualization, visualization techniques",
                "number-of-pages": "5",
                "page": "33–37",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Usability of Big Data Resources in Visual Search Interfaces of Repositories Based on KOS",
                "URL": "https://doi.org/10.1145/3264560.3264567"
            }
        },
        {
            "10.1145/3344948.3344987": {
                "id": "10.1145/3344948.3344987",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Díaz-de-Arcaya",
                        "given": "Josu"
                    },
                    {
                        "family": "Miñon",
                        "given": "Raül"
                    },
                    {
                        "family": "Torre-Bastida",
                        "given": "Ana I."
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            9,
                            9
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            9,
                            9
                        ]
                    ]
                },
                "abstract": "An industry transformation is being boosted by Big Data and Cloud technologies. We present a Big Data architecture, which expands the life cycle of data processing through the Edge, Fog and Cloud computing layers. The proposed architecture takes advantage of the strengths of each: the Cloud layer executes heavy analytical processes, the Fog is responsible for the ingestion and performing aggregations, and the Edge manages devices and actuators. The proposed architecture tackles two main goals, 1) latencies and response times can be reduced by bringing the analytics closer to where the data is generated and 2) the use of computing resources is optimised. In order to conceptualise this architecture, an orchestration module is proposed with the goal of optimising the deployment of analytical workloads across the three layers, by evaluating their computing resources. In addition to this, another module is designed to monitor the performance of such workloads allowing the redistribution of tasks assigned to each node. These modules will be implemented in a real case scenario in the train domain.",
                "call-number": "10.1145/3344948.3344987",
                "collection-title": "ECSA '19",
                "container-title": "Proceedings of the 13th European Conference on Software Architecture - Volume 2",
                "DOI": "10.1145/3344948.3344987",
                "event-place": "Paris, France",
                "ISBN": "9781450371421",
                "keyword": "IoT, data analytics, big data, fog computing, cloud computing, predictive maintenance",
                "number-of-pages": "4",
                "page": "173–176",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Towards an architecture for big data analytics leveraging edge/fog paradigms",
                "URL": "https://doi.org/10.1145/3344948.3344987"
            }
        },
        {
            "10.1145/3341069.3342996": {
                "id": "10.1145/3341069.3342996",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Liu",
                        "given": "Jun"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            6,
                            22
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            6,
                            22
                        ]
                    ]
                },
                "abstract": "In the era of big data, mass production, analysis and application of data have become a new trend. In the long-term design, production, operation and testing process of aerospace enterprises, a large number of valuable data have been generated. Collection and analysis of these data can improve the management of aerospace enterprises and gain competitive advantages. With the increase of semi-structured and unstructured data produced by aerospace enterprises year by year, how to store and analyze data, how to mine and share knowledge has become a major problem. The existing knowledge management system cannot meet the diversified needs of users only by traditional database technology. It also needs to combine distributed computing and storage technology to solve the problems of knowledge storage, knowledge sharing, knowledge mining, knowledge retrieval and recommendation in big data environment. Aerospace enterprises need to build a knowledge management system based on big data technology to support knowledge innovation and knowledge application. From the perspective of data operation and relying on Hadoop ecosystem related big data technology, this paper constructs a knowledge management framework model for aerospace enterprises based on Hadoop.",
                "call-number": "10.1145/3341069.3342996",
                "collection-title": "HPCCT 2019",
                "container-title": "Proceedings of the 2019 3rd High Performance Computing and Cluster Technologies Conference",
                "DOI": "10.1145/3341069.3342996",
                "event-place": "Guangzhou, China",
                "ISBN": "9781450371858",
                "keyword": "knowledge management system, Big data, Hadoop, knowledge management",
                "number-of-pages": "5",
                "page": "172–176",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Research on Knowledge Management Technology of Aerospace Engineering Based on Big Data",
                "URL": "https://doi.org/10.1145/3341069.3342996"
            }
        },
        {
            "10.1145/3284103.3284123": {
                "id": "10.1145/3284103.3284123",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Li",
                        "given": "Tao"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            11,
                            6
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            11,
                            6
                        ]
                    ]
                },
                "abstract": "As the transportation services represented by DiDi have entered the mobile Internet, the data volume of transportation services on various network platforms and social media has increased dramatically, which indicates that the era of big data of transportation system has come. This paper proposes a transportation market prosperity index system based on big data analytics. First, the construction principle of the prosperity index is established; second, the quintessential prosperity indexes are selected; third, we formulate the index data acquisition method, data processing and index calculation method; fourth, the calculation of transportation diffusion index and traffic composite index is developed. The contribution of this paper is that we can grasp the degree of economic development of the transportation market from both qualitative and quantitative perspectives and show its development trend based on the mining and analysis of big data on the Internet. Then, we can provide foundation for the government to formulate relative policies and decision-making for relevant enterprises.",
                "call-number": "10.1145/3284103.3284123",
                "collection-number": "17",
                "collection-title": "Safety and Resilience'18",
                "container-title": "Proceedings of the 4th ACM SIGSPATIAL International Workshop on Safety and Resilience",
                "DOI": "10.1145/3284103.3284123",
                "event-place": "Seattle, WA, USA",
                "ISBN": "9781450360449",
                "keyword": "Transportation market, Prosperity index, Big data analytics",
                "number": "Article 17",
                "number-of-pages": "6",
                "page": "1–6",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Using Big Data Analytics to Build Prosperity Index of Transportation Market",
                "URL": "https://doi.org/10.1145/3284103.3284123"
            }
        },
        {
            "10.1145/2213598.2213606": {
                "id": "10.1145/2213598.2213606",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Endler",
                        "given": "Gregor"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2012,
                            5,
                            20
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2012,
                            5,
                            20
                        ]
                    ]
                },
                "abstract": "The trend to merge medical practices into cooperatively operating networks and organizational units like Medical Supply Centers generates new challenges for an adequate IT support. In particular, new use cases for common economic planning, controlling and treatment coordination arise. This requires consolidation of data originating from heterogeneous and autonomous software systems. Heterogeneity and autonomy are core reasons for low data quality. The intuitive approach of initially integrating heterogeneous systems into a federated system creates a very high upfront effort before the system can become operable and does not adequately consider the fact that data quality requirements might change over time. To remedy this, we propose an approach for continuous data quality improvement which enables a demand driven step by step system integration. By adapting the generic Total Data Quality Management process to healthcare specific use cases, we are developing an extended model for continuous data quality management in cooperative healthcare settings. The IT tools which are needed to provide the information that drives this process are currently in development within a government supported project involving both industry and academia.",
                "call-number": "10.1145/2213598.2213606",
                "collection-title": "PhD '12",
                "container-title": "Proceedings of the on SIGMOD/PODS 2012 PhD Symposium",
                "DOI": "10.1145/2213598.2213606",
                "event-place": "Scottsdale, Arizona, USA",
                "ISBN": "9781450313261",
                "keyword": "co-operative healthcare delivery, demand-driven improvement, data quality management",
                "number-of-pages": "6",
                "page": "21–26",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Data quality and integration in collaborative environments",
                "URL": "https://doi.org/10.1145/2213598.2213606"
            }
        },
        {
            "10.1145/3314221.3314650": {
                "id": "10.1145/3314221.3314650",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Wang",
                        "given": "Chenxi"
                    },
                    {
                        "family": "Cui",
                        "given": "Huimin"
                    },
                    {
                        "family": "Cao",
                        "given": "Ting"
                    },
                    {
                        "family": "Zigman",
                        "given": "John"
                    },
                    {
                        "family": "Volos",
                        "given": "Haris"
                    },
                    {
                        "family": "Mutlu",
                        "given": "Onur"
                    },
                    {
                        "family": "Lv",
                        "given": "Fang"
                    },
                    {
                        "family": "Feng",
                        "given": "Xiaobing"
                    },
                    {
                        "family": "Xu",
                        "given": "Guoqing Harry"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            6,
                            8
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            6,
                            8
                        ]
                    ]
                },
                "abstract": "Modern data-parallel systems such as Spark rely increasingly on in-memory computing that can significantly improve the efficiency of iterative algorithms. To process real-world datasets, modern data-parallel systems often require extremely large amounts of memory, which are both costly and energy-inefficient. Emerging non-volatile memory (NVM) technologies offers high capacity compared to DRAM and low energy compared to SSDs. Hence, NVMs have the potential to fundamentally change the dichotomy between DRAM and durable storage in Big Data processing. However, most Big Data applications are written in managed languages (e.g., Scala and Java) and executed on top of a managed runtime (e.g., the Java Virtual Machine) that already performs various dimensions of memory management. Supporting hybrid physical memories adds in a new dimension, creating unique challenges in data replacement and migration. This paper proposes Panthera, a semantics-aware, fully automated memory management technique for Big Data processing over hybrid memories. Panthera analyzes user programs on a Big Data system to infer their coarse-grained access patterns, which are then passed down to the Panthera runtime for efficient data placement and migration. For Big Data applications, the coarse-grained data division is accurate enough to guide GC for data layout, which hardly incurs data monitoring and moving overhead. We have implemented Panthera in OpenJDK and Apache Spark. An extensive evaluation with various datasets and applications demonstrates that Panthera reduces energy by 32 – 52% at only a 1 – 9% execution time overhead.",
                "call-number": "10.1145/3314221.3314650",
                "collection-title": "PLDI 2019",
                "container-title": "Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation",
                "DOI": "10.1145/3314221.3314650",
                "event-place": "Phoenix, AZ, USA",
                "ISBN": "9781450367127",
                "keyword": "garbage collection, Big Data systems, memory management, hybrid memories",
                "number-of-pages": "16",
                "page": "347–362",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Panthera: holistic memory management for big data processing over hybrid memories",
                "URL": "https://doi.org/10.1145/3314221.3314650"
            }
        },
        {
            "10.1145/2908216.2908222": {
                "id": "10.1145/2908216.2908222",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Geslevich-Packin",
                        "given": "Nizan"
                    },
                    {
                        "family": "Lev-Aretz",
                        "given": "Yafit"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            3,
                            28
                        ]
                    ]
                },
                "abstract": "Social netbanks hold much potential as alternative financial institutions that could better serve marginalized populations. But they bring with them a number of regulatory complications and informational risks. We explore the rise of big data and social netbanks, and describe some of the challenges they present.",
                "call-number": "10.1145/2908216.2908222",
                "container-title": "SIGCAS Comput. Soc.",
                "DOI": "10.1145/2908216.2908222",
                "ISSN": "0095-2737",
                "issue": "1",
                "keyword": "netbanks, banks, social networks, regulation",
                "number-of-pages": "5",
                "page": "36–40",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "0March 2016",
                "title": "Big data and social netbanks: what happens when tech companies become financial companies?",
                "URL": "https://doi.org/10.1145/2908216.2908222",
                "volume": "46"
            }
        },
        {
            "10.1145/3510858.3510975": {
                "id": "10.1145/3510858.3510975",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Zheng",
                        "given": "Zhiming"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            12,
                            18
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2021,
                            12,
                            18
                        ]
                    ]
                },
                "abstract": "How to use big data technology to effectively excavate and identify data information contained in user behaviors and further innovate services has become a development trend of the Internet big data. With the further increase of the amount of data, the configuration parameters involved further increase, and the optimization of configuration parameters has become the main bottleneck limiting the performance of MapReduce. Hadoop configuration involves many parameters, which have a great impact on the running jobs. These parameters just determine the overall performance of the cluster. This paper uses Hadoop technology to the Internet big data combining the optimization model. The construction process of Hadoop cluster environment is described in detail. Hadoop is applied to a file publishing system. For files of different orders of magnitude, the time-consuming operation of file upload is compared when the number of clusters is different. The experimental results show that the larger the amount of data and the number of cluster nodes, the stronger the ability of Hadoop cluster to process data. The results prove that this method effectively can solve the problems of the complex information of big data and improve the service efficiency for big data processing.",
                "call-number": "10.1145/3510858.3510975",
                "collection-title": "ICASIT 2021",
                "container-title": "2021 International Conference on Aviation Safety and Information Technology",
                "DOI": "10.1145/3510858.3510975",
                "event-place": "Changsha, China",
                "ISBN": "9781450390422",
                "number-of-pages": "4",
                "page": "405–408",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Application Analysis of Hadoop in Big Data Processing",
                "URL": "https://doi.org/10.1145/3510858.3510975"
            }
        },
        {
            "10.1145/3468264.3473135": {
                "id": "10.1145/3468264.3473135",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Jesse",
                        "given": "Kevin"
                    },
                    {
                        "family": "Devanbu",
                        "given": "Premkumar T."
                    },
                    {
                        "family": "Ahmed",
                        "given": "Toufique"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            8,
                            20
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2021,
                            8,
                            20
                        ]
                    ]
                },
                "abstract": "TypeScript is a widely used optionally-typed language where developers can adopt “pay as you go” typing: they can add types as desired, and benefit from static typing. The “type annotation tax” or manual effort required to annotate new or existing TypeScript can be reduced by a variety of automatic methods. Probabilistic machine-learning (ML) approaches work quite well. ML approaches use different inductive biases, ranging from simple token sequences to complex graphical neural network (GNN) models capturing syntax and semantic relations. More sophisticated inductive biases are hand-engineered to exploit the formal nature of software. Rather than deploying fancy inductive biases for code, can we just use “big data” to learn natural patterns relevant to typing? We find evidence suggesting that this is the case. We present TypeBert, demonstrating that even with simple token-sequence inductive bias used in BERT-style models and enough data, type-annotation performance of the most sophisticated models can be surpassed.",
                "call-number": "10.1145/3468264.3473135",
                "collection-title": "ESEC/FSE 2021",
                "container-title": "Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
                "DOI": "10.1145/3468264.3473135",
                "event-place": "Athens, Greece",
                "ISBN": "9781450385626",
                "keyword": "Type inference, TypeScript, deep learning, transfer learning",
                "number-of-pages": "4",
                "page": "1483–1486",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Learning type annotation: is big data enough?",
                "URL": "https://doi.org/10.1145/3468264.3473135"
            }
        },
        {
            "10.1145/3436286.3436400": {
                "id": "10.1145/3436286.3436400",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Lu",
                        "given": "Wanting"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2020,
                            4,
                            28
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2020,
                            4,
                            28
                        ]
                    ]
                },
                "abstract": "With the rapid development of the Internet and the popularity of smart mobile terminals, individuals can publish and access data anytime, anywhere, accurately and quickly, which makes the scale and type of data grow rapidly, and the era of big data has come quietly. At present, WeChat Official Account has become a platform for various industries to carry out marketing activities by virtue of its advantages of accuracy, speed and effectiveness, and tourist attractions have also opened WeChat Official Account to carry out marketing publicity activities. Wuhan Garden Expo Park officially opened on September 26, 2015, is one of the 12 Garden Expo Parks in China, its WeChat Official Account has been opened and operated so far, although it has a certain social influence, but there is still room for improvement. This paper, with Wuhan Garden Expo Park WeChat Official Account as the research object, first introduced the overview of Wuhan Garden Expo Park WeChat Official Account, based on the WeChat Communication Index WCI of Wuhan Park Expo Park influence analysis of the influence of WeChat Official Account, explore the constraints to enhance its influence, finally proposed the design of iconic avatar, stable tweet frequency, adjusting the content and boosting quantity of tweets, in order to further enhance the influence of WeChat Official Account of Wuhan Garden Expo Park Scenic Area and better serve users.",
                "call-number": "10.1145/3436286.3436400",
                "collection-title": "ISBDAI '20",
                "container-title": "Proceedings of the 2020 2nd International Conference on Big Data and Artificial Intelligence",
                "DOI": "10.1145/3436286.3436400",
                "event-place": "Johannesburg, South Africa",
                "ISBN": "9781450376457",
                "keyword": "Big Data Era, Influence, WeChat Communication Index, WeChat Official Account, Wuhan Garden Expo Park Scenic Area",
                "number-of-pages": "6",
                "page": "249–254",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Research on the Influence of the WeChat Official Account in Wuhan Garden Expo Park Scenic Area in the Era of Big Data",
                "URL": "https://doi.org/10.1145/3436286.3436400"
            }
        },
        {
            "10.1145/3070607.3075961": {
                "id": "10.1145/3070607.3075961",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Sroka",
                        "given": "Jacek"
                    },
                    {
                        "family": "Leśniewski",
                        "given": "Artur"
                    },
                    {
                        "family": "Kowaluk",
                        "given": "Mirosław"
                    },
                    {
                        "family": "Stencel",
                        "given": "Krzysztof"
                    },
                    {
                        "family": "Tyszkiewicz",
                        "given": "Jerzy"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            5,
                            14
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            5,
                            14
                        ]
                    ]
                },
                "abstract": "The motivation for our research is the need for a simple and accessible method to process large datasets available to the public. We aim to significantly lower the technological barrier, which currently prevents average users from analyzing big datasets available as CSV files. Spreadsheets are perfectly suited for this task, being the most popular and accessible data analysis tool. We present ideally balanced algorithms for specifying MapReduce computations of high number of range queries. With our algorithms, it will be possible to automatically perform analysis defined with a spreadsheet on data of size exceeding the dimensions of the spreadsheet grid by orders of magnitude.",
                "call-number": "10.1145/3070607.3075961",
                "collection-number": "1",
                "collection-title": "BeyondMR'17",
                "container-title": "Proceedings of the 4th ACM SIGMOD Workshop on Algorithms and Systems for MapReduce and Beyond",
                "DOI": "10.1145/3070607.3075961",
                "event-place": "Chicago, IL, USA",
                "ISBN": "9781450350198",
                "keyword": "spreadsheet, Big data, range queries, Hadoop, minimal algorithms, MapReduce",
                "number": "Article 1",
                "number-of-pages": "4",
                "page": "1–4",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Towards minimal algorithms for big data analytics with spreadsheets",
                "URL": "https://doi.org/10.1145/3070607.3075961"
            }
        },
        {
            "10.1109/UCC.2014.101": {
                "id": "10.1109/UCC.2014.101",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Self",
                        "given": "Richard J."
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2014,
                            12,
                            8
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2014,
                            12,
                            8
                        ]
                    ]
                },
                "abstract": "The Cloud, Big Data and many emerging technologies are now being considered by many educational establishments as candidates for deriving benefits for both students in their learning and also for the organisation in terms of more effective and efficient operation. This paper considers the governance strategies which need to be developed and implemented in order to ensure that the technologies can be safely incorporated into the technical and operational infrastructure. It demonstrates that synthesising ISO 27002 with a new framework of 12 Vs of Big Data provides an effective approach to identifying some important aspects of new technologies that do not naturally arise from traditional frameworks.",
                "call-number": "10.1109/UCC.2014.101",
                "collection-title": "UCC '14",
                "container-title": "Proceedings of the 2014 IEEE/ACM 7th International Conference on Utility and Cloud Computing",
                "DOI": "10.1109/UCC.2014.101",
                "ISBN": "9781479978816",
                "keyword": "governance, big data, strategy, cloud, emerging technology, education",
                "number-of-pages": "6",
                "page": "630–635",
                "publisher": "IEEE Computer Society",
                "publisher-place": "USA",
                "title": "Governance Strategies for the Cloud, Big Data, and Other Technologies in Education",
                "URL": "https://doi.org/10.1109/UCC.2014.101"
            }
        },
        {
            "10.1145/3469213.3470709": {
                "id": "10.1145/3469213.3470709",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Li",
                        "given": "Punan"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            5,
                            28
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2021,
                            5,
                            28
                        ]
                    ]
                },
                "abstract": "This paper presents an optimal source data quantization approach. The corresponding algorithm in this paper can obtain the optimal result for source symbols by considering the correlation between source symbol values, and overcome the defect that the context quantization of minimum description length is only applicable to binary cases. We use a tree structure to represent the context quantization process. The similar measure is description length which is used to present the conditional probability model for judgment. For merging of some conditional probability distributions, the optimal tree structure with the smallest description length can be found. With the help of dynamic programming, our algorithm can get the optimized models. The experimental results are: under the reasonable computational complexity, the results of this algorithm are similar to or better than those of previous algorithms.",
                "call-number": "10.1145/3469213.3470709",
                "collection-number": "276",
                "collection-title": "ICAIIS 2021",
                "container-title": "2021 2nd International Conference on Artificial Intelligence and Information Systems",
                "DOI": "10.1145/3469213.3470709",
                "event-place": "Chongqing, China",
                "ISBN": "9781450390200",
                "keyword": "DP Programming, quantization, Big data",
                "number": "Article 276",
                "number-of-pages": "4",
                "page": "1–4",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Optimal Quantization for Big Data Based on the Dynamic Programming",
                "URL": "https://doi.org/10.1145/3469213.3470709"
            }
        },
        {
            "10.1145/2025528.2025537": {
                "id": "10.1145/2025528.2025537",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Khan",
                        "given": "Azam"
                    },
                    {
                        "family": "Hornbæk",
                        "given": "Kasper"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2011,
                            9,
                            18
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2011,
                            9,
                            18
                        ]
                    ]
                },
                "abstract": "As sensor networks in buildings continue to grow in number and heterogeneity, occupants can become empowered to better control their environment for comfort maximization and energy minimization. Since buildings are the primary consumers of energy and are the dominant cause of greenhouse gases, apps that help occupants to understand and control their interactions with a building could be extremely beneficial to society. However, the massive raw data sets that could be collected must be aggregated and visualized to be usable which presents significant data handling, information visualization, and interaction challenges. In the context of Project Dasher, a prototype building site for exploring these issues, we discuss lessons learned and challenges ahead to develop ubiquitous computing support for sustainability.",
                "call-number": "10.1145/2025528.2025537",
                "collection-title": "LARGE '11",
                "container-title": "Proceedings of the 2nd international workshop on Research in the large",
                "DOI": "10.1145/2025528.2025537",
                "event-place": "Beijing, China",
                "ISBN": "9781450309240",
                "keyword": "data aggregation, massive data sets, building information model, sustainability, augmented reality, app",
                "number-of-pages": "4",
                "page": "29–32",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Big data from the built environment",
                "URL": "https://doi.org/10.1145/2025528.2025537"
            }
        },
        {
            "10.1145/3319619.3322045": {
                "id": "10.1145/3319619.3322045",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Jurczuk",
                        "given": "Krzysztof"
                    },
                    {
                        "family": "Czajkowski",
                        "given": "Marcin"
                    },
                    {
                        "family": "Kretowski",
                        "given": "Marek"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            7,
                            13
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            7,
                            13
                        ]
                    ]
                },
                "abstract": "This paper identifies scalability bounds of the evolutionary induced decision trees (DT)s. In order to conquer the barriers concerning the large-scale data we propose a novel multi-GPU approach. It incorporates the knowledge of the global DT induction and EA parallelization. The search for a tree structure and tests is performed sequentially by a CPU, while the fitness calculations are delegated to GPUs, thus the core evolution is unchanged. The results show that the evolutionary induction is accelerated several thousand times by using up to 4 GPUs on datasets with up to 1 billion objects.",
                "call-number": "10.1145/3319619.3322045",
                "collection-title": "GECCO '19",
                "container-title": "Proceedings of the Genetic and Evolutionary Computation Conference Companion",
                "DOI": "10.1145/3319619.3322045",
                "event-place": "Prague, Czech Republic",
                "ISBN": "9781450367486",
                "keyword": "parallel computing, decision trees, CUDA, scalability bounds, graphics processing unit (GPU), big data, evolutionary data mining",
                "number-of-pages": "2",
                "page": "175–176",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Multi-GPU approach for big data mining: global induction of decision trees",
                "URL": "https://doi.org/10.1145/3319619.3322045"
            }
        },
        {
            "10.1145/3424978.3425000": {
                "id": "10.1145/3424978.3425000",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Yuan",
                        "given": "Lufeng"
                    },
                    {
                        "family": "Gao",
                        "given": "Xiaoxin"
                    },
                    {
                        "family": "Wang",
                        "given": "Sining"
                    },
                    {
                        "family": "Wang",
                        "given": "Jun"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2020,
                            10,
                            20
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2020,
                            10,
                            20
                        ]
                    ]
                },
                "abstract": "Extra-large enterprises, due to their huge scale and complex businesses, face serious challenges in the big data time. This paper introduces the Operating and Monitoring Information System (OMIS) in the State Grid Corporation of China to try to use big data in the extra-large enterprises. OMIS consists of full coverage data flow path, compound general library of model and algorithm, multi-mode computing platform and interface components. It solves a series of key problems that are data barrier, transmission, analysis, computing, usability in extra large enterprises. OMIS has connected headquarters, provinces and cities, covered 27 provinces. Benefited from interface components, a programme for data extraction, model train and parallel computing can be implemented by several codes conveniently in OMIS. In the experiments, OMIS can extract 1.73TB line loss data in about 93 hours from 27 provinces, provide multiple algorithms to detect abnormal low-voltage substation areas, support high performance computing by parallelization. Finally, OMIS reaches 3.71 speedup ratio on five nodes.",
                "call-number": "10.1145/3424978.3425000",
                "collection-number": "22",
                "collection-title": "CSAE 2020",
                "container-title": "Proceedings of the 4th International Conference on Computer Science and Application Engineering",
                "DOI": "10.1145/3424978.3425000",
                "event-place": "Sanya, China",
                "ISBN": "9781450377720",
                "keyword": "Monitoring, Multi-source, Heterogeneous, Operating, Data extraction, Extra-large enterprise, Big data, Parallelization",
                "number": "Article 22",
                "number-of-pages": "5",
                "page": "1–5",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Research on Multi-source Heterogeneous Big Data in Extra-large Enterprises",
                "URL": "https://doi.org/10.1145/3424978.3425000"
            }
        },
        {
            "10.1145/2663204.2669985": {
                "id": "10.1145/2663204.2669985",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Tosun",
                        "given": "Cafer"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2014,
                            11,
                            12
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2014,
                            11,
                            12
                        ]
                    ]
                },
                "abstract": "Smart phones and mobile technologies have changed software usage dramatically. Ease of use and simplicity has made software accessible to a huge number of users. In addition, technological advancements in multimodal interaction are opening new frontiers in software. Users are interacting with software systems through multiple channels such as gestures and speech. Touch screens, cameras, sensors, and wearable devices are enablers of this interaction. The user expectation is that the interaction with business software also becomes as simple as the interaction with consumer software. In particular, through the usage of mobile devices, consumer and business software is coming closer together. Next generation software systems and applications will have to enable smart, seamless and contextual multimodal interaction capabilities. New tools, technologies and solutions will be required to increase the ease of use and to build the user experience of the future.",
                "call-number": "10.1145/2663204.2669985",
                "collection-title": "ICMI '14",
                "container-title": "Proceedings of the 16th International Conference on Multimodal Interaction",
                "DOI": "10.1145/2663204.2669985",
                "event-place": "Istanbul, Turkey",
                "ISBN": "9781450328852",
                "keyword": "human-computer interaction, smart city, augmented reality, interactive software, user experience, internet of things, user-centered software, connected living, sap hana",
                "number-of-pages": "1",
                "page": "282",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Smart Multimodal Interaction through Big Data",
                "URL": "https://doi.org/10.1145/2663204.2669985"
            }
        },
        {
            "10.1145/2688072": {
                "id": "10.1145/2688072",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Nair",
                        "given": "Ravi"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2014,
                            12,
                            23
                        ]
                    ]
                },
                "call-number": "10.1145/2688072",
                "container-title": "Commun. ACM",
                "DOI": "10.1145/2688072",
                "ISSN": "0001-0782",
                "issue": "1",
                "number-of-pages": "1",
                "page": "104",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "January 2015",
                "title": "Big data needs approximate computing: technical perspective",
                "URL": "https://doi.org/10.1145/2688072",
                "volume": "58"
            }
        },
        {
            "10.1145/3341620.3341624": {
                "id": "10.1145/3341620.3341624",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Jia",
                        "given": "Fengsheng"
                    },
                    {
                        "family": "Gao",
                        "given": "Yang"
                    },
                    {
                        "family": "Wang",
                        "given": "Yuming"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            6,
                            11
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            6,
                            11
                        ]
                    ]
                },
                "abstract": "The integration and application of aerospace product quality data resources is an important way to carry out quality improvement, quality evaluation and precise management. Standardization is the basis for promoting quality data resources integration. The unified and normative standard system is the guarantee for efficient development of integration standards. Firstly, we analyzed the features of quality data resources according to the status quo of integration. Integration structure of quality data resources in terms of vertical and horizontal integration was proposed by adopting the methods of \"decomposition-integration\" and \"classification-association\". Secondly, we constructed a three-dimensions architecture of quality data resource integration using the method of system engineering methodology, from the layer dimension (basis, common, special), technical dimension (description, collection, storage, transmission, processing, comprehensive management) and category dimension (rocket, spacecraft). Thirdly, we worked out 20 lists about basis and common standard by adopting the top-down approach. Some standard development suggestions are proposed based on the characteristics of quality data resources and standard research strategies. Finally, we applied the quality problem data resource standard construction and application to verify the proposed method.",
                "call-number": "10.1145/3341620.3341624",
                "collection-title": "BDE 2019",
                "container-title": "Proceedings of the 2019 International Conference on Big Data Engineering",
                "DOI": "10.1145/3341620.3341624",
                "event-place": "Hong Kong, Hong Kong",
                "ISBN": "9781450360913",
                "keyword": "standard system, aerospace products, system planning, quality data resource integration",
                "number-of-pages": "7",
                "page": "16–22",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Study on Standard System of Aerospace Quality Data Resources Integration under the Background of Big Data",
                "URL": "https://doi.org/10.1145/3341620.3341624"
            }
        },
        {
            "10.1145/3175628.3175647": {
                "id": "10.1145/3175628.3175647",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Abbad",
                        "given": "Hicham"
                    },
                    {
                        "family": "Bouchaib",
                        "given": "Radi"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            10,
                            25
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            10,
                            25
                        ]
                    ]
                },
                "abstract": "Due to the emergence of the Internet of Things, social network and the mobile applications, the data produced is very important and diversified. These data are very useful to design a smart city based on machine learning and statistical algorithms used by real time analytics or predictive analytics. This paper presents a framework headlines that helps cities to take profit from data to improve the life quality of citizens in several areas: mobility and transportation, economy, environment and energy, public safety, health, education and others.This framework describe different data sources (IoT, software applications, sensors,..) used to generate the information. It also describes, the processing steps that make data usable, the different machine learning statistical algorithms and data mining techniques used to extract the insights. This framework, combine two complementary approaches: Techno-centric approach based on a city populated by sensors that collect a massive data to drive all the urban services, and the collaborative approach that addresses citizens more directly.",
                "call-number": "10.1145/3175628.3175647",
                "collection-number": "17",
                "collection-title": "SCAMS '17",
                "container-title": "Proceedings of the Mediterranean Symposium on Smart City Application",
                "DOI": "10.1145/3175628.3175647",
                "event-place": "Tangier, Morocco",
                "ISBN": "9781450352116",
                "keyword": "analytics, internet of things (IoT), machine learning, big data, smart cities",
                "number": "Article 17",
                "number-of-pages": "5",
                "page": "1–5",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Towards a big data analytics framework for smart cities",
                "URL": "https://doi.org/10.1145/3175628.3175647"
            }
        },
        {
            "10.1145/3456887.3457506": {
                "id": "10.1145/3456887.3457506",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Sun",
                        "given": "Yan"
                    },
                    {
                        "family": "Zhao",
                        "given": "Jing"
                    },
                    {
                        "family": "Yin",
                        "given": "Weiwei"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            5,
                            25
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2021,
                            5,
                            25
                        ]
                    ]
                },
                "abstract": "With the rapid development of Internet of things and communication technology and the arrival of big data era, the construction of smart tourism becomes possible. The construction of tourism destination smart tourism platform is in line with the current strategic goal of China's tourism development. Based on the background of big data, this paper expounds the connotation of big data and smart tourism. This paper analyzes the development and existing problems of the existing smart tourism platform, and constructs a data service platform for the purpose of smart tourism prediction and feedback. According to the data platform, this paper proposes to build a security early warning mechanism for scenic spots based on Tourism big data, which can realize the interconnection between inside and outside the scenic spots. The system can predict the hot degree and tourist saturation of scenic spots in advance, and expand the space of safety warning mechanism. It can further build the safety emergency mechanism, rescue mechanism and security mechanism of the scenic spot, so as to realize the safe and normal operation of the scenic spot.",
                "call-number": "10.1145/3456887.3457506",
                "collection-title": "CIPAE 2021",
                "container-title": "2021 2nd International Conference on Computers, Information Processing and Advanced Education",
                "DOI": "10.1145/3456887.3457506",
                "event-place": "Ottawa, ON, Canada",
                "ISBN": "9781450389969",
                "keyword": "Hot Spots, Security Early Warning, Smart Tourism Platform, Big Data",
                "number-of-pages": "5",
                "page": "1273–1277",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Construction of Big Data Security Early Warning Visualization Model for Smart Tourism",
                "URL": "https://doi.org/10.1145/3456887.3457506"
            }
        },
        {
            "10.1145/3383913.3383917": {
                "id": "10.1145/3383913.3383917",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Ruoyu",
                        "given": "Ren"
                    },
                    {
                        "family": "Shulin",
                        "given": "Yang"
                    },
                    {
                        "family": "Qi",
                        "given": "Zhang"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            9,
                            19
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            9,
                            19
                        ]
                    ]
                },
                "abstract": "With the development of network information technology, the era of big data brings new opportunities for digital publishing industry. Big data publishing has become one of the trends of future publishing, and has become the core competitiveness of publishing industry, and also brings new challenges for digital publishing. Due to the increasing popularity of data sharing in the era of big data, the problem of network infringement has also been gradually paid attention to, so the research on copyright protection is very important. Based on the characteristics and current situation of the development of digital publishing in the big data environment, this paper analyzes in detail the main problems and difficulties of copyright protection in the big data era. On this basis, this paper puts forward the era of big data is copyright protection of digital publishing strategy, in order to further improve the construction of copyright protection of digital publishing system.",
                "call-number": "10.1145/3383913.3383917",
                "collection-title": "IHIP 2019",
                "container-title": "Proceedings of the 2019 2nd International Conference on Information Hiding and Image Processing",
                "DOI": "10.1145/3383913.3383917",
                "event-place": "London, United Kingdom",
                "ISBN": "9781450372879",
                "keyword": "big data, copyright protection, digital publishing",
                "number-of-pages": "5",
                "page": "39–43",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Research on Copyright Protection of Digital Publishing in the Era of Big Data",
                "URL": "https://doi.org/10.1145/3383913.3383917"
            }
        },
        {
            "10.1145/3318265.3318268": {
                "id": "10.1145/3318265.3318268",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Li",
                        "given": "Zhongwei"
                    },
                    {
                        "family": "Duan",
                        "given": "Feng"
                    },
                    {
                        "family": "Che",
                        "given": "Hao"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            3,
                            8
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            3,
                            8
                        ]
                    ]
                },
                "abstract": "As scale-out execution of big data analytics has become predominate datacenter workloads, it is of paramount importance to faithfully characterize the scaling properties for such workloads. To date, the most widely cited scaling laws for big data analytics is the traditional Amdahl's law, which was discovered well before the era of big data analytics. A key observation made in this paper is that both the system and workload models underlying the traditional scaling laws are too simplistic to fully characterize the scaling properties for big data analytics workloads. In this paper, we put forward a Unified Scaling model for Big data Analytics (USBA), based on a multi-stage system model and a discretized workload model. USBA allows for flexible workload scaling unifying the fixed-size and fixed-time workload models underlying Amdahl's and Gustafson's laws, respectively, and flexible system scaling in terms of both number of stages and degree of parallelism per stage. Moreover, to faithfully characterize the scaling properties for big data analytics workloads, USBA accounts for variabilities of task response times and barrier synchronization. Finally, application of USBA to the scaling analysis of four Spark-based data mining and graph benchmarks demonstrates that USBA is able to adequately characterize the scaling design space and predict the scaling properties of real-world big data analytics workloads. This makes it possible to use USBA as a useful tool to facilitate job resource provisioning for big data analytics in datacenters.",
                "call-number": "10.1145/3318265.3318268",
                "collection-title": "HP3C '19",
                "container-title": "Proceedings of the 3rd International Conference on High Performance Compilation, Computing and Communications",
                "DOI": "10.1145/3318265.3318268",
                "event-place": "Xi&apos;an, China",
                "ISBN": "9781450366380",
                "keyword": "Gustafson's law, Amdahl's law, performance modeling, spark, big data analytics, MapReduce",
                "number-of-pages": "11",
                "page": "67–77",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "A unified scaling model in the era of big data analytics",
                "URL": "https://doi.org/10.1145/3318265.3318268"
            }
        },
        {
            "10.1145/2896825.2896831": {
                "id": "10.1145/2896825.2896831",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Scavuzzo",
                        "given": "Marco"
                    },
                    {
                        "family": "Tamburri",
                        "given": "Damian A."
                    },
                    {
                        "family": "Di Nitto",
                        "given": "Elisabetta"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            5,
                            14
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2016,
                            5,
                            14
                        ]
                    ]
                },
                "abstract": "The recent growing interest on highly-available data-intensive applications sparked the need for flexible and portable storage technologies, e.g., NoSQL databases. Unfortunately, the lack of standard interfaces and architectures for NoSQLs makes it difficult and expensive to create portable applications, which results in vendor lock-in. Building on previous work, we aim at providing guaranteed fault-tolerant techniques and supporting architectures to port or migrate data to and across heterogeneous NoSQL technology. To prove the effectiveness of our approach we evaluate it on an industrial case-study. We conclude that our method and supporting architecture offer an efficient and fault-tolerant mechanism for NoSQL portability and interoperation.",
                "call-number": "10.1145/2896825.2896831",
                "collection-title": "BIGDSE '16",
                "container-title": "Proceedings of the 2nd International Workshop on BIG Data Software Engineering",
                "DOI": "10.1145/2896825.2896831",
                "event-place": "Austin, Texas",
                "ISBN": "9781450341523",
                "number-of-pages": "7",
                "page": "26–32",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Providing big data applications with fault-tolerant data migration across heterogeneous NoSQL databases",
                "URL": "https://doi.org/10.1145/2896825.2896831"
            }
        },
        {
            "10.1145/3419635.3419675": {
                "id": "10.1145/3419635.3419675",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Liu",
                        "given": "Yan"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2020,
                            10,
                            16
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2020,
                            10,
                            16
                        ]
                    ]
                },
                "abstract": "The goal of humanistic value of big data technology is to promote the development of individual freedom, inner balance, comprehensive education and cultural harmony. According to the present situation of big data technology, this paper makes a preliminary exploration on the way to realize the humanistic value in the big data labor practice education. In other words, the humanistic value of big data technology can be realized through three ways: persisting in putting people first, optimizing the technological environment of big data, and integrating the technological resources of big data. Through the above research on the humanistic value of big data technology, this paper applies the application of big data algorithm in labor practice to make it better serve human beings.",
                "call-number": "10.1145/3419635.3419675",
                "collection-title": "CIPAE 2020",
                "container-title": "Proceedings of the 2020 International Conference on Computers, Information Processing and Advanced Education",
                "DOI": "10.1145/3419635.3419675",
                "event-place": "Ottawa, ON, Canada",
                "ISBN": "9781450387729",
                "keyword": "Labor practice, Humanistic value, Big data technology",
                "number-of-pages": "4",
                "page": "359–362",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Research on Humanistic Value of Labor Practice Education under Big Data Technology",
                "URL": "https://doi.org/10.1145/3419635.3419675"
            }
        },
        {
            "10.1145/3014812.3014869": {
                "id": "10.1145/3014812.3014869",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Yang",
                        "given": "Jie"
                    },
                    {
                        "family": "Ma",
                        "given": "Jun"
                    },
                    {
                        "family": "Howard",
                        "given": "Sarah K."
                    },
                    {
                        "family": "Ciao",
                        "given": "Matthew"
                    },
                    {
                        "family": "Srikhanta",
                        "given": "Rangan"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            1,
                            31
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            1,
                            30
                        ]
                    ]
                },
                "abstract": "Last decade has witnessed the dramatic expansion of online user-generated content. Making full use of this data to discover behaviour patterns has become an increasingly appealing research topic. In this pilot study, a big data analytic framework is proposed, particularly taking streaming data from students' activity on their laptop usage as an illustrative example. Three modules are implemented to harvest raw streaming records, storage heterogeneous data, and apply the fuzzy representation and rule-mining algorithm for a modelling purpose.The efficiency of the proposed framework is then evaluated using a nationwide streaming dataset. The exploratory simulation of results demonstrates the flexibility and applicability of the proposed framework for processing complex streaming data, and revealing patterns from digital engagement which be used to inform decision makers.",
                "call-number": "10.1145/3014812.3014869",
                "collection-number": "55",
                "collection-title": "ACSW '17",
                "container-title": "Proceedings of the Australasian Computer Science Week Multiconference",
                "DOI": "10.1145/3014812.3014869",
                "event-place": "Geelong, Australia",
                "ISBN": "9781450347686",
                "keyword": "user-generated content, streaming data, rule mining, big data analytics",
                "number": "Article 55",
                "number-of-pages": "4",
                "page": "1–4",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "A big data analytic framework for investigating streaming educational data",
                "URL": "https://doi.org/10.1145/3014812.3014869"
            }
        },
        {
            "10.1145/3357384.3358124": {
                "id": "10.1145/3357384.3358124",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Salloum",
                        "given": "Salman"
                    },
                    {
                        "family": "Wu",
                        "given": "Yinxu"
                    },
                    {
                        "family": "Huang",
                        "given": "Joshua Zhexue"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            11,
                            3
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            11,
                            3
                        ]
                    ]
                },
                "abstract": "To break the in-memory bottleneck and facilitate online sampling in cluster computing frameworks, we propose a new sampling-based system for approximate big data analysis on computing clusters. We address both computational and statistical aspects of big data across the main layers of cluster computing frameworks: big data storage, big data management, big data online sampling, big data processing, and big data exploration and analysis. We use the new Random Sample Partition (RSP) distributed data model to store a big data set as a set of ready-to-use random sample data blocks in Hadoop Distributed File System (HDFS), called RSP blocks. With this system, only a few RSP blocks are selected and processed using a sequential algorithm in a distributed data-parallel manner to produce approximate results for the entire data set. In this paper, we present a prototype RSP-based system and demonstrate its advantages. Our experiments show that RSP blocks can be used to get approximate models and summary statistics as well as estimate the proportions of inconsistent values without computing the entire data or running expensive online sampling operations. This new system enables big data exploration and analysis where the entire data set cannot be computed.",
                "call-number": "10.1145/3357384.3358124",
                "collection-title": "CIKM '19",
                "container-title": "Proceedings of the 28th ACM International Conference on Information and Knowledge Management",
                "DOI": "10.1145/3357384.3358124",
                "event-place": "Beijing, China",
                "ISBN": "9781450369763",
                "keyword": "approximate computing, cluster computing, random sample partition, big data, block-level sampling",
                "number-of-pages": "4",
                "page": "2481–2484",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "A Sampling-Based System for Approximate Big Data Analysis on Computing Clusters",
                "URL": "https://doi.org/10.1145/3357384.3358124"
            }
        },
        {
            "10.1145/2389686.2389688": {
                "id": "10.1145/2389686.2389688",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Megler",
                        "given": "V. M."
                    },
                    {
                        "family": "Maier",
                        "given": "David"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2012,
                            11,
                            2
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2012,
                            11,
                            2
                        ]
                    ]
                },
                "abstract": "For decades, scientists bemoaned the scarcity of observational data to analyze and against which to test their models. Exponential growth in data volumes from ever-cheaper environmental sensors has provided scientists with the answer to their prayers: \"big data\". Now, scientists face a new challenge: with terabytes, petabytes or exabytes of data at hand, stored in thousands of heterogeneous datasets, how can scientists find the datasets most relevant to their research interests? If they cannot find the data, then they may as well never have collected it; that data is lost to them. Our research addresses this challenge, using an existing scientific archive as our test-bed. We approach this problem in a new way: by adapting Information Retrieval techniques, developed for searching text documents, into the world of (primarily numeric) scientific data. We propose an approach that uses a blend of automated and \"semi-curated\" methods to extract metadata from large archives of scientific data. We then perform searches over the extracted metadata, returning results ranked by similarity to the query terms. We briefly describe an implementation performed at an ocean observatory to validate the proposed approach. We propose performance and scalability research to explore how continued archive growth will affect our goal of interactive response, no matter the scale.",
                "call-number": "10.1145/2389686.2389688",
                "collection-title": "PIKM '12",
                "container-title": "Proceedings of the 5th Ph.D. workshop on Information and knowledge",
                "DOI": "10.1145/2389686.2389688",
                "event-place": "Maui, Hawaii, USA",
                "ISBN": "9781450317191",
                "keyword": "scientific data, ranked data search",
                "number-of-pages": "8",
                "page": "1–8",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "When big data leads to lost data",
                "URL": "https://doi.org/10.1145/2389686.2389688"
            }
        },
        {
            "10.1145/3377170.3377278": {
                "id": "10.1145/3377170.3377278",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Zhai",
                        "given": "Chenggong"
                    },
                    {
                        "family": "Fei",
                        "given": "Xiande"
                    },
                    {
                        "family": "Yang",
                        "given": "Zhiwei"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            12,
                            20
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            12,
                            20
                        ]
                    ]
                },
                "abstract": "Based on the in-depth analysis of the necessity and feasibility of the optimization of big data-based quilt support, this paper puts forward the overall framework and implementation concept of the optimization of big data-based quilt support, and describes how to build the basic matching of the optimization of big data-based quilt support and the concept of big data information system system. Design of integrated platform for clothing and accouterment support based on big data that collects, stores, processes, analyzes and mines the supply data of conscription bedding by using big data, cloud computing and other information technologies, which plays an important role in deepening the reform of military bedding and strengthening the scientific management of bedding.",
                "call-number": "10.1145/3377170.3377278",
                "collection-title": "ICIT 2019",
                "container-title": "Proceedings of the 2019 7th International Conference on Information Technology: IoT and Smart City",
                "DOI": "10.1145/3377170.3377278",
                "event-place": "Shanghai, China",
                "ISBN": "9781450376631",
                "keyword": "Integrated Platform, Big Data, Clothing and Accouterment",
                "number-of-pages": "5",
                "page": "54–58",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Design of Integrated Platform for Clothing and Accouterment Support Based on Big Data",
                "URL": "https://doi.org/10.1145/3377170.3377278"
            }
        },
        {
            "10.1145/3437963.3441654": {
                "id": "10.1145/3437963.3441654",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Segal",
                        "given": "Eran"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            3,
                            8
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2021,
                            3,
                            8
                        ]
                    ]
                },
                "abstract": "The recent availability of diverse health data resources on large cohorts of human individuals presents many challenges and opportunities. I will present our work aimed at developing machine learning algorithms for predicting future onset of disease and identifying causal drivers of disease based on nationwide electronic health record data as well as data from high-throughput omics profiling technologies such as genetics, microbiome, and metabolomics. Our models provide novel insights into potential drivers of obesity, diabetes, and heart disease, and identify hundreds of novel markers at the microbiome, metabolite, and immune system level. Overall, our predictive models can be translated into personalized disease prevention and treatment plans, and to the development of new therapeutic modalities based on metabolites and the microbiome.",
                "call-number": "10.1145/3437963.3441654",
                "collection-title": "WSDM '21",
                "container-title": "Proceedings of the 14th ACM International Conference on Web Search and Data Mining",
                "DOI": "10.1145/3437963.3441654",
                "event-place": "Virtual Event, Israel",
                "ISBN": "9781450382977",
                "keyword": "microbiome, electronic health records, genetics, machine learning, metabolomics",
                "number-of-pages": "1",
                "page": "3",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Harnessing Big Data for Personalized Medicine",
                "URL": "https://doi.org/10.1145/3437963.3441654"
            }
        },
        {
            "10.1145/3284103.3284112": {
                "id": "10.1145/3284103.3284112",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Xing",
                        "given": "Han"
                    },
                    {
                        "family": "Zhang",
                        "given": "Ke"
                    },
                    {
                        "family": "Yang",
                        "given": "Zifan"
                    },
                    {
                        "family": "Sun",
                        "given": "Lianying"
                    },
                    {
                        "family": "Liu",
                        "given": "Yi"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            11,
                            6
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            11,
                            6
                        ]
                    ]
                },
                "abstract": "Traffic state estimation helps urban traffic control and management. In this paper, a traffic state estimation model based on the fusion of Hidden Markov model and SEA algorithm is proposed considering the randomness and volatility of traffic systems. Traffic data of average travel speed in selected city were collected, and the mean and fluctuation values of average travel speed in adjacent time windows were calculated. With Hidden Markov model, the system state network is defined according to mean values and fluctuation values. The operation efficiency of traffic system, as well as stability and trend values, were calculated with System Effectiveness Analysis (SEA) algorithm based on system state network. Calculation results show that the method perform well and can be applied to both traffic state assessment of certain road sections and large scale road networks.",
                "call-number": "10.1145/3284103.3284112",
                "collection-number": "9",
                "collection-title": "Safety and Resilience'18",
                "container-title": "Proceedings of the 4th ACM SIGSPATIAL International Workshop on Safety and Resilience",
                "DOI": "10.1145/3284103.3284112",
                "event-place": "Seattle, WA, USA",
                "ISBN": "9781450360449",
                "keyword": "HMM, Traffic State Estimation, SEA, System State Network",
                "number": "Article 9",
                "number-of-pages": "5",
                "page": "1–5",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Traffic State Estimation with Big Data",
                "URL": "https://doi.org/10.1145/3284103.3284112"
            }
        },
        {
            "10.1145/3292500.3340400": {
                "id": "10.1145/3292500.3340400",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Heckerman",
                        "given": "David"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            7,
                            25
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            7,
                            25
                        ]
                    ]
                },
                "abstract": "There are two aspects of data that make them big: sample size and dimensionality. The advantages of large sample size have long been touted. In contrast, high dimensionality has typically been seen as an obstacle to successful analysis. In this talk, using the area of genomics as an example, I will illustrate some of the advantages of high dimensionality.",
                "call-number": "10.1145/3292500.3340400",
                "collection-title": "KDD '19",
                "container-title": "Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining",
                "DOI": "10.1145/3292500.3340400",
                "event-place": "Anchorage, AK, USA",
                "ISBN": "9781450362016",
                "keyword": "Invited Talk",
                "number-of-pages": "1",
                "page": "3172",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Exploiting High Dimensionality in Big Data",
                "URL": "https://doi.org/10.1145/3292500.3340400"
            }
        },
        {
            "10.1145/2939502.2939518": {
                "id": "10.1145/2939502.2939518",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Fisher",
                        "given": "Danyel"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            6,
                            26
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2016,
                            6,
                            26
                        ]
                    ]
                },
                "abstract": "As datasets grow to tera- and petabyte sizes, exploratory data visualization becomes very difficult: a screen is limited to a few million pixels, and main memory to a few tens of millions of data points. Yet these very large scale analyses are of tremendous interest to industry and academia. This paper discusses some of the major challenges involved in data analytics at scale, including issues of computation, communication, and rendering. It identifies techniques for handling large scale data, grouped into \"look at less of it,\" and \"look at it faster.\" Using these techniques involves a number of difficult design tradeoffs for both the ways that data can be represented, and the ways that users can interact with the visualizations.",
                "call-number": "10.1145/2939502.2939518",
                "collection-number": "16",
                "collection-title": "HILDA '16",
                "container-title": "Proceedings of the Workshop on Human-In-the-Loop Data Analytics",
                "DOI": "10.1145/2939502.2939518",
                "event-place": "San Francisco, California",
                "ISBN": "9781450342070",
                "keyword": "data analysis, data visualization, big data",
                "number": "Article 16",
                "number-of-pages": "5",
                "page": "1–5",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Big data exploration requires collaboration between visualization and data infrastructures",
                "URL": "https://doi.org/10.1145/2939502.2939518"
            }
        },
        {
            "10.5555/2819009.2819014": {
                "id": "10.5555/2819009.2819014",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Zhou",
                        "given": "Hucheng"
                    },
                    {
                        "family": "Lou",
                        "given": "Jian-Guang"
                    },
                    {
                        "family": "Zhang",
                        "given": "Hongyu"
                    },
                    {
                        "family": "Lin",
                        "given": "Haibo"
                    },
                    {
                        "family": "Lin",
                        "given": "Haoxiang"
                    },
                    {
                        "family": "Qin",
                        "given": "Tingting"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2015,
                            5,
                            16
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2015,
                            5,
                            16
                        ]
                    ]
                },
                "abstract": "Big Data computing platform has evolved to be a multi-tenant service. The service quality matters because system failure or performance slowdown could adversely affect business and user experience. To date, there is few study in literature on service quality issues of production Big Data computing platform. In this paper, we present an empirical study on the service quality issues of Microsoft ProductA, which is a company-wide multi-tenant Big Data computing platform, serving thousands of customers from hundreds of teams. ProductA has a well-defined escalation process (i.e., incident management process), which helps customers report service quality issues on 24/7 basis. This paper investigates the common symptom, causes and mitigation of service quality issues in Big Data platform. We conduct a comprehensive empirical study on 210 real service quality issues of ProductA. Our major findings include (1) 21.0% of escalations are caused by hardware faults; (2) 36.2% are caused by system side defects; (3) 37.2% are due to customer side faults. We also studied the general diagnosis process and the commonly adopted mitigation solutions. Our study results provide valuable guidance on improving existing development and maintenance practice of production Big Data platform, and motivate tool support.",
                "call-number": "10.5555/2819009.2819014",
                "collection-title": "ICSE '15",
                "container-title": "Proceedings of the 37th International Conference on Software Engineering - Volume 2",
                "event-place": "Florence, Italy",
                "keyword": "fault tolerance, escalations, quality issues, empirical study, big data computing",
                "number-of-pages": "10",
                "page": "17–26",
                "publisher": "IEEE Press",
                "title": "An empirical study on quality issues of production big data platform"
            }
        },
        {
            "10.1145/2451856.2451869": {
                "id": "10.1145/2451856.2451869",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Kim",
                        "given": "Jeffrey"
                    },
                    {
                        "family": "Lund",
                        "given": "Arnie"
                    },
                    {
                        "family": "Dombrowski",
                        "given": "Caroline"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2013,
                            5,
                            1
                        ]
                    ]
                },
                "call-number": "10.1145/2451856.2451869",
                "container-title": "interactions",
                "DOI": "10.1145/2451856.2451869",
                "ISSN": "1072-5520",
                "issue": "3",
                "number-of-pages": "4",
                "page": "48–51",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "May + June 2013",
                "title": "Telling the story in big data",
                "URL": "https://doi.org/10.1145/2451856.2451869",
                "volume": "20"
            }
        },
        {
            "10.1145/3130983": {
                "id": "10.1145/3130983",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Yang",
                        "given": "Su"
                    },
                    {
                        "family": "Wang",
                        "given": "Minjie"
                    },
                    {
                        "family": "Wang",
                        "given": "Wenshan"
                    },
                    {
                        "family": "Sun",
                        "given": "Yi"
                    },
                    {
                        "family": "Gao",
                        "given": "Jun"
                    },
                    {
                        "family": "Zhang",
                        "given": "Weishan"
                    },
                    {
                        "family": "Zhang",
                        "given": "Jiulong"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            9,
                            11
                        ]
                    ]
                },
                "abstract": "This study aims at revealing how commercial hotness of urban commercial districts (UCDs) is shaped by social contexts of surrounding areas so as to render predictive business planning. We define social contexts for a given region as the number of visitors, the region functions, the population and buying power of local residents, the average price of services, and the rating scores of customers, which are computed from heterogeneous data including taxi GPS trajectories, point of interests, geographical data, and user-generated comments. Then, we apply sparse representation to discover the impactor factor of each variable of the social contexts in terms of predicting commercial activeness of UCDs under a linear predictive model. The experiments show that a linear correlation between social contexts and commercial activeness exists for Beijing and Shanghai based on an average prediction accuracy of 77.69% but the impact factors of social contexts vary from city to city, where the key factors are rich life services, diversity of restaurants, good shopping experience, large number of local residents with relatively high purchasing power, and convenient transportation. This study reveals the underlying mechanism of urban business ecosystems, and promise social context-aware business planning over heterogeneous urban big data.",
                "call-number": "10.1145/3130983",
                "collection-number": "119",
                "container-title": "Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.",
                "DOI": "10.1145/3130983",
                "issue": "3",
                "keyword": "Social Intelligence, Urban Informatics, Economic Ecosystems, Context Awareness, Crowdsourcing",
                "number": "Article 119",
                "number-of-pages": "20",
                "page": "1–20",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "September 2017",
                "title": "Predicting Commercial Activeness over Urban Big Data",
                "URL": "https://doi.org/10.1145/3130983",
                "volume": "1"
            }
        },
        {
            "10.1145/2757218.2757221": {
                "id": "10.1145/2757218.2757221",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Eckroth",
                        "given": "Joshua"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2015,
                            5,
                            13
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2015,
                            5,
                            13
                        ]
                    ]
                },
                "abstract": "Many disciplines are confronting the challenge of \"big data,\" i.e., databases or data streams that are so massive or deliver data at such high velocity that a single commodity machine is unable to store or process the data. Mining and analyzing big data requires specialized algorithms and methodologies due to the fundamentally distributed and parallel natures of the workloads. To our knowledge, existing pedagogies in most disciplines do not prepare students to work with big data. We aim to develop a cross-disciplinary pedagogy that prepares all students to tackle data mining and analysis challenges of the future.",
                "call-number": "10.1145/2757218.2757221",
                "collection-number": "7",
                "collection-title": "CIC '15",
                "container-title": "Proceedings of The 2015 NSF Workshop on Curricular Development for Computing in Context",
                "DOI": "10.1145/2757218.2757221",
                "event-place": "DeLand, FL, USA",
                "ISBN": "9781450335973",
                "number": "Article 7",
                "number-of-pages": "1",
                "page": "1",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Towards a Cross-Disciplinary Pedagogy for Big Data",
                "URL": "https://doi.org/10.1145/2757218.2757221"
            }
        },
        {
            "10.1145/2783258.2788573": {
                "id": "10.1145/2783258.2788573",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Zheng",
                        "given": "Yu"
                    },
                    {
                        "family": "Yi",
                        "given": "Xiuwen"
                    },
                    {
                        "family": "Li",
                        "given": "Ming"
                    },
                    {
                        "family": "Li",
                        "given": "Ruiyuan"
                    },
                    {
                        "family": "Shan",
                        "given": "Zhangqing"
                    },
                    {
                        "family": "Chang",
                        "given": "Eric"
                    },
                    {
                        "family": "Li",
                        "given": "Tianrui"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2015,
                            8,
                            10
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2015,
                            8,
                            10
                        ]
                    ]
                },
                "abstract": "In this paper, we forecast the reading of an air quality monitoring station over the next 48 hours, using a data-driven method that considers current meteorological data, weather forecasts, and air quality data of the station and that of other stations within a few hundred kilometers. Our predictive model is comprised of four major components: 1) a linear regression-based temporal predictor to model the local factors of air quality, 2) a neural network-based spatial predictor to model global factors, 3) a dynamic aggregator combining the predictions of the spatial and temporal predictors according to meteorological data, and 4) an inflection predictor to capture sudden changes in air quality. We evaluate our model with data from 43 cities in China, surpassing the results of multiple baseline methods. We have deployed a system with the Chinese Ministry of Environmental Protection, providing 48-hour fine-grained air quality forecasts for four major Chinese cities every hour. The forecast function is also enabled on Microsoft Bing Map and MS cloud platform Azure. Our technology is general and can be applied globally for other cities.",
                "call-number": "10.1145/2783258.2788573",
                "collection-title": "KDD '15",
                "container-title": "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
                "DOI": "10.1145/2783258.2788573",
                "event-place": "Sydney, NSW, Australia",
                "ISBN": "9781450336642",
                "keyword": "air quality forecast, big data, urban air, urban computing",
                "number-of-pages": "10",
                "page": "2267–2276",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Forecasting Fine-Grained Air Quality Based on Big Data",
                "URL": "https://doi.org/10.1145/2783258.2788573"
            }
        },
        {
            "10.1145/3368691.3368713": {
                "id": "10.1145/3368691.3368713",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Mouchili",
                        "given": "Mama Nsangou"
                    },
                    {
                        "family": "Atwood",
                        "given": "John William"
                    },
                    {
                        "family": "Aljawarneh",
                        "given": "Shadi"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            12,
                            2
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            12,
                            2
                        ]
                    ]
                },
                "abstract": "A Call Data Record (CDR) is produced for each call (or other interaction) handled by a telephone company. CDRs have traditionally been used for billing and network engineering purposes. Given how mobile phones have become an integral part of - and have undoubtedly transformed - the everyday life of a great part of the earth's population, and given that 90% or more of phone subscriptions are registered in any city, if CDRs are collected from the mobile phone and cellular networks, and combined with other data from the organization or from elsewhere, this will allow managers to identify trends, detect patterns, and glean other valuable findings from the data.This paper highlights the applicability of CDR-based big data, to gathering such insights. By stitching events together into clusters of related events across runtime environments and/or geographies, raw data becomes business insight to make decisions either to understand customer needs, to mitigate problems, or to ultimately gain a competitive advantage.",
                "call-number": "10.1145/3368691.3368713",
                "collection-number": "22",
                "collection-title": "DATA '19",
                "container-title": "Proceedings of the Second International Conference on Data Science, E-Learning and Information Systems",
                "DOI": "10.1145/3368691.3368713",
                "event-place": "Dubai, United Arab Emirates",
                "ISBN": "9781450372848",
                "keyword": "data analytics, traffic management, big data, smart city, insights, random forest, CDR, city management, Hadoop, mining algorithms, traffic congestion, JSON",
                "number": "Article 22",
                "number-of-pages": "7",
                "page": "1–7",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Call data record based big data analytics for smart cities",
                "URL": "https://doi.org/10.1145/3368691.3368713"
            }
        },
        {
            "10.1145/2629568": {
                "id": "10.1145/2629568",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Glowalla",
                        "given": "Paul"
                    },
                    {
                        "family": "Sunyaev",
                        "given": "Ali"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2014,
                            9,
                            4
                        ]
                    ]
                },
                "abstract": "Data quality is critical to organizational success. In order to improve and sustain data quality in the long term, process-driven data quality management (PDDQM) seeks to redesign processes that create or modify data. Consequently, process modeling is mandatory for PDDQM. Current research examines process modeling languages with respect to representational capabilities. However, there is a gap, since process modeling languages for PDDQM are not considered. We address this research gap by providing a synthesis of the varying applications of process modeling languages for PDDQM. We conducted a keyword-based literature review in conferences as well as 74 highranked information systems and computer science journals, reviewing 1,555 articles from 1995 onwards. For practitioners, it is possible to integrate the quality perspective within broadly applied process models. For further research, we derive representational requirements for PDDQM that should be integrated within existing process modeling languages. However, there is a need for further representational analysis to examine the adequacy of upcoming process modeling languages. New or enhanced process modeling languages may substitute for PDDQM-specific process modeling languages and facilitate development of a broadly applicable and accepted process modeling language for PDDQM.",
                "call-number": "10.1145/2629568",
                "collection-number": "7",
                "container-title": "J. Data and Information Quality",
                "DOI": "10.1145/2629568",
                "ISSN": "1936-1955",
                "issue": "1-2",
                "keyword": "conceptual modeling, process modeling, data quality, Information quality, data and knowledge visualization.",
                "number": "Article 7",
                "number-of-pages": "30",
                "page": "1–30",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "August 2014",
                "title": "Process-driven data quality management: A critical review on the application of process modeling languages",
                "URL": "https://doi.org/10.1145/2629568",
                "volume": "5"
            }
        },
        {
            "10.1145/3512576.3512618": {
                "id": "10.1145/3512576.3512618",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Sardjono",
                        "given": "Wahyu"
                    },
                    {
                        "family": "Retnowardhani",
                        "given": "Astari"
                    },
                    {
                        "family": "Emil Kaburuan",
                        "given": "Robert"
                    },
                    {
                        "family": "Rahmasari",
                        "given": "Aninda"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            12,
                            22
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2021,
                            12,
                            22
                        ]
                    ]
                },
                "abstract": "Industry 4.0 is the pioneer of the Internet of Things (IoT). The Internet of Things (IoT) are often heard and successful in business revolution from all sectors. The IoT are widely used in numerous sectors including medical services and have become the rise of Internet of Medical Things (IoMT). One of the implementations is the Electronic Health Record (EHR) systems. Previously the health records were used in traditional manner such as print-out health record of a patient and stored to an archive room. With the innovation of EHR, patients’ health records are digitalized which provides advantages from space efficiency and paperless forms. EHR helps medical service management to provide better healthcare services. With the integration of Artificial Intelligence (AI) and Big Data Analysis in EHR, healthcare services provide more accurate and reliable diagnosis.",
                "call-number": "10.1145/3512576.3512618",
                "collection-title": "ICIT 2021",
                "container-title": "2021 The 9th International Conference on Information Technology: IoT and Smart City",
                "DOI": "10.1145/3512576.3512618",
                "event-place": "Guangzhou, China",
                "ISBN": "9781450384971",
                "keyword": "internet of things, artificial intelligence, electronic medical records, big data, Analysis",
                "number-of-pages": "7",
                "page": "231–237",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Artificial intelligence and big data analysis implementation in electronic medical records",
                "URL": "https://doi.org/10.1145/3512576.3512618"
            }
        },
        {
            "10.1145/3063955.3063995": {
                "id": "10.1145/3063955.3063995",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Zhang",
                        "given": "Shuzhuang"
                    },
                    {
                        "family": "Luo",
                        "given": "Hao"
                    },
                    {
                        "family": "Wu",
                        "given": "Zhigang"
                    },
                    {
                        "family": "Wang",
                        "given": "Yi"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            5,
                            12
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            5,
                            12
                        ]
                    ]
                },
                "abstract": "Quantiles and Cardinality queries are important tools to analyze statistical information from big data streams. Due to the features of the streams, such as huge volume and high velocity, it is a challenging problem to quickly provide responses for the two types of queries using constrained space over big data streams. In this paper, we propose a composed sketch framework, which can support both quantiles queries and cardinality queries over the data streams. We introduce cardinality estimators into a baseline q-digest structure and propose unified sketch merging and query processing operations. Our approach can support these two types of queries simultaneously. We conduct detailed theoretical and experimental analysis in terms of query accuracy and query response time. The analytical and experimental results show that our approach can obtain accurate estimates quicker than traditional method and system in big data streams environments, and it just produces less than 0.8‰ storage overhead in TB-scale real-world data sets.",
                "call-number": "10.1145/3063955.3063995",
                "collection-number": "39",
                "collection-title": "ACM TUR-C '17",
                "container-title": "Proceedings of the ACM Turing 50th Celebration Conference - China",
                "DOI": "10.1145/3063955.3063995",
                "event-place": "Shanghai, China",
                "ISBN": "9781450348737",
                "keyword": "quantiles query, big data, sketch, cardinality query, data stream",
                "number": "Article 39",
                "number-of-pages": "10",
                "page": "1–10",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Composed sketch framework for quantiles and cardinality queries over big data streams",
                "URL": "https://doi.org/10.1145/3063955.3063995"
            }
        },
        {
            "10.1145/2910019.2910033": {
                "id": "10.1145/2910019.2910033",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Netten",
                        "given": "Niels"
                    },
                    {
                        "family": "van den Braak",
                        "given": "Susan"
                    },
                    {
                        "family": "Choenni",
                        "given": "Sunil"
                    },
                    {
                        "family": "van Someren",
                        "given": "Maarten"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            3,
                            1
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2016,
                            3,
                            1
                        ]
                    ]
                },
                "abstract": "Crisis response organizations operate in very dynamic environments, in which it is essential for responders to acquire all information critical to their task execution in time. In reality, the responders are often faced with information overload, incomplete information, or a combination of both. This hampers their decision-making process, workflow, situational awareness and, consequently, effective execution of collaborative crisis response. Therefore, getting the right information to the right person at the right time is of crucial importance.The task of processing all data during crisis response situations and determining for whom at a particular moment the information is relevant is not straightforward. When developing an information system to support this task, some important challenges have to be taken into account. These challenges relate to the structure and truthfulness of the used data, the assessment of information relevance, and the dissemination of relevant information in time. While methods and techniques from big data can be used to collect and integrate data, machine learning can be used to build a model for relevance assessments. An example implementation of such a framework of big data is the TAID software system that collects and integrates data communicated between first responders and may send information to crisis responders that were not addressed in the initial communication. As an example of the impact of TAID on crisis response, we show its effect in a simulated crisis response scenario.",
                "call-number": "10.1145/2910019.2910033",
                "collection-title": "ICEGOV '15-16",
                "container-title": "Proceedings of the 9th International Conference on Theory and Practice of Electronic Governance",
                "DOI": "10.1145/2910019.2910033",
                "event-place": "Montevideo, Uruguay",
                "ISBN": "9781450336406",
                "keyword": "Big Data, Crisis Response for Public Safety, Information Distribution, Machine Learning, Relevance Assessments",
                "number-of-pages": "10",
                "page": "266–275",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "A Big Data Approach to Support Information Distribution in Crisis Response",
                "URL": "https://doi.org/10.1145/2910019.2910033"
            }
        },
        {
            "10.1145/2676536.2676543": {
                "id": "10.1145/2676536.2676543",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Li",
                        "given": "Xun"
                    },
                    {
                        "family": "Li",
                        "given": "Wenwen"
                    },
                    {
                        "family": "Anselin",
                        "given": "Luc"
                    },
                    {
                        "family": "Rey",
                        "given": "Sergio"
                    },
                    {
                        "family": "Koschinsky",
                        "given": "Julia"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2014,
                            11,
                            4
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2014,
                            11,
                            4
                        ]
                    ]
                },
                "abstract": "Spatial analysis of Big data is a key component of Cyber-GIS. However, how to utilize existing cyberinfrastructure (e.g. large computing clusters) to perform parallel and distributed spatial analysis on Big data remains a huge challenge. Problems such as efficient spatial weights creation, spatial statistics and spatial regression of Big data still need investigation. In this research, we propose a MapReduce algorithm for creating contiguity-based spatial weights. This algorithm provides the ability to create spatial weights from very large spatial datasets efficiently by using computing resources that are organized in the Hadoop framework. It works in the paradigm of MapReduce: mappers are distributed in computing clusters to find contiguous neighbors in parallel, then reducers collect the results and generate the weights matrix. To test the performance of this algorithm, we design experiment to create contiguity-based weights matrix from artificial spatial data with up to 190 million polygons using Amazon's Hadoop framework called Elastic MapReduce. The experiment demonstrates the scalability of this parallel algorithm which utilizes large computing clusters to solve the problem of creating contiguity weights on Big data.",
                "call-number": "10.1145/2676536.2676543",
                "collection-title": "BigSpatial '14",
                "container-title": "Proceedings of the 3rd ACM SIGSPATIAL International Workshop on Analytics for Big Geospatial Data",
                "DOI": "10.1145/2676536.2676543",
                "event-place": "Dallas, Texas",
                "ISBN": "9781450331326",
                "keyword": "mapreduce, spatial weights, big data",
                "number-of-pages": "4",
                "page": "50–53",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "A MapReduce algorithm to create contiguity weights for spatial analysis of big data",
                "URL": "https://doi.org/10.1145/2676536.2676543"
            }
        },
        {
            "10.1145/2642769.2642798": {
                "id": "10.1145/2642769.2642798",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Gray",
                        "given": "Ian"
                    },
                    {
                        "family": "Chan",
                        "given": "Yu"
                    },
                    {
                        "family": "Audsley",
                        "given": "Neil C."
                    },
                    {
                        "family": "Wellings",
                        "given": "Andy"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2014,
                            9,
                            9
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2014,
                            9,
                            9
                        ]
                    ]
                },
                "abstract": "Existing programming models for distributed and cloud-based systems tend to abstract away from the architectures of individual target nodes, concentrating instead on higher-level issues of algorithm representation (MapReduce etc.). However, as programmers begin to tackle the issue of Big Data, increasing data volumes are forcing developers to reconsider this approach and to optimise their software heavily. JUNIPER is an EU-funded project which assists Big Data developers to create architecture-aware software in a way that is suitable for the target domain, and provides higher performance, portability, and real-time guarantees.",
                "call-number": "10.1145/2642769.2642798",
                "collection-title": "EuroMPI/ASIA '14",
                "container-title": "Proceedings of the 21st European MPI Users' Group Meeting",
                "DOI": "10.1145/2642769.2642798",
                "event-place": "Kyoto, Japan",
                "ISBN": "9781450328753",
                "number-of-pages": "6",
                "page": "151–156",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Architecture-Awareness for Real-Time Big Data Systems",
                "URL": "https://doi.org/10.1145/2642769.2642798"
            }
        },
        {
            "10.1145/3006299.3006304": {
                "id": "10.1145/3006299.3006304",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Hassaan",
                        "given": "Mohamed"
                    },
                    {
                        "family": "Elghandour",
                        "given": "Iman"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            12,
                            6
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2016,
                            12,
                            6
                        ]
                    ]
                },
                "abstract": "It is important to analyze and predict meteorological phenomena in real-time. Parallel programming by exploiting thousands of threads in GPUs can be efficiently used to speed up the execution of many applications. However, GPUs have limitations when used for processing big data, which can be better analyzed using distributed computing platforms such as Hadoop and Spark. In this paper, we propose DAMB a system that processes streamed data on a heterogeneous cluster of CPUs and GPUs in real-time. The core of DAMB is SparkGPU, a platform that extends Apache Spark to allow it to manage a heterogeneous cluster that has both CPUs and GPUs and to execute tasks on GPUs. DAMB also provides data visualization tools that present the analyzed data in an interactive way in real-time. As a case study, we focus on a meteorological application that analyzes lightening discharges. We show that DAMB can successfully process and analyze the meteorological data streamed to it and visualize the results in real-time on a cluster of size 12 nodes, each is equipped with one or more GPU cards. This is a speedup of two orders of magnitude as compared to a sequential program implementation for the same application.",
                "call-number": "10.1145/3006299.3006304",
                "collection-title": "BDCAT '16",
                "container-title": "Proceedings of the 3rd IEEE/ACM International Conference on Big Data Computing, Applications and Technologies",
                "DOI": "10.1145/3006299.3006304",
                "event-place": "Shanghai, China",
                "ISBN": "9781450346177",
                "keyword": "GPU programming, heterogeneous clusters, in-memory cluster computing",
                "number-of-pages": "10",
                "page": "168–177",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "A real-time big data analysis framework on a CPU/GPU heterogeneous cluster: a meteorological application case study",
                "URL": "https://doi.org/10.1145/3006299.3006304"
            }
        },
        {
            "10.1145/3544109.3544142": {
                "id": "10.1145/3544109.3544142",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Guo",
                        "given": "Jian"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2022,
                            4,
                            14
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2022,
                            4,
                            14
                        ]
                    ]
                },
                "call-number": "10.1145/3544109.3544142",
                "collection-title": "IPEC '22",
                "container-title": "Proceedings of the 3rd Asia-Pacific Conference on Image Processing, Electronics and Computers",
                "DOI": "10.1145/3544109.3544142",
                "event-place": "Dalian, China",
                "ISBN": "9781450395786",
                "number-of-pages": "4",
                "page": "183–186",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Financial Big Data Analysis Service System",
                "URL": "https://doi.org/10.1145/3544109.3544142"
            }
        },
        {
            "10.1145/3210506.3210514": {
                "id": "10.1145/3210506.3210514",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Fang",
                        "given": "Yinjie"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            4,
                            14
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            4,
                            14
                        ]
                    ]
                },
                "abstract": "This paper combines prospect theory with real option pricing model to construct a new value assessment model for big data assets. For the high uncertainty of future earnings and risk of big data assets, this paper analyzes that their value characteristics are in line with the American call option firstly. On this basis, we use the value function in the prospect theory to calculate decision makers' subjective judgments on the value of underlying big data assets under each state, and use the weighting function to calculate the decision makers' subjective judgment on the weight of expansion right, downsize right and abandon right. In example, we use least-squares Monte Carlo simulation method to perform a simulation which verifies the real option pricing method based on perspective of prospect theory can obtain more reasonable assessment result for big data assets.",
                "call-number": "10.1145/3210506.3210514",
                "collection-title": "EBIMCS '18",
                "container-title": "Proceedings of the 2018 International Conference on E-Business, Information Management and Computer Science",
                "DOI": "10.1145/3210506.3210514",
                "event-place": "Hong Kong, Hong Kong",
                "ISBN": "9781450363808",
                "keyword": "Assessment, Big Data Asset, Real Option, Big Data, Prospect Theory",
                "number-of-pages": "6",
                "page": "40–45",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "The Real Option Approach for Assessment of Big Data Asset Based on Prospect Theory",
                "URL": "https://doi.org/10.1145/3210506.3210514"
            }
        },
        {
            "10.1145/3320326.3320356": {
                "id": "10.1145/3320326.3320356",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Daki",
                        "given": "Houda"
                    },
                    {
                        "family": "El Hannani",
                        "given": "Asmaa"
                    },
                    {
                        "family": "Ouahmane",
                        "given": "Hassan"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            3,
                            27
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            3,
                            27
                        ]
                    ]
                },
                "abstract": "Recently, educational institutions suffer from high electrical consumption due to their new practices and activities. One of the promising solutions to overcome this challenge is to improve their energy management strategies using smart grids which ensure efficiency, reliability and energy saving. For this same reason, the National School of Applied Sciences of El Jadida -- Morocco has decided to install a private smart grid based on photovoltaic panels that will cover 40% of its electricity needs. But the problem that arises when using this new approach is the high level of complexity in term of data management due to the variety, veracity and the volume of the data. So, to meet these needs the use of Big Data technologies is required. In this paper, we propose a Big Data solution based on Lambda architecture to handle electrical consumption data in the National School of Applied Sciences of El Jadida -- Morocco. This system collects all parameters that might influence electrical consumption with Kafka, then it applies Spark libraries to analyze it. The solution allows also electrical energy forecasting using Spark machine learning library and the data persistence using HBase storage system.",
                "call-number": "10.1145/3320326.3320356",
                "collection-number": "24",
                "collection-title": "NISS19",
                "container-title": "Proceedings of the 2nd International Conference on Networking, Information Systems & Security",
                "DOI": "10.1145/3320326.3320356",
                "event-place": "Rabat, Morocco",
                "ISBN": "9781450366458",
                "keyword": "Electrical forecasting, Smart grid, Lambda architecture, Machine learning, Big Data, Kafka, HBase storage system, Spark",
                "number": "Article 24",
                "number-of-pages": "6",
                "page": "1–6",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Big-Data Architecture for Electrical Consumption Forecasting in Educational Institutions Buildings",
                "URL": "https://doi.org/10.1145/3320326.3320356"
            }
        },
        {
            "10.1145/3448748.3448776": {
                "id": "10.1145/3448748.3448776",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Cui",
                        "given": "Boao"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            1,
                            22
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2021,
                            1,
                            22
                        ]
                    ]
                },
                "abstract": "The two main characteristics of venture capital are high risk and participation in management. Risk identification and risk evaluation before investing, risk supervise and control are important process that affect the success of venture investment. First of all, a risk evaluation index system is constructed. Partial correlation analysis is used to explore the indicators that can significantly affect the success of a company's investment, and to provide suggestions for the types of risks that start-ups should focus on controlling during the startup period. Then the principal component analysis method and the Logistic regression analysis method are combined to predict the success rate of investment, which can make up for the deficiency of the Logistic model and improve the prediction accuracy rate. Then use the test set data to calculate the accuracy of the model, and conduct an Omnibus test of the model coefficients to verify the significance of the equation. Then the SE-DEA model is constructed to calculate and compare the efficiency values of enterprises that accept different levels of post-investment services, and test the robustness of the SE-DEA model by adjusting the input and output indicators. Then through the Mann-Whitney U test and analysis, it is concluded that if the post-investment service is to have a good effect, how VC should choose the degree of intervention according to the state of the enterprise. That is to say, which indicators of the enterprise can be optimized by VC intervention in management. Finally, the models are evaluated and future research prospects in related fields are proposed.",
                "call-number": "10.1145/3448748.3448776",
                "collection-title": "BIC 2021",
                "container-title": "Proceedings of the 2021 International Conference on Bioinformatics and Intelligent Computing",
                "DOI": "10.1145/3448748.3448776",
                "event-place": "Harbin, China",
                "ISBN": "9781450390002",
                "keyword": "Big data, models, risk management",
                "number-of-pages": "9",
                "page": "173–181",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Research on Big Data Risk Control Model of Venture Capital",
                "URL": "https://doi.org/10.1145/3448748.3448776"
            }
        },
        {
            "10.1145/2382416.2382418": {
                "id": "10.1145/2382416.2382418",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Weber",
                        "given": "Samuel"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2012,
                            10,
                            15
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2012,
                            10,
                            15
                        ]
                    ]
                },
                "abstract": "The ability to collect and organize large data sets has proven to be transformational: instead of just a linear improvement of older techniques, the ability to effectively process huge amounts of information creates radically new abilities and opportunities. Unfortunately, these abilities come with new security, privacy and legal risks: privacy that was previously protected only by the fact that data gathering was difficult can now be trivially violated. This talk will describe research challenges that arise in this field.",
                "call-number": "10.1145/2382416.2382418",
                "collection-title": "BADGERS '12",
                "container-title": "Proceedings of the 2012 ACM Workshop on Building analysis datasets and gathering experience returns for security",
                "DOI": "10.1145/2382416.2382418",
                "event-place": "Raleigh, North Carolina, USA",
                "ISBN": "9781450316613",
                "keyword": "privacy, legal aspects, security",
                "number-of-pages": "2",
                "page": "1–2",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Big data privacy and security challenges",
                "URL": "https://doi.org/10.1145/2382416.2382418"
            }
        },
        {
            "10.1145/3331453.3360973": {
                "id": "10.1145/3331453.3360973",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Dong",
                        "given": "Wei"
                    },
                    {
                        "family": "Xiao",
                        "given": "Litian"
                    },
                    {
                        "family": "Niu",
                        "given": "Shengfen"
                    },
                    {
                        "family": "Niu",
                        "given": "Jianjun"
                    },
                    {
                        "family": "Wang",
                        "given": "Fei"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            10,
                            22
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            10,
                            22
                        ]
                    ]
                },
                "abstract": "At the space launch site, the big data of the launch support system comes from the construction of the launch site, the ground service, the comprehensive support process, and launch mission organization and command. The big data is extensive sources, various types, large scale, and rapid growth. The big data application can improve the data processing and management efficiency for the launch support system. Then the application can enhance the support capability of flight mission and success rate. This paper analyzes the existing data application of launch support system. The challenges and requirements of big data application are studied by the construction of intelligent launch site. The application pattern and target are put forward from four aspects of launch mission organization and command, mission application, comprehensive support, and information security. The classification of big data is proposed for a launch support system. The architecture of big data application system is designed, which meets the application pattern and target. It lays a foundation for the future big data project at the launch site.",
                "call-number": "10.1145/3331453.3360973",
                "collection-number": "23",
                "collection-title": "CSAE 2019",
                "container-title": "Proceedings of the 3rd International Conference on Computer Science and Application Engineering",
                "DOI": "10.1145/3331453.3360973",
                "event-place": "Sanya, China",
                "ISBN": "9781450362948",
                "keyword": "Big data, Space launch site, Application research, Launch support system",
                "number": "Article 23",
                "number-of-pages": "6",
                "page": "1–6",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Application Research of Big Data for Launch Support System at Space Launch Site",
                "URL": "https://doi.org/10.1145/3331453.3360973"
            }
        },
        {
            "10.5555/2616606.2617095": {
                "id": "10.5555/2616606.2617095",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Wang",
                        "given": "Yu"
                    },
                    {
                        "family": "Li",
                        "given": "Boxun"
                    },
                    {
                        "family": "Luo",
                        "given": "Rong"
                    },
                    {
                        "family": "Chen",
                        "given": "Yiran"
                    },
                    {
                        "family": "Xu",
                        "given": "Ningyi"
                    },
                    {
                        "family": "Yang",
                        "given": "Huazhong"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2014,
                            3,
                            24
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2014,
                            3,
                            24
                        ]
                    ]
                },
                "abstract": "The world is experiencing a data revolution to discover knowledge in big data. Large scale neural networks are one of the mainstream tools of big data analytics. Processing big data with large scale neural networks includes two phases: the training phase and the operation phase. Huge computing power is required to support the training phase. And the energy efficiency (power efficiency) is one of the major considerations of the operation phase. We first explore the computing power of GPUs for big data analytics and demonstrate an efficient GPU implementation of the training phase of large scale recurrent neural networks (RNNs). We then introduce a promising ultra-high energy efficient implementation of neural networks' operation phase by taking advantage of the emerging memristor technique. Experiment results show that the proposed GPU implementation of RNNs is able to achieve 2 ~ 11× speed-up compared with the basic CPU implementation. And the scaled-up recurrent neural network trained with GPUs realizes an accuracy of 47% on the Microsoft Research Sentence Completion Challenge, the best result achieved by a single RNN on the same dataset. In addition, the proposed memristor-based implementation of neural networks demonstrates power efficiency of > 400 GFLOPS/W and achieves energy savings of 22× on the HMAX model compared with its pure digital implementation counterpart.",
                "call-number": "10.5555/2616606.2617095",
                "collection-number": "345",
                "collection-title": "DATE '14",
                "container-title": "Proceedings of the conference on Design, Automation & Test in Europe",
                "event-place": "Dresden, Germany",
                "ISBN": "9783981537024",
                "number": "Article 345",
                "number-of-pages": "2",
                "page": "1–2",
                "publisher": "European Design and Automation Association",
                "publisher-place": "Leuven, BEL",
                "title": "Energy efficient neural networks for big data analytics"
            }
        },
        {
            "10.1145/2463676.2467801": {
                "id": "10.1145/2463676.2467801",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Aboulnaga",
                        "given": "Ashraf"
                    },
                    {
                        "family": "Babu",
                        "given": "Shivnath"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2013,
                            6,
                            22
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2013,
                            6,
                            22
                        ]
                    ]
                },
                "call-number": "10.1145/2463676.2467801",
                "collection-title": "SIGMOD '13",
                "container-title": "Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data",
                "DOI": "10.1145/2463676.2467801",
                "event-place": "New York, New York, USA",
                "ISBN": "9781450320375",
                "keyword": "analytics, mapreduce, parallel database systems, workload management",
                "number-of-pages": "4",
                "page": "929–932",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Workload management for big data analytics",
                "URL": "https://doi.org/10.1145/2463676.2467801"
            }
        },
        {
            "10.1145/3494885.3494891": {
                "id": "10.1145/3494885.3494891",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Lu",
                        "given": "Siyang"
                    },
                    {
                        "family": "Chen",
                        "given": "Yihong"
                    },
                    {
                        "family": "Zhu",
                        "given": "Xiaolin"
                    },
                    {
                        "family": "Wang",
                        "given": "Ziyi"
                    },
                    {
                        "family": "Ou",
                        "given": "Yangjun"
                    },
                    {
                        "family": "Xie",
                        "given": "Yuhang"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            10,
                            22
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2021,
                            10,
                            22
                        ]
                    ]
                },
                "abstract": "The traditional support vector machines perform well in classification and prediction on small and medium-sized data sets, but there are some problems such as the low training efficiency and the low accuracy in large sample number, high dimension and large-scale data sets. Meanwhile, with the rise of distributed computing platforms such as the Spark suitable for big data analyses, more and more scholars at home and abroad turn their research direction to the distributed machine learning algorithms Therefore, in order to carry out the research on support vector machine for big data analyses, this paper explores the related researches and current situations of support vector machine, including: in-depth analysis of the algorithm principle of support vector machines, systematical investigation of the improved methods of support vector machines for the big data analyses, and distributed support vector machines under the Spark platform. Then, combined with the parallelization mechanism of the Spark, the some future research directions of support vector machine are investigated: for optimizing the accuracy of training results, some special matrix calculation skills should be added; In term of the research on SVM under the Spark platform, some better optimization methods from the perspective of dimension and partition can be found.",
                "call-number": "10.1145/3494885.3494891",
                "collection-title": "CSSE 2021",
                "container-title": "2021 4th International Conference on Computer Science and Software Engineering (CSSE 2021)",
                "DOI": "10.1145/3494885.3494891",
                "event-place": "Singapore, Singapore",
                "ISBN": "9781450390675",
                "keyword": "Spark, SVM, Machine Learning, Distribution",
                "number-of-pages": "7",
                "page": "31–37",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Exploring Support Vector Machines for Big Data Analyses",
                "URL": "https://doi.org/10.1145/3494885.3494891"
            }
        },
        {
            "10.1145/3318299.3318384": {
                "id": "10.1145/3318299.3318384",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Zhou",
                        "given": "LiangQi"
                    },
                    {
                        "family": "Xu",
                        "given": "HongZhen"
                    },
                    {
                        "family": "Wei",
                        "given": "Li"
                    },
                    {
                        "family": "Zhang",
                        "given": "Quan"
                    },
                    {
                        "family": "Zhou",
                        "given": "Fei"
                    },
                    {
                        "family": "Li",
                        "given": "ZhuoPei"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            2,
                            22
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            2,
                            22
                        ]
                    ]
                },
                "abstract": "Air quality has always been a hot issue of concern to the people, the environmental protection department and the government. Among the massive air quality data, abnormal data can interfere with subsequent experiments and analysis. Therefore, it is necessary to detect abnormal data to improve the accuracy of the data. However, traditional air outlier detection methods require at least one year's data to make inferences about air quality. This paper firstly analyzes the characteristics of air quality big data, and then proposes a framework based on Bayesian non-parametric clustering, namely Dirichlet Process (DP) clustering framework, to realize the outlier detection of air quality. The framework optimizes Gaussian mixture model into infinite Gaussian mixture model according to the results of data analysis, and uses neural network to cluster the data processed by infinite Gaussian mixture model, which effectively improves the clustering accuracy and avoids the need of collecting a large number of training data.",
                "call-number": "10.1145/3318299.3318384",
                "collection-title": "ICMLC '19",
                "container-title": "Proceedings of the 2019 11th International Conference on Machine Learning and Computing",
                "DOI": "10.1145/3318299.3318384",
                "event-place": "Zhuhai, China",
                "ISBN": "9781450366007",
                "keyword": "neural Network, Air quality, outlier detection, Bayesian clustering, Dirichlet process",
                "number-of-pages": "5",
                "page": "317–321",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Air Big Data Outlier Detection Based on Infinite Gauss Bayesian and CNN",
                "URL": "https://doi.org/10.1145/3318299.3318384"
            }
        },
        {
            "10.1145/3371425.3371435": {
                "id": "10.1145/3371425.3371435",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Liu",
                        "given": "He"
                    },
                    {
                        "family": "Wang",
                        "given": "Xiaohui"
                    },
                    {
                        "family": "Lei",
                        "given": "Shuya"
                    },
                    {
                        "family": "Zhang",
                        "given": "Xi"
                    },
                    {
                        "family": "Liu",
                        "given": "Weiwei"
                    },
                    {
                        "family": "Qin",
                        "given": "Ming"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            12,
                            19
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            12,
                            19
                        ]
                    ]
                },
                "abstract": "Data quality assessment plays an important role in electricity consumption big data. It can help business people master the overall data situation, which can provide a strong guarantee for subsequent data improvement, analysis and decision. According to the electrical data quality issues, we design a rule-based data quality assessment architecture for electrical big data. It includes six types of data quality assessment indexes (such as comprehensiveness, accuracy, completeness), and the related data quality rules (such as non-empty rule and range rule), which can be used to guide the electrical data quality inspection. Meanwhile, for the accuracy, we propose an outlier detection method based on time time-relevant k-means, which is used to detect the voltage, curve and power data issues in electricity data. The experimental and simulation results show that the proposed architecture and method can work well for the comprehensive data quality assessment of electrical data.",
                "call-number": "10.1145/3371425.3371435",
                "collection-number": "40",
                "collection-title": "AIIPCC '19",
                "container-title": "Proceedings of the International Conference on Artificial Intelligence, Information Processing and Cloud Computing",
                "DOI": "10.1145/3371425.3371435",
                "event-place": "Sanya, China",
                "ISBN": "9781450376334",
                "keyword": "outlier, electrical data, quality assessment, data quality",
                "number": "Article 40",
                "number-of-pages": "6",
                "page": "1–6",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "A rule based data quality assessment architecture and application for electrical data",
                "URL": "https://doi.org/10.1145/3371425.3371435"
            }
        },
        {
            "10.1145/3123024.3124411": {
                "id": "10.1145/3123024.3124411",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Babar",
                        "given": "Muhammad"
                    },
                    {
                        "family": "Arif",
                        "given": "Fahim"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            9,
                            11
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            9,
                            11
                        ]
                    ]
                },
                "abstract": "The extensive growth of the Internet of Things (IoT) is providing direction towards the smart urban. The smart urban is favored because it improves the standard of living of the citizens and provides excellence in the community services. The services may include but not limited to health, parking, transport, water, environment, power, and so forth. The diverse and heterogeneous environment of IoT and smart urban is challenged by real-time data processing and decision-making. In this research article, we propose IoT based smart urban architecture using Big Data analytics. The proposed architecture is divided into three different tiers: (1) data acquisition and aggregation, (2) data computation and processing, and (3) decision making and application. The proposed architecture is implemented and validated on Hadoop Ecosystem using reliable and authentic datasets. The research shows that the proposed system presents valuable imminent into the community development systems to get better the existing smart urban architecture.",
                "call-number": "10.1145/3123024.3124411",
                "collection-title": "UbiComp '17",
                "container-title": "Proceedings of the 2017 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2017 ACM International Symposium on Wearable Computers",
                "DOI": "10.1145/3123024.3124411",
                "event-place": "Maui, Hawaii",
                "ISBN": "9781450351904",
                "keyword": "smart city, big data analytics, IoT",
                "number-of-pages": "6",
                "page": "397–402",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Smart urban planning using big data analytics based internet of things",
                "URL": "https://doi.org/10.1145/3123024.3124411"
            }
        },
        {
            "10.1145/2983402.2983431": {
                "id": "10.1145/2983402.2983431",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Sidhu",
                        "given": "Ravneet Kaur"
                    },
                    {
                        "family": "Saroa",
                        "given": "Charanjiv Singh"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            9,
                            21
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2016,
                            9,
                            21
                        ]
                    ]
                },
                "abstract": "The data generated by today's enterprises has been increasing at exponential rates in size from most recent couple of years. Also, the need to process and break down the substantial volumes of data has likewise expanded. In order to handle this enormous amount of data and to analyze the same, an open-source usage of Apache system, Hadoop is utilized now-a-days. Hadoop presented a utility computing model which offer replacement of traditional databases and processing techniques. Scalability and high availability of MapReduce makes it the first choice for big data analysis. This paper provides a brief introduction to HDFS and MapReduce. After studying them in detail, it later made to work on related tasks and store the cached result of mapper function which can be used as an input for general reducers. By this additional triggering agent, we were able to achieve the analysis result in approximately half the actual time.",
                "call-number": "10.1145/2983402.2983431",
                "collection-title": "VisionNet'16",
                "container-title": "Proceedings of the Third International Symposium on Computer Vision and the Internet",
                "DOI": "10.1145/2983402.2983431",
                "event-place": "Jaipur, India",
                "ISBN": "9781450343015",
                "keyword": "MapReduce, Hadoop, HDFS, Big Data",
                "number-of-pages": "4",
                "page": "106–109",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Efficient Batch Processing of Related Big Data Tasks using Persistent MapReduce Technique",
                "URL": "https://doi.org/10.1145/2983402.2983431"
            }
        },
        {
            "10.1145/2745844.2745889": {
                "id": "10.1145/2745844.2745889",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Mirhoseini",
                        "given": "Azalia"
                    },
                    {
                        "family": "Songhori",
                        "given": "Ebrahim M."
                    },
                    {
                        "family": "Darvish Rouhani",
                        "given": "Bita"
                    },
                    {
                        "family": "Koushanfar",
                        "given": "Farinaz"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2015,
                            6,
                            15
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2015,
                            6,
                            15
                        ]
                    ]
                },
                "abstract": "This paper proposes a domain-specific solution for iterative learning of big and dense (non-sparse) datasets. A large host of learning algorithms, including linear and regularized regression techniques, rely on iterative updates on the data connectivity matrix in order to converge to a solution. The performance of such algorithms often severely degrade when it comes to large and dense data. Massive dense datasets not only induce obligatory large number of arithmetics, but they also incur unwanted message passing cost across the processing nodes. Our key observation is that despite the seemingly dense structures, in many applications, data can be transformed into a new space where sparse structures become revealed. We propose a scalable data transformation scheme that enables creating versatile sparse representations of the data. The transformation can be tuned to benefit the underlying platform's cost and constraints. Our evaluations demonstrate significant improvement in energy usage, runtime, and mem",
                "call-number": "10.1145/2745844.2745889",
                "collection-title": "SIGMETRICS '15",
                "container-title": "Proceedings of the 2015 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems",
                "DOI": "10.1145/2745844.2745889",
                "event-place": "Portland, Oregon, USA",
                "ISBN": "9781450334860",
                "keyword": "subspace sampling, sparse factorization, big and dense data, performance optimization",
                "number-of-pages": "2",
                "page": "453–454",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Flexible Transformations For Learning Big Data",
                "URL": "https://doi.org/10.1145/2745844.2745889"
            }
        },
        {
            "10.1145/2796314.2745889": {
                "id": "10.1145/2796314.2745889",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Mirhoseini",
                        "given": "Azalia"
                    },
                    {
                        "family": "Songhori",
                        "given": "Ebrahim M."
                    },
                    {
                        "family": "Darvish Rouhani",
                        "given": "Bita"
                    },
                    {
                        "family": "Koushanfar",
                        "given": "Farinaz"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2015,
                            6,
                            15
                        ]
                    ]
                },
                "abstract": "This paper proposes a domain-specific solution for iterative learning of big and dense (non-sparse) datasets. A large host of learning algorithms, including linear and regularized regression techniques, rely on iterative updates on the data connectivity matrix in order to converge to a solution. The performance of such algorithms often severely degrade when it comes to large and dense data. Massive dense datasets not only induce obligatory large number of arithmetics, but they also incur unwanted message passing cost across the processing nodes. Our key observation is that despite the seemingly dense structures, in many applications, data can be transformed into a new space where sparse structures become revealed. We propose a scalable data transformation scheme that enables creating versatile sparse representations of the data. The transformation can be tuned to benefit the underlying platform's cost and constraints. Our evaluations demonstrate significant improvement in energy usage, runtime, and mem",
                "call-number": "10.1145/2796314.2745889",
                "container-title": "SIGMETRICS Perform. Eval. Rev.",
                "DOI": "10.1145/2796314.2745889",
                "ISSN": "0163-5999",
                "issue": "1",
                "keyword": "subspace sampling, sparse factorization, performance optimization, big and dense data",
                "number-of-pages": "2",
                "page": "453–454",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "June 2015",
                "title": "Flexible Transformations For Learning Big Data",
                "URL": "https://doi.org/10.1145/2796314.2745889",
                "volume": "43"
            }
        },
        {
            "10.1145/3352411.3352450": {
                "id": "10.1145/3352411.3352450",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Heng",
                        "given": "Li"
                    },
                    {
                        "family": "Longfu",
                        "given": "Zhou"
                    },
                    {
                        "family": "Qian",
                        "given": "Yang"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            7,
                            19
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            7,
                            19
                        ]
                    ]
                },
                "abstract": "Big data technology can efficiently collect, store, analyze and mine the massive safety control data which is generated in the daily operation of refined oil depot. It can also realize the early warning of safety risk, complete the safety assessment as well as help make the safety decision of refined oil depot. This paper will firstly introduce the traditional safety control information systems of refined oil depot. Secondly, the study will dive into the different data types of safety control, and then will construct the big data platform for security control. By discussing the key technologies of security control big data platform, this research aims to help build big data of refined oil depot, to promote the development of big data technology within the sector of safety control of refined oil depot, and finally to make it as a \"sharp weapon \" in safety control of refined oil depot.",
                "call-number": "10.1145/3352411.3352450",
                "collection-title": "DSIT 2019",
                "container-title": "Proceedings of the 2019 2nd International Conference on Data Science and Information Technology",
                "DOI": "10.1145/3352411.3352450",
                "event-place": "Seoul, Republic of Korea",
                "ISBN": "9781450371414",
                "keyword": "Big data technology, Big data platform, Refined oil depot, Safety control",
                "number-of-pages": "6",
                "page": "249–254",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Research on Safety Control of Refined Oil Depot based on Big Data Technology",
                "URL": "https://doi.org/10.1145/3352411.3352450"
            }
        },
        {
            "10.1145/3241748.3241773": {
                "id": "10.1145/3241748.3241773",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Dai",
                        "given": "Hong"
                    },
                    {
                        "family": "Tao",
                        "given": "Ye"
                    },
                    {
                        "family": "Shi",
                        "given": "Tian-Wei"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            7,
                            5
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            7,
                            5
                        ]
                    ]
                },
                "abstract": "Put forward design of micro course mobile teaching system through analyzing the change of knowledge acquisition way and teaching role in the big data environment. The system platform includes the learner terminal platform and the teacher terminal platform. Learners use mobile terminals to active learn utilizing fragments of time. Mine the value information to take advantage of collecting education data in the process of using system. At the same time, the paper sets forth learning resource organization model. Form a complete data base of teaching process through the integration of a variety of teaching resources. Carry out data mining analysis according to the learner's behavior of micro course mobile learning. The results of the mining analysis provide a basis for teachers to adjust teaching content and improve teaching methods. The paper also presents the design of system network structure from the perspective of big data. The system architecture has been implemented. The paper proposes an innovative education evaluation mechanism through the mining analysis of education data. Form the objective and innovative evaluation mechanism between teachers and learners.",
                "call-number": "10.1145/3241748.3241773",
                "collection-title": "ICEBT '18",
                "container-title": "Proceedings of the 2018 2nd International Conference on E-Education, E-Business and E-Technology",
                "DOI": "10.1145/3241748.3241773",
                "event-place": "Beijing, China",
                "ISBN": "9781450364812",
                "keyword": "Mobile Learning, Evaluation Mechanism, Micro Course, Big Data",
                "number-of-pages": "4",
                "page": "48–51",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Research on Mobile Learning and Micro Course in the Big Data Environment",
                "URL": "https://doi.org/10.1145/3241748.3241773"
            }
        },
        {
            "10.1145/2740908.2745843": {
                "id": "10.1145/2740908.2745843",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Yuan",
                        "given": "Nicholas Jing"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2015,
                            5,
                            18
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2015,
                            5,
                            18
                        ]
                    ]
                },
                "abstract": "In recent years, with the rapid development of positioning technologies, online social networks, sensors and smart devices, large scale human behavioral data are now readily available. The growing availability of such behavioral data provides us unprecedented opportunities to gain more in depth understanding of users in both the physical world and cyber world, especially in online social networks. In this talk, I will introduce our recent research efforts in social and urban mining based on large-scale human behavioral datasets showcased by two projects: 1) LifeSpec: Modeling the spectrum of urban lifestyles based on heterogeneous online social network data. 2) L2P: Inferring demographic attributes from location check-ins.",
                "call-number": "10.1145/2740908.2745843",
                "collection-title": "WWW '15 Companion",
                "container-title": "Proceedings of the 24th International Conference on World Wide Web",
                "DOI": "10.1145/2740908.2745843",
                "event-place": "Florence, Italy",
                "ISBN": "9781450334730",
                "keyword": "performance, algorithms, experimentation",
                "number-of-pages": "1",
                "page": "1103",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Mining Social and Urban Big Data",
                "URL": "https://doi.org/10.1145/2740908.2745843"
            }
        },
        {
            "10.1145/3335656.3335688": {
                "id": "10.1145/3335656.3335688",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Li",
                        "given": "Minxuan"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            4,
                            28
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            4,
                            28
                        ]
                    ]
                },
                "abstract": "With the growing maturity of web crawler technology and the advent of the era of big data, when you want to study some problems, you can directly get all the data related to them through web crawlers and other means, but it is more important to mining and filter the data to get valuable data for the research content. This study which based on the word list of keywords uses CRN network to construct semantic distance table and TOPSIS evaluation system to sort data to make sure researchers can obtain quantitative screening data with research value and to provide researchers with scientific screening methods.",
                "call-number": "10.1145/3335656.3335688",
                "collection-title": "ICDMML 2019",
                "container-title": "Proceedings of the 2019 International Conference on Data Mining and Machine Learning",
                "DOI": "10.1145/3335656.3335688",
                "event-place": "Hong Kong, Hong Kong",
                "ISBN": "9781450360906",
                "keyword": "Big Data, Data screening, TOPSIS evaluation system, Data Mining, Keyword list of NIMBY event, CRN network",
                "number-of-pages": "5",
                "page": "70–74",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Innovation of Data Mining & Screening System under Big Data: Take a case as NIMBY",
                "URL": "https://doi.org/10.1145/3335656.3335688"
            }
        },
        {
            "10.1145/3017995.3017998": {
                "id": "10.1145/3017995.3017998",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Mahapatra",
                        "given": "Tanmaya"
                    },
                    {
                        "family": "Gerostathopoulos",
                        "given": "Ilias"
                    },
                    {
                        "family": "Prehofer",
                        "given": "Christian"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            11,
                            7
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2016,
                            11,
                            7
                        ]
                    ]
                },
                "abstract": "The increasing number and sensing capabilities of connected devices offer unique opportunities for developing sophisticated applications that employ data analysis as part of their business logic to make informed decisions based on sensed data. So far, mashup tools have been successful in supporting application development for Internet of Things. At the same time, Big Data analytics tools have allowed the analysis of very large and diverse data sets. The problem is that there is no consolidated development approach for integrating the two fields, IoT mashups and Big Data analytics. Such integration should go beyond merely specifying IoT mashups that only act as data providers. Mashup developers should also be able to specify Big Data analytics jobs and consume their results within a single application model. In this paper, we contribute to the direction of integrating Big Data analytics with IoT mashup tools by highlighting the need for such integration and the challenges that it entails via concrete examples. We also provide a research and development roadmap that can pave the way forward.",
                "call-number": "10.1145/3017995.3017998",
                "collection-title": "WoT '16",
                "container-title": "Proceedings of the Seventh International Workshop on the Web of Things",
                "DOI": "10.1145/3017995.3017998",
                "event-place": "Stuttgart, Germany",
                "ISBN": "9781450348744",
                "keyword": "IoT mashups, Big Data analytics, Development support",
                "number-of-pages": "6",
                "page": "11–16",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Towards Integration of Big Data Analytics in Internet of Things Mashup Tools",
                "URL": "https://doi.org/10.1145/3017995.3017998"
            }
        },
        {
            "10.1145/3268891.3268892": {
                "id": "10.1145/3268891.3268892",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Liu",
                        "given": "Zhao-ge"
                    },
                    {
                        "family": "Li",
                        "given": "Xiang-yang"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            8,
                            22
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            8,
                            22
                        ]
                    ]
                },
                "abstract": "In community safety service, big data governance is the prime mode to achieve community safety big data sharing and service value increasing. Although existing researches have preliminarily established the general big data governance framework, identification and governance of specific sharing problems lack comprehensive and systematic scenario description. Applying software engineering method, this paper proposes a kind of scenario expression model of big data governance in community safety service. Considering the common features of big data governance scenarios, construct the meta-scenario model of big data governance in community safety services. Considering the scenario expression difference under different levels, scales and particle sizes, construct the full view scenario model of big data governance in community safety services by meta-models nesting to complete the scenario expression under different applying situation. Finally, a use case is proposed to verify the rationality and effectiveness of the scenario expression models.",
                "call-number": "10.1145/3268891.3268892",
                "collection-title": "ICICM '18",
                "container-title": "Proceedings of the 8th International Conference on Information Communication and Management",
                "DOI": "10.1145/3268891.3268892",
                "event-place": "Edinburgh, United Kingdom",
                "ISBN": "9781450365024",
                "keyword": "scenario model, community safety, big data governance, safety service",
                "number-of-pages": "6",
                "page": "44–49",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Full View Scenario Model of Big Data Governance in Community Safety Service",
                "URL": "https://doi.org/10.1145/3268891.3268892"
            }
        },
        {
            "10.1145/3299887.3299892": {
                "id": "10.1145/3299887.3299892",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Hirzel",
                        "given": "Martin"
                    },
                    {
                        "family": "Baudart",
                        "given": "Guillaume"
                    },
                    {
                        "family": "Bonifati",
                        "given": "Angela"
                    },
                    {
                        "family": "Della Valle",
                        "given": "Emanuele"
                    },
                    {
                        "family": "Sakr",
                        "given": "Sherif"
                    },
                    {
                        "family": "Akrivi Vlachou",
                        "given": "Akrivi"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            12,
                            11
                        ]
                    ]
                },
                "abstract": "This paper is a survey of recent stream processing languages, which are programming languages for writing applications that analyze data streams. Data streams, or continuous data flows, have been around for decades. But with the advent of the big-data era, the size of data streams has increased dramatically. Analyzing big data streams yields immense advantages across all sectors of our society. To analyze streams, one needs to write a stream processing application. This paper showcases several languages designed for this purpose, articulates underlying principles, and outlines open challenges.",
                "call-number": "10.1145/3299887.3299892",
                "container-title": "SIGMOD Rec.",
                "DOI": "10.1145/3299887.3299892",
                "ISSN": "0163-5808",
                "issue": "2",
                "number-of-pages": "12",
                "page": "29–40",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "June 2018",
                "title": "Stream Processing Languages in the Big Data Era",
                "URL": "https://doi.org/10.1145/3299887.3299892",
                "volume": "47"
            }
        },
        {
            "10.1145/2905055.2905211": {
                "id": "10.1145/2905055.2905211",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Saxena",
                        "given": "Ankur"
                    },
                    {
                        "family": "Kaushik",
                        "given": "Neeraj"
                    },
                    {
                        "family": "Kaushik",
                        "given": "Nidhi"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            3,
                            4
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2016,
                            3,
                            4
                        ]
                    ]
                },
                "abstract": "In the time of big data techniques with spring framework on java, web servers or application server as the significant channel in big data should be updated to meet execution and force imperatives. Significant endeavors have been put resources into web server or application server conveyance and web storing procedures, but very few efforts have been paid to improve hardware-favored web type services. Big Data with spring framework in java is a promising business and computing model in web framework. Spring is the most popular open source Java application Framework. It combines all the industry-standard frameworks (for e.g. Struts and Hibernate) and approaches into one bundle.The expense and working costs of data centers have skyrocketed with the increase in computing capacity.Big data is a concept that defines the large volume of both structured and unstructured data -- that inundates a business on a day-to-day environment. This research argues the need to provide novel method and tools to bolster programming engineers meaning to enhance vitality productivity and minimize the subsequent from outlining, creating, sending and running programming in Big Data with spring framework.",
                "call-number": "10.1145/2905055.2905211",
                "collection-number": "5",
                "collection-title": "ICTCS '16",
                "container-title": "Proceedings of the Second International Conference on Information and Communication Technology for Competitive Strategies",
                "DOI": "10.1145/2905055.2905211",
                "event-place": "Udaipur, India",
                "ISBN": "9781450339629",
                "keyword": "spring, Java, Hadoop Framework, J2ee, Big Data",
                "number": "Article 5",
                "number-of-pages": "6",
                "page": "1–6",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Implementing and Analyzing Big Data Techniques withSpring Frame Work in Java& J2EEBased Application",
                "URL": "https://doi.org/10.1145/2905055.2905211"
            }
        },
        {
            "10.1145/1370788.1370799": {
                "id": "10.1145/1370788.1370799",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Liebchen",
                        "given": "Gernot A."
                    },
                    {
                        "family": "Shepperd",
                        "given": "Martin"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2008,
                            5,
                            12
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2008,
                            5,
                            12
                        ]
                    ]
                },
                "abstract": "OBJECTIVE - to assess the extent and types of techniques used to manage quality within software engineering data sets. We consider this a particularly interesting question in the context of initiatives to promote sharing and secondary analysis of data sets. METHOD - we perform a systematic review of available empirical software engineering studies. RESULTS - only 23 out of the many hundreds of studies assessed, explicitly considered data quality. CONCLUSIONS - first, the community needs to consider the quality and appropriateness of the data set being utilised; not all data sets are equal. Second, we need more research into means of identifying, and ideally repairing, noisy cases. Third, it should become routine to use sensitivity analysis to assess conclusion stability with respect to the assumptions that must be made concerning noise levels.",
                "call-number": "10.1145/1370788.1370799",
                "collection-title": "PROMISE '08",
                "container-title": "Proceedings of the 4th international workshop on Predictor models in software engineering",
                "DOI": "10.1145/1370788.1370799",
                "event-place": "Leipzig, Germany",
                "ISBN": "9781605580364",
                "keyword": "empirical research, data sets, prediction, data quality",
                "number-of-pages": "6",
                "page": "39–44",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Data sets and data quality in software engineering",
                "URL": "https://doi.org/10.1145/1370788.1370799"
            }
        },
        {
            "10.1145/2660168.2660177": {
                "id": "10.1145/2660168.2660177",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Rose",
                        "given": "Stephen"
                    },
                    {
                        "family": "Tuppen",
                        "given": "Sandra"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2014,
                            9,
                            12
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2014,
                            9,
                            12
                        ]
                    ]
                },
                "abstract": "This position paper sets out the possibility of a musicology based on the analysis of musical-bibliographical metadata as Big Data. It outlines the work underway, as part of the AHRC-funded project A Big Data History of Music, to align seven major datasets of musical-bibliographical metadata. After discussing some of the technical challenges of data alignment, it suggests how analysis and visualization of this data might transform musicological understandings of cultural transmission and canon formation.",
                "call-number": "10.1145/2660168.2660177",
                "collection-title": "DLfM '14",
                "container-title": "Proceedings of the 1st International Workshop on Digital Libraries for Musicology",
                "DOI": "10.1145/2660168.2660177",
                "event-place": "London, United Kingdom",
                "ISBN": "9781450330022",
                "keyword": "music publishing, canon, metadata, Musicology, bibliography",
                "number-of-pages": "3",
                "page": "1–3",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Prospects for a Big Data History of Music",
                "URL": "https://doi.org/10.1145/2660168.2660177"
            }
        },
        {
            "10.1145/3388142.3388164": {
                "id": "10.1145/3388142.3388164",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Safari",
                        "given": "Zohreh"
                    },
                    {
                        "family": "Mursi",
                        "given": "Khalid T."
                    },
                    {
                        "family": "Zhuang",
                        "given": "Yu"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2020,
                            3,
                            9
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2020,
                            3,
                            9
                        ]
                    ]
                },
                "abstract": "For a large volume of data, the clustering algorithm is of significant importance to categorize and analyze data. Accordingly, choosing the optimal number of clusters (K) is an essential factor, but it also is a tricky problem in big data analysis. More importantly, it is to efficiently determine the best K automatically, which is the main issue in clustering algorithms. Indeed, considering both the quality and efficiency of the clustering algorithm during defining K can be a trade-off that is our primary purpose to overcome. K-Means is still one of the popular clustering algorithms, which has a shortcoming that K needs to be pre-set. We introduce a new process with fewer K-Means running, which selects the most promising time to run the K-Means algorithm. To achieve this goal, we applied Bisecting K-Means and a different splitting measure, which all are contributed to efficiently determine the number of clusters automatically while maintaining the quality of clustering for a large set of high dimensional data. We carried out our experimental studies on different data sets and found that our procedure has the flexibility of choosing different criteria for determining the optimal K under each of them. Experiments indicate higher efficiency through decreasing of computation cost compared with the Ray&Turi method or with the use of only the K-Means algorithm.",
                "call-number": "10.1145/3388142.3388164",
                "collection-title": "ICCDA 2020",
                "container-title": "Proceedings of the 2020 the 4th International Conference on Compute and Data Analysis",
                "DOI": "10.1145/3388142.3388164",
                "event-place": "Silicon Valley, CA, USA",
                "ISBN": "9781450376440",
                "keyword": "Bisecting K-Means, Big Data, Clustering, Cluster Validity, K-Means",
                "number-of-pages": "8",
                "page": "50–57",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Fast Automatic Determination of Cluster Numbers for High Dimensional Big Data",
                "URL": "https://doi.org/10.1145/3388142.3388164"
            }
        },
        {
            "10.1145/2989214": {
                "id": "10.1145/2989214",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Dong",
                        "given": "Mianxiong"
                    },
                    {
                        "family": "Piuri",
                        "given": "Vincenzo"
                    },
                    {
                        "family": "Chan",
                        "given": "Shueng-Han Gary"
                    },
                    {
                        "family": "Jain",
                        "given": "Ramesh"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            9,
                            21
                        ]
                    ]
                },
                "call-number": "10.1145/2989214",
                "collection-number": "70",
                "container-title": "ACM Trans. Multimedia Comput. Commun. Appl.",
                "DOI": "10.1145/2989214",
                "ISSN": "1551-6857",
                "issue": "5s",
                "number": "Article 70",
                "number-of-pages": "3",
                "page": "1–3",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "December 2016",
                "title": "Introduction to Special Issue on Multimedia Big Data: Networking",
                "URL": "https://doi.org/10.1145/2989214",
                "volume": "12"
            }
        },
        {
            "10.1145/2912160.2912205": {
                "id": "10.1145/2912160.2912205",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Ghosh",
                        "given": "Debopriya"
                    },
                    {
                        "family": "Chun",
                        "given": "Soon Ae"
                    },
                    {
                        "family": "Shafiq",
                        "given": "Basit"
                    },
                    {
                        "family": "Adam",
                        "given": "Nabil R."
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            6,
                            8
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2016,
                            6,
                            8
                        ]
                    ]
                },
                "abstract": "One of the challenges governments and communities face to achieve smart city goals is dealing with enormous amount of data available - sensors, devices, social media, Web activities and commerce, tracking devices, all generate enormous amount of data, so called Big Data. Our goal is to empower the city government and its citizens to create a safer city by enabling crime and risk analysis of unstructured crime reports, criminal history of suspects, auto-license data, location-specific data, etc. for crime fighting efforts. We present intelligent solutions for Data-based Smart City Platform in Newark, NJ. We used a Machine Learning approach to automate and help crime analysts identify the connected entities and events by collecting, integrating and analyzing diverse data sources to generate alerts and predictions for new knowledge and insights that lead to better decision making and optimized actions.",
                "call-number": "10.1145/2912160.2912205",
                "collection-title": "dg.o '16",
                "container-title": "Proceedings of the 17th International Digital Government Research Conference on Digital Government Research",
                "DOI": "10.1145/2912160.2912205",
                "event-place": "Shanghai, China",
                "ISBN": "9781450343398",
                "keyword": "crime analysis, machine learning, smart city, Local Government",
                "number-of-pages": "9",
                "page": "58–66",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Big Data-based Smart City Platform: Real-Time Crime Analysis",
                "URL": "https://doi.org/10.1145/2912160.2912205"
            }
        },
        {
            "10.1145/3105831.3105842": {
                "id": "10.1145/3105831.3105842",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Santos",
                        "given": "Maribel Yasmina"
                    },
                    {
                        "family": "Costa",
                        "given": "Carlos"
                    },
                    {
                        "family": "Galvão",
                        "given": "João"
                    },
                    {
                        "family": "Andrade",
                        "given": "Carina"
                    },
                    {
                        "family": "Martinho",
                        "given": "Bruno Augusto"
                    },
                    {
                        "family": "Lima",
                        "given": "Francisca Vale"
                    },
                    {
                        "family": "Costa",
                        "given": "Eduarda"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            7,
                            12
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            7,
                            12
                        ]
                    ]
                },
                "abstract": "Big Data is currently conceptualized as data whose volume, variety or velocity impose significant difficulties in traditional techniques and technologies. Big Data Warehousing is emerging as a new concept for Big Data analytics. In this context, SQL-on-Hadoop systems increased notoriety, providing Structured Query Language (SQL) interfaces and interactive queries on Hadoop. A benchmark based on a denormalized version of the TPC-H is used to compare the performance of Hive on Tez, Spark, Presto and Drill. Some key contributions of this work include: the direct comparison of a vast set of technologies; unlike previous scientific works, SQL-on-Hadoop systems were connected to Hive tables instead of raw files; allow to understand the behaviour of these systems in scenarios with ever-increasing requirements, but not-so-good hardware. Besides these benchmark results, this paper also makes available interesting findings regarding an architecture and infrastructure in SQL-on-Hadoop for Big Data Warehousing, helping practitioners and fostering future research.",
                "call-number": "10.1145/3105831.3105842",
                "collection-title": "IDEAS '17",
                "container-title": "Proceedings of the 21st International Database Engineering & Applications Symposium",
                "DOI": "10.1145/3105831.3105842",
                "event-place": "Bristol, United Kingdom",
                "ISBN": "9781450352208",
                "keyword": "Hadoop, Hive, Spark, Data Warehouse, Drill, Presto, Big Data, SQL-on-Hadoop, Big Data Warehousing, Benchmark",
                "number-of-pages": "11",
                "page": "242–252",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Evaluating SQL-on-Hadoop for Big Data Warehousing on Not-So-Good Hardware",
                "URL": "https://doi.org/10.1145/3105831.3105842"
            }
        },
        {
            "10.1145/3234698.3234758": {
                "id": "10.1145/3234698.3234758",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Chehbi-Gamoura",
                        "given": "Samia"
                    },
                    {
                        "family": "Derrouiche",
                        "given": "Ridha"
                    },
                    {
                        "family": "Malhotra",
                        "given": "Manisha"
                    },
                    {
                        "family": "Koruca",
                        "given": "Halil-Ibrahim"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            6,
                            19
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            6,
                            19
                        ]
                    ]
                },
                "abstract": "With the Big Data management, and the propagation of business lines into complex networks, activities are ever more subject to disasters than ever. It is nearly impossible to forecast their happening and degree of related costs. Accordingly, organizations try to collaborate in risk management. This paper outlines and discusses a generic approach based on Fuzzy Cognitive Maps (FCM) for cross-management of Disaster Recovery Plans (DRP). A set of basics of disaster planning is also provided. The proposed approach is focused on risk assessment methodology. The method is able to aggregate all assessment variables of the whole stakeholders involved in the business network. The main outcomes of this study aim to support networked enterprises in improving risk readiness capability and disaster recovery. Finally, we indicate the open challenges for further researches and an outlook on our future research.",
                "call-number": "10.1145/3234698.3234758",
                "collection-number": "60",
                "collection-title": "ICEMIS '18",
                "container-title": "Proceedings of the Fourth International Conference on Engineering & MIS 2018",
                "DOI": "10.1145/3234698.3234758",
                "event-place": "Istanbul, Turkey",
                "ISBN": "9781450363921",
                "keyword": "adaptive management, Disaster Recovery Plan, networked enterprises, Fuzzy Cognitive Map, Big Data",
                "number": "Article 60",
                "number-of-pages": "8",
                "page": "1–8",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Adaptive Management Approach for more Availability of Big Data Business Analytics",
                "URL": "https://doi.org/10.1145/3234698.3234758"
            }
        },
        {
            "10.1109/CCGrid.2016.85": {
                "id": "10.1109/CCGrid.2016.85",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Nicolae",
                        "given": "Bogdan"
                    },
                    {
                        "family": "Costa",
                        "given": "Carlos"
                    },
                    {
                        "family": "Misale",
                        "given": "Claudia"
                    },
                    {
                        "family": "Katrinis",
                        "given": "Kostas"
                    },
                    {
                        "family": "Park",
                        "given": "Yoonho"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            5,
                            16
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2016,
                            5,
                            16
                        ]
                    ]
                },
                "abstract": "Big data analytics is an indispensable tool in transforming science, engineering, medicine, healthcare, finance and ultimately business itself. With the explosion of data sizes and need for shorter time-to-solution, in-memory platforms such as Apache Spark gain increasing popularity. However, this introduces important challenges, among which data shuffling is particularly difficult: on one hand it is a key part of the computation that has a major impact on the overall performance and scalability so its efficiency is paramount, while on the other hand it needs to operate with scarce memory in order to leave as much memory available for data caching. In this context, efficient scheduling of data transfers such that it addresses both dimensions of the problem simultaneously is non-trivial. State-of-the-art solutions often rely on simple approaches that yield sub-optimal performance and resource usage. This paper contributes a novel shuffle data transfer strategy that dynamically adapts to the computation with minimal memory utilization, which we briefly underline as a series of design principles.",
                "call-number": "10.1109/CCGrid.2016.85",
                "collection-title": "CCGRID '16",
                "container-title": "Proceedings of the 16th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing",
                "DOI": "10.1109/CCGrid.2016.85",
                "event-place": "Cartagena, Columbia",
                "ISBN": "9781509024520",
                "keyword": "data shuffling, elastic buffering, big data analytics, memory-efficient I/O",
                "number-of-pages": "4",
                "page": "409–412",
                "publisher": "IEEE Press",
                "title": "Towards memory-optimized data shuffling patterns for big data analytics",
                "URL": "https://doi.org/10.1109/CCGrid.2016.85"
            }
        },
        {
            "10.1109/CCGrid.2015.174": {
                "id": "10.1109/CCGrid.2015.174",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Cuzzocrea",
                        "given": "Alfredo"
                    },
                    {
                        "family": "Moussa",
                        "given": "Rim"
                    },
                    {
                        "family": "Xu",
                        "given": "Guandong"
                    },
                    {
                        "family": "Grasso",
                        "given": "Giorgio Mario"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2015,
                            5,
                            4
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2015,
                            5,
                            4
                        ]
                    ]
                },
                "abstract": "Following our previous research results, in this paper we provide two authoritative application scenarios that build on top of OLAP*, a middleware for parallel processing of OLAP queries that truly realizes effective and efficiently OLAP over Big Data. We have provided two authoritative case studies, namely parallel OLAP data cube processing and virtual OLAP data cube design, for which we also propose a comprehensive performance evaluation and analysis. Derived analysis clearly confirms the benefits of our proposed framework.",
                "call-number": "10.1109/CCGrid.2015.174",
                "collection-title": "CCGRID '15",
                "container-title": "Proceedings of the 15th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing",
                "DOI": "10.1109/CCGrid.2015.174",
                "event-place": "Shenzhen, China",
                "ISBN": "9781479980062",
                "number-of-pages": "7",
                "page": "921–927",
                "publisher": "IEEE Press",
                "title": "Cloud-based OLAP over big data: application scenarios and performance analysis",
                "URL": "https://doi.org/10.1109/CCGrid.2015.174"
            }
        },
        {
            "10.1145/3492324": {
                "id": "10.1145/3492324",
                "type": "BOOK",
                "issued": {
                    "date-parts": [
                        [
                            2021
                        ]
                    ]
                },
                "call-number": "10.1145/3492324",
                "container-title-short": "BDCAT '21",
                "event-place": "Leicester, United Kingdom",
                "genre": "proceeding",
                "ISBN": "9781450391641",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "2021 IEEE/ACM 8th International Conference on Big Data Computing, Applications and Technologies (BDCAT '21)"
            }
        },
        {
            "10.1145/3554729": {
                "id": "10.1145/3554729",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Yang",
                        "given": "Tianbao"
                    },
                    {
                        "family": "Ying",
                        "given": "Yiming"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2022,
                            7,
                            25
                        ]
                    ]
                },
                "abstract": "Area under the ROC curve, a.k.a. AUC, is a measure of choice for assessing the performance of a classifier for imbalanced data. AUC maximization refers to a learning paradigm that learns a predictive model by directly maximizing its AUC score. It has been studied for more than two decades dating back to late 90s and a huge amount of work has been devoted to AUC maximization since then. Recently, stochastic AUC maximization for big data and deep AUC maximization (DAM) for deep learning have received increasing attention and yielded dramatic impact for solving real-world problems. However, to the best our knowledge there is no comprehensive survey of related works for AUC maximization. This paper aims to address the gap by reviewing the literature in the past two decades. We not only give a holistic view of the literature but also present detailed explanations and comparisons of different papers from formulations to algorithms and theoretical guarantees. We also identify and discuss remaining and emerging issues for DAM, and provide suggestions on topics for future work.",
                "call-number": "10.1145/3554729",
                "container-title": "ACM Comput. Surv.",
                "DOI": "10.1145/3554729",
                "ISSN": "0360-0300",
                "keyword": "AUC, ROC, big data, deep learning",
                "note": "Just Accepted",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "AUC Maximization in the Era of Big Data and AI: A Survey",
                "URL": "https://doi.org/10.1145/3554729"
            }
        },
        {
            "10.1145/3463858.3463880": {
                "id": "10.1145/3463858.3463880",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Gui",
                        "given": "Yong"
                    },
                    {
                        "family": "Leng",
                        "given": "Sheng"
                    },
                    {
                        "family": "Dai",
                        "given": "Zhiqiang"
                    },
                    {
                        "family": "Wu",
                        "given": "Jiyuan"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            1,
                            8
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2021,
                            1,
                            8
                        ]
                    ]
                },
                "abstract": "The state of the tool wear is an important factor that affects the processing quality and production efficiency. With the development of a large number of automation equipment in the workshop, the on-line monitoring of tool wear is increasingly important for manufacturing industries. However, there are several challenges, such as the real-time storage and efficient processing of large amounts of signal data, in the process of on-line monitoring of tool wear. In this paper, a framework for big data driven on-line monitoring of tool wear was proposed to address these challenges. Then, two key technologies of the proposed framework including self-driving data acquisition and storage, processing of real-time and non-real time data were developed for the big data analytics for tool wear. Finally, a study of experiment was presented to demonstrate the proof-of-concept of the proposed framework. The results show that the proposed framework was feasible to be adopted in on-line monitoring of tool wear and can be used to make decisions on the time to change tool.",
                "call-number": "10.1145/3463858.3463880",
                "collection-title": "ICIEA 2021-Europe",
                "container-title": "2021 The 8th International Conference on Industrial Engineering and Applications(Europe)",
                "DOI": "10.1145/3463858.3463880",
                "event-place": "Barcelona, Spain",
                "ISBN": "9781450389921",
                "keyword": "on-line monitoring, tool wear, big data, self-driving",
                "number-of-pages": "5",
                "page": "1–5",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "A Framework for Big Data Driven On-Line Monitoring of Tool Wear",
                "URL": "https://doi.org/10.1145/3463858.3463880"
            }
        },
        {
            "10.1145/3017680.3022436": {
                "id": "10.1145/3017680.3022436",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Deb",
                        "given": "Debzani"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            3,
                            8
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            3,
                            8
                        ]
                    ]
                },
                "abstract": "Big data and cloud computing (BDCloud) collectively offer a paradigm shift in the way businesses are now acquiring, using and managing information technology. With the fast growth of this paradigm, we argue that each and every CS and IT students should be equipped with foundation knowledge in this collective paradigm and should possess hand-on-experiences in managing big data applications in clouds to acquire skills that are necessary to meet current and future industry demands. This poster presents our research that proposes gradual and systematic integration of big data and cloud computing related topics into multiple core (required) courses of CS/IT curriculum. The poster, supported by a NSF grant, will be useful for CS/IT students and their instructors as it identifies big data and cloud computing related topics that are important to cover, finds a sequence of the prescribed topics that can be incorporated into existing core courses most effectively, and suggests specific core courses in which their coverage might find an appropriate context. The poster further identifies the major challenges this proposed intervention may encounter and provides a deeper analysis of them. Finally, the poster describes our experience of implementing one such course with proposed interventions during Fall of 2016 semester. The pre- post- test results that measure student opinion and understanding of big data and cloud computing topics are presented in the poster and demonstrate improved student interest and learning.",
                "call-number": "10.1145/3017680.3022436",
                "collection-title": "SIGCSE '17",
                "container-title": "Proceedings of the 2017 ACM SIGCSE Technical Symposium on Computer Science Education",
                "DOI": "10.1145/3017680.3022436",
                "event-place": "Seattle, Washington, USA",
                "ISBN": "9781450346986",
                "keyword": "CS curriculum, IT curriculum, cloud computing, big data",
                "number-of-pages": "1",
                "page": "706",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "On the Integration of Big Data and Cloud Computing Topics (Abstract Only)",
                "URL": "https://doi.org/10.1145/3017680.3022436"
            }
        },
        {
            "10.1145/253769.253804": {
                "id": "10.1145/253769.253804",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Strong",
                        "given": "Diane M."
                    },
                    {
                        "family": "Lee",
                        "given": "Yang W."
                    },
                    {
                        "family": "Wang",
                        "given": "Richard Y."
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            1997,
                            5,
                            1
                        ]
                    ]
                },
                "call-number": "10.1145/253769.253804",
                "container-title": "Commun. ACM",
                "DOI": "10.1145/253769.253804",
                "ISSN": "0001-0782",
                "issue": "5",
                "number-of-pages": "8",
                "page": "103–110",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "May 1997",
                "title": "Data quality in context",
                "URL": "https://doi.org/10.1145/253769.253804",
                "volume": "40"
            }
        },
        {
            "10.1145/1651291.1651303": {
                "id": "10.1145/1651291.1651303",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Rodic",
                        "given": "Jasna"
                    },
                    {
                        "family": "Baranovic",
                        "given": "Mirta"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2009,
                            11,
                            6
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2009,
                            11,
                            6
                        ]
                    ]
                },
                "abstract": "Many data quality projects are integrated into data warehouse projects without enough time allocated for the data quality part, which leads to a need for a quicker data quality process implementation that can be easily adopted as the first stage of data warehouse implementation. We will see that many data quality rules can be implemented in a similar way, and thus generated based on metadata tables that store information about the rules. These generated rules are then used to check data in designated tables and mark erroneous records, or to do certain updates of invalid data. We will also store information about the rules violations in order to provide analysis of such data. This could give a significant insight into our source systems. Entire data quality process will be integrated into ETL process in order to achieve load of data warehouse that is as automated, as correct and as quick as possible. Only small number of records would be left for manual inspection and reprocessing.",
                "call-number": "10.1145/1651291.1651303",
                "collection-title": "DOLAP '09",
                "container-title": "Proceedings of the ACM twelfth international workshop on Data warehousing and OLAP",
                "DOI": "10.1145/1651291.1651303",
                "event-place": "Hong Kong, China",
                "ISBN": "9781605588018",
                "keyword": "generator, data quality, oracle, metadata, rules",
                "number-of-pages": "8",
                "page": "65–72",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Generating data quality rules and integration into ETL process",
                "URL": "https://doi.org/10.1145/1651291.1651303"
            }
        },
        {
            "10.1145/3293614.3293624": {
                "id": "10.1145/3293614.3293624",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Santos",
                        "given": "Anne C. M."
                    },
                    {
                        "family": "Pereira",
                        "given": "Ávner J. S."
                    },
                    {
                        "family": "Oliveira",
                        "given": "Manoela R."
                    },
                    {
                        "family": "Macedo",
                        "given": "Hendrik T."
                    },
                    {
                        "family": "Nascimento",
                        "given": "Rogério P. C."
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            11,
                            12
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            11,
                            12
                        ]
                    ]
                },
                "abstract": "The use of Big Data and Open Data has been increasing and becoming a tendency in the last years. Big Data is about collect, store and analysis and interpretation of datasets so big and complex that traditional applications of data processing are not appropriate to your treatment. Open Data is related to opening of data: by opening, it is understood that data must be public and available free. Looking for information transparency, government data should be open. Every day more cities and countries are opening your data. The Open Data emerge as a special paradigm in smart cities. The main goal of Big and Open Data technologies in a smart city is provide system development that can be useful to the citizens. In this work, we analyze the state of utilization of Big Data and Open data in technological solutions with interoperability between software products to smart cities. 79 publications were found and 25 of them were selected, listing some technologies able to answer the research questions of this work.",
                "call-number": "10.1145/3293614.3293624",
                "collection-number": "3",
                "collection-title": "EATIS '18",
                "container-title": "Proceedings of the Euro American Conference on Telematics and Information Systems",
                "DOI": "10.1145/3293614.3293624",
                "event-place": "Fortaleza, Brazil",
                "ISBN": "9781450365727",
                "keyword": "Big Data, Smart City, Open Data",
                "number": "Article 3",
                "number-of-pages": "7",
                "page": "1–7",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Building Software Products with use Open Data and Big Data in Smart Cities",
                "URL": "https://doi.org/10.1145/3293614.3293624"
            }
        },
        {
            "10.1145/2882903.2903744": {
                "id": "10.1145/2882903.2903744",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "LeFevre",
                        "given": "Jeff"
                    },
                    {
                        "family": "Liu",
                        "given": "Rui"
                    },
                    {
                        "family": "Inigo",
                        "given": "Cornelio"
                    },
                    {
                        "family": "Paz",
                        "given": "Lupita"
                    },
                    {
                        "family": "Ma",
                        "given": "Edward"
                    },
                    {
                        "family": "Castellanos",
                        "given": "Malu"
                    },
                    {
                        "family": "Hsu",
                        "given": "Meichun"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            6,
                            14
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2016,
                            6,
                            26
                        ]
                    ]
                },
                "abstract": "Enterprise customers increasingly require greater flexibility in the way they access and process their Big Data while at the same time they continue to request advanced analytics and access to diverse data sources. Yet customers also still require the robustness of enterprise class analytics for their mission-critical data. In this paper, we present our initial efforts toward a solution that satisfies the above requirements by integrating the HPE Vertica enterprise database with Apache Spark's open source big data computation engine. In particular, it enables fast, reliable transferring of data between Vertica and Spark; and deploying Machine Learning models created by Spark into Vertica for predictive analytics on Vertica data. This integration provides a fabric on which our customers get the best of both worlds: it extends Vertica's extensive SQL analytics capabilities with Spark's machine learning library (MLlib), giving Vertica users access to a wide range of ML functions; it also enables customers to leverage Spark as an advanced ETL engine for all data that require the guarantees offered by Vertica.",
                "call-number": "10.1145/2882903.2903744",
                "collection-title": "SIGMOD '16",
                "container-title": "Proceedings of the 2016 International Conference on Management of Data",
                "DOI": "10.1145/2882903.2903744",
                "event-place": "San Francisco, California, USA",
                "ISBN": "9781450335317",
                "keyword": "spark, database, connector, big data, vertica, analytics, PMML",
                "number-of-pages": "13",
                "page": "63–75",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Building the Enterprise Fabric for Big Data with Vertica and Spark Integration",
                "URL": "https://doi.org/10.1145/2882903.2903744"
            }
        },
        {
            "10.1145/3448748.3448989": {
                "id": "10.1145/3448748.3448989",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Lv",
                        "given": "Haiyan"
                    },
                    {
                        "family": "Li",
                        "given": "Zhiqiang"
                    },
                    {
                        "family": "Wen",
                        "given": "Baoqiang"
                    },
                    {
                        "family": "Wan",
                        "given": "Chauan"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            1,
                            22
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2021,
                            1,
                            22
                        ]
                    ]
                },
                "abstract": "The Google Hadoop platform Map/Reduce task scheduling and distribution mechanism of the Hadoop distributed computing framework applied to cloud computing and big data. The Quartz open source job scheduler regularly crawls into the websites of different tourist attractions, and stores the tourist attractions prices calculated by the price comparison algorithm to the Database HBase distribution Computing System. When the user enters the planned departure place, departure date, tourist attractions and other specific conditions, the cloud platform price comparison strategy system will display tourist routes according to certain logic, and generate price comparison data for tourist attractions, from the travel start point to the travel destination. The price comparison strategy of clothing, food, housing, transportation and consumption generates cost prices, recommends the best travel planning plan for customers, helps users choose the most economical tourist attractions and tourist routes to make quick choices, and obtain satisfactory returns for short vacations or holidays to avoid delay in decision-making time.",
                "call-number": "10.1145/3448748.3448989",
                "collection-title": "BIC 2021",
                "container-title": "Proceedings of the 2021 International Conference on Bioinformatics and Intelligent Computing",
                "DOI": "10.1145/3448748.3448989",
                "event-place": "Harbin, China",
                "ISBN": "9781450390002",
                "keyword": "Cloud computing, Big Data, Task scheduling and distribution mechanism, Price comparison strategy",
                "number-of-pages": "9",
                "page": "159–167",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "A Big Data Platform Tourism Price Strategy Method with Map/Reduce",
                "URL": "https://doi.org/10.1145/3448748.3448989"
            }
        },
        {
            "10.1145/3297156.3297249": {
                "id": "10.1145/3297156.3297249",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Samoylov",
                        "given": "Alexey"
                    },
                    {
                        "family": "Sergeev",
                        "given": "Nikolay"
                    },
                    {
                        "family": "Kucherova",
                        "given": "Margarita"
                    },
                    {
                        "family": "Denisov",
                        "given": "Boris"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            12,
                            8
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            12,
                            8
                        ]
                    ]
                },
                "abstract": "The success of data preparation for Big Data analytics directly depends on the quality of data integration from heterogeneous data sources. Extract, Transform and Load (ETL) systems have proved to be an efficient solution for this task. But to the moment, in the stages of data selection, definition of extraction rules and transformation, the decision is usually made exclusively by a data specialist. This, in turn, causes such problems as redundancy and inconsistency of imported data, narrow specialization of rules (up to uniqueness) with a limited number of analytical models and known requirements for the data mart. This paper presents the concept of solving the problem by providing methodological support for Big Data preparation procedure to efficiently collect data from a priory unknown heterogeneous data sources.",
                "call-number": "10.1145/3297156.3297249",
                "collection-title": "CSAI '18",
                "container-title": "Proceedings of the 2018 2nd International Conference on Computer Science and Artificial Intelligence",
                "DOI": "10.1145/3297156.3297249",
                "event-place": "Shenzhen, China",
                "ISBN": "9781450366069",
                "keyword": "heterogeneous data sources, Big Data, knowledge extraction, modeling, semantics, ETL, data integration",
                "number-of-pages": "5",
                "page": "131–135",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Methodology of Big Data Integration from A Priori Unknown Heterogeneous Data Sources",
                "URL": "https://doi.org/10.1145/3297156.3297249"
            }
        },
        {
            "10.1145/3299869.3320240": {
                "id": "10.1145/3299869.3320240",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Giannakouris",
                        "given": "Victor"
                    },
                    {
                        "family": "Fernandez",
                        "given": "Alejandro"
                    },
                    {
                        "family": "Simitsis",
                        "given": "Alkis"
                    },
                    {
                        "family": "Babu",
                        "given": "Shivnath"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            6,
                            25
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            6,
                            25
                        ]
                    ]
                },
                "abstract": "More than 10,000 enterprises worldwide use the big data stack composed of multiple distributed systems. At Unravel, we build the next-generation APM platform for the big data stack, and we have worked with a representative sample of these enterprises that covers most industry verticals. This sample covers the spectrum of choices for deploying the big data stack across on-premises datacenters, private and public cloud deployments, and hybrid combinations of these. In this paper, we present a solution for assisting enterprises planning the migration of their big data stacks from on-premises deployments to the cloud. Our solution is goal driven and adapts to various migration scenarios. We present the system architecture we built and several cloud mapping options. We also describe a demonstration script that involves practical, real-world use-cases of the path to cloud adoption.",
                "call-number": "10.1145/3299869.3320240",
                "collection-title": "SIGMOD '19",
                "container-title": "Proceedings of the 2019 International Conference on Management of Data",
                "DOI": "10.1145/3299869.3320240",
                "event-place": "Amsterdam, Netherlands",
                "ISBN": "9781450356435",
                "keyword": "cloud migration, big data stack, application performance management",
                "number-of-pages": "4",
                "page": "1909–1912",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Cost-Effective, Workload-Adaptive Migration of Big Data Applications to the Cloud",
                "URL": "https://doi.org/10.1145/3299869.3320240"
            }
        }
    ]
}