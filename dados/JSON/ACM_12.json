{
    "exportedDoiLength": 100,
    "fileName": "acm",
    "style": "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<style xmlns=\"http://purl.org/net/xbiblio/csl\" class=\"in-text\" version=\"1.0\" demote-non-dropping-particle=\"sort-only\" default-locale=\"en-US\">\n    <!-- This style was edited with the Visual CSL Editor (http://editor.citationstyles.org/visualEditor/) -->\n    <info>\n        <title>BibTeX ACM citation style</title>\n        <id>http://www.zotero.org/styles/bibtex-acm-citation-style</id>\n        <link href=\"http://www.zotero.org/styles/bibtex-acm-citation-style\" rel=\"self\"/>\n        <link href=\"http://www.bibtex.org/\" rel=\"documentation\"/>\n        <author>\n            <name>Markus Schaffner</name>\n        </author>\n        <contributor>\n            <name>Richard Karnesky</name>\n            <email>karnesky+zotero@gmail.com</email>\n            <uri>http://arc.nucapt.northwestern.edu/Richard_Karnesky</uri>\n        </contributor>\n        <category citation-format=\"author-date\"/>\n        <category field=\"generic-base\"/>\n        <updated>2018-06-11T10:52:49+00:00</updated>\n        <rights license=\"http://creativecommons.org/licenses/by-sa/3.0/\">This work is licensed under a Creative Commons Attribution-ShareAlike 3.0 License</rights>\n    </info>\n    <macro name=\"zotero2bibtexType\">\n        <choose>\n            <if type=\"BILL BOOK GRAPHIC LEGAL_CASE LEGISLATION MOTION_PICTURE SONG\" match=\"any\">\n                <choose>\n                    <if genre=\"rfc\"  match=\"any\">\n                       <text value=\"rfc\"/>\n                    </if>\n                     <else-if  genre=\"bibliography\" match=\"any\">\n                        <text value=\"bibliography\"/>\n                    </else-if>\n                    <else-if  genre=\"play_drama\" match=\"any\">\n                        <text value=\"playdrama\"/>\n                    </else-if>\n                    <else-if  genre=\"proceeding\" match=\"any\">\n                        <text value=\"proceedings\"/>\n                    </else-if>\n                    <else-if  genre=\"tech_brief\" match=\"any\">\n                        <text value=\"tech-brief\"/>\n                    </else-if>\n                    <else>\n                       <text value=\"book\"/>\n                    </else>\n                </choose>\n            </if>\n            <else-if type=\"CHAPTER\" match=\"any\">\n                <text value=\"inbook\"/>\n            </else-if>\n            <else-if type=\"ARTICLE ARTICLE_JOURNAL ARTICLE_MAGAZINE ARTICLE_NEWSPAPER\" match=\"any\">\n                <text value=\"article\"/>\n            </else-if>\n            <else-if type=\"THESIS\" match=\"any\">\n                <choose>\n                    <if variable=\"genre\">\n                        <text variable=\"genre\" text-case=\"lowercase\" strip-periods=\"true\"/>\n                    </if>\n                </choose>\n                <text value=\"thesis\"/>\n            </else-if>\n            <else-if type=\"PAPER_CONFERENCE\" match=\"any\">\n                <text value=\"inproceedings\"/>\n            </else-if>\n            <else-if type=\"REPORT\" match=\"any\">\n                <text value=\"techreport\"/>\n            </else-if>\n            <else-if type=\"DATASET\" match=\"any\">\n                <choose>\n                    <if genre=\"software\"  match=\"any\">\n                       <text value=\"software\"/>\n                    </if>\n                    <else>\n                       <text value=\"dataset\"/>\n                    </else>\n                </choose>\n            </else-if>\n            <else>\n                <text value=\"misc\"/>\n            </else>\n        </choose>\n    </macro>\n    <macro name=\"citeKey\">\n           <text variable=\"call-number\"/>\n    </macro>\n    <macro name=\"editor-short\">\n        <names variable=\"editor\">\n            <name form=\"short\" delimiter=\":\" delimiter-precedes-last=\"always\"/>\n        </names>\n    </macro>\n    <macro name=\"author-short\">\n        <names variable=\"author\">\n            <name form=\"short\" delimiter=\":\" delimiter-precedes-last=\"always\"/>\n        </names>\n    </macro>\n    <macro name=\"issued-year\">\n        <date variable=\"issued\">\n            <date-part name=\"year\"/>\n        </date>\n    </macro>\n    <macro name=\"issued-month\">\n        <choose>\n            <if type=\"ARTICLE\" match=\"any\">\n                <date variable=\"issued\">\n                    <date-part name=\"month\" form=\"short\" strip-periods=\"true\" text-case=\"lowercase\"/>\n                </date>\n            </if>\n        </choose>\n    </macro>\n    <macro name=\"issue-date\">\n        <choose>\n            <if type=\"ARTICLE ARTICLE_JOURNAL\" match=\"any\">\n                <choose>\n                    <if variable=\"note\" match=\"none\">\n                        <choose>\n                            <if variable=\"source\">\n                                 <text variable=\"source\"/>\n                            </if>\n                               <else>\n                                 <date date-parts=\"year-month\" form=\"text\" variable=\"issued\"/>\n                               </else>\n                        </choose>\n                    </if>\n                </choose>\n            </if>\n        </choose>\n    </macro>\n    <macro name=\"author\">\n        <names variable=\"author\">\n            <name sort-separator=\", \" delimiter=\" and \" delimiter-precedes-last=\"always\" name-as-sort-order=\"all\"/>\n            <label form=\"long\" text-case=\"capitalize-first\"/>\n        </names>\n    </macro>\n    <macro name=\"editor-translator\">\n        <names variable=\"editor translator\" delimiter=\", \">\n            <name sort-separator=\", \" delimiter=\" and \" delimiter-precedes-last=\"always\" name-as-sort-order=\"all\"/>\n            <label form=\"long\" text-case=\"capitalize-first\"/>\n        </names>\n    </macro>\n    <macro name=\"title\">\n        <choose>\n            <if genre=\"proceeding\" match=\"any\">\n                <text variable=\"container-title-short\" suffix=\": \"/>\n                <text variable=\"title\" text-case=\"title\"/>\n            </if>\n            <else-if type=\"ARTICLE_JOURNAL\" match=\"none\">\n                <text variable=\"title\" text-case=\"title\"/>\n            </else-if>\n        </choose>\n    </macro>\n    <macro name=\"volume\">\n        <choose>\n            <if type=\"ARTICLE BOOK ARTICLE_JOURNAL\" match=\"any\">\n                <text variable=\"volume\" prefix=\"volume = {\" suffix=\"}\"/>\n            </if>\n        </choose>\n    </macro>\n    <macro name=\"DOI\">\n        <choose>\n            <if type=\"ARTICLE PAPER_CONFERENCE\" match=\"any\">\n                <text variable=\"DOI\" prefix=\"doi = {\" suffix=\"}\"/>\n            </if>\n        </choose>\n    </macro>\n    <macro name=\"abstract\">\n        <if match=\"any\" variable=\"abstract\">\n            <text variable=\"abstract\"/>\n        </if>\n    </macro>\n    <macro name=\"URL\">\n        <text variable=\"URL\" prefix=\"url = {\" suffix=\"}\"/>\n    </macro>\n    <macro name=\"container-title\">\n        <choose>\n            <if type=\"CHAPTER PAPER_CONFERENCE\" match=\"any\">\n                <text variable=\"container-title\" prefix=\"booktitle = {\" suffix=\"}\" text-case=\"title\"/>\n            </if>\n            <else-if type=\"ARTICLE ARTICLE_JOURNAL\" match=\"any\">\n                <text variable=\"container-title\" prefix=\"journal = {\" suffix=\"}\" text-case=\"title\"/>\n            </else-if>\n        </choose>\n    </macro>\n    <macro name=\"pages\">\n        <group delimiter=\",&#10;\">\n            <choose>\n                <if match=\"any\" variable=\"collection-number\">\n                    <text variable=\"collection-number\" prefix=\"articleno = {\" suffix=\"}\"/>\n                </if>\n                <else>\n                    <text variable=\"page\" prefix=\"pages = {\" suffix=\"}\"/>\n                </else>\n            </choose>\n            <text variable=\"number-of-pages\" prefix=\"numpages = {\" suffix=\"}\"/>\n        </group>\n    </macro>\n    <macro name=\"edition\">\n        <text variable=\"edition\"/>\n    </macro>\n    <macro name=\"editor\">\n        <choose>\n            <if match=\"any\" type=\"THESIS\">\n                <names variable=\"editor\" delimiter=\", \" prefix=\"advisor = {\" suffix=\"}\">\n                    <name delimiter=\" and \" delimiter-precedes-last=\"always\" name-as-sort-order=\"all\"/>\n                </names>\n            </if>\n            <else>\n               <names variable=\"editor\" delimiter=\", \" prefix=\"editor = {\" suffix=\"}\">\n                   <name delimiter=\" and \" delimiter-precedes-last=\"always\" name-as-sort-order=\"all\"/>\n               </names>\n            </else>\n        </choose>\n    </macro>\n    <macro name=\"keyword\">\n        <choose>\n            <if type=\"ARTICLE PAPER_CONFERENCE DATASET\" match=\"any\">\n                <text variable=\"keyword\" prefix=\"keywords = {\" suffix=\"}\"/>\n            </if>\n        </choose>\n    </macro>\n    <citation et-al-min=\"11\" et-al-use-first=\"10\" disambiguate-add-year-suffix=\"true\" disambiguate-add-names=\"false\" disambiguate-add-givenname=\"false\" collapse=\"year\">\n        <layout delimiter=\"_\">\n            <text macro=\"citeKey\"/>\n        </layout>\n    </citation>\n    <bibliography hanging-indent=\"false\">\n        <layout>\n            <group display=\"right-inline\">\n                <text macro=\"zotero2bibtexType\" prefix=\"@\"/>\n                <group prefix=\"{\" suffix=\"&#10;}\" delimiter=\",&#10;\">\n                    <text macro=\"citeKey\"/>\n                    <text macro=\"author\" prefix=\"author = {\" suffix=\"}\"/>\n                    <text macro=\"editor\"/>\n                    <text macro=\"title\" prefix=\"title = {\" suffix=\"}\"/>\n                    <text macro=\"issued-year\" prefix=\"year = {\" suffix=\"}\"/>\n                    <text macro=\"issue-date\" prefix=\"issue_date = {\" suffix=\"}\"/>\n                    <text variable=\"ISBN\" prefix=\"isbn = {\" suffix=\"}\"/>\n                    <text variable=\"publisher\" prefix=\"publisher = {\" suffix=\"}\"/>\n                    <text variable=\"publisher-place\" prefix=\"address = {\" suffix=\"}\"/>\n                    <text variable=\"chapter-number\" prefix=\"chapter = {\" suffix=\"}\"/>\n                    <text macro=\"edition\" prefix=\"edition = {\" suffix=\"}\"/>\n                    <text macro=\"volume\"/>\n                    <text variable=\"issue\" prefix=\"number = {\" suffix=\"}\"/>\n                    <text variable=\"ISSN\" prefix=\"issn = {\" suffix=\"}\"/>\n                    <text variable=\"archive_location\" prefix=\"archiveLocation = {\" suffix=\"}\"/>\n                    <text macro=\"URL\"/>\n                    <text macro=\"DOI\"/>\n                    <text macro=\"abstract\" prefix=\"abstract = {\" suffix=\"}\"/>\n                    <text variable=\"note\" prefix=\"note = {\" suffix=\"}\"/>\n                    <text macro=\"container-title\"/>\n                    <text macro=\"issued-month\" prefix=\"month = {\" suffix=\"}\"/>\n                    <text macro=\"pages\"/>\n                    <text macro=\"keyword\"/>\n                    <text variable=\"event-place\" prefix=\"location = {\" suffix=\"}\"/>\n                    <choose>\n                        <if type=\"PAPER_CONFERENCE\" match=\"any\">\n                            <text variable=\"collection-title\" prefix=\"series = {\" suffix=\"}\"/>\n                        </if>\n                        <else>\n                            <text variable=\"collection-title\" prefix=\"collection = {\" suffix=\"}\"/>\n                        </else>\n                    </choose>\n                </group>\n            </group>\n        </layout>\n    </bibliography>\n</style>\n",
    "suffix": "bib",
    "locale": "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<locale xmlns=\"http://purl.org/net/xbiblio/csl\" version=\"1.0\" xml:lang=\"en-US\">\n  <info>\n    <rights license=\"http://creativecommons.org/licenses/by-sa/3.0/\">This work is licensed under a Creative Commons Attribution-ShareAlike 3.0 License</rights>\n    <updated>2012-07-04T23:31:02+00:00</updated>\n  </info>\n  <style-options punctuation-in-quote=\"true\"\n                 leading-noise-words=\"a,an,the\"\n                 name-as-sort-order=\"ja zh kr my hu vi\"\n                 name-never-short=\"ja zh kr my hu vi\"/>\n  <date form=\"text\">\n    <date-part name=\"month\" suffix=\" \"/>\n    <date-part name=\"day\" suffix=\", \"/>\n    <date-part name=\"year\"/>\n  </date>\n  <date form=\"numeric\">\n    <date-part name=\"month\" form=\"numeric-leading-zeros\" suffix=\"/\"/>\n    <date-part name=\"day\" form=\"numeric-leading-zeros\" suffix=\"/\"/>\n    <date-part name=\"year\"/>\n  </date>\n  <terms>\n    <term name=\"radio-broadcast\">radio broadcast</term>\n    <term name=\"television-broadcast\">television broadcast</term>\n    <term name=\"podcast\">podcast</term>\n    <term name=\"instant-message\">instant message</term>\n    <term name=\"email\">email</term>\n    <term name=\"number-of-volumes\">\n      <single>volume</single>\n      <multiple>volumes</multiple>\n    </term>\n    <term name=\"accessed\">accessed</term>\n    <term name=\"and\">and</term>\n    <term name=\"and\" form=\"symbol\">&amp;</term>\n    <term name=\"and others\">and others</term>\n    <term name=\"anonymous\">anonymous</term>\n    <term name=\"anonymous\" form=\"short\">anon.</term>\n    <term name=\"at\">at</term>\n    <term name=\"available at\">available at</term>\n    <term name=\"by\">by</term>\n    <term name=\"circa\">circa</term>\n    <term name=\"circa\" form=\"short\">c.</term>\n    <term name=\"cited\">cited</term>\n    <term name=\"edition\">\n      <single>edition</single>\n      <multiple>editions</multiple>\n    </term>\n    <term name=\"edition\" form=\"short\">ed.</term>\n    <term name=\"et-al\">et al.</term>\n    <term name=\"forthcoming\">forthcoming</term>\n    <term name=\"from\">from</term>\n    <term name=\"ibid\">ibid.</term>\n    <term name=\"in\">in</term>\n    <term name=\"in press\">in press</term>\n    <term name=\"internet\">internet</term>\n    <term name=\"interview\">interview</term>\n    <term name=\"letter\">letter</term>\n    <term name=\"no date\">no date</term>\n    <term name=\"no date\" form=\"short\">n.d.</term>\n    <term name=\"online\">online</term>\n    <term name=\"presented at\">presented at the</term>\n    <term name=\"reference\">\n      <single>reference</single>\n      <multiple>references</multiple>\n    </term>\n    <term name=\"reference\" form=\"short\">\n      <single>ref.</single>\n      <multiple>refs.</multiple>\n    </term>\n    <term name=\"retrieved\">retrieved</term>\n    <term name=\"scale\">scale</term>\n    <term name=\"version\">version</term>\n\n    <!-- ANNO DOMINI; BEFORE CHRIST -->\n    <term name=\"ad\">AD</term>\n    <term name=\"bc\">BC</term>\n\n    <!-- PUNCTUATION -->\n    <term name=\"open-quote\">“</term>\n    <term name=\"close-quote\">”</term>\n    <term name=\"open-inner-quote\">‘</term>\n    <term name=\"close-inner-quote\">’</term>\n    <term name=\"page-range-delimiter\">–</term>\n\n    <!-- ORDINALS -->\n    <term name=\"ordinal\">th</term>\n    <term name=\"ordinal-01\">st</term>\n    <term name=\"ordinal-02\">nd</term>\n    <term name=\"ordinal-03\">rd</term>\n    <term name=\"ordinal-11\">th</term>\n    <term name=\"ordinal-12\">th</term>\n    <term name=\"ordinal-13\">th</term>\n\n    <!-- LONG ORDINALS -->\n    <term name=\"long-ordinal-01\">first</term>\n    <term name=\"long-ordinal-02\">second</term>\n    <term name=\"long-ordinal-03\">third</term>\n    <term name=\"long-ordinal-04\">fourth</term>\n    <term name=\"long-ordinal-05\">fifth</term>\n    <term name=\"long-ordinal-06\">sixth</term>\n    <term name=\"long-ordinal-07\">seventh</term>\n    <term name=\"long-ordinal-08\">eighth</term>\n    <term name=\"long-ordinal-09\">ninth</term>\n    <term name=\"long-ordinal-10\">tenth</term>\n\n    <!-- LONG LOCATOR FORMS -->\n    <term name=\"book\">\n      <single>book</single>\n      <multiple>books</multiple>\n    </term>\n    <term name=\"chapter\">\n      <single>chapter</single>\n      <multiple>chapters</multiple>\n    </term>\n    <term name=\"column\">\n      <single>column</single>\n      <multiple>columns</multiple>\n    </term>\n    <term name=\"figure\">\n      <single>figure</single>\n      <multiple>figures</multiple>\n    </term>\n    <term name=\"folio\">\n      <single>folio</single>\n      <multiple>folios</multiple>\n    </term>\n    <term name=\"issue\">\n      <single>number</single>\n      <multiple>numbers</multiple>\n    </term>\n    <term name=\"line\">\n      <single>line</single>\n      <multiple>lines</multiple>\n    </term>\n    <term name=\"note\">\n      <single>note</single>\n      <multiple>notes</multiple>\n    </term>\n    <term name=\"opus\">\n      <single>opus</single>\n      <multiple>opera</multiple>\n    </term>\n    <term name=\"page\">\n      <single>page</single>\n      <multiple>pages</multiple>\n    </term>\n    <term name=\"paragraph\">\n      <single>paragraph</single>\n      <multiple>paragraph</multiple>\n    </term>\n    <term name=\"part\">\n      <single>part</single>\n      <multiple>parts</multiple>\n    </term>\n    <term name=\"section\">\n      <single>section</single>\n      <multiple>sections</multiple>\n    </term>\n    <term name=\"sub verbo\">\n      <single>sub verbo</single>\n      <multiple>sub verbis</multiple>\n    </term>\n    <term name=\"verse\">\n      <single>verse</single>\n      <multiple>verses</multiple>\n    </term>\n    <term name=\"volume\">\n      <single>volume</single>\n      <multiple>volumes</multiple>\n    </term>\n\n    <!-- SHORT LOCATOR FORMS -->\n    <term name=\"book\" form=\"short\">bk.</term>\n    <term name=\"chapter\" form=\"short\">chap.</term>\n    <term name=\"column\" form=\"short\">col.</term>\n    <term name=\"figure\" form=\"short\">fig.</term>\n    <term name=\"folio\" form=\"short\">f.</term>\n    <term name=\"issue\" form=\"short\">no.</term>\n    <term name=\"line\" form=\"short\">l.</term>\n    <term name=\"note\" form=\"short\">n.</term>\n    <term name=\"opus\" form=\"short\">op.</term>\n    <term name=\"page\" form=\"short\">\n      <single>p.</single>\n      <multiple>pp.</multiple>\n    </term>\n    <term name=\"paragraph\" form=\"short\">para.</term>\n    <term name=\"part\" form=\"short\">pt.</term>\n    <term name=\"section\" form=\"short\">sec.</term>\n    <term name=\"sub verbo\" form=\"short\">\n      <single>s.v.</single>\n      <multiple>s.vv.</multiple>\n    </term>\n    <term name=\"verse\" form=\"short\">\n      <single>v.</single>\n      <multiple>vv.</multiple>\n    </term>\n    <term name=\"volume\" form=\"short\">\n      <single>vol.</single>\n      <multiple>vols.</multiple>\n    </term>\n\n    <!-- SYMBOL LOCATOR FORMS -->\n    <term name=\"paragraph\" form=\"symbol\">\n      <single>¶</single>\n      <multiple>¶¶</multiple>\n    </term>\n    <term name=\"section\" form=\"symbol\">\n      <single>§</single>\n      <multiple>§§</multiple>\n    </term>\n\n    <!-- LONG ROLE FORMS -->\n    <term name=\"director\">\n      <single>director</single>\n      <multiple>directors</multiple>\n    </term>\n    <term name=\"editor\">\n      <single>editor</single>\n      <multiple>editors</multiple>\n    </term>\n    <term name=\"editorial-director\">\n      <single>editor</single>\n      <multiple>editors</multiple>\n    </term>\n    <term name=\"illustrator\">\n      <single>illustrator</single>\n      <multiple>illustrators</multiple>\n    </term>\n    <term name=\"translator\">\n      <single>translator</single>\n      <multiple>translators</multiple>\n    </term>\n    <term name=\"editortranslator\">\n      <single>editor &amp; translator</single>\n      <multiple>editors &amp; translators</multiple>\n    </term>\n\n    <!-- SHORT ROLE FORMS -->\n    <term name=\"director\" form=\"short\">\n      <single>dir.</single>\n      <multiple>dirs.</multiple>\n    </term>\n    <term name=\"editor\" form=\"short\">\n      <single>ed.</single>\n      <multiple>eds.</multiple>\n    </term>\n    <term name=\"editorial-director\" form=\"short\">\n      <single>ed.</single>\n      <multiple>eds.</multiple>\n    </term>\n    <term name=\"illustrator\" form=\"short\">\n      <single>ill.</single>\n      <multiple>ills.</multiple>\n    </term>\n    <term name=\"translator\" form=\"short\">\n      <single>tran.</single>\n      <multiple>trans.</multiple>\n    </term>\n    <term name=\"editortranslator\" form=\"short\">\n      <single>ed. &amp; tran.</single>\n      <multiple>eds. &amp; trans.</multiple>\n    </term>\n\n    <!-- VERB ROLE FORMS -->\n    <term name=\"director\" form=\"verb\">directed by</term>\n    <term name=\"editor\" form=\"verb\">edited by</term>\n    <term name=\"editorial-director\" form=\"verb\">edited by</term>\n    <term name=\"illustrator\" form=\"verb\">illustrated by</term>\n    <term name=\"interviewer\" form=\"verb\">interview by</term>\n    <term name=\"recipient\" form=\"verb\">to</term>\n    <term name=\"reviewed-author\" form=\"verb\">by</term>\n    <term name=\"translator\" form=\"verb\">translated by</term>\n    <term name=\"editortranslator\" form=\"verb\">edited &amp; translated by</term>\n\n    <!-- SHORT VERB ROLE FORMS -->\n    <term name=\"container-author\" form=\"verb-short\">by</term>\n    <term name=\"director\" form=\"verb-short\">dir.</term>\n    <term name=\"editor\" form=\"verb-short\">ed.</term>\n    <term name=\"editorial-director\" form=\"verb-short\">ed.</term>\n    <term name=\"illustrator\" form=\"verb-short\">illus.</term>\n    <term name=\"translator\" form=\"verb-short\">trans.</term>\n    <term name=\"editortranslator\" form=\"verb-short\">ed. &amp; trans.</term>\n\n    <!-- LONG MONTH FORMS -->\n    <term name=\"month-01\">January</term>\n    <term name=\"month-02\">February</term>\n    <term name=\"month-03\">March</term>\n    <term name=\"month-04\">April</term>\n    <term name=\"month-05\">May</term>\n    <term name=\"month-06\">June</term>\n    <term name=\"month-07\">July</term>\n    <term name=\"month-08\">August</term>\n    <term name=\"month-09\">September</term>\n    <term name=\"month-10\">October</term>\n    <term name=\"month-11\">November</term>\n    <term name=\"month-12\">December</term>\n\n    <!-- SHORT MONTH FORMS -->\n    <term name=\"month-01\" form=\"short\">Jan.</term>\n    <term name=\"month-02\" form=\"short\">Feb.</term>\n    <term name=\"month-03\" form=\"short\">Mar.</term>\n    <term name=\"month-04\" form=\"short\">Apr.</term>\n    <term name=\"month-05\" form=\"short\">May</term>\n    <term name=\"month-06\" form=\"short\">Jun.</term>\n    <term name=\"month-07\" form=\"short\">Jul.</term>\n    <term name=\"month-08\" form=\"short\">Aug.</term>\n    <term name=\"month-09\" form=\"short\">Sep.</term>\n    <term name=\"month-10\" form=\"short\">Oct.</term>\n    <term name=\"month-11\" form=\"short\">Nov.</term>\n    <term name=\"month-12\" form=\"short\">Dec.</term>\n\n    <!-- SEASONS -->\n    <term name=\"season-01\">Spring</term>\n    <term name=\"season-02\">Summer</term>\n    <term name=\"season-03\">Autumn</term>\n    <term name=\"season-04\">Winter</term>\n  </terms>\n</locale>\n",
    "contentType": "Application/x-bibtex",
    "items": [
        {
            "10.1145/3299869.3319898": {
                "id": "10.1145/3299869.3319898",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Ge",
                        "given": "Chang"
                    },
                    {
                        "family": "Li",
                        "given": "Yinan"
                    },
                    {
                        "family": "Eilebrecht",
                        "given": "Eric"
                    },
                    {
                        "family": "Chandramouli",
                        "given": "Badrish"
                    },
                    {
                        "family": "Kossmann",
                        "given": "Donald"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            6,
                            25
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            6,
                            25
                        ]
                    ]
                },
                "abstract": "There has been a recent flurry of interest in providing query capability on raw data in today's big data systems. These raw data must be parsed before processing or use in analytics. Thus, a fundamental challenge in distributed big data systems is that of efficient parallel parsing of raw data. The difficulties come from the inherent ambiguity while independently parsing chunks of raw data without knowing the context of these chunks. Specifically, it can be difficult to find the beginnings and ends of fields and records in these chunks of raw data. To parallelize parsing, this paper proposes a speculation-based approach for the CSV format, arguably the most commonly used raw data format. Due to the syntactic and statistical properties of the format, speculative parsing rarely fails and therefore parsing is efficiently parallelized in a distributed setting. Our speculative approach is also robust, meaning that it can reliably detect syntax errors in CSV data. We experimentally evaluate the speculative, distributed parsing approach in Apache Spark using more than 11,000 real-world datasets, and show that our parser produces significant performance benefits over existing methods.",
                "call-number": "10.1145/3299869.3319898",
                "collection-title": "SIGMOD '19",
                "container-title": "Proceedings of the 2019 International Conference on Management of Data",
                "DOI": "10.1145/3299869.3319898",
                "event-place": "Amsterdam, Netherlands",
                "ISBN": "9781450356435",
                "keyword": "distributed, csv, parsing, parallel",
                "number-of-pages": "17",
                "page": "883–899",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Speculative Distributed CSV Data Parsing for Big Data Analytics",
                "URL": "https://doi.org/10.1145/3299869.3319898"
            }
        },
        {
            "10.1145/3473141.3473248": {
                "id": "10.1145/3473141.3473248",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Cong",
                        "given": "Xiaoqi"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            6,
                            4
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2021,
                            6,
                            4
                        ]
                    ]
                },
                "abstract": "The wide application of big data and the rapid development of e-commerce are constantly changing the business model and financial management model of enterprises.In the era of big data, the electronic invoices, online orders, purchases, wages, cash flow, electronic taxation and other operating data and financial data of electronic enterprises and business enterprises will be more transparent, and enterprise financial management is facing intelligent opportunities and challenges. The application of artificial intelligence technology in e-commerce companies in the era of big data puts forward higher requirements on the comprehensive capabilities of corporate financial personnel and financial management processes. This article conducts an in-depth analysis of the financial management risks of e-commerce enterprises in the current big data environment, and further studies the application of big data in the financial risk management of e-commerce enterprises in the era of big data, risk factors, the quality of financial personnel, business process reengineering and other key issues, Expounds the relationship between big data and financial risk management, analyzes and summarizes the financial risk management strategies adopted by e-commerce companies, builds the overall structure of the big data analysis platform for e-commerce companies, and effectively improves e-commerce companies’ response to financial risks Ability and enhance the company's market competitiveness.",
                "call-number": "10.1145/3473141.3473248",
                "collection-title": "ICFET '21",
                "container-title": "Proceedings of the 7th International Conference on Frontiers of Educational Technologies",
                "DOI": "10.1145/3473141.3473248",
                "event-place": "Bangkok, Thailand",
                "ISBN": "9781450389723",
                "keyword": "Big Data, Financial risk management, E-commerce, Competitive ability",
                "number-of-pages": "5",
                "page": "195–199",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Research on Financial Risk Management of E-commerce Enterprises in the Era of Big Data",
                "URL": "https://doi.org/10.1145/3473141.3473248"
            }
        },
        {
            "10.1145/2910674.2910685": {
                "id": "10.1145/2910674.2910685",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Sideris",
                        "given": "Costas"
                    },
                    {
                        "family": "Shaikh",
                        "given": "Sakib"
                    },
                    {
                        "family": "Kalantarian",
                        "given": "Haik"
                    },
                    {
                        "family": "Sarrafzadeh",
                        "given": "Majid"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            6,
                            29
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2016,
                            6,
                            29
                        ]
                    ]
                },
                "abstract": "In this paper, we present a big data plarform for knowledge categorization in Electronic Health Records and examine its application to automatic assignment of ICD-9 codes. Our platform relies on reusable, adaptable components that can perform knowledge extraction at a large scale. For the ICD-9 automatic assignment, we build and validate our approach using data from the MIMIC II Clinical Database that contains over 20,000 discharge summaries. We show that our platform can achieve state of the art performance in this dataset and that the classification results improve with more data. Overall, in the first level of the ICD-9 hierarchy our algorithm achieves an average precision of 79.7% for an average recall of 70.2%.",
                "call-number": "10.1145/2910674.2910685",
                "collection-number": "77",
                "collection-title": "PETRA '16",
                "container-title": "Proceedings of the 9th ACM International Conference on PErvasive Technologies Related to Assistive Environments",
                "DOI": "10.1145/2910674.2910685",
                "event-place": "Corfu, Island, Greece",
                "ISBN": "9781450343374",
                "keyword": "big data, ICD, knowledge extraction",
                "number": "Article 77",
                "number-of-pages": "2",
                "page": "1–2",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "A Big-Data platform for Medical Knowledge Extraction from Electronic Health Records: Automatic Assignment of ICD-9 Codes",
                "URL": "https://doi.org/10.1145/2910674.2910685"
            }
        },
        {
            "10.1145/2912152.2912159": {
                "id": "10.1145/2912152.2912159",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Matsuoka",
                        "given": "Satoshi"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            6,
                            1
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2016,
                            6,
                            1
                        ]
                    ]
                },
                "abstract": "Rapid growth in the use cases and demands for extreme computing and huge data processing is leading to convergence of the two infrastructures. Tokyo Tech.'s TSUBAME3.0, a 2017 addition to the highly successful TSUBAME2.5, will aim to deploy a series of innovative technologies, including ultra-efficient liquid cooling and power control, petabytes of non-volatile memory, as well as low cost Petabit-class interconnect. To address the challenges of such technology adoption, proper system architecture, software stack, and algorithm must be desgined and developed; these are being addressed by several of our ongoing research projects as well as prototypes, such as the TSUBAME-KFC/DL prototype which became #1 in the world in power efficiency on the Green500 twice in a row, the Billion-way Resiliency project that is investigating effective methods for future resilient supercomputers, as well as the Extreme Big Data (EBD) project which is looking at co-design development of convergent system stack given future extreme data and computing workloads. We are already successful in developing various algorithms and sottware substrates to manipulate big data elements directly on extreme supercomputers, such as graphs, tables (sort), trees, files, etc. and in fact became #1 in the world on the Graph 500 twice including the latest Nov. 2015 version. Our recent focus is also how to ssupport new workloads in categorizing big data represented by deep learning, and there we are collaborating with several partners such as DENSO to improve the scalability and predictability of such workloads; recent trial allowed scalablity to utilize 1146 GPUs for the entire week for a CNN workload. For TSUBAME3 and 2.5 combined we espect to increase such capabilities to over 80 Petaflops in early 2017, or 7 times faster than the K computer.",
                "call-number": "10.1145/2912152.2912159",
                "collection-title": "DIDC '16",
                "container-title": "Proceedings of the ACM International Workshop on Data-Intensive Distributed Computing",
                "DOI": "10.1145/2912152.2912159",
                "event-place": "Kyoto, Japan",
                "ISBN": "9781450343527",
                "keyword": "scalability, algorithms, rapid growth",
                "number-of-pages": "1",
                "page": "1",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Towards Convergence of Extreme Computing and Big Data Centers",
                "URL": "https://doi.org/10.1145/2912152.2912159"
            }
        },
        {
            "10.5555/2555523.2555560": {
                "id": "10.5555/2555523.2555560",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Whitmer",
                        "given": "Barbara"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2013,
                            11,
                            18
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2013,
                            11,
                            18
                        ]
                    ]
                },
                "abstract": "The Centre for Innovation in Information Visualization and Data-Driven Design (CIVDDD) is a Big Data research project collaboration funded by the Ontario Research Fund -- Research Excellence (ORF-RE). Research collaborators in the project include York University, OCAD University, the University of Toronto, and private sector partners (PSPs) to develop the next generation of data discovery, design, analytics, and visualization techniques for new computational tools, representational strategies, and interfaces. As the preeminent research hub for information analytics and scientific visualization in Ontario, CIVDDD has fifteen research teams in the four theme areas of Bioinformatics and Medical Applications, Interactive Visualization, Textual Visualization, and Scientific Visualization. The Workshop included a brief overview of CIVDDD research by the Principal Investigator Dr. Amir Asif, followed by three CIVDDD team presentations and demonstrations related to CASCON 2013 themes. These included: Graph Analytics and Biological Network Structures (Big Data and Cloud Computing), Social Media Data Visualization (Social Computing), and Dynamic Carbon Mapping in Urban Environments (Mobile Computing). Each Workshop presentation contained academic researchers and their private sector partner research collaborators. Each presentation was followed by a demonstration of the research application or visualization, and Q&A. An open discussion concluded the Workshop.",
                "call-number": "10.5555/2555523.2555560",
                "collection-title": "CASCON '13",
                "container-title": "Proceedings of the 2013 Conference of the Center for Advanced Studies on Collaborative Research",
                "event-place": "Ontario, Canada",
                "number-of-pages": "3",
                "page": "344–346",
                "publisher": "IBM Corp.",
                "publisher-place": "USA",
                "title": "CIVDDD collaborative research in big data analytics and visualization"
            }
        },
        {
            "10.1145/3535735.3535737": {
                "id": "10.1145/3535735.3535737",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Tian Le",
                        "given": "Zhang"
                    },
                    {
                        "family": "Weixin",
                        "given": "Ren"
                    },
                    {
                        "family": "Yue",
                        "given": "Zhang"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2022,
                            4,
                            14
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2022,
                            4,
                            14
                        ]
                    ]
                },
                "abstract": "With the advent of the information age, the rapid development of new-generation information technologies such as cloud computing and cloud storage has also led to changes in the education field, especially the massive open online courses (MOOC). At the same time, the society ‘s demand for talents with computer technology is increasing. The undergraduate education in the field of computer science has gradually become the focus of undergraduate education in the information age. However, even if MOOC is used as a supplement to offline education, the learning effect varies from person to person due to the differences in students ‘ learning methods and abilities. An improved MOOC model based on sequential recommendation algorithm and big data proposed in this study can provide an optimization idea for such problems. In the model testing session, this study randomly selected some undergraduates in 2020 grade majoring in computer science at the University of Electronic Science and Technology of China for comparative experiments, proving that the MOOC improvement program based on sequential recommendation algorithms and big data can effectively improve students ‘ academic performance and contribute to the promotion of educational equity.",
                "call-number": "10.1145/3535735.3535737",
                "collection-title": "ICIEI '22",
                "container-title": "Proceedings of the 7th International Conference on Information and Education Innovations",
                "DOI": "10.1145/3535735.3535737",
                "event-place": "Belgrade, Serbia",
                "ISBN": "9781450396196",
                "keyword": "MOOC, sequential recommendation algorithm, big data, undergraduate education",
                "number-of-pages": "5",
                "page": "13–17",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Research on Improvement Scheme of MOOC Based on Sequential Recommendation Algorithm and Big Data",
                "URL": "https://doi.org/10.1145/3535735.3535737"
            }
        },
        {
            "10.1145/269012.269023": {
                "id": "10.1145/269012.269023",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Orr",
                        "given": "Ken"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            1998,
                            2,
                            1
                        ]
                    ]
                },
                "call-number": "10.1145/269012.269023",
                "container-title": "Commun. ACM",
                "DOI": "10.1145/269012.269023",
                "ISSN": "0001-0782",
                "issue": "2",
                "number-of-pages": "6",
                "page": "66–71",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "Feb. 1998",
                "title": "Data quality and systems theory",
                "URL": "https://doi.org/10.1145/269012.269023",
                "volume": "41"
            }
        },
        {
            "10.1145/3434778": {
                "id": "10.1145/3434778",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Khalemsky",
                        "given": "A."
                    },
                    {
                        "family": "Gelbard",
                        "given": "R."
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            6,
                            2
                        ]
                    ]
                },
                "abstract": "In dynamic and big data environments the visualization of a segmentation process over time often does not enable the user to simultaneously track entire pieces. The key points are sometimes incomparable, and the user is limited to a static visual presentation of a certain point. The proposed visualization concept, called ExpanDrogram, is designed to support dynamic classifiers that run in a big data environment subject to changes in data characteristics. It offers a wide range of features that seek to maximize the customization of a segmentation problem. The main goal of the ExpanDrogram visualization is to improve comprehensiveness by combining both the individual and segment levels, illustrating the dynamics of the segmentation process over time, providing “version control” that enables the user to observe the history of changes, and more. The method is illustrated using different datasets, with which we demonstrate multiple segmentation parameters, as well as multiple display layers, to highlight points such as new trend detection, outlier detection, tracking changes in original segments, and zoom in/out for more/less detail. The datasets vary in size from a small one to one of more than 12 million records.",
                "call-number": "10.1145/3434778",
                "collection-number": "11",
                "container-title": "J. Data and Information Quality",
                "DOI": "10.1145/3434778",
                "ISSN": "1936-1955",
                "issue": "2",
                "keyword": "version control, taxonomy, dendrogram, cluster analysis, Dynamic segmentation, temporal data visualization",
                "number": "Article 11",
                "number-of-pages": "27",
                "page": "1–27",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "June 2021",
                "title": "ExpanDrogram: Dynamic Visualization of Big Data Segmentation over Time",
                "URL": "https://doi.org/10.1145/3434778",
                "volume": "13"
            }
        },
        {
            "10.1145/3454127.3456615": {
                "id": "10.1145/3454127.3456615",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Al Marouni",
                        "given": "Yasmina"
                    },
                    {
                        "family": "Bentaleb",
                        "given": "Youssef"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            4,
                            1
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2021,
                            4,
                            1
                        ]
                    ]
                },
                "abstract": "Partial Least Squares Regression (PLSR) is a data analysis method in high-dimensional settings, it is used as a dimension reduction method and also as a tool of linear regression. However, it has some problems in a big data context when the data is too large and has been designed to handle only quantitative variables.In this paper, we will present PLSR, then discuss adaptations and extensions of PLS regression to overcome these problems so that it can be more use-full in a big data context.",
                "call-number": "10.1145/3454127.3456615",
                "collection-number": "34",
                "collection-title": "NISS2021",
                "container-title": "Proceedings of the 4th International Conference on Networking, Information Systems & Security",
                "DOI": "10.1145/3454127.3456615",
                "event-place": "KENITRA, AA, Morocco",
                "ISBN": "9781450388719",
                "keyword": "big data, data types, PLS regression",
                "number": "Article 34",
                "number-of-pages": "5",
                "page": "1–5",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "State of art of PLS Regression for non quantitative data and in Big Data context",
                "URL": "https://doi.org/10.1145/3454127.3456615"
            }
        },
        {
            "10.1145/3442555.3442562": {
                "id": "10.1145/3442555.3442562",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Li",
                        "given": "Zheng"
                    },
                    {
                        "family": "Lin",
                        "given": "Chen"
                    },
                    {
                        "family": "Wang",
                        "given": "Chien-Hua"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2020,
                            11,
                            27
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2020,
                            11,
                            27
                        ]
                    ]
                },
                "abstract": "Leisure and tourism activities have become an indispensable way for people to relieve pressure nowadays, and the tourism industry has also seen the rise owing to the stable development of economy. The most obvious is the vigorous development of the homestay market. The tourists as demand for information generated, homestay owners recognize the individual a homestay owner can master the tourists as information is limited. It's easier on the human decision-making confusion. Therefore, through the application of big data analysis, it can assist the owner to make the best analysis and decision-making. This paper takes the homestay industry of Kinmen as an example to discuss how owners can collect and construct build their own big data for business and customer satisfaction, and then regularly statistical analysis, customer group analysis, internal problem improvement, marketing strategies and future development directions, etc. Next, taking association rules as example and the WEKA was used to analyze customer satisfaction. It is expected to be conducive to the improvement of the operation of the homestay industry, and to meet the needs of accommodation tourists and enhance the willingness of tourists to stay again.",
                "call-number": "10.1145/3442555.3442562",
                "collection-title": "ICCIP 2020",
                "container-title": "2020 the 6th International Conference on Communication and Information Processing",
                "DOI": "10.1145/3442555.3442562",
                "event-place": "Tokyo, Japan",
                "ISBN": "9781450388092",
                "keyword": "WEKA, Kinmen, customer satisfaction, Homestay, big data",
                "number-of-pages": "5",
                "page": "38–42",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Research on the Application of Big Data Analysis in the Homestay Industry in Kinmen",
                "URL": "https://doi.org/10.1145/3442555.3442562"
            }
        },
        {
            "10.1145/3358331.3358341": {
                "id": "10.1145/3358331.3358341",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Zhang",
                        "given": "Lihong"
                    },
                    {
                        "family": "Wang",
                        "given": "Juan"
                    },
                    {
                        "family": "He",
                        "given": "Wei"
                    },
                    {
                        "family": "Zhang",
                        "given": "Peng"
                    },
                    {
                        "family": "Zhang",
                        "given": "Shuangshi"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            10,
                            17
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            10,
                            17
                        ]
                    ]
                },
                "abstract": "Based on the 200 rumor hot lists in recent three years, which were refuted by WeChat platform rumor filter, this paper conducted qualitative and quantitative analysis on the word frequency distribution, theme characteristics and hot categories of rumors, and proposed the rumor governance strategy based on big data thinking. The research results help us to grasp the mechanism and rules of rumor generation and propagation under the background of new media, and the proposed rumor governance strategies can also provide guidance for the online rumor regulatory authorities.",
                "call-number": "10.1145/3358331.3358341",
                "collection-number": "10",
                "collection-title": "AIAM 2019",
                "container-title": "Proceedings of the 2019 International Conference on Artificial Intelligence and Advanced Manufacturing",
                "DOI": "10.1145/3358331.3358341",
                "event-place": "Dublin, Ireland",
                "ISBN": "9781450372022",
                "keyword": "rumor, governance, TF-IDF, WeChat, word frequency",
                "number": "Article 10",
                "number-of-pages": "4",
                "page": "1–4",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "WeChat Rumor Analysis and Governance Based on Big Data",
                "URL": "https://doi.org/10.1145/3358331.3358341"
            }
        },
        {
            "10.1145/3286606.3286782": {
                "id": "10.1145/3286606.3286782",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Bouhafer",
                        "given": "Fadwa"
                    },
                    {
                        "family": "Heyouni",
                        "given": "Mohammed"
                    },
                    {
                        "family": "El Haddadi",
                        "given": "Anass"
                    },
                    {
                        "family": "Boulouard",
                        "given": "Zakaria"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            10,
                            10
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            10,
                            10
                        ]
                    ]
                },
                "abstract": "The development of dyamic information analysis, like incremental clustering, is becoming a very important concern in big data. In this paper, we will propose a new incremental clustering algorithm, called \"ACO-FFDP-Incremental-Cluster\". This algorithm is a combination between \"FFDP\" a large graph visualization algorithm developed by our team, and \"ACO Algorithm\". FFDP will set an equilibrium positioning of the large graph; then it will provide the nodes final positions as a vector of coordinates. ACO algorithm will take this vector into consideration and try to find the best clustering configuration possible for new data.",
                "call-number": "10.1145/3286606.3286782",
                "collection-number": "5",
                "collection-title": "SCA '18",
                "container-title": "Proceedings of the 3rd International Conference on Smart City Applications",
                "DOI": "10.1145/3286606.3286782",
                "event-place": "Tetouan, Morocco",
                "ISBN": "9781450365628",
                "keyword": "Incremental clustering, Ant Colony Optimization, Swarm Intelligence",
                "number": "Article 5",
                "number-of-pages": "7",
                "page": "1–7",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "ACO-FFDP in incremental clustering for big data analysis",
                "URL": "https://doi.org/10.1145/3286606.3286782"
            }
        },
        {
            "10.14778/2733004.2733056": {
                "id": "10.14778/2733004.2733056",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Zhang",
                        "given": "Rui"
                    },
                    {
                        "family": "Jain",
                        "given": "Reshu"
                    },
                    {
                        "family": "Sarkar",
                        "given": "Prasenjit"
                    },
                    {
                        "family": "Rupprecht",
                        "given": "Lukas"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2014,
                            8,
                            1
                        ]
                    ]
                },
                "abstract": "As we come to terms with various big data challenges, one vital issue remains largely untouched. That is the optimal multiplexing and prioritization of different big data applications sharing the same underlying infrastructure, for example, a public cloud platform. Given these demanding applications and the necessary practice to avoid over-provisioning, resource contention between applications is inevitable. Priority must be given to important applications (or sub workloads in an application) in these circumstances.This demo highlights the compelling impact prioritization could make, using an example application that recommends promising combinations of stocks to purchase based on relevant Twitter sentiment. The application consists of a batch job and an interactive query, ran simultaneously. Our underlying solution provides a unique capability to identify and differentiate application workloads throughout a complex big data platform. Its current implementation is based on Apache Hadoop and the IBM GPFS distributed storage system. The demo showcases the superior interactive query performance achievable by prioritizing its workloads and thereby avoiding I/O bandwidth contention. The query time is 3.6 × better compared to no prioritization. Such a performance is within 0.3% of that of an idealistic system where the query runs without contention. The demo is conducted on around 3 months of Twitter data, pertinent to the S & P 100 index, with about 4 × 1012 potential stock combinations considered.",
                "call-number": "10.14778/2733004.2733056",
                "container-title": "Proc. VLDB Endow.",
                "DOI": "10.14778/2733004.2733056",
                "ISSN": "2150-8097",
                "issue": "13",
                "number-of-pages": "4",
                "page": "1665–1668",
                "publisher": "VLDB Endowment",
                "source": "August 2014",
                "title": "Getting your big data priorities straight: a demonstration of priority-based QoS using social-network-driven stock recommendation",
                "URL": "https://doi.org/10.14778/2733004.2733056",
                "volume": "7"
            }
        },
        {
            "10.1145/3284179.3284206": {
                "id": "10.1145/3284179.3284206",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Martínez-Abad",
                        "given": "Fernando"
                    },
                    {
                        "family": "Gamazo",
                        "given": "Adriana"
                    },
                    {
                        "family": "Rodríguez-Conde",
                        "given": "María José"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            10,
                            24
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            10,
                            24
                        ]
                    ]
                },
                "abstract": "The search for non-contextual process factors associated with school effectiveness has become a line of research with very broad dissemination, primarily from the ubiquity of research that includes analysis of secondary data from large-scale evaluations. With the aim of detecting ICT factors related to school effectiveness from the Spanish sample of the PISA 2015 evaluation, this work applies statistical techniques of data mining, specifically decision trees, for the optimization of the process. The results demonstrate that, while the availability and excessive use of ICTs, both in the educational environment and in the home, are factors associated with low effectiveness, other more personal characteristics of the pupils, such as their perception of self-efficacy in the management of ICT, or their own interest in the use of technologies, can be considered as factors in the protection of school effectiveness. We conclude by discussing the relationship between school effectiveness and academic performance through the analysis of previous studies, focusing on the common elements detected within both perspectives.",
                "call-number": "10.1145/3284179.3284206",
                "collection-title": "TEEM'18",
                "container-title": "Proceedings of the Sixth International Conference on Technological Ecosystems for Enhancing Multiculturality",
                "DOI": "10.1145/3284179.3284206",
                "event-place": "Salamanca, Spain",
                "ISBN": "9781450365185",
                "keyword": "Information and Communication Technologies, School Effectiveness, Data Mining, Secondary Education, Educational Evaluation",
                "number-of-pages": "6",
                "page": "145–150",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Big Data in Education: Detection of ICT Factors Associated with School Effectiveness with Data Mining Techniques",
                "URL": "https://doi.org/10.1145/3284179.3284206"
            }
        },
        {
            "10.14778/2824032.2824141": {
                "id": "10.14778/2824032.2824141",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Walter",
                        "given": "Todd"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2015,
                            8,
                            1
                        ]
                    ]
                },
                "abstract": "In ancient texts, 40 was a magic number. It meant a \"lot\" or \"a long time.\" 40 years represented the time it took for a new generation to arise. A look back at 40 years of VLDB suggests that this applies to database researchers as well -- the young researchers of the early VLDBs are now the old folks of the database world, and a new generation is creating afresh. Over this period many plateaus of \"Big Data\" have challenged the database community and been conquered. But there is still no free lunch -- database research is really the science of trade-offs, many of which are no different today than 40 years ago. And of course the evolution of hardware technology continues to swing the trade-off pendulum while enabling new plateaus to be reached. Todd will take a look back at customer big data plateaus of the past. He will look at where we are today, then use his crystal ball and the lessons of the past to extrapolate the next several plateaus -- how they will be the same and how will they be different. Along the way we will have a little fun with some VLDB and Teradata history.",
                "call-number": "10.14778/2824032.2824141",
                "container-title": "Proc. VLDB Endow.",
                "DOI": "10.14778/2824032.2824141",
                "ISSN": "2150-8097",
                "issue": "12",
                "number-of-pages": "1",
                "page": "2057",
                "publisher": "VLDB Endowment",
                "source": "August 2015",
                "title": "Big plateaus of big data on the big island",
                "URL": "https://doi.org/10.14778/2824032.2824141",
                "volume": "8"
            }
        },
        {
            "10.1145/3144592.3144597": {
                "id": "10.1145/3144592.3144597",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "de Laat",
                        "given": "Paul B."
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            9,
                            25
                        ]
                    ]
                },
                "abstract": "Decision-making assisted by algorithms developed by machine learning is increasingly determining our lives. Unfortunately, full opacity about the process is the norm. Can transparency contribute to restoring accountability for such systems? Several objections are examined: the loss of privacy when data sets become public, the perverse effects of disclosure of the very algorithms themselves ('gaming the system' in particular), the potential loss of competitive edge, and the limited gains in answerability to be expected since sophisticated algorithms are inherently opaque. It is concluded that transparency is certainly useful, but only up to a point: extending it to the public at large is normally not to be advised. Moreover, in order to make algorithmic decisions understandable, models of machine learning to be used should either be interpreted ex post or be interpretable by design ex ante.",
                "call-number": "10.1145/3144592.3144597",
                "container-title": "SIGCAS Comput. Soc.",
                "DOI": "10.1145/3144592.3144597",
                "ISSN": "0095-2737",
                "issue": "3",
                "keyword": "accountability, machine learning, opacity, algorithm, transparency, interpretability",
                "number-of-pages": "15",
                "page": "39–53",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "September 2017",
                "title": "Big data and algorithmic decision-making: can transparency restore accountability?",
                "URL": "https://doi.org/10.1145/3144592.3144597",
                "volume": "47"
            }
        },
        {
            "10.1145/2428616.2431055": {
                "id": "10.1145/2428616.2431055",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Kumar",
                        "given": "Arun"
                    },
                    {
                        "family": "Niu",
                        "given": "Feng"
                    },
                    {
                        "family": "Ré",
                        "given": "Christopher"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2013,
                            1,
                            22
                        ]
                    ]
                },
                "abstract": "The rise of big data presents both big opportunities and big challenges in domains ranging from enterprises to sciences. The opportunities include better-informed business decisions, more efficient supply-chain management and resource allocation, more effective targeting of products and advertisements, better ways to \"organize the world’s information,\" faster turnaround of scientific discoveries, etc.",
                "call-number": "10.1145/2428616.2431055",
                "container-title": "Queue",
                "DOI": "10.1145/2428616.2431055",
                "ISSN": "1542-7730",
                "issue": "1",
                "number-of-pages": "17",
                "page": "30–46",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "January 2013",
                "title": "Hazy: Making it Easier to Build and Maintain Big-data Analytics: Racing to unleash the full potential of big data with the latest statistical and machine-learning techniques.",
                "URL": "https://doi.org/10.1145/2428616.2431055",
                "volume": "11"
            }
        },
        {
            "10.1145/3254147": {
                "id": "10.1145/3254147",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Antoniu",
                        "given": "Gabriel"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            6,
                            1
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2016,
                            6,
                            1
                        ]
                    ]
                },
                "call-number": "10.1145/3254147",
                "collection-title": "ScienceCloud '16",
                "container-title": "Proceedings of the ACM 7th Workshop on Scientific Cloud Computing",
                "DOI": "10.1145/3254147",
                "event-place": "Kyoto, Japan",
                "ISBN": "9781450343534",
                "page": "",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Session details: HPC and Big Data Convergence",
                "URL": "https://doi.org/10.1145/3254147"
            }
        },
        {
            "10.1145/3149572.3149603": {
                "id": "10.1145/3149572.3149603",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Xiang",
                        "given": "Gao"
                    },
                    {
                        "family": "Fang",
                        "given": "Wang"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            10,
                            9
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            10,
                            9
                        ]
                    ]
                },
                "abstract": "With the development of information technology, ChangQing drilling engineering company has accumulated a large number of data about drilling, and the research foundation of drilling big data technology had been formed. But now, huge volumes of data are distributed in different heterogeneous data sources due to the long-term decentralized construction, which is hard to realize the comprehensive analysis of related data. In this paper, aiming at the practical problems, a data integration and business intelligent- project based on drilling big data has been put forward. Referencing to the knowledge of this field, the system applies kettle which is a data integration tool to realize the integration of ETL heterogeneous data resource, establishes the data warehouse based on theme, and uses fine report which is a business intelligence tools to organize the views of drilling big data according to different user's requires, shows flexibly in multi-perspective view, thus provides powerful data support for user's drilling decision.",
                "call-number": "10.1145/3149572.3149603",
                "collection-title": "ICIME 2017",
                "container-title": "Proceedings of the 9th International Conference on Information Management and Engineering",
                "DOI": "10.1145/3149572.3149603",
                "event-place": "Barcelona, Spain",
                "ISBN": "9781450353373",
                "keyword": "Data warehouse, Business Intelligence, Data Integration, Multi-source heterogeneous big data",
                "number-of-pages": "5",
                "page": "64–68",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "The research of Data Integration and Business Intelligent based on drilling big data",
                "URL": "https://doi.org/10.1145/3149572.3149603"
            }
        },
        {
            "10.1145/3194554.3194633": {
                "id": "10.1145/3194554.3194633",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Chang",
                        "given": "Mu-Tien"
                    },
                    {
                        "family": "Choi",
                        "given": "I. Stephen"
                    },
                    {
                        "family": "Niu",
                        "given": "Dimin"
                    },
                    {
                        "family": "Zheng",
                        "given": "Hongzhong"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            5,
                            30
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            5,
                            30
                        ]
                    ]
                },
                "abstract": "This paper presents a performance analysis framework for studying emerging memories. The key component of the framework is a memory-latency programmable emulator, which is based on a FPGA-attached server system. The emulator allows users extend read and/or write latency. In addition, we use regression models to enable system performance studies for memory latencies beyond hardware limitations. Finally, we demonstrate Spark application case studies, analyzing the impact of two key characteristics of emerging memories: extended memory access times and enlarged memory capacities. Results show that the benefit of high capacity memory could outweigh the performance loss due to longer memory latency.",
                "call-number": "10.1145/3194554.3194633",
                "collection-title": "GLSVLSI '18",
                "container-title": "Proceedings of the 2018 on Great Lakes Symposium on VLSI",
                "DOI": "10.1145/3194554.3194633",
                "event-place": "Chicago, IL, USA",
                "ISBN": "9781450357241",
                "keyword": "big data, emerging memories, performance analysis",
                "number-of-pages": "4",
                "page": "439–442",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Performance Impact of Emerging Memory Technologies on Big Data Applications: A Latency-Programmable System Emulation Approach",
                "URL": "https://doi.org/10.1145/3194554.3194633"
            }
        },
        {
            "10.1145/2591062.2591088": {
                "id": "10.1145/2591062.2591088",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Camilli",
                        "given": "Matteo"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2014,
                            5,
                            31
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2014,
                            5,
                            31
                        ]
                    ]
                },
                "abstract": "Formal verification requires high performance data processing software for extracting knowledge from the unprecedented amount of data coming from analyzed systems. Since cloud based computing resources have became easily accessible, there is an opportunity for verification techniques and tools to undergo a deep technological transition to exploit the new available architectures. This has created an increasing interest in parallelizing and distributing verification techniques. In this paper we introduce a distributed approach which exploits techniques typically used by the bigdata community to enable verification of very complex systems using bigdata approaches and cloud computing facilities.",
                "call-number": "10.1145/2591062.2591088",
                "collection-title": "ICSE Companion 2014",
                "container-title": "Companion Proceedings of the 36th International Conference on Software Engineering",
                "DOI": "10.1145/2591062.2591088",
                "event-place": "Hyderabad, India",
                "ISBN": "9781450327688",
                "keyword": "Big Data, Formal Verification, MapReduce, CTL",
                "number-of-pages": "4",
                "page": "638–641",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Formal verification problems in a big data world: towards a mighty synergy",
                "URL": "https://doi.org/10.1145/2591062.2591088"
            }
        },
        {
            "10.1145/2554850.2555076": {
                "id": "10.1145/2554850.2555076",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Evermann",
                        "given": "Joerg"
                    },
                    {
                        "family": "Assadipour",
                        "given": "Ghazal"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2014,
                            3,
                            24
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2014,
                            3,
                            24
                        ]
                    ]
                },
                "abstract": "Process mining is an approach to extract process models from event logs. Given the distributed nature of modern information systems, event logs are likely to be distributed across different physical machines. Map-Reduce is a scalable approach for efficient computations on distributed data. In this paper we present the design of a Map-Reduce implementation of the Alpha process mining algorithm, to take advantage of the scalability of the Map-Reduce approach. We provide a experimental results that show the performance and scalability of our implementation.",
                "call-number": "10.1145/2554850.2555076",
                "collection-title": "SAC '14",
                "container-title": "Proceedings of the 29th Annual ACM Symposium on Applied Computing",
                "DOI": "10.1145/2554850.2555076",
                "event-place": "Gyeongju, Republic of Korea",
                "ISBN": "9781450324694",
                "keyword": "map-reduce, process mining, alpha algorithm, workflow management",
                "number-of-pages": "3",
                "page": "1414–1416",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Big data meets process mining: implementing the alpha algorithm with map-reduce",
                "URL": "https://doi.org/10.1145/2554850.2555076"
            }
        },
        {
            "10.1145/3055635.3056660": {
                "id": "10.1145/3055635.3056660",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Cahyani",
                        "given": "Anggita Dian"
                    },
                    {
                        "family": "Budiharto",
                        "given": "Widodo"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            2,
                            24
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            2,
                            24
                        ]
                    ]
                },
                "abstract": "The information available in the form of data in Human Resources (HR) to be analyzed will vary greatly depending on the type of organization. Fast and effective Human Resources Systems facilitates the success of an organization. Data science in HR can help determine whether there are patterns of churn in our data that could help predict future churn and find the right candidate of employee. In this paper, we describe the novel model of Intelligent Human Resources Systems (IHRS) using Big Data to analyze and predict about future status of the employees based on the Support Vector Machine (SVM).",
                "call-number": "10.1145/3055635.3056660",
                "collection-title": "ICMLC 2017",
                "container-title": "Proceedings of the 9th International Conference on Machine Learning and Computing",
                "DOI": "10.1145/3055635.3056660",
                "event-place": "Singapore, Singapore",
                "ISBN": "9781450348171",
                "keyword": "Human resources, data science, Psychology, SVM, big data",
                "number-of-pages": "4",
                "page": "137–140",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Modeling Intelligent Human Resources Systems (IRHS) using Big Data and Support Vector Machine (SVM)",
                "URL": "https://doi.org/10.1145/3055635.3056660"
            }
        },
        {
            "10.1145/3341069.3342980": {
                "id": "10.1145/3341069.3342980",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Islam",
                        "given": "M. D. Samiul"
                    },
                    {
                        "family": "Liu",
                        "given": "Daizong"
                    },
                    {
                        "family": "Wang",
                        "given": "Kewei"
                    },
                    {
                        "family": "Zhou",
                        "given": "Pan"
                    },
                    {
                        "family": "Yu",
                        "given": "Li"
                    },
                    {
                        "family": "Wu",
                        "given": "Dapeng"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            6,
                            22
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            6,
                            22
                        ]
                    ]
                },
                "abstract": "The medical services in Bangladesh are shortage nowadays; people are suffering from getting the correct treatment from the hospital. With the low proportion of the doctors and the low per capita salary in Bangladesh, patients need to spend more money to get the appropriate treatments. Therefore, it is necessary to apply modern information technologies by which the scaffold between the patients and specialists can be reduced, and the patients can take proper treatment at a lower cost. Fortunately, we can solve this critical problem by utilizing interaction among electrical devices. With the big data collected from these devices, machine learning is a powerful tool for the data analytics because of its high accuracy, lower computational costs, and lower power consumption. This research is based on a case of study by the incorporation of the database, mobile application, web application and develops a novel platform through which the patients and the doctors can interact. In addition, the platform helps to store the patients' health data to make the final prediction using machine learning methods to get the proper healthcare treatment with the help of the machines and the doctors. The experiment result shows the high accuracy over 95% of the disease detection using machine learning methods, with the cost 90% lower than the local hospital in Bangladesh, which provides the strong support to implement of our platform in the remote area of the country.",
                "call-number": "10.1145/3341069.3342980",
                "collection-title": "HPCCT 2019",
                "container-title": "Proceedings of the 2019 3rd High Performance Computing and Cluster Technologies Conference",
                "DOI": "10.1145/3341069.3342980",
                "event-place": "Guangzhou, China",
                "ISBN": "9781450371858",
                "keyword": "Big Data, Data Mining, Disease Prediction, Healthcare, Machine Learning",
                "number-of-pages": "8",
                "page": "139–146",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "A Case Study of HealthCare Platform using Big Data Analytics and Machine Learning",
                "URL": "https://doi.org/10.1145/3341069.3342980"
            }
        },
        {
            "10.5555/2946645.3007028": {
                "id": "10.5555/2946645.3007028",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Richtárik",
                        "given": "Peter"
                    },
                    {
                        "family": "Takáč",
                        "given": "Martin"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            1,
                            1
                        ]
                    ]
                },
                "abstract": "In this paper we develop and analyze Hydra: HYbriD cooRdinAte descent method for solving loss minimization problems with big data. We initially partition the coordinates (features) and assign each partition to a different node of a cluster. At every iteration, each node picks a random subset of the coordinates from those it owns, independently from the other computers, and in parallel computes and applies updates to the selected coordinates based on a simple closed-form formula. We give bounds on the number of iterations sufficient to approximately solve the problem with high probability, and show how it depends on the data and on the partitioning. We perform numerical experiments with a LASSO instance described by a 3TB matrix.",
                "call-number": "10.5555/2946645.3007028",
                "container-title": "J. Mach. Learn. Res.",
                "ISSN": "1532-4435",
                "issue": "1",
                "keyword": "parallel coordinate descent, boosting, distributed algorithms, stochastic methods",
                "number-of-pages": "25",
                "page": "2657–2681",
                "publisher": "JMLR.org",
                "source": "January 2016",
                "title": "Distributed coordinate descent method for learning with big data",
                "volume": "17"
            }
        },
        {
            "10.1145/3110218": {
                "id": "10.1145/3110218",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Zhang",
                        "given": "Qingchen"
                    },
                    {
                        "family": "Yang",
                        "given": "Laurence T."
                    },
                    {
                        "family": "Chen",
                        "given": "Zhikui"
                    },
                    {
                        "family": "Li",
                        "given": "Peng"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            9,
                            24
                        ]
                    ]
                },
                "abstract": "With the ongoing development of sensor devices and network techniques, big data are being generated from the cyber-physical systems. Because of sensor equipment occasional failure and network transmission unreliability, a large number of low-quality data, such as noisy data and incomplete data, is collected from the cyber-physical systems. Low-quality data pose a remarkable challenge on deep learning models for big data feature learning. As a novel deep learning model, the deep computation model achieves superior performance for big data feature learning. However, it is difficult for the deep computation model to learn dependable features for low-quality data, since it uses the nonlinear function as the encoder. In this article, a dependable deep computation model is proposed for feature learning on low-quality big data in cyber-physical systems. Specially, a regularity is added into the objective function of the deep computation model to obtain reliable features in the intermediate-level representation space. Furthermore, a learning algorithm based on the back-propagation strategy is devised to train the parameters of the proposed model. Finally, experiments are conducted on three representative datasets and a real dataset to evaluate the effectiveness of the dependable deep computation model for low-quality big data feature learning. Results show that the proposed model achieves a remarkable result for the tasks of classification, restoration, and prediction, proving the potential of this work for practical applications in cyber-physical systems.",
                "call-number": "10.1145/3110218",
                "collection-number": "11",
                "container-title": "ACM Trans. Cyber-Phys. Syst.",
                "DOI": "10.1145/3110218",
                "ISSN": "2378-962X",
                "issue": "1",
                "keyword": "feature learning, back-propagation algorithm, dependable deep computation model, Cyber-physical systems, big data",
                "number": "Article 11",
                "number-of-pages": "17",
                "page": "1–17",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "January 2019",
                "title": "Dependable Deep Computation Model for Feature Learning on Big Data in Cyber-Physical Systems",
                "URL": "https://doi.org/10.1145/3110218",
                "volume": "3"
            }
        },
        {
            "10.1145/3257759": {
                "id": "10.1145/3257759",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "April",
                        "given": "Alain"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            4,
                            11
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2016,
                            4,
                            11
                        ]
                    ]
                },
                "call-number": "10.1145/3257759",
                "collection-title": "DH '16",
                "container-title": "Proceedings of the 6th International Conference on Digital Health Conference",
                "DOI": "10.1145/3257759",
                "event-place": "Montréal, Québec, Canada",
                "ISBN": "9781450342247",
                "page": "",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Session details: Big Data Analytics for Health",
                "URL": "https://doi.org/10.1145/3257759"
            }
        },
        {
            "10.1145/3473714.3473822": {
                "id": "10.1145/3473714.3473822",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Mu",
                        "given": "Xiangwei"
                    },
                    {
                        "family": "Jiang",
                        "given": "Jingjing"
                    },
                    {
                        "family": "Zhu",
                        "given": "Guoqing"
                    },
                    {
                        "family": "Li",
                        "given": "Kequan"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            6,
                            18
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2021,
                            6,
                            18
                        ]
                    ]
                },
                "abstract": "There are path planning issues under restrictions such as time windows when the express delivery point is delivered, and the delivery address of express parcels in actual delivery may be scattered. The volume of express parcels is extremely uneven for the express delivery point. When the amount of express delivery exceeds the load of the express delivery point too much, pure path planning becomes meaningless. Therefore, considering the actual situation and taking the distribution route and the maximum delivery volume of the express point as the main constraints, a possible route planning is conducted in the paper according to the express delivery address before the distribution point enters the express point. Then, based on the big data of the Internet of Things, the distribution area of the express delivery point is dynamically divided, and the express delivery that meets the maximum delivery volume and optimal route planning is allocated to the corresponding neighboring express delivery points. Moreover, the overall equilibrium optimization of each express delivery point is performed to obtain the final dynamically divided distribution area. Experiments have proved that the optimization algorithm in the paper has higher calculation accuracy than other methods, which can achieve more efficient arrangements for the division of logistics distribution areas and improve the distribution rate of load balancing and the quality of delivery services.",
                "call-number": "10.1145/3473714.3473822",
                "collection-title": "ICCIR 2021",
                "container-title": "Proceedings of the 2021 International Conference on Control and Intelligent Robotics",
                "DOI": "10.1145/3473714.3473822",
                "event-place": "Guangzhou, China",
                "ISBN": "9781450390231",
                "keyword": "Logistics and distribution, Internet of Things, Area division, Ant colony algorithm, Tabu algorithm, Big data, Genetic algorithm",
                "number-of-pages": "7",
                "page": "619–625",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Research on Dynamic Division Method of Distribution Area under Big Data Environment",
                "URL": "https://doi.org/10.1145/3473714.3473822"
            }
        },
        {
            "10.1145/3457682.3457775": {
                "id": "10.1145/3457682.3457775",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Ke",
                        "given": "Chen"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            2,
                            26
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2021,
                            2,
                            26
                        ]
                    ]
                },
                "abstract": "The development of big data technology has brought a series of new content, opportunities and challenges to the library, and scholars have conducted many studies around this. This study obtained 98 related papers from the core collection of Web of Science, using the knowledge map research method, and using the CiteSpace software to analyze the number of annual papers, journals, authors, institutions, keywords and topic changes. The results show that scholars’ attention to this field has gradually increased, and the number of annual papers has increased year by year. China is the country with the highest contribution to the research, and the contribution of Chinese scholars is higher than that of other countries. Big data, university library, data management and information service are the key research contents of this field. In the end, this paper makes a research prospect, and scholars should further strengthen the research on user behavior, user portrait and intellectual property risk.",
                "call-number": "10.1145/3457682.3457775",
                "collection-title": "ICMLC 2021",
                "container-title": "2021 13th International Conference on Machine Learning and Computing",
                "DOI": "10.1145/3457682.3457775",
                "event-place": "Shenzhen, China",
                "ISBN": "9781450389310",
                "keyword": "Big Data, Library, Knowledge Map",
                "number-of-pages": "8",
                "page": "271–278",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Visualization Analysis of Library Research in the Context of Big Data Based on Knowledge Map",
                "URL": "https://doi.org/10.1145/3457682.3457775"
            }
        },
        {
            "10.1145/3335484.3335494": {
                "id": "10.1145/3335484.3335494",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Chen",
                        "given": "Wei-Yu"
                    },
                    {
                        "family": "Lu",
                        "given": "Peggy Joy"
                    },
                    {
                        "family": "Shiau",
                        "given": "Steven"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            5,
                            10
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            5,
                            10
                        ]
                    ]
                },
                "abstract": "With big data analytics and open data mining becoming increasingly important in this information explosion era, a highly efficient approach to providing an integrated service is by combining these two topics. Therefore, to maximize the convenience of Taiwan's open data utilization and to enrich users' experiences with big data analytics, this paper proposes the Integrated Platform for Government Open Data (IPGOD). The platform consists of a \"Data System\" based on a cloud data warehouse and an \"Analytics System\" based on machine learning utilities; these two systems can work individually or in an integrated manner. Moreover, we leverage the Apache Spark cloud platform to enhance low latency response and high performance. The experimental results demonstrate that the proposed IPGOD realizes the open data warehouse effectively and derives machine learning visualization in a user-friendly and intelligent way.",
                "call-number": "10.1145/3335484.3335494",
                "collection-title": "ICBDC '19",
                "container-title": "Proceedings of the 4th International Conference on Big Data and Computing",
                "DOI": "10.1145/3335484.3335494",
                "event-place": "Guangzhou, China",
                "ISBN": "9781450362788",
                "keyword": "Cloud computing, Open data, Machine learning, Big data mining",
                "number-of-pages": "6",
                "page": "11–16",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "IPGOD: An Integrated Visualization Platform Based on Big Data Mining and Cloud Computing",
                "URL": "https://doi.org/10.1145/3335484.3335494"
            }
        },
        {
            "10.14778/3303753.3303762": {
                "id": "10.14778/3303753.3303762",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Xu",
                        "given": "Lijie"
                    },
                    {
                        "family": "Guo",
                        "given": "Tian"
                    },
                    {
                        "family": "Dou",
                        "given": "Wensheng"
                    },
                    {
                        "family": "Wang",
                        "given": "Wei"
                    },
                    {
                        "family": "Wei",
                        "given": "Jun"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            1,
                            1
                        ]
                    ]
                },
                "abstract": "Popular big data frameworks, ranging from Hadoop MapReduce to Spark, rely on garbage-collected languages, such as Java and Scala. Big data applications are especially sensitive to the effectiveness of garbage collection (i.e., GC), because they usually process a large volume of data objects that lead to heavy GC overhead. Lacking in-depth understanding of GC performance has impeded performance improvement in big data applications. In this paper, we conduct the first comprehensive evaluation on three popular garbage collectors, i.e., Parallel, CMS, and G1, using four representative Spark applications. By thoroughly investigating the correlation between these big data applications' memory usage patterns and the collectors' GC patterns, we obtain many findings about GC inefficiencies. We further propose empirical guidelines for application developers, and insightful optimization strategies for designing big-data-friendly garbage collectors.",
                "call-number": "10.14778/3303753.3303762",
                "container-title": "Proc. VLDB Endow.",
                "DOI": "10.14778/3303753.3303762",
                "ISSN": "2150-8097",
                "issue": "5",
                "number-of-pages": "14",
                "page": "570–583",
                "publisher": "VLDB Endowment",
                "source": "January 2019",
                "title": "An experimental evaluation of garbage collectors on big data applications",
                "URL": "https://doi.org/10.14778/3303753.3303762",
                "volume": "12"
            }
        },
        {
            "10.1145/2993318.2993351": {
                "id": "10.1145/2993318.2993351",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Meissner",
                        "given": "Roy"
                    },
                    {
                        "family": "Junghanns",
                        "given": "Kurt"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            9,
                            12
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2016,
                            9,
                            12
                        ]
                    ]
                },
                "abstract": "One approach to continuously achieve a certain data quality level is to use an integration pipeline that continuously checks and monitors the quality of a data set according to defined metrics. This approach is inspired by Continuous Integration pipelines, that have been introduced in the area of software development and DevOps to perform continuous source code checks. By investigating in possible tools to use and discussing the specific requirements for RDF data sets, an integration pipeline is derived that joins current approaches of the areas of software-development and semantic-web as well as reuses existing tools. As these tools have not been built explicitly for CI usage, we evaluate their usability and propose possible workarounds and improvements. Furthermore, a real-world usage scenario is discussed, outlining the benefit of the usage of such a pipeline.",
                "call-number": "10.1145/2993318.2993351",
                "collection-title": "SEMANTiCS 2016",
                "container-title": "Proceedings of the 12th International Conference on Semantic Systems",
                "DOI": "10.1145/2993318.2993351",
                "event-place": "Leipzig, Germany",
                "ISBN": "9781450347525",
                "keyword": "Quality Monitoring, DevOps, RDF, Continuous Integration, Data Integration, Data Quality, Instant Feedback",
                "number-of-pages": "4",
                "page": "189–192",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Using DevOps Principles to Continuously Monitor RDF Data Quality",
                "URL": "https://doi.org/10.1145/2993318.2993351"
            }
        },
        {
            "10.1145/3018896.3018975": {
                "id": "10.1145/3018896.3018975",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Hussein",
                        "given": "Ashraf S."
                    },
                    {
                        "family": "Khan",
                        "given": "Hamayun A."
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            3,
                            22
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            3,
                            22
                        ]
                    ]
                },
                "abstract": "The field of Big Data Analytics (BDA) is advancing rapidly, and it is finding adoption in diverse areas such as Health, Commerce, Logistics, Retail and Manufacturing to name a few. Adoption of BDA techniques in the field of Higher Education is new, and it is steadily increasing. In this work, BDA techniques have been applied to track the Key Academic Performance Indicators (KAPIs) related to students at the Arab Open University (AOU) and to support the corresponding decisions in this regard. Since the AOU is a Pan Arab multi-campus distributed institution operating in 8 countries and makes extensive use of a wide range of cloud based applications to manage the students' life cycle, hence it is an ideal candidate for adoption of BDA techniques to track students' KAPIs across the AOU multiple country campuses. In order to achieve this objective, we have used IBM Watson Analytics (WA) platform to track the students' KAPIs. As a pilot project, we have focused in this work on the Information Technology and Computing (ITC) academic programme across the AOU. The Exploration and Business Intelligence BDA capabilities of WA have enabled us to analyze and track the academic KAPIs of the ITC students across AOU country campuses while the Predictive Analytics (PA) has led to identifying the dominant factors behind some of our problems such as students drop out rates. One of the most promising outcomes is the decision support dashboards such as the one related to the Student Risk Factor (SRF). By identifying At Risk Students, such dashboard can act as an \"Early Alert System\" to enable the AOU management to take corrective action to provide needed support to such students.",
                "call-number": "10.1145/3018896.3018975",
                "collection-number": "75",
                "collection-title": "ICC '17",
                "container-title": "Proceedings of the Second International Conference on Internet of things, Data and Cloud Computing",
                "DOI": "10.1145/3018896.3018975",
                "event-place": "Cambridge, United Kingdom",
                "ISBN": "9781450347747",
                "keyword": "academic key performance indicators, watson analytics, educational data analytics, big data analytics, student information systems",
                "number": "Article 75",
                "number-of-pages": "8",
                "page": "1–8",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Students' performance tracking in distributed open education using big data analytics",
                "URL": "https://doi.org/10.1145/3018896.3018975"
            }
        },
        {
            "10.1145/1529282.1529334": {
                "id": "10.1145/1529282.1529334",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Hüner",
                        "given": "Kai M."
                    },
                    {
                        "family": "Ofner",
                        "given": "Martin"
                    },
                    {
                        "family": "Otto",
                        "given": "Boris"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2009,
                            3,
                            8
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2009,
                            3,
                            8
                        ]
                    ]
                },
                "abstract": "High-quality corporate data is a prerequisite for world-wide business process harmonization, global spend analysis, integrated service management, and compliance with regulatory and legal requirements. Corporate Data Quality Management (CDQM) describes the quality oriented organization and control of a company's key data assets such as material, customer, and vendor data. With regard to the aforementioned business drivers, companies demand an instrument to assess the progress and performance of their CDQM initiative. This paper proposes a reference model for CDQM maturity assessment. The model is intended to be used for supporting the build process of CDQM. A case study shows how the model has been successfully implemented in a real-world scenario.",
                "call-number": "10.1145/1529282.1529334",
                "collection-title": "SAC '09",
                "container-title": "Proceedings of the 2009 ACM symposium on Applied Computing",
                "DOI": "10.1145/1529282.1529334",
                "event-place": "Honolulu, Hawaii",
                "ISBN": "9781605581668",
                "keyword": "corporate data quality, data quality management, design research, action research, maturity models, reference modeling",
                "number-of-pages": "8",
                "page": "231–238",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Towards a maturity model for corporate data quality management",
                "URL": "https://doi.org/10.1145/1529282.1529334"
            }
        },
        {
            "10.1145/3255779": {
                "id": "10.1145/3255779",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Candan",
                        "given": "Selcuk K."
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2014,
                            6,
                            18
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2014,
                            6,
                            18
                        ]
                    ]
                },
                "call-number": "10.1145/3255779",
                "collection-title": "SIGMOD '14",
                "container-title": "Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data",
                "DOI": "10.1145/3255779",
                "event-place": "Snowbird, Utah, USA",
                "ISBN": "9781450323765",
                "page": "",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Session details: Research session 28: big data",
                "URL": "https://doi.org/10.1145/3255779"
            }
        },
        {
            "10.1145/3411564.3411612": {
                "id": "10.1145/3411564.3411612",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Souza",
                        "given": "Thiago Vieira de"
                    },
                    {
                        "family": "Farias",
                        "given": "Kleinner"
                    },
                    {
                        "family": "Bischoff",
                        "given": "Vinicius"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2020,
                            11,
                            3
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2020,
                            11,
                            3
                        ]
                    ]
                },
                "abstract": "In recent years, the capacity of big data analytics (BDA) has attracted the significant attention of researchers linked to academia and industry professionals. This capacity is related to the possibility of managing informations advanced to reach its supply chain. In other words, information technology uses integrated systems, which facilitates innovation and the diffusion of knowledge throughout this supply chain. However, researchers and professionals still need to explore the capacity potential of BDA, in order to improve supply chain operational decision-making skills. This work classifies the state-of-the-art literature that applied BDA to the supply chain management (SCM). A Systematic Mapping Study was elaborated based on literature guidelines. A total of 50 primary studies were selected through a filtering process from initially 5,437 studies. These primary studies were used to answer the six research questions. The result of the classification showed that 64% of the studies are related to supply-chain management; most of the studies carried out empirical research; and approximately 50% of the primary studies investigated models for optimization process. This research provides to academics and industry professionals the gaps and future challenges related to BDA for SCM.",
                "call-number": "10.1145/3411564.3411612",
                "collection-number": "14",
                "collection-title": "SBSI'20",
                "container-title": "XVI Brazilian Symposium on Information Systems",
                "DOI": "10.1145/3411564.3411612",
                "event-place": "São Bernardo do Campo, Brazil",
                "ISBN": "9781450388733",
                "keyword": "Data Analysis, Big Data Analytics, Information Systems, Supply-Chain",
                "number": "Article 14",
                "number-of-pages": "8",
                "page": "1–8",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Big Data Analytics applied in Supply Chain Management: A Systematic Mapping Study",
                "URL": "https://doi.org/10.1145/3411564.3411612"
            }
        },
        {
            "10.1145/3467691.3467697": {
                "id": "10.1145/3467691.3467697",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Sassi",
                        "given": "Imad"
                    },
                    {
                        "family": "Anter",
                        "given": "Samir"
                    },
                    {
                        "family": "Bekkhoucha",
                        "given": "Abdelkrim"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            4,
                            9
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2021,
                            4,
                            9
                        ]
                    ]
                },
                "abstract": "A new fast parallel constrained Viterbi algorithm for big data is proposed in this paper. We provide a detailed analysis of its performance on big data frameworks. This performance analysis includes the evaluation of execution time, speedup, and prediction accuracy. Additionally, we compare the impact of the proposed approach on the performance of our parallel constrained algorithm with other benchmark versions. We use synthetic data and real-world data in our experiments to describe the behavior of our algorithm for different data sizes and different numbers of nodes. We demonstrate that this algorithm is fast, highly efficient, and scalable when it runs on spark framework and its prediction quality is acceptable since there is no deterioration or reduction observed.",
                "call-number": "10.1145/3467691.3467697",
                "collection-title": "ICRSA 2021",
                "container-title": "2021 4th International Conference on Robot Systems and Applications",
                "DOI": "10.1145/3467691.3467697",
                "event-place": "Chengdu, China",
                "ISBN": "9781450384940",
                "keyword": "fast parallel constrained, big data, spark, optimization, Hmm, Viterbi, scalability",
                "number-of-pages": "6",
                "page": "50–55",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Fast Parallel Constrained Viterbi Algorithm for Big Data with Applications to Financial Time Series",
                "URL": "https://doi.org/10.1145/3467691.3467697"
            }
        },
        {
            "10.1145/3429351.3431745": {
                "id": "10.1145/3429351.3431745",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Tavakolisomeh",
                        "given": "Sanaz"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2020,
                            12,
                            7
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2020,
                            12,
                            7
                        ]
                    ]
                },
                "abstract": "Memory management is responsible for allocating and releasing the memory used by applications (e.g., in the Java Virtual Machine-JVM). There are several garbage collectors (GCs) each designed to target different performance metrics, making it very hard for developers to decide which GC to use for a particular application. We start with a review of existing GC algorithms. Then, we intend to evaluate throughput, pause time, and memory usage in existing JVM GCs using benchmark suites like DaCapo and Renaissance. The goal is to find the trade-offs between the above mentioned performance metrics to have a better understanding of which GC helps fulfilling certain application requirements.",
                "call-number": "10.1145/3429351.3431745",
                "collection-title": "Middleware'20 Doctoral Symposium",
                "container-title": "Proceedings of the 21st International Middleware Conference Doctoral Symposium",
                "DOI": "10.1145/3429351.3431745",
                "event-place": "Delft, Netherlands",
                "ISBN": "9781450382007",
                "keyword": "Garbage Collection Algorithm, Big Data, Garbage Collector, JVM, Memory Management, Cloud",
                "number-of-pages": "4",
                "page": "22–25",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Selecting a JVM Garbage Collector for Big Data and Cloud Services",
                "URL": "https://doi.org/10.1145/3429351.3431745"
            }
        },
        {
            "10.1145/3486611.3491121": {
                "id": "10.1145/3486611.3491121",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Mo",
                        "given": "Yunjeong"
                    },
                    {
                        "family": "Zhao",
                        "given": "Dong"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            11,
                            17
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2021,
                            11,
                            17
                        ]
                    ]
                },
                "abstract": "Occupant behavior is multifaceted, and a systematic approach is required to understand occupant behavior comprehensively. This research aims to define a structure of the relationship between energy consumption, building technology, and occupant behavior, using the Occupant Behavior Prediction Model. The model can predict and explain occupant energy usage-related activities. A machine learning approach is used to develop the model, and datasets from the American Time Use Survey (ATUS) are used to verify the model. The results show that the energy use activities with higher predictive performances are more stable and habitual compared to the ones with lower predictive performances. The prediction accuracy achieved by this model for these habitual activities reached as high as 99%. The findings imply that the building systems and control strategies need to be adjusted to accommodate habitual energy use behaviors, rather than changing the behaviors. In addition, educational interventions seem more effective on the less habitual behaviors, which often change.",
                "call-number": "10.1145/3486611.3491121",
                "collection-title": "BuildSys '21",
                "container-title": "Proceedings of the 8th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation",
                "DOI": "10.1145/3486611.3491121",
                "event-place": "Coimbra, Portugal",
                "ISBN": "9781450391146",
                "keyword": "urban scale data analysis, residential building, big data, energy use prediction, occupant behavior",
                "number-of-pages": "4",
                "page": "349–352",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Application of occupant behavior prediction model on residential big data analysis",
                "URL": "https://doi.org/10.1145/3486611.3491121"
            }
        },
        {
            "10.1145/2955129.2955190": {
                "id": "10.1145/2955129.2955190",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Zhang",
                        "given": "Fuquan"
                    },
                    {
                        "family": "Mao",
                        "given": "Zijing"
                    },
                    {
                        "family": "Ding",
                        "given": "Gangyi"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            8,
                            15
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2016,
                            8,
                            15
                        ]
                    ]
                },
                "abstract": "The temperature sensor network in intelligent building classified collection of big data processing has the problem of big data redundancy interference, which results in unable to determine the fixed filter thresholds. This paper proposed Chaos differential disturbance based fuzzy C-means clustering model for big temperature sensing data classification tasks. It requires to analyze temperature sensor in the intelligent building big distributed structure model of data in a database storage system, the big data information flow feature fusion and time series analysis. Based on traditional fuzzy c-means clustering processing, we introduced chaos disturbance to avoid the classification into local convergence and local optimum, and therefore improve the performance of data clustering. The testing results show that our proposed classification method effectively reduces the error rate for classification tasks of temperature data in intelligent building and have achieved the best performance among the existing algorithms.",
                "call-number": "10.1145/2955129.2955190",
                "collection-number": "52",
                "collection-title": "MISNC, SI, DS 2016",
                "container-title": "Proceedings of the The 3rd Multidisciplinary International Social Networks Conference on SocialInformatics 2016, Data Science 2016",
                "DOI": "10.1145/2955129.2955190",
                "event-place": "Union, NJ, USA",
                "ISBN": "9781450341295",
                "keyword": "Temperature sensor, Intelligent building, Classification, Big data",
                "number": "Article 52",
                "number-of-pages": "5",
                "page": "1–5",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Simulation and Analysis of Classification Optimization model of Temperature Sensing Big Data in Intelligent Building",
                "URL": "https://doi.org/10.1145/2955129.2955190"
            }
        },
        {
            "10.1145/3349341.3349375": {
                "id": "10.1145/3349341.3349375",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Liu",
                        "given": "Yongfu"
                    },
                    {
                        "family": "Zhao",
                        "given": "Xin"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            7,
                            12
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            7,
                            12
                        ]
                    ]
                },
                "abstract": "Starting from the characteristics of the big data era, the researcher reveals the trend of information-based teaching in the big data era, and the changes of resource view, teaching view and teacher development view brought about by the information-based teaching reform, which is of great significance for deepening the reform of modern distance education curriculum and building a new teaching team. Combining with the actual situation of information-based teaching in the era of big data change, it is considered that flipping classroom, MOOC and micro-course are the first wave of data change education. The significant feature of flipping classroom and micro-course is the innovation of information-based teaching in the field of education in the era of cloud computing and big data.",
                "call-number": "10.1145/3349341.3349375",
                "collection-title": "AICS 2019",
                "container-title": "Proceedings of the 2019 International Conference on Artificial Intelligence and Computer Science",
                "DOI": "10.1145/3349341.3349375",
                "event-place": "Wuhan, Hubei, China",
                "ISBN": "9781450371506",
                "keyword": "MOOC, Big Data, Distance Education",
                "number-of-pages": "4",
                "page": "93–96",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Research on the Application of \"MOOC\" in Modern Distance Education under the Background of Big Data",
                "URL": "https://doi.org/10.1145/3349341.3349375"
            }
        },
        {
            "10.1145/3211890.3211914": {
                "id": "10.1145/3211890.3211914",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Quiñones",
                        "given": "Eduardo"
                    },
                    {
                        "family": "Bertogna",
                        "given": "Marko"
                    },
                    {
                        "family": "Hadad",
                        "given": "Erez"
                    },
                    {
                        "family": "Ferrer",
                        "given": "Ana Juan"
                    },
                    {
                        "family": "Chiantore",
                        "given": "Luca"
                    },
                    {
                        "family": "Reboa",
                        "given": "Alfredo"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            6,
                            4
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            6,
                            4
                        ]
                    ]
                },
                "call-number": "10.1145/3211890.3211914",
                "collection-title": "SYSTOR '18",
                "container-title": "Proceedings of the 11th ACM International Systems and Storage Conference",
                "DOI": "10.1145/3211890.3211914",
                "event-place": "Haifa, Israel",
                "ISBN": "9781450358491",
                "number-of-pages": "1",
                "page": "130",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Big Data Analytics for Smart Cities: The H2020 CLASS Project",
                "URL": "https://doi.org/10.1145/3211890.3211914"
            }
        },
        {
            "10.1145/3383972.3384034": {
                "id": "10.1145/3383972.3384034",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Feng",
                        "given": "Mingchen"
                    },
                    {
                        "family": "Zheng",
                        "given": "Jiangbin"
                    },
                    {
                        "family": "Ren",
                        "given": "Jinchang"
                    },
                    {
                        "family": "Liu",
                        "given": "Yanqin"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2020,
                            2,
                            15
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2020,
                            2,
                            15
                        ]
                    ]
                },
                "abstract": "Road traffic accident (RTA) is a big issue to our society due to it is among the main causes of traffic congestion, human death, health problems, environmental pollution, and economic losses. Facing these fatal and unexpected traffic accidents, understanding what happened and discover factors that relate to them and then make alarms in advance play critical roles for possibly effective traffic management and reduction of accidents. This paper presents our work to establish a novel big data analytics platform for UK traffic accident analysis using machine learning and deep learning techniques. Our system consists of three parts in which we first cluster accident incidents in an interactive Google map to highlight some hotspots and then narratively visualize accident attributes to uncover potentially related factors, finally we explored several state-of-the-art machine learning, deep learning and time series forecasting models to predict the number of road accidents in the future. The experimental results show that our big data processing platform can not only effectively handle large amount of data but also give new insights into what happened and reasonably prediction of what will happen in the future to assist decision making, which will undoubtedly show its great value as a generic platform for other big data analytics fields.",
                "call-number": "10.1145/3383972.3384034",
                "collection-title": "ICMLC 2020",
                "container-title": "Proceedings of the 2020 12th International Conference on Machine Learning and Computing",
                "DOI": "10.1145/3383972.3384034",
                "event-place": "Shenzhen, China",
                "ISBN": "9781450376426",
                "keyword": "Deep Learning, Traffic Accident Analysis, Big Data Analytics, Time series Forecasting",
                "number-of-pages": "5",
                "page": "225–229",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Towards Big Data Analytics and Mining for UK Traffic Accident Analysis, Visualization & Prediction",
                "URL": "https://doi.org/10.1145/3383972.3384034"
            }
        },
        {
            "10.1145/3194188.3194204": {
                "id": "10.1145/3194188.3194204",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Wang",
                        "given": "Siyu"
                    },
                    {
                        "family": "Xue",
                        "given": "Heru"
                    },
                    {
                        "family": "Jiang",
                        "given": "Xinhua"
                    },
                    {
                        "family": "Zhou",
                        "given": "Yanqing"
                    },
                    {
                        "family": "Duan",
                        "given": "Xiaodong"
                    },
                    {
                        "family": "Bai",
                        "given": "Mingyue"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            2,
                            23
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            2,
                            23
                        ]
                    ]
                },
                "abstract": "Aiming at the water shortage and Inefficient water resources management of Xilin River Basin. we put forward the Management System of Steppe-Watershed Multiple Water Resources Based on Big Data. In this paper, we mainly did two things, the first one is the build of the big data storage platform. The sensors sent back the real-time data, which will be stored in the MySQL temporary database. Every week, these data will be synchronized to the big data platform for permanent storage and backup. The second is the visualization of the current data. Based on the data we have got, we designed different diagrams accomplished by using Echarts for the visualization of the current data. Additional, for the later analysis of the history data, we designed a data processing flow in the big platform for processing enormous amounts of data.",
                "call-number": "10.1145/3194188.3194204",
                "collection-title": "ICEBA 2018",
                "container-title": "Proceedings of the 2018 International Conference on E-Business and Applications",
                "DOI": "10.1145/3194188.3194204",
                "event-place": "Da Nang, Viet Nam",
                "ISBN": "9781450363686",
                "keyword": "water resources management, Big data, data storage, Visualization",
                "number-of-pages": "4",
                "page": "78–81",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Construction of Information Management System of Steppe-watershed Multiple Water Resources Based on Big Data",
                "URL": "https://doi.org/10.1145/3194188.3194204"
            }
        },
        {
            "10.1145/3018896.3018913": {
                "id": "10.1145/3018896.3018913",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Chaoui",
                        "given": "Habiba"
                    },
                    {
                        "family": "Makdoun",
                        "given": "Ibtissam"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            3,
                            22
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            3,
                            22
                        ]
                    ]
                },
                "abstract": "When Big data and cloud computing join forces together, several domains like: healthcare, disaster prediction and decision making become easier and much more beneficial to users in term of information gathering, although cloud computing will reduce time and cost of analyzing information for big data, it may harm the confidentiality and integrity of the sensitive data, for instance, in healthcare, when analyzing disease's spreading area, the name of the infected people must remain secure, hence the obligation to adopt a secure model that protect sensitive data from malicious users. Several case studies on the integration of big data in cloud computing, urge on how easier it would be to analyze and manage big data in this complex envronement. Companies must consider outsourcing their sensitive data to the cloud to take advantage of its beneficial resources such as huge storage, fast calculation, and availability, yet cloud computing might harm the security of data stored and computed in it (confidentiality, integrity). Therefore, strict paradigm must be adopted by organization to obviate their outsourced data from being stolen, damaged or lost. In this paper, we compare between the existing models to secure big data implementation in the cloud computing. Then, we propose our own model to secure Big Data on the cloud computing environement, considering the lifecycle of data from uploading, storage, calculation to its destruction.",
                "call-number": "10.1145/3018896.3018913",
                "collection-number": "18",
                "collection-title": "ICC '17",
                "container-title": "Proceedings of the Second International Conference on Internet of things, Data and Cloud Computing",
                "DOI": "10.1145/3018896.3018913",
                "event-place": "Cambridge, United Kingdom",
                "ISBN": "9781450347747",
                "keyword": "safe data destruction, big data, functional encryption, authentication protocols, search over encrypted data, cloud computing",
                "number": "Article 18",
                "number-of-pages": "11",
                "page": "1–11",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "A new secure model for the use of cloud computing in big data analytics",
                "URL": "https://doi.org/10.1145/3018896.3018913"
            }
        },
        {
            "10.1145/3401895.3402092": {
                "id": "10.1145/3401895.3402092",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Silva",
                        "given": "Rodrigo Dantas da"
                    },
                    {
                        "family": "de Araújo",
                        "given": "Jean Jar Pereira"
                    },
                    {
                        "family": "de Paiva",
                        "given": "Álvaro Ferreira Pires"
                    },
                    {
                        "family": "de Medeiros Valentim",
                        "given": "Ricardo Alexsandro"
                    },
                    {
                        "family": "Coutinho",
                        "given": "Karilany Dantas"
                    },
                    {
                        "family": "de Paiva",
                        "given": "Jailton Carlos"
                    },
                    {
                        "family": "Roussanaly",
                        "given": "Azim"
                    },
                    {
                        "family": "Boyer",
                        "given": "Anne"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2020,
                            11,
                            25
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2020,
                            11,
                            25
                        ]
                    ]
                },
                "abstract": "For many decades society did need to monitor and assess the standard of living of the population. In the 1950s, the United Nations (UN) saw this need and proposed 12 areas that should be evaluated, the first of which is listed under \"Health and Demography\", which focuses on what is expressed as the level of a population's health. Decades have passed and great results have been gained from similar initiatives such as reducing mortality from infectious diseases and even eradicating some others. In the age of the digital society, needs have grown. Monitoring demands that once perished from data to become concrete now suffer from the opposite effect, the excess of data from everywhere. Healthcare systems around the world use many different information systems, collecting and generating hundreds of data at unimaginable speed. We are billions of people on the planet and most of us are connected to the virtual world, sharing information, experiences and events with some kind of cloud. In this information age, the ability to aggregate and process this data is a major factor in raising public health to a new level. The development of tools capable of analyzing a large volume of data in seconds and producing knowledge for targeted decision making can help in the fight against specific diseases, in the process of continuing education of professionals, in the formation of new professionals, in the elaboration of new policies. with the specific locoregional look, in the analysis of hidden trends in front of so much information faced in everyday life and other possibilities. The present work proposes an architecture capable of storing and manipulating seeking to standardize the variables in order to allow to correlate this large amount of data in a systematic way, providing to several services and researchers the possibility of consuming health, social, economic and educational data for the promotion of public health.",
                "call-number": "10.1145/3401895.3402092",
                "collection-number": "58",
                "collection-title": "EATIS '20",
                "container-title": "Proceedings of the 10th Euro-American Conference on Telematics and Information Systems",
                "DOI": "10.1145/3401895.3402092",
                "event-place": "Aveiro, Portugal",
                "ISBN": "9781450377119",
                "keyword": "epidemiology, healthcare surveillance, syphilis, big data",
                "number": "Article 58",
                "number-of-pages": "6",
                "page": "1–6",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "A big data architecture to a multiple purpose in healthcare surveillance: the Brazilian syphilis case",
                "URL": "https://doi.org/10.1145/3401895.3402092"
            }
        },
        {
            "10.5555/1812530.1812588": {
                "id": "10.5555/1812530.1812588",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Chen",
                        "given": "Kuang"
                    },
                    {
                        "family": "Chen",
                        "given": "Harr"
                    },
                    {
                        "family": "Conway",
                        "given": "Neil"
                    },
                    {
                        "family": "Dolan",
                        "given": "Heather"
                    },
                    {
                        "family": "Hellerstein",
                        "given": "Joseph M."
                    },
                    {
                        "family": "Parikh",
                        "given": "Tapan S."
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2009,
                            4,
                            17
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2009,
                            4,
                            17
                        ]
                    ]
                },
                "abstract": "Organizations in developing regions want to efficiently collect digital data, but standard data gathering practices from the developed world are often inappropriate. Traditional techniques for form design and data quality are expensive and labour-intensive. We propose a new data-driven approach to form design, execution (filling) and quality assurance. We demonstrate USHER, an end-to-end system that automatically generates data entry forms that enforce and maintain data quality constraints during execution. The system features a probabilistic engine that drives form-user interactions to encourage correct answers.",
                "call-number": "10.5555/1812530.1812588",
                "collection-title": "ICTD'09",
                "container-title": "Proceedings of the 3rd international conference on Information and communication technologies and development",
                "event-place": "Doha, Qatar",
                "ISBN": "9781424446629",
                "number-of-pages": "1",
                "page": "487",
                "publisher": "IEEE Press",
                "title": "Improving data quality with dynamic forms"
            }
        },
        {
            "10.1145/2740908.2778845": {
                "id": "10.1145/2740908.2778845",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Cappiello",
                        "given": "Cinzia"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2015,
                            5,
                            18
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2015,
                            5,
                            18
                        ]
                    ]
                },
                "abstract": "n today's information era, every day more and more information is generated and people, on the one hand, have advantages due the increasing support in decision processes and, on the other hand, are experiencing difficulties in the selection of the right data to use. That is, users may leverage on more data but at the same time they may not be able to fully value such data since they lack the necessary knowledge about their provenance and quality. The data quality research area provides quality assessment and improvement methods that can be a valuable support for users that have to deal with the complexity of Web content. In fact, such methods help users to identify the suitability of information for their purposes. Most of the methods and techniques proposed, however, address issues for structured data and/or for defined contexts. Clearly, they cannot be easily used on the Web, where data come from heterogeneous sources and the context of use is most of the times unknown.In this keynote, the need for new assessment techniques is highlighted together with the importance of tracking data provenance as well as the reputation and trustworthiness of the sources. In fact, it is well known that the increase of data volume often corresponds to an increase of value, but to maximize such value the data sources to be used have to carefully analyzed, selected and integrated depending on the specific context of use. The talk discusses the data quality dimensions necessary to analyze different Web data sources and provides a set of illustrative examples that show how to maximize the quality of gathered information.",
                "call-number": "10.1145/2740908.2778845",
                "collection-title": "WWW '15 Companion",
                "container-title": "Proceedings of the 24th International Conference on World Wide Web",
                "DOI": "10.1145/2740908.2778845",
                "event-place": "Florence, Italy",
                "ISBN": "9781450334730",
                "keyword": "web quality, data quality, data quality assessment",
                "number-of-pages": "1",
                "page": "1433",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "On the Role of Data Quality in Improving Web Information Value",
                "URL": "https://doi.org/10.1145/2740908.2778845"
            }
        },
        {
            "10.1145/3254069": {
                "id": "10.1145/3254069",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "McCoy",
                        "given": "Kathleen"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            10,
                            23
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2016,
                            10,
                            23
                        ]
                    ]
                },
                "call-number": "10.1145/3254069",
                "collection-title": "ASSETS '16",
                "container-title": "Proceedings of the 18th International ACM SIGACCESS Conference on Computers and Accessibility",
                "DOI": "10.1145/3254069",
                "event-place": "Reno, Nevada, USA",
                "ISBN": "9781450341240",
                "page": "",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Session details: Big Data and Blind Users",
                "URL": "https://doi.org/10.1145/3254069"
            }
        },
        {
            "10.1145/3449301.3449322": {
                "id": "10.1145/3449301.3449322",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Wen",
                        "given": "Yana"
                    },
                    {
                        "family": "Wei",
                        "given": "Tingyue"
                    },
                    {
                        "family": "Cui",
                        "given": "Kewei"
                    },
                    {
                        "family": "Ling",
                        "given": "Bai"
                    },
                    {
                        "family": "Zhang",
                        "given": "Yahao"
                    },
                    {
                        "family": "Huang",
                        "given": "Meng"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2020,
                            11,
                            20
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2020,
                            11,
                            20
                        ]
                    ]
                },
                "abstract": "In the era of big data, people's visual needs for data expression are increasing. In order to achieve better big data display effects, this article introduced the use of text clustering algorithms to achieve data crawling and Echarts technology to realize big data visualization. This system used mvvm's architecture and vue framework development platform, ThinkPHP was used as the background framework, and ES6 related technologies and specifications were used for application development. This system used Echarts, IView, GIS technology and JavaScript development methods to demonstrate economic big data module functions on the web side; Applied CSS3, HTML5, GIS technology to implement project achievement module and university alliance module; Applied Echarts, HTML5, JS function library technology to achieve national information module. This system used stored procedure, database index optimization technology to achieve rapid screening of massive data, and dynamically update and displayed related data through two-way data binding. This system combined real-time location technology with GIS technology to measure the distance between the user and the destination, and automatically plan the tour route to provide related services. This system can provide feasibility suggestions for strategic researchers or experts in related areas of the “Belt and Road”, and provide theoretical basis and technical support.",
                "call-number": "10.1145/3449301.3449322",
                "collection-title": "ICRAI 2020",
                "container-title": "2020 6th International Conference on Robotics and Artificial Intelligence",
                "DOI": "10.1145/3449301.3449322",
                "event-place": "Singapore, Singapore",
                "ISBN": "9781450388597",
                "keyword": "One Belt One Road, Keywords-component, Text clustering algorithm, big data visualization",
                "number-of-pages": "5",
                "page": "121–125",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Research on Belt and Road Big Data Visualization Based on Text Clustering Algorithm",
                "URL": "https://doi.org/10.1145/3449301.3449322"
            }
        },
        {
            "10.1145/3510858.3510934": {
                "id": "10.1145/3510858.3510934",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Zhao",
                        "given": "Yiming"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            12,
                            18
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2021,
                            12,
                            18
                        ]
                    ]
                },
                "abstract": "With the development and wide application of machine learning technology, the use of machine learning technology for economic algorithm technology research has become a new type of financial technology field. Today's financial big data has penetrated into all walks of life and has become an important factor of production. The extraction and application of massive amounts of data by humans heralds the arrival of a new wave of productivity growth and consumer surplus. Big data originally refers to a large number of data sets generated through batch processing or web search index analysis. This paper uses machine learning technology to explore and research big data financial algorithms, analyze risk control measures, report on the improvement and perfection of traditional finance, and analyze and study the future development of big data finance. The main research content of this paper is the analysis of big data financial algorithm technology by machine learning algorithms. Machine learning technology is one of the main methods to solve big data mining problems. Machine learning technology is a process of self-improvement using the system itself, so that computer programs can automatically improve performance through accumulated experience. This paper analyzes the relevant theories and characteristics of machine learning algorithms, and integrates them into the research of big data economic algorithm technology. The final result of the research shows that when the data volume is 1G, the training time of SVM is 8 minutes, while the training time of Bayesian is 12 minutes, and the data volume is relatively small. The SVM algorithm still has obvious advantages in training time.",
                "call-number": "10.1145/3510858.3510934",
                "collection-title": "ICASIT 2021",
                "container-title": "2021 International Conference on Aviation Safety and Information Technology",
                "DOI": "10.1145/3510858.3510934",
                "event-place": "Changsha, China",
                "ISBN": "9781450390422",
                "number-of-pages": "5",
                "page": "218–222",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Big Data Financial Algorithm Technology Based on Machine Learning Technology",
                "URL": "https://doi.org/10.1145/3510858.3510934"
            }
        },
        {
            "10.5555/3320516.3321154": {
                "id": "10.5555/3320516.3321154",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Uhrmacher",
                        "given": "Adelinde"
                    },
                    {
                        "family": "Sanchez",
                        "given": "Susan M."
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            12,
                            9
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            12,
                            9
                        ]
                    ]
                },
                "call-number": "10.5555/3320516.3321154",
                "collection-title": "WSC '18",
                "container-title": "Proceedings of the 2018 Winter Simulation Conference",
                "event-place": "Gothenburg, Sweden",
                "ISBN": "978153866570",
                "page": "",
                "publisher": "IEEE Press",
                "title": "Session details: Advanced tutorials: Inferential big data"
            }
        },
        {
            "10.1145/3255781": {
                "id": "10.1145/3255781",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Abiteboul",
                        "given": "Serge"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2014,
                            6,
                            18
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2014,
                            6,
                            18
                        ]
                    ]
                },
                "call-number": "10.1145/3255781",
                "collection-title": "PODS '14",
                "container-title": "Proceedings of the 33rd ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems",
                "DOI": "10.1145/3255781",
                "event-place": "Snowbird, Utah, USA",
                "ISBN": "9781450323758",
                "page": "",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Session details: Web queries and big data",
                "URL": "https://doi.org/10.1145/3255781"
            }
        },
        {
            "10.1109/CCGrid.2015.170": {
                "id": "10.1109/CCGrid.2015.170",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Cuzzocrea",
                        "given": "Alfredo"
                    },
                    {
                        "family": "Mumolo",
                        "given": "Enzo"
                    },
                    {
                        "family": "Corona",
                        "given": "Pietro"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2015,
                            5,
                            4
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2015,
                            5,
                            4
                        ]
                    ]
                },
                "abstract": "We propose Cloud-based machine learning tools for enhanced Big Data applications, where the main idea is that of predicting the \"next\" workload occurring against the target Cloud infrastructure via an innovative ensemble-based approach that combine the effectiveness of different well-known classifiers in order to enhance the whole accuracy of the final classification, which is very relevant at now in the specific context of Big Data. So-called workload categorization problem plays a critical role towards improving the efficiency and the reliability of Cloud-based big data applications. Implementation-wise, our method proposes deploying Cloud entities that participate to the distributed classification approach on top of virtual machines, which represent classical \"commodity\" settings for Cloud-based big data applications. Preliminary experimental assessment and analysis clearly confirm the benefits deriving from our classification framework.",
                "call-number": "10.1109/CCGrid.2015.170",
                "collection-title": "CCGRID '15",
                "container-title": "Proceedings of the 15th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing",
                "DOI": "10.1109/CCGrid.2015.170",
                "event-place": "Shenzhen, China",
                "ISBN": "9781479980062",
                "number-of-pages": "7",
                "page": "908–914",
                "publisher": "IEEE Press",
                "title": "Cloud-based machine learning tools for enhanced big data applications",
                "URL": "https://doi.org/10.1109/CCGrid.2015.170"
            }
        },
        {
            "10.1109/SC.2014.66": {
                "id": "10.1109/SC.2014.66",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Chung",
                        "given": "I-Hsin"
                    },
                    {
                        "family": "Sainath",
                        "given": "Tara N."
                    },
                    {
                        "family": "Ramabhadran",
                        "given": "Bhuvana"
                    },
                    {
                        "family": "Picheny",
                        "given": "Michael"
                    },
                    {
                        "family": "Gunnels",
                        "given": "John"
                    },
                    {
                        "family": "Austel",
                        "given": "Vernon"
                    },
                    {
                        "family": "Chauhari",
                        "given": "Upendra"
                    },
                    {
                        "family": "Kingsbury",
                        "given": "Brian"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2014,
                            11,
                            16
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2014,
                            11,
                            16
                        ]
                    ]
                },
                "abstract": "Deep Neural Networks (DNNs) have recently been shown to significantly outperform existing machine learning techniques in several pattern recognition tasks. DNNs are the state-of-the-art models used in image recognition, object detection, classification and tracking, and speech and language processing applications. The biggest drawback to DNNs has been the enormous cost in computation and time taken to train the parameters of the networks - often a tenfold increase relative to conventional technologies. Such training time costs can be mitigated by the application of parallel computing algorithms and architectures. However, these algorithms often run into difficulties because of the cost of inter-processor communication bottlenecks. In this paper, we describe how to enable Parallel Deep Neural Network Training on the IBM Blue Gene/Q (BG/Q) computer system. Specifically, we explore DNN training using the data-parallel Hessian-free 2nd order optimization algorithm. Such an algorithm is particularly well-suited to parallelization across a large set of loosely coupled processors. BG/Q, with its excellent inter-processor communication characteristics, is an ideal match for this type of algorithm. The paper discusses how issues regarding programming model and data-dependent imbalances are addressed. Results on large-scale speech tasks show that the performance on BG/Q scales linearly up to 4096 processes with no loss in accuracy. This allows us to train neural networks using billions of training examples in a few hours.",
                "call-number": "10.1109/SC.2014.66",
                "collection-title": "SC '14",
                "container-title": "Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",
                "DOI": "10.1109/SC.2014.66",
                "event-place": "New Orleans, Louisana",
                "ISBN": "9781479955008",
                "keyword": "speech recognition, big data, high performance computing",
                "number-of-pages": "9",
                "page": "745–753",
                "publisher": "IEEE Press",
                "title": "Parallel deep neural network training for big data on blue gene/Q",
                "URL": "https://doi.org/10.1109/SC.2014.66"
            }
        },
        {
            "10.5555/645919.672811": {
                "id": "10.5555/645919.672811",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Patterson",
                        "given": "Blake"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            1993,
                            8,
                            24
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            1993,
                            8,
                            24
                        ]
                    ]
                },
                "call-number": "10.5555/645919.672811",
                "collection-title": "VLDB '93",
                "container-title": "Proceedings of the 19th International Conference on Very Large Data Bases",
                "ISBN": "155860152X",
                "page": "709",
                "publisher": "Morgan Kaufmann Publishers Inc.",
                "publisher-place": "San Francisco, CA, USA",
                "title": "The Need for Data Quality"
            }
        },
        {
            "10.5555/1564131.1564137": {
                "id": "10.5555/1564131.1564137",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Hsueh",
                        "given": "Pei-Yun"
                    },
                    {
                        "family": "Melville",
                        "given": "Prem"
                    },
                    {
                        "family": "Sindhwani",
                        "given": "Vikas"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2009,
                            6,
                            5
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2009,
                            6,
                            5
                        ]
                    ]
                },
                "abstract": "Annotation acquisition is an essential step in training supervised classifiers. However, manual annotation is often time-consuming and expensive. The possibility of recruiting annotators through Internet services (e.g., Amazon Mechanic Turk) is an appealing option that allows multiple labeling tasks to be outsourced in bulk, typically with low overall costs and fast completion rates. In this paper, we consider the difficult problem of classifying sentiment in political blog snippets. Annotation data from both expert annotators in a research lab and non-expert annotators recruited from the Internet are examined. Three selection criteria are identified to select high-quality annotations: noise level, sentiment ambiguity, and lexical uncertainty. Analysis confirm the utility of these criteria on improving data quality. We conduct an empirical study to examine the effect of noisy annotations on the performance of sentiment classification models, and evaluate the utility of annotation selection on classification accuracy and efficiency.",
                "call-number": "10.5555/1564131.1564137",
                "collection-title": "HLT '09",
                "container-title": "Proceedings of the NAACL HLT 2009 Workshop on Active Learning for Natural Language Processing",
                "event-place": "Boulder, Colorado",
                "number-of-pages": "9",
                "page": "27–35",
                "publisher": "Association for Computational Linguistics",
                "publisher-place": "USA",
                "title": "Data quality from crowdsourcing: a study of annotation selection criteria"
            }
        },
        {
            "10.1145/3159652.3160602": {
                "id": "10.1145/3159652.3160602",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Teng",
                        "given": "Shang-Hua"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            2,
                            2
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            2,
                            2
                        ]
                    ]
                },
                "abstract": "In the age of network sciences and machine learning, efficient algorithms are now in higher demand more than ever before. Big Data fundamentally challenges the classical notion of efficient algorithms: Algorithms that used to be considered efficient, according to polynomial-time characterization, may no longer be adequate for solving today»s problems. It is not just desirable, but essential, that efficient algorithms should be scalable. In other words, their complexity should be nearly linear or sub-linear with respect to the problem size. Thus, scalability, not just polynomial-time computability, should be elevated as the central complexity notion for characterizing efficient computation. In this talk, I will highlight a family of fundamental algorithmic techniques for designing provably-good scalable algorithms: (1) scalable primitives and scalable reduction, (2) spectral approximation of graphs and matrices, (3) sparsification by multilevel structures, (4) advanced sampling, (5) local network exploration. For the first, I will focus on the emerging Laplacian Paradigm, that has led to breakthroughs in scalable algorithms for several fundamental problems in network analysis, machine learning, and scientific computing. I will then illustrate these algorithmic techniques with four recent applications: (1) sampling from graphic models, (2) network centrality approximation, (3) social-influence analysis (4) local clustering. Mathematical and algorithmic solution to these problems exemplify the fusion of combinatorial, numerical, and statistical thinking in data and network analysis.",
                "call-number": "10.1145/3159652.3160602",
                "collection-title": "WSDM '18",
                "container-title": "Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining",
                "DOI": "10.1145/3159652.3160602",
                "event-place": "Marina Del Rey, CA, USA",
                "ISBN": "9781450355810",
                "keyword": "machine learning, big data, local algorithms, scalable algorithms, network sciences, advanced sampling, graph sparsification",
                "number-of-pages": "2",
                "page": "6–7",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Scalable Algorithms in the Age of Big Data and Network Sciences: Characterization, Primitives, and Techniques",
                "URL": "https://doi.org/10.1145/3159652.3160602"
            }
        },
        {
            "10.1145/3500931.3500938": {
                "id": "10.1145/3500931.3500938",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Liu",
                        "given": "Dan"
                    },
                    {
                        "family": "Xu",
                        "given": "Han"
                    },
                    {
                        "family": "Liu",
                        "given": "Bo"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            10,
                            29
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2021,
                            10,
                            29
                        ]
                    ]
                },
                "abstract": "This paper analyzes the core technologies in the construction of the epidemic prevention and control platform, including big data technology, artificial intelligence technology, Echarts data visualization technology, Python language, YAPI technology. Key points of application in the construction plan include cross-industry big data platform, macro-oriented prevention and control model, real-time epidemic prevention and control map, clinical symptom screening system and multi-platform information linkage. This paper studies the optimization measures to improve the comprehensive sharing of information, standardize the prevention and control information data standards, and optimize the information collection interface. The purpose is to accumulate corresponding value data and constantly improve the content of the epidemic prevention and control platform construction plan.",
                "call-number": "10.1145/3500931.3500938",
                "collection-title": "ISAIMS 2021",
                "container-title": "Proceedings of the 2nd International Symposium on Artificial Intelligence for Medicine Sciences",
                "DOI": "10.1145/3500931.3500938",
                "event-place": "Beijing, China",
                "ISBN": "9781450395588",
                "keyword": "Artificial intelligence technology, Big data technology, Epidemic Prevention and Control Platform",
                "number-of-pages": "7",
                "page": "28–34",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Big data and artificial intelligence platform construction plan for epidemic prevention and control",
                "URL": "https://doi.org/10.1145/3500931.3500938"
            }
        },
        {
            "10.1145/2684200.2684333": {
                "id": "10.1145/2684200.2684333",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Hassan",
                        "given": "Sabri"
                    },
                    {
                        "family": "Pernul",
                        "given": "Günther"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2014,
                            12,
                            4
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2014,
                            12,
                            4
                        ]
                    ]
                },
                "abstract": "We are currently living in the age of big data with ever growing volumes of heterogeneous and fast moving data. Whether they are mobile devices, internal or external systems or cloud-based systems data is generated, stored, processed and distributed in many different systems. This leads to various information security and privacy risks. To address these issues, especially from the viewpoint of data management and data governance we propose a conceptual analysis model. Thereby, our model takes into account the dimension of data storage location together with their respective risks and costs while considering the strategic value and sensitivity of data assets. For demonstrating our approach we developed a visual analytics web application which is based on parallel sets visualizations. By being able to interactively explore the analysis dimensions users are supported in developing enhanced situational awareness for making decisions in the context of secure and economical data storage.",
                "call-number": "10.1145/2684200.2684333",
                "collection-title": "iiWAS '14",
                "container-title": "Proceedings of the 16th International Conference on Information Integration and Web-based Applications & Services",
                "DOI": "10.1145/2684200.2684333",
                "event-place": "Hanoi, Viet Nam",
                "ISBN": "9781450330015",
                "keyword": "Distributed Systems, Data Management, Data Governance, Cloud Storage, Big Data, Visual Analytics, Information Security",
                "number-of-pages": "5",
                "page": "180–184",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Efficiently Managing the Security and Costs of Big Data Storage using Visual Analytics",
                "URL": "https://doi.org/10.1145/2684200.2684333"
            }
        },
        {
            "10.1145/3482632.3483150": {
                "id": "10.1145/3482632.3483150",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Zhang",
                        "given": "Haiyan"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            9,
                            24
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2021,
                            9,
                            24
                        ]
                    ]
                },
                "abstract": "With the development of big data's Japanese teaching reform in colleges and universities, the evaluation of Japanese teaching quality in colleges and universities has become more and more important. The evaluation of Japanese teaching quality in colleges and universities involves all aspects of teaching activities and the vital interests of teachers and students. The scientific and reasonable evaluation of Japanese teaching in colleges and universities not only provides a good foundation for the management of Japanese teaching in colleges and universities. It is also of great significance to the development of teachers and students' ability. In order to improve the evaluation of teachers' Japanese teaching quality and provide reasonable reference for the examination of education management department, this paper uses the relevant theory and SPSS software of factor analysis. The data of Japanese teaching quality evaluation of college students are analyzed statistically, and the method of data statistical analysis has certain application prospect and space.",
                "call-number": "10.1145/3482632.3483150",
                "collection-title": "ICISCAE 2021",
                "container-title": "2021 4th International Conference on Information Systems and Computer Aided Education",
                "DOI": "10.1145/3482632.3483150",
                "event-place": "Dalian, China",
                "ISBN": "9781450390255",
                "number-of-pages": "5",
                "page": "1356–1360",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Analysis of Japanese Teaching Data Based on Big Data",
                "URL": "https://doi.org/10.1145/3482632.3483150"
            }
        },
        {
            "10.1145/2925995.2926010": {
                "id": "10.1145/2925995.2926010",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Jäger",
                        "given": "Alexandra"
                    },
                    {
                        "family": "Breu",
                        "given": "Ruth"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            7,
                            25
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2016,
                            7,
                            25
                        ]
                    ]
                },
                "abstract": "Data quality is of utmost importance in large product databases. This is especially true for food products, since potentially health-critical data is contained. With growing database size, manual quality assurance becomes infeasible. GS1 Sync, governed by GS1 Austria, is rapidly becoming the largest national food product database. In order to support manual quality assurance, we have conceptualized a process to conduct product data quality assurance in an automatic way, based on defining rules and classifying product data. In order to evaluate our approach, we have implemented a prototype and performed a proof-of-concept. Although our research is still a work-in-progress, we were able to show that our approach is able to find a substantial number of issues that did not appear during manual control.",
                "call-number": "10.1145/2925995.2926010",
                "collection-number": "19",
                "collection-title": "KMO '16",
                "container-title": "Proceedings of the The 11th International Knowledge Management in Organizations Conference on The changing face of Knowledge Management Impacting Society",
                "DOI": "10.1145/2925995.2926010",
                "event-place": "Hagen, Germany",
                "ISBN": "9781450340649",
                "keyword": "GS1 Sync, Product Data Classification, Product Data Quality",
                "number": "Article 19",
                "number-of-pages": "6",
                "page": "1–6",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Providing Support for Automated Product Data Quality Assurance: A Case Study",
                "URL": "https://doi.org/10.1145/2925995.2926010"
            }
        },
        {
            "10.1145/2107536.2107538": {
                "id": "10.1145/2107536.2107538",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Müller",
                        "given": "Heiko"
                    },
                    {
                        "family": "Freytag",
                        "given": "Johann-Christoph"
                    },
                    {
                        "family": "Leser",
                        "given": "Ulf"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2012,
                            3,
                            2
                        ]
                    ]
                },
                "abstract": "In many domains, data cleaning is hampered by our limited ability to specify a comprehensive set of integrity constraints to assist in identification of erroneous data. An alternative approach to improve data quality is to exploit different data sources that contain information about the same set of objects. Such overlapping sources highlight hot-spots of poor data quality through conflicting data values and immediately provide alternative values for conflict resolution. In order to derive a dataset of high quality, we can merge the overlapping sources based on a quality assessment of the conflicting values. The quality of the resulting dataset, however, is highly dependent on our ability to asses the quality of conflicting values effectively.The main objective of this article is to introduce methods that aid the developer of an integrated system over overlapping, but contradicting sources in the task of improving the quality of data. Value conflicts between contradicting sources are often systematic, caused by some characteristic of the different sources. Our goal is to identify such systematic differences and outline data patterns that occur in conjunction with them. Evaluated by an expert user, the regularities discovered provide insights into possible conflict reasons and help to assess the quality of inconsistent values. The contributions of this article are two concepts of systematic conflicts: contradiction patterns and minimal update sequences. Contradiction patterns resemble a special form of association rules that summarize characteristic data properties for conflict occurrence. We adapt existing association rule mining algorithms for mining contradiction patterns. Contradiction patterns, however, view each class of conflicts in isolation, sometimes leading to largely overlapping patterns. Sequences of set-oriented update operations that transform one data source into the other are compact descriptions for all regular differences among the sources. We consider minimal update sequences as the most likely explanation for observed differences between overlapping data sources. Furthermore, the order of operations within the sequences point out potential dependencies between systematic differences. Finding minimal update sequences, however, is beyond reach in practice. We show that the problem already is NP-complete for a restricted set of operations. In the light of this intractability result, we present heuristics that lead to convincing results for all examples we considered.",
                "call-number": "10.1145/2107536.2107538",
                "collection-number": "15",
                "container-title": "J. Data and Information Quality",
                "DOI": "10.1145/2107536.2107538",
                "ISSN": "1936-1955",
                "issue": "4",
                "keyword": "quality assessment, Conflict resolution, semantic distance measure, data cleaning",
                "number": "Article 15",
                "number-of-pages": "38",
                "page": "1–38",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "February 2012",
                "title": "Improving data quality by source analysis",
                "URL": "https://doi.org/10.1145/2107536.2107538",
                "volume": "2"
            }
        },
        {
            "10.1145/3269206.3269208": {
                "id": "10.1145/3269206.3269208",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Nidzwetzki",
                        "given": "Jan Kristof"
                    },
                    {
                        "family": "Güting",
                        "given": "Ralf Hartmut"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            10,
                            17
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            10,
                            17
                        ]
                    ]
                },
                "abstract": "BBoxDB is a distributed and highly available key-bounding-box-value store which enhances the classical key-value data model with an axis-parallel bounding box. The bounding box describes the location of the values in an n-dimensional space, and enables BBoxDB to efficiently distribute multi-dimensional data across a cluster of nodes. Well-known geometric algorithms (such as the K-D Tree) are used to create distribution regions (multi-dimensional shards). Distribution regions are created dynamically, based on the stored data. BBoxDB stores data of multiple tables co-partitioned, which enables efficient distributed spatial joins. Spatial joins on co-partitioned tables can be executed without data shuffling between nodes. A two-level index structure is employed to retrieve stored data quickly. We demonstrate the interaction with the system, the dynamic creation of distribution regions and the data redistribution feature of BBoxDB.",
                "call-number": "10.1145/3269206.3269208",
                "collection-title": "CIKM '18",
                "container-title": "Proceedings of the 27th ACM International Conference on Information and Knowledge Management",
                "DOI": "10.1145/3269206.3269208",
                "event-place": "Torino, Italy",
                "ISBN": "9781450360142",
                "keyword": "multi-dimensional data store, spatial join, multi-dimensional big data, co-partitioned data, distributed system",
                "number-of-pages": "4",
                "page": "1867–1870",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "BBoxDB - A Scalable Data Store for Multi-Dimensional Big Data",
                "URL": "https://doi.org/10.1145/3269206.3269208"
            }
        },
        {
            "10.1145/2627534.2627560": {
                "id": "10.1145/2627534.2627560",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Whitworth",
                        "given": "Jeff"
                    },
                    {
                        "family": "Suthaharan",
                        "given": "Shan"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2014,
                            4,
                            17
                        ]
                    ]
                },
                "abstract": "The data source that produces data continuously in high volume and high velocity with large varieties of data types creates Big Data, and causes problems and challenges to Machine Learning (ML) techniques that help extract, analyze and visualize important information. To overcome these problems and challenges, we propose to make use of the hybrid networking model that consists of multiple components such as Hadoop distributed file system (HDFS), cloud storage system, security module and ML unit. Processing of Big Data in this networking environment with ML technique requires user interaction and additional storage hence some artificial delay between the arrivals of data domains through external storage can help HDFSto process the Big Data efficiently. To address this problem we suggest using public cloud for data storage which will induce meaningful time delay to the data while making use of its storage capability. However, the use of public cloud will lead to security vulnerability to the data transmission and storage. Therefore, we need some form of security algorithm that provides a flexible key-based encryption technique that can provide tradeoffs between time-delay, security strength and storage risks. In this paper we propose a model for using public cloud provider trust levels to select encryption types for data storage for use within a Big Data analytics network topology.",
                "call-number": "10.1145/2627534.2627560",
                "container-title": "SIGMETRICS Perform. Eval. Rev.",
                "DOI": "10.1145/2627534.2627560",
                "ISSN": "0163-5999",
                "issue": "4",
                "keyword": "retrievability, hybrid cloud, encryption, big data, machine learning",
                "number-of-pages": "4",
                "page": "82–85",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "March 2014",
                "title": "Security problems and challenges in a machine learning-based hybrid big data processing network systems",
                "URL": "https://doi.org/10.1145/2627534.2627560",
                "volume": "41"
            }
        },
        {
            "10.1145/3209582.3209599": {
                "id": "10.1145/3209582.3209599",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Gong",
                        "given": "Xiaowen"
                    },
                    {
                        "family": "Shroff",
                        "given": "Ness"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            6,
                            26
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            6,
                            26
                        ]
                    ]
                },
                "abstract": "Mobile data crowdsourcing has found a broad range of applications (e.g., spectrum sensing, environmental monitoring) by leveraging the \"wisdom\" of a potentially large crowd of \"workers\" (i.e., mobile users). A key metric of crowdsourcing is data accuracy, which relies on the quality of the participating workers' data (e.g., the probability that the data is equal to the ground truth). However, the data quality of a worker can be its own private information (which the worker learns, e.g., based on its location) that it may have incentive to misreport, which can in turn mislead the crowdsourcing requester about the accuracy of the data. This issue is further complicated by the fact that the worker can also manipulate its effort made in the crowdsourcing task and the data reported to the requester, which can also mislead the requester. In this paper, we devise truthful crowdsourcing mechanisms for Quality, Effort, and Data Elicitation (QEDE), which incentivize strategic workers to truthfully report their private worker quality and data to the requester, and make truthful effort as desired by the requester. The truthful design of the QEDE mechanisms overcomes the lack of ground truth and the coupling in the joint elicitation of worker quality, effort, and data. Under the QEDE mechanisms, we characterize the socially optimal and the requester's optimal task assignments, and analyze their performance. We show that the requester's optimal assignment is determined by the largest \"virtual valuation\" rather than the highest quality among workers, which depends on the worker's quality and the quality's distribution. We evaluate the QEDE mechanisms using simulations which demonstrate the truthfulness of the mechanisms and the performance of the optimal task assignments.",
                "call-number": "10.1145/3209582.3209599",
                "collection-title": "Mobihoc '18",
                "container-title": "Proceedings of the Eighteenth ACM International Symposium on Mobile Ad Hoc Networking and Computing",
                "DOI": "10.1145/3209582.3209599",
                "event-place": "Los Angeles, CA, USA",
                "ISBN": "9781450357708",
                "keyword": "Mobile data crowdsourcing, incentive mechanism, data quality",
                "number-of-pages": "10",
                "page": "161–170",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Incentivizing Truthful Data Quality for Quality-Aware Mobile Data Crowdsourcing",
                "URL": "https://doi.org/10.1145/3209582.3209599"
            }
        },
        {
            "10.1145/3424978.3425002": {
                "id": "10.1145/3424978.3425002",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Liu",
                        "given": "Jianhua"
                    },
                    {
                        "family": "Gao",
                        "given": "Taotao"
                    },
                    {
                        "family": "Du",
                        "given": "Yunxia"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2020,
                            10,
                            20
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2020,
                            10,
                            20
                        ]
                    ]
                },
                "abstract": "Because the traffic flow on the urban road network continues to gather to the congested area, which exceeds the regional traffic carrying capacity, causing the traffic condition to deteriorate gradually. Based on the spatial proximity characteristics of the urban road network and relying on big data analysis technology, a traffic balance control method based on the urban regional capacity is designed and implemented. Through the collected data of urban vehicle travel, the correlation between regional capacity and traffic state is analyzed, the maximum bearing capacity of the region is determined, and the key adjacent areas that affect the regional state are identified. In the process of actual traffic control, real-time traffic flow data of urban road network is collected. When the regional traffic volume is close to the capacity threshold, the traffic flow in key adjacent areas is controlled and dynamically allocated to effectively prevent the continuous accumulation and state deterioration of regional traffic flow. Finally, the algorithm simulation test based on big data analysis platform is conducted. The results show that the variance of traffic volume in each area is reduced by 15% and the average congestion duration is reduced by 12%.",
                "call-number": "10.1145/3424978.3425002",
                "collection-number": "24",
                "collection-title": "CSAE 2020",
                "container-title": "Proceedings of the 4th International Conference on Computer Science and Application Engineering",
                "DOI": "10.1145/3424978.3425002",
                "event-place": "Sanya, China",
                "ISBN": "9781450377720",
                "keyword": "Dynamic distribution, Big data analysis, Traffic state, Equilibrium control",
                "number": "Article 24",
                "number-of-pages": "5",
                "page": "1–5",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Research on Equilibrium Control Method of Urban Road Network Based on Big Data",
                "URL": "https://doi.org/10.1145/3424978.3425002"
            }
        },
        {
            "10.1145/2628194.2628231": {
                "id": "10.1145/2628194.2628231",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Ceci",
                        "given": "Michelangelo"
                    },
                    {
                        "family": "Cassavia",
                        "given": "Nunziato"
                    },
                    {
                        "family": "Corizzo",
                        "given": "Roberto"
                    },
                    {
                        "family": "Dicosta",
                        "given": "Pietro"
                    },
                    {
                        "family": "Malerba",
                        "given": "Donato"
                    },
                    {
                        "family": "Maria",
                        "given": "Gaspare"
                    },
                    {
                        "family": "Masciari",
                        "given": "Elio"
                    },
                    {
                        "family": "Pastura",
                        "given": "Camillo"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2014,
                            7,
                            7
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2014,
                            7,
                            7
                        ]
                    ]
                },
                "abstract": "The problem of accurately predicting the energy production from renewable sources has recently received an increasing attention from both the industrial and the research communities. It presents several challenges, such as facing with the rate data are provided by sensors, the heterogeneity of the data collected, power plants efficiency, as well as uncontrollable factors, such as weather conditions and user consumption profiles. In this paper we describe Vi-POC (Virtual Power Operating Center), a project conceived to assist energy producers and decision makers in the energy market. In this paper we present the Vi-POC project and how we face with challenges posed by the specific application. The solutions we propose have roots both in big data management and in stream data mining.",
                "call-number": "10.1145/2628194.2628231",
                "collection-title": "IDEAS '14",
                "container-title": "Proceedings of the 18th International Database Engineering & Applications Symposium",
                "DOI": "10.1145/2628194.2628231",
                "event-place": "Porto, Portugal",
                "ISBN": "9781450326278",
                "number-of-pages": "4",
                "page": "326–329",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Innovative power operating center management exploiting big data techniques",
                "URL": "https://doi.org/10.1145/2628194.2628231"
            }
        },
        {
            "10.1109/CCGrid.2013.100": {
                "id": "10.1109/CCGrid.2013.100",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Tracey",
                        "given": "David"
                    },
                    {
                        "family": "Sreenan",
                        "given": "Cormac"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2013,
                            5,
                            13
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2013,
                            5,
                            13
                        ]
                    ]
                },
                "abstract": "Wireless Sensor Networks (WSNs) increasingly enable applications and services to interact with the physical world. Such services may be located across the Internet from the sensing network. Cloud services and big data approaches may be used to store and analyse this data to improve scalability and availability, which will be required for the billions of devices envisaged in the Internet of Things (IoT). The potential of WSNs is limited by the relatively low number deployed and the difficulties imposed by their heterogeneous nature and limited (or proprietary) development environments and interfaces. This paper proposes a set of requirements for achieving a pervasive, integrated information system of WSNs and associated services. It also presents an architecture which is termed holistic as it considers the flow of the data from sensors through to services. The architecture provides a set of abstractions for the different types of sensors and services. It has been designed for implementation on a resource constrained node and to be extensible to server environments. This paper presents a 'C' implementation of the core architecture, including services on Linux and Contiki (using the Constrained Application Protocol (CoAP)) and a Linux service to integrate with the Hadoop HBase datastore.",
                "call-number": "10.1109/CCGrid.2013.100",
                "collection-title": "CCGRID '13",
                "container-title": "Proceedings of the 13th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing",
                "DOI": "10.1109/CCGrid.2013.100",
                "event-place": "Delft, Netherlands",
                "ISBN": "9780768549965",
                "keyword": "cloud computing, tuple space, protocols, information model, wireless sensor networks, big data",
                "number-of-pages": "8",
                "page": "546–553",
                "publisher": "IEEE Press",
                "title": "A holistic architecture for the internet of things, sensing services and big data",
                "URL": "https://doi.org/10.1109/CCGrid.2013.100"
            }
        },
        {
            "10.1145/3164541.3164560": {
                "id": "10.1145/3164541.3164560",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Nakashima",
                        "given": "Kenji"
                    },
                    {
                        "family": "Kon",
                        "given": "Joichiro"
                    },
                    {
                        "family": "Yamaguchi",
                        "given": "Saneyasu"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            1,
                            5
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            1,
                            5
                        ]
                    ]
                },
                "abstract": "For utilizing private big data, such as DNA data, encryption and anonymization are essential for preserving privacy. However, encryption and anonymization sometimes increase the size of data largely. Thus, increasing I/O performance for large-scale data is important. Caching data with Solid State Drive (SSD) is a popular method for by using SSD as cache, which is a proposed method for improving access performance of data in Hard Disk Drive (HDD). In this paper, we focus on SSD cache and discuss a method for improving I/O performance of a big data application using SSD cache. First, we evaluate the basic I/O performance with and without SSD cache. Second, we reveal the behavior of flash cache. Third, we propose a method for improving I/O performance of a large scale DNA application with SSD cache. Fourth, we evaluate the proposed method and demonstrate that the method can improve I/O performance effectively.",
                "call-number": "10.1145/3164541.3164560",
                "collection-number": "90",
                "collection-title": "IMCOM '18",
                "container-title": "Proceedings of the 12th International Conference on Ubiquitous Information Management and Communication",
                "DOI": "10.1145/3164541.3164560",
                "event-place": "Langkawi, Malaysia",
                "ISBN": "9781450363853",
                "keyword": "anonymized analysis, Big data, SSD cache",
                "number": "Article 90",
                "number-of-pages": "7",
                "page": "1–7",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "I/O Performance Improvement of Secure Big Data Analyses with Application Support on SSD Cache",
                "URL": "https://doi.org/10.1145/3164541.3164560"
            }
        },
        {
            "10.1145/3418896": {
                "id": "10.1145/3418896",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Christophides",
                        "given": "Vassilis"
                    },
                    {
                        "family": "Efthymiou",
                        "given": "Vasilis"
                    },
                    {
                        "family": "Palpanas",
                        "given": "Themis"
                    },
                    {
                        "family": "Papadakis",
                        "given": "George"
                    },
                    {
                        "family": "Stefanidis",
                        "given": "Kostas"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2020,
                            12,
                            6
                        ]
                    ]
                },
                "abstract": "One of the most critical tasks for improving data quality and increasing the reliability of data analytics is Entity Resolution (ER), which aims to identify different descriptions that refer to the same real-world entity. Despite several decades of research, ER remains a challenging problem. In this survey, we highlight the novel aspects of resolving Big Data entities when we should satisfy more than one of the Big Data characteristics simultaneously (i.e., Volume and Velocity with Variety). We present the basic concepts, processing steps, and execution strategies that have been proposed by database, semantic Web, and machine learning communities in order to cope with the loose structuredness, extreme diversity, high speed, and large scale of entity descriptions used by real-world applications. We provide an end-to-end view of ER workflows for Big Data, critically review the pros and cons of existing methods, and conclude with the main open research directions.",
                "call-number": "10.1145/3418896",
                "collection-number": "127",
                "container-title": "ACM Comput. Surv.",
                "DOI": "10.1145/3418896",
                "ISSN": "0360-0300",
                "issue": "6",
                "keyword": "batch and incremental entity resolution workflows, crowdsourcing, Entity blocking and matching, block processing, strongly and nearly similar entities, deep learning",
                "number": "Article 127",
                "number-of-pages": "42",
                "page": "1–42",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "November 2021",
                "title": "An Overview of End-to-End Entity Resolution for Big Data",
                "URL": "https://doi.org/10.1145/3418896",
                "volume": "53"
            }
        },
        {
            "10.1145/3380688.3380702": {
                "id": "10.1145/3380688.3380702",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Amaechi",
                        "given": "Eloanyi Samson"
                    },
                    {
                        "family": "Van Pham",
                        "given": "Hai"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2020,
                            1,
                            17
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2020,
                            1,
                            17
                        ]
                    ]
                },
                "abstract": "Current developments in technologies occupy a central role in weather forecasting and the Internet-of-Things for both organizations and the IT sector. Big-data analytics and the classification of data (derived from many sources including importantly the Internet-of-Things) provides significant information on which organizations can optimize their current and future business planning. This paper considers convolutional neural networks and data classification as it relates to big-data and presents a novel approach to weather forecasting. The proposed approach targets the enhancement of convolutional neural networks and data classification to enable improved classification performance for big-data classifiers. Our contribution combines the positive benefits of convolutional neural networks with expert knowledge represented by fuzzy rules for prepared data sets in time series, the aim being to achieve improvements in the predictive quality of weather forecasting. Experimental testing demonstrates that the proposed enhanced convolutional network approach achieves a high level of accuracy in weather forecasting when compared to alternative methods evaluated.",
                "call-number": "10.1145/3380688.3380702",
                "collection-title": "ICMLSC 2020",
                "container-title": "Proceedings of the 4th International Conference on Machine Learning and Soft Computing",
                "DOI": "10.1145/3380688.3380702",
                "event-place": "Haiphong City, Viet Nam",
                "ISBN": "9781450376310",
                "keyword": "Big data, Internet of Things (loT), Convolutional Neural Network, Neural Network, fuzzy rules",
                "number-of-pages": "5",
                "page": "25–29",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Enhancement of Convolutional Neural Networks Classifier Performance in the Classification of IoT Big Data",
                "URL": "https://doi.org/10.1145/3380688.3380702"
            }
        },
        {
            "10.14778/2536222.2536235": {
                "id": "10.14778/2536222.2536235",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Bellamkonda",
                        "given": "Srikanth"
                    },
                    {
                        "family": "Li",
                        "given": "Hua-Gang"
                    },
                    {
                        "family": "Jagtap",
                        "given": "Unmesh"
                    },
                    {
                        "family": "Zhu",
                        "given": "Yali"
                    },
                    {
                        "family": "Liang",
                        "given": "Vince"
                    },
                    {
                        "family": "Cruanes",
                        "given": "Thierry"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2013,
                            8,
                            1
                        ]
                    ]
                },
                "abstract": "This paper showcases some of the newly introduced parallel execution methods in Oracle RDBMS. These methods provide highly scalable and adaptive evaluation for the most commonly used SQL operations - joins, group-by, rollup/cube, grouping sets, and window functions. The novelty of these techniques is their use of multi-stage parallelization models, accommodation of optimizer mistakes, and the runtime parallelization and data distribution decisions. These parallel plans adapt based on the statistics gathered on the real data at query execution time. We realized enormous performance gains from these adaptive parallelization techniques. The paper also discusses our approach to parallelize queries with operations that are inherently serial. We believe all these techniques will make their way into big data analytics and other massively parallel database systems.",
                "call-number": "10.14778/2536222.2536235",
                "container-title": "Proc. VLDB Endow.",
                "DOI": "10.14778/2536222.2536235",
                "ISSN": "2150-8097",
                "issue": "11",
                "number-of-pages": "12",
                "page": "1102–1113",
                "publisher": "VLDB Endowment",
                "source": "August 2013",
                "title": "Adaptive and big data scale parallel execution in oracle",
                "URL": "https://doi.org/10.14778/2536222.2536235",
                "volume": "6"
            }
        },
        {
            "10.5555/2390524.2390640": {
                "id": "10.5555/2390524.2390640",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Zhang",
                        "given": "Ce"
                    },
                    {
                        "family": "Niu",
                        "given": "Feng"
                    },
                    {
                        "family": "Ré",
                        "given": "Christopher"
                    },
                    {
                        "family": "Shavlik",
                        "given": "Jude"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2012,
                            7,
                            8
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2012,
                            7,
                            8
                        ]
                    ]
                },
                "abstract": "Classically, training relation extractors relies on high-quality, manually annotated training data, which can be expensive to obtain. To mitigate this cost, NLU researchers have considered two newly available sources of less expensive (but potentially lower quality) labeled data from distant supervision and crowd sourcing. There is, however, no study comparing the relative impact of these two sources on the precision and recall of post-learning answers. To fill this gap, we empirically study how state-of-the-art techniques are affected by scaling these two sources. We use corpus sizes of up to 100 million documents and tens of thousands of crowd-source labeled examples. Our experiments show that increasing the corpus size for distant supervision has a statistically significant, positive impact on quality (F1 score). In contrast, human feedback has a positive and statistically significant, but lower, impact on precision and recall.",
                "call-number": "10.5555/2390524.2390640",
                "collection-title": "ACL '12",
                "container-title": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1",
                "event-place": "Jeju Island, Korea",
                "number-of-pages": "10",
                "page": "825–834",
                "publisher": "Association for Computational Linguistics",
                "publisher-place": "USA",
                "title": "Big data versus the crowd: looking for relationships in all the right places"
            }
        },
        {
            "10.1145/3463531.3463539": {
                "id": "10.1145/3463531.3463539",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Leo Handoko",
                        "given": "Bambang"
                    },
                    {
                        "family": "Edward Riantono",
                        "given": "Ignatius"
                    },
                    {
                        "family": "Wigna Sunarto",
                        "given": "Felicia"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            4,
                            14
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2021,
                            4,
                            14
                        ]
                    ]
                },
                "abstract": "During the COVID-19 pandemics, auditors are required to do remote audit from their home or office. With the limitation of movement of the auditor, big data is the solution for the auditor as the data availability in large amounts in order to make the right decision where the process can be done quickly in effective and efficient ways. In order to help auditors do their jobs, it requires a supporting factor on doing remote audit which is technology, organization and environment. The possibilities to do audit with population based increased the fraud detection and quality of audit generated by the audit process. This article intends to define the differences, roles, fraud detection and impact of big data analytics on audit quality generated from remote audit process. This study used questionnaire to the respondents who have criteria of having experience in audit process, in order to determine which factors influence remote audit process.",
                "call-number": "10.1145/3463531.3463539",
                "collection-title": "ICETT 2021",
                "container-title": "2021 7th International Conference on Education and Training Technologies",
                "DOI": "10.1145/3463531.3463539",
                "event-place": "Macau, China",
                "ISBN": "9781450389662",
                "keyword": "TOE Framework, Big Data, Analysis, Remote, Audit",
                "number-of-pages": "7",
                "page": "53–59",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Determinants Affecting Intention of Use of Big Data Analytics on Remote Audits: TOE Framework Approach",
                "URL": "https://doi.org/10.1145/3463531.3463539"
            }
        },
        {
            "10.5555/3172795.3172829": {
                "id": "10.5555/3172795.3172829",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Huang",
                        "given": "Yu"
                    },
                    {
                        "family": "Chiang",
                        "given": "Fei"
                    },
                    {
                        "family": "Maier",
                        "given": "Albert"
                    },
                    {
                        "family": "Petitclerc",
                        "given": "Martin"
                    },
                    {
                        "family": "Saillet",
                        "given": "Yannick"
                    },
                    {
                        "family": "Spisic",
                        "given": "Damir"
                    },
                    {
                        "family": "Zuzarte",
                        "given": "Calisto"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            11,
                            6
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            11,
                            6
                        ]
                    ]
                },
                "abstract": "Deduplication is a costly and tedious task that involves identifying duplicate records in a dataset. High duplication rates lead to poor data quality, where data ambiguity occurs as to whether two records refer to the same entity. Existing deduplication techniques compare a set of attribute values, and verify whether given similarity thresholds are satisfied. While potential duplicate records are identified, these techniques do not provide users with any information about the degree of duplication, i.e., the varying levels of closeness among the attribute values and between records that define the duplicates.In this paper, we present a duplication metric that quantifies the level of duplication for an attribute value, and within an attribute. This metric can be used by analysts to understand the distribution and similarity of values during the data cleaning process. We present a deduplication framework that differentiates terms during similarity matching step, and is agnostic to the ordering of values within a record. We compare our framework against two existing approaches, and show that we achieve improved accuracy and performance over real data collections.",
                "call-number": "10.5555/3172795.3172829",
                "collection-title": "CASCON '17",
                "container-title": "Proceedings of the 27th Annual International Conference on Computer Science and Software Engineering",
                "event-place": "Markham, Ontario, Canada",
                "number-of-pages": "7",
                "page": "272–278",
                "publisher": "IBM Corp.",
                "publisher-place": "USA",
                "title": "Quantifying duplication to improve data quality"
            }
        },
        {
            "10.1145/3495018.3495293": {
                "id": "10.1145/3495018.3495293",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Hu",
                        "given": "Bofei"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            10,
                            23
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2021,
                            10,
                            23
                        ]
                    ]
                },
                "abstract": "The distributed optical fiber sensor has the unique advantages of small size, light weight, electrical insulation, anti-electromagnetic interference and long-distance transmission and sensing. It is widely used in the fields of transportation, industry and mining, national defense, military, biological medical, aerospace and so on. This paper mainly analyzes the status quo and significance of using big data technology to solve the problems existing in distributed optical fiber sensing power cable and pipeline safety monitoring. Considering the characteristics of distributed fiber optic sensing data and big data processing technology, an overall framework of distributed fiber optic sensing big data storage, management and processing is constructed, which adopts a three-layer architecture of data acquisition layer, data storage management layer and data processing layer, and combined with the actual application scenarios to build a big data cluster.",
                "call-number": "10.1145/3495018.3495293",
                "collection-title": "AIAM2021",
                "container-title": "2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture",
                "DOI": "10.1145/3495018.3495293",
                "event-place": "Manchester, United Kingdom",
                "ISBN": "9781450385046",
                "number-of-pages": "5",
                "page": "868–872",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Distributed Sensing of Animation Art Style under Big Data Technology",
                "URL": "https://doi.org/10.1145/3495018.3495293"
            }
        },
        {
            "10.1145/3325773.3325779": {
                "id": "10.1145/3325773.3325779",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Li",
                        "given": "Zhiyong"
                    },
                    {
                        "family": "Li",
                        "given": "Tao"
                    },
                    {
                        "family": "Zhu",
                        "given": "Fangdong"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            3,
                            23
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            3,
                            23
                        ]
                    ]
                },
                "abstract": "Password authentication is the most widely used authentication method in information systems. The traditional proactive password detection method is generally implemented by counting password length, character class number and computing password information entropy to improve password security. However, passwords that pass proactive password detection do not represent that they are secure. In this paper, based on the research of the characteristics of password distribution under big data, we propose an online password guessing method, which collects a dataset of guessing passwords composed of weak passwords, high frequency passwords and personal information related passwords. It is used to guess the 13k password dataset leaked in China's largest ticketing website, China Railways 12306 website. The experimental results show that even if our guess object has passed the strict proactive password detection, we can construct a guessing password dataset contain only 100 passwords, and effectively guess 4.84% of the passwords.",
                "call-number": "10.1145/3325773.3325779",
                "collection-title": "ISMSI 2019",
                "container-title": "Proceedings of the 2019 3rd International Conference on Intelligent Systems, Metaheuristics & Swarm Intelligence",
                "DOI": "10.1145/3325773.3325779",
                "event-place": "Male, Maldives",
                "ISBN": "9781450372114",
                "keyword": "Password security, proactive password check, password guessing attack",
                "number-of-pages": "4",
                "page": "59–62",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "An Online Password Guessing Method Based on Big Data",
                "URL": "https://doi.org/10.1145/3325773.3325779"
            }
        },
        {
            "10.14778/2367502.2367512": {
                "id": "10.14778/2367502.2367512",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Rabl",
                        "given": "Tilmann"
                    },
                    {
                        "family": "Gómez-Villamor",
                        "given": "Sergio"
                    },
                    {
                        "family": "Sadoghi",
                        "given": "Mohammad"
                    },
                    {
                        "family": "Muntés-Mulero",
                        "given": "Victor"
                    },
                    {
                        "family": "Jacobsen",
                        "given": "Hans-Arno"
                    },
                    {
                        "family": "Mankovskii",
                        "given": "Serge"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2012,
                            8,
                            1
                        ]
                    ]
                },
                "abstract": "As the complexity of enterprise systems increases, the need for monitoring and analyzing such systems also grows. A number of companies have built sophisticated monitoring tools that go far beyond simple resource utilization reports. For example, based on instrumentation and specialized APIs, it is now possible to monitor single method invocations and trace individual transactions across geographically distributed systems. This high-level of detail enables more precise forms of analysis and prediction but comes at the price of high data rates (i.e., big data). To maximize the benefit of data monitoring, the data has to be stored for an extended period of time for ulterior analysis. This new wave of big data analytics imposes new challenges especially for the application performance monitoring systems. The monitoring data has to be stored in a system that can sustain the high data rates and at the same time enable an up-to-date view of the underlying infrastructure. With the advent of modern key-value stores, a variety of data storage systems have emerged that are built with a focus on scalability and high data rates as predominant in this monitoring use case.In this work, we present our experience and a comprehensive performance evaluation of six modern (open-source) data stores in the context of application performance monitoring as part of CA Technologies initiative. We evaluated these systems with data and workloads that can be found in application performance monitoring, as well as, on-line advertisement, power monitoring, and many other use cases. We present our insights not only as performance results but also as lessons learned and our experience relating to the setup and configuration complexity of these data stores in an industry setting.",
                "call-number": "10.14778/2367502.2367512",
                "container-title": "Proc. VLDB Endow.",
                "DOI": "10.14778/2367502.2367512",
                "ISSN": "2150-8097",
                "issue": "12",
                "number-of-pages": "12",
                "page": "1724–1735",
                "publisher": "VLDB Endowment",
                "source": "August 2012",
                "title": "Solving big data challenges for enterprise application performance management",
                "URL": "https://doi.org/10.14778/2367502.2367512",
                "volume": "5"
            }
        },
        {
            "10.1145/3383845.3383900": {
                "id": "10.1145/3383845.3383900",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Wang",
                        "given": "Cuihong"
                    },
                    {
                        "family": "Wang",
                        "given": "Fengzhou"
                    },
                    {
                        "family": "He",
                        "given": "Shu"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2020,
                            1,
                            31
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2020,
                            1,
                            31
                        ]
                    ]
                },
                "abstract": "In the era of big data, a company can collect useful data to obtain relevant information timely, strengthen the cost management of its supply chain, and use intelligent, digital, and sophisticated analysis methods to enhance its core business competitiveness. Firstly, this article analyzes the problems in the supply chain links such as procurement, sales, production, and logistics, and then constructs a model of enterprise supply chain system that combines a big data platform and a supply chain, and designs an enterprise supply chain cost management model in the context of big data. It also focuses on the procedures and measures of cost management in the internal supply chain and external supply chain, providing a reference for cost control in the era of big data.",
                "call-number": "10.1145/3383845.3383900",
                "collection-title": "ICCMB 2020",
                "container-title": "Proceedings of the 2020 the 3rd International Conference on Computers in Management and Business",
                "DOI": "10.1145/3383845.3383900",
                "event-place": "Tokyo, Japan",
                "ISBN": "9781450376778",
                "keyword": "Big data, cost management, supply chain costs",
                "number-of-pages": "6",
                "page": "19–24",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Conceptualization on the Cost Management Model of Enterprise Supply Chain Under the background of Big Data",
                "URL": "https://doi.org/10.1145/3383845.3383900"
            }
        },
        {
            "10.1145/3026480.3026491": {
                "id": "10.1145/3026480.3026491",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Ying",
                        "given": "Yang"
                    },
                    {
                        "family": "Xialing",
                        "given": "Tang"
                    },
                    {
                        "family": "Wei",
                        "given": "Tang"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            1,
                            5
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            1,
                            5
                        ]
                    ]
                },
                "abstract": "With the rapid development of computer and network technology, big data analytics has been more and more widely used in public management. In the process of people obtaining and making use of cultural resources, the data of information behavior which closely related to the cultural resources is produced. These data reflect the cultural needs of people. So if the government takes the analysis of these data into account while purchasing cultural resources, the time efficiency and efficiency of fund will be improved. The governmental culture resources purchase management based on the big data analysis of public information behavior is put forward after research the public information behavior model and the book resources purchase way based on people's reading behavior -PDA(Patron - driven - acquisition).",
                "call-number": "10.1145/3026480.3026491",
                "collection-title": "IC4E '17",
                "container-title": "Proceedings of the 8th International Conference on E-Education, E-Business, E-Management and E-Learning",
                "DOI": "10.1145/3026480.3026491",
                "event-place": "Kuala Lumpur, Malaysia",
                "ISBN": "9781450348218",
                "keyword": "public cultural service, big data, culture resources purchase management, information behavior, PDA",
                "number-of-pages": "4",
                "page": "72–75",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Study on Governmental Cultural Resources Purchase Management Based on Public Information Behavior Big Data",
                "URL": "https://doi.org/10.1145/3026480.3026491"
            }
        },
        {
            "10.1145/2668260.2668264": {
                "id": "10.1145/2668260.2668264",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Al-Quzlan",
                        "given": "Tuqya"
                    },
                    {
                        "family": "Hamdi-Cherif",
                        "given": "Aboubekeur"
                    },
                    {
                        "family": "Kara-Mohamed",
                        "given": "Chafia"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2014,
                            9,
                            15
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2014,
                            9,
                            15
                        ]
                    ]
                },
                "abstract": "To address one of the most challenging ecosystems issues at the cellular level, this paper surveys the fuzzy methods used in gene regulatory networks (GRNs) inference. GRNs represent causal relationships between genes that have a direct influence on the life and the development of living organisms, and provide a useful contribution to the understanding of the cellular functions as well as the mechanisms of diseases. The ecosystems impacted by GRN inference span various levels from cell to society -- globally.",
                "call-number": "10.1145/2668260.2668264",
                "collection-title": "MEDES '14",
                "container-title": "Proceedings of the 6th International Conference on Management of Emergent Digital EcoSystems",
                "DOI": "10.1145/2668260.2668264",
                "event-place": "Buraidah, Al Qassim, Saudi Arabia",
                "ISBN": "9781450327671",
                "keyword": "Link and graph mining, Big data, GRN inference, Fuzzy systems, Soft computing, Gene regulatory networks (GRNs)",
                "number-of-pages": "3",
                "page": "201–203",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Big data fuzzy management methods in gene regulatory networks inference: a review",
                "URL": "https://doi.org/10.1145/2668260.2668264"
            }
        },
        {
            "10.1145/3377672.3378049": {
                "id": "10.1145/3377672.3378049",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Jie",
                        "given": "Liu"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            12,
                            8
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            12,
                            8
                        ]
                    ]
                },
                "abstract": "OBJECTIVE:We scientifically analyze knowledge structure, development stages, research hotspots and research frontiers of tourism big data in China to provide practical and useful references for researchers to understand the research status and development trends of this field.METHODS:Published journal literatures were retrieved. A scientific collaboration analysis was conducted to visualize the relations of authors and institutions. A co-occurrence analysis was used to visualize the network of key words that was classified by the clustering analysis. Burst detection was conducted to visualize emerging words across the entire research field.RESULTS:We retrieved 964 literatures, from which 668 literatures were identified after screening. Wang Dong has published the most papers. A cooperative group of scientific research institutions with Beijing Union University as the core has been formed. The key words were classified into 6 clusters, and the frequency of \"tourism industry\" is the largest, and top 14 key words with the highest emergence intensity were detected.CONCLUSIONS:The literature of tourism big data research in China has been increasing rapidly since 2016. Three cooperative groups with Wang Dong, Liu Ligang and Pan Xinqin as the core respectively were formed, and a cooperative group of scientific research institutions with Beijing Union University as the core has been formed. The research hotspots of tourism big data in China mainly focus on six aspects: tourism industry development, key technologies of tourism big data, global tourism, tourism public service, tourists behavior, problems and countermeasures. The evolution of in this field can basically be divided into three stages: exploration (before 2012), start-up (2013-2016) and rapid development (from 2017 to present).",
                "call-number": "10.1145/3377672.3378049",
                "collection-title": "AMME 2019",
                "container-title": "Proceedings of the 2019 Annual Meeting on Management Engineering",
                "DOI": "10.1145/3377672.3378049",
                "event-place": "Kuala Lumpur, Malaysia",
                "ISBN": "9781450362481",
                "keyword": "Tourism Big Data, Scientific Collaboration, Knowledge Map, Citespace, Co-occurrence Analysis, Burst Detection",
                "number-of-pages": "10",
                "page": "144–153",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Knowledge Maps of Tourism Big Data Research in China Based on Visualization Analysis",
                "URL": "https://doi.org/10.1145/3377672.3378049"
            }
        },
        {
            "10.1145/3469890": {
                "id": "10.1145/3469890",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Lv",
                        "given": "Zhihan"
                    },
                    {
                        "family": "Lou",
                        "given": "Ranran"
                    },
                    {
                        "family": "Feng",
                        "given": "Hailin"
                    },
                    {
                        "family": "Chen",
                        "given": "Dongliang"
                    },
                    {
                        "family": "Lv",
                        "given": "Haibin"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            10,
                            5
                        ]
                    ]
                },
                "abstract": "Two-dimensional1 arrays of bi-component structures made of cobalt and permalloy elliptical dots with thickness of 25 nm, length 1 mm and width of 225 nm, have been prepared by a self-aligned shadow deposition technique. Brillouin light scattering has been exploited to study the frequency dependence of thermally excited magnetic eigenmodes on the intensity of the external magnetic field, applied along the easy axis of the elements.Scientific information technology has been developed rapidly. Here, the purposes are to make people's lives more convenient and ensure information management and classification. The machine learning algorithm is improved to obtain the optimized Light Gradient Boosting Machine (LightGBM) algorithm. Then, an Android-based intelligent support information management system is designed based on LightGBM for the big data analysis and classification management of information in the intelligent support information management system. The system is designed with modules of employee registration and login, company announcement notice, attendance and attendance management, self-service, and daily tools with the company as the subject. Furthermore, the performance of the constructed information management system is analyzed through simulations. Results demonstrate that the training time of the optimized LightGBM algorithm can stabilize at about 100s, and the test time can stabilize at 0.68s. Besides, its accuracy rate can reach 89.24%, which is at least 3.6% higher than other machine learning algorithms. Moreover, the acceleration efficiency analysis of each algorithm suggests that the optimized LightGBM algorithm is suitable for processing large amounts of data; its acceleration effect is more apparent, and its acceleration ratio is higher than other algorithms. Hence, the constructed intelligent support information management system can reach a high accuracy while ensuring the error, with apparent acceleration effect. Therefore, this model can provide an experimental reference for information classification and management in various fields.",
                "call-number": "10.1145/3469890",
                "collection-number": "7",
                "container-title": "ACM Trans. Manage. Inf. Syst.",
                "DOI": "10.1145/3469890",
                "ISSN": "2158-656X",
                "issue": "1",
                "keyword": "accuracy rate, intelligent support information system, lightGBM, big data analysis, Machine learning",
                "number": "Article 7",
                "number-of-pages": "21",
                "page": "1–21",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "March 2022",
                "title": "Novel Machine Learning for Big Data Analytics in Intelligent Support Information Management Systems",
                "URL": "https://doi.org/10.1145/3469890",
                "volume": "13"
            }
        },
        {
            "10.1145/2786983": {
                "id": "10.1145/2786983",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Bartoli",
                        "given": "Alberto"
                    },
                    {
                        "family": "Lorenzo",
                        "given": "Andrea De"
                    },
                    {
                        "family": "Medvet",
                        "given": "Eric"
                    },
                    {
                        "family": "Tarlao",
                        "given": "Fabiano"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2015,
                            10,
                            19
                        ]
                    ]
                },
                "call-number": "10.1145/2786983",
                "collection-number": "13",
                "container-title": "J. Data and Information Quality",
                "DOI": "10.1145/2786983",
                "ISSN": "1936-1955",
                "issue": "4",
                "keyword": "String processing, Programming by example",
                "number": "Article 13",
                "number-of-pages": "4",
                "page": "1–4",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "October 2015",
                "title": "Data Quality Challenge: Toward a Tool for String Processing by Examples",
                "URL": "https://doi.org/10.1145/2786983",
                "volume": "6"
            }
        },
        {
            "10.5555/3417639.3417658": {
                "id": "10.5555/3417639.3417658",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "DePratti",
                        "given": "Roland"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2020,
                            4,
                            1
                        ]
                    ]
                },
                "abstract": "In building curriculum in new areas of computer science, often the tools introduced in the course are an important component. This is especially true in the area of big data, where the complexity of the problems the area tackles is high. In the 4 years since its inception, my big data course has gone through two major redesigns and has settled on a tool set including: the Hadoop platform, Spark processing engine, the Python programming language, Eclipse IDE, and Jupyter Notebooks. Many of the changes were driven by input from professional peers on big data teams, who were struggling with the complexity resulting from the low-level programming model used by MapReduce. Jupyter Notebook, a type of computational notebook, was added to the course to introduce students to the Python programming language. Data scientists and researchers have found computational notebooks an effective tool to manage their work by providing a way to track their thinking process, their code, and conclusions in one web document. To assess the effectiveness of using Jupyter Notebook in a big data course, students' views on the use of computational notebooks and traditional textbooks were captured and statistically analyzed.",
                "call-number": "10.5555/3417639.3417658",
                "container-title": "J. Comput. Sci. Coll.",
                "ISSN": "1937-4771",
                "issue": "8",
                "number-of-pages": "13",
                "page": "208–220",
                "publisher": "Consortium for Computing Sciences in Colleges",
                "publisher-place": "Evansville, IN, USA",
                "source": "April 2020",
                "title": "Jupyter notebooks versus a textbook in a big data course",
                "volume": "35"
            }
        },
        {
            "10.1145/3419635.3419736": {
                "id": "10.1145/3419635.3419736",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Guo",
                        "given": "Jianliang"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2020,
                            10,
                            16
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2020,
                            10,
                            16
                        ]
                    ]
                },
                "abstract": "This article takes English teaching behavior analysis as a breakthrough point, combines big data mining technology, optimizes English teaching plan analysis methods, and uses data mining methods on the basis of inductive analysis to compare the K nearest neighbour algorithm and the support vector machine algorithm. The algorithm is combined with the support vector machine algorithm to obtain the advantages and disadvantages of the new algorithm. A classification prediction model is built. Through the built prediction model, the prediction of English teaching integration is optimized. The classifier's prediction results for accuracy, precision, and recall are 89.63%, 90.63%, and 71.01% respectively. This can provide a basis for educators to optimize decision-making and optimize teaching methods.",
                "call-number": "10.1145/3419635.3419736",
                "collection-title": "CIPAE 2020",
                "container-title": "Proceedings of the 2020 International Conference on Computers, Information Processing and Advanced Education",
                "DOI": "10.1145/3419635.3419736",
                "event-place": "Ottawa, ON, Canada",
                "ISBN": "9781450387729",
                "keyword": "Integration and optimization, Big data, English teaching, Data mining",
                "number-of-pages": "5",
                "page": "504–508",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Empirical Analysis for English Teaching Integration and Optimization Based on Big Data Mining Technology",
                "URL": "https://doi.org/10.1145/3419635.3419736"
            }
        },
        {
            "10.1145/3482632.3483163": {
                "id": "10.1145/3482632.3483163",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Zhang",
                        "given": "Ruolong"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            9,
                            24
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2021,
                            9,
                            24
                        ]
                    ]
                },
                "abstract": "The advent of the era of big data has brought us many shocks. Especially for police teaching, it is both a challenge and an opportunity. In the current police tactics teaching, due to the peculiarities of the troops and the necessity of teaching methods, uniformity is necessary. In the implementation of teaching, some teachers walked into a teaching misunderstanding. At the same time, big data technology has strong application capabilities and promotion power for the ever-increasing scale of police data, which can better improve work efficiency and serve the masses. The purpose of this article is to study the application of big data technology in police tactics teaching. In this paper, by clarifying the police reform brought by big data and responding to the challenges of the big data era, researching data mining technology and algorithms, using outbound data and case data in police data as data sources, the data is preprocessed and multi-dimensional data modeling can be applied to police tactics teaching. This article starts from the actual work of police activities, with both theoretical accumulation and practical exploration. Theory and practice are closely integrated, and from the perspective of practical applications, focusing on the current organic combination of information and communication technology and police work, and finally come up with a solution to the shortcomings of the traditional police work system. Provide ideas for the information integration work carried out by police work. Experimental research shows that the biggest improvement is the risk-avoidance action. Before the use of big data technology, the learning efficiency was only 70.6%, and after the improvement, it was as high as 85.7%, an improvement of 15.1%. Generally speaking, after the improvement, both the basic theory teaching of police tactics and the basic movement teaching have made great progress.",
                "call-number": "10.1145/3482632.3483163",
                "collection-title": "ICISCAE 2021",
                "container-title": "2021 4th International Conference on Information Systems and Computer Aided Education",
                "DOI": "10.1145/3482632.3483163",
                "event-place": "Dalian, China",
                "ISBN": "9781450390255",
                "number-of-pages": "5",
                "page": "1414–1418",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "The Application of Big Data Technology in Police Tactics Teaching",
                "URL": "https://doi.org/10.1145/3482632.3483163"
            }
        },
        {
            "10.1145/2647748": {
                "id": "10.1145/2647748",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Efros",
                        "given": "Alexei A."
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2014,
                            9,
                            1
                        ]
                    ]
                },
                "call-number": "10.1145/2647748",
                "container-title": "Commun. ACM",
                "DOI": "10.1145/2647748",
                "ISSN": "0001-0782",
                "issue": "9",
                "number-of-pages": "1",
                "page": "92",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "September 2014",
                "title": "Portraiture in the age of big data: technical perspective",
                "URL": "https://doi.org/10.1145/2647748",
                "volume": "57"
            }
        },
        {
            "10.1145/2508859.2516701": {
                "id": "10.1145/2508859.2516701",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Dong",
                        "given": "Changyu"
                    },
                    {
                        "family": "Chen",
                        "given": "Liqun"
                    },
                    {
                        "family": "Wen",
                        "given": "Zikai"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2013,
                            11,
                            4
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2013,
                            11,
                            4
                        ]
                    ]
                },
                "abstract": "Large scale data processing brings new challenges to the design of privacy-preserving protocols: how to meet the increasing requirements of speed and throughput of modern applications, and how to scale up smoothly when data being protected is big. Efficiency and scalability become critical criteria for privacy preserving protocols in the age of Big Data. In this paper, we present a new Private Set Intersection (PSI) protocol that is extremely efficient and highly scalable compared with existing protocols. The protocol is based on a novel approach that we call oblivious Bloom intersection. It has linear complexity and relies mostly on efficient symmetric key operations. It has high scalability due to the fact that most operations can be parallelized easily. The protocol has two versions: a basic protocol and an enhanced protocol, the security of the two variants is analyzed and proved in the semi-honest model and the malicious model respectively. A prototype of the basic protocol has been built. We report the result of performance evaluation and compare it against the two previously fastest PSI protocols. Our protocol is orders of magnitude faster than these two protocols. To compute the intersection of two million-element sets, our protocol needs only 41 seconds (80-bit security) and 339 seconds (256-bit security) on moderate hardware in parallel mode.",
                "call-number": "10.1145/2508859.2516701",
                "collection-title": "CCS '13",
                "container-title": "Proceedings of the 2013 ACM SIGSAC conference on Computer & communications security",
                "DOI": "10.1145/2508859.2516701",
                "event-place": "Berlin, Germany",
                "ISBN": "9781450324779",
                "keyword": "private set intersection, bloom filters",
                "number-of-pages": "12",
                "page": "789–800",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "When private set intersection meets big data: an efficient and scalable protocol",
                "URL": "https://doi.org/10.1145/2508859.2516701"
            }
        },
        {
            "10.1145/3053600.3053621": {
                "id": "10.1145/3053600.3053621",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Apte",
                        "given": "Varsha"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            4,
                            18
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            4,
                            18
                        ]
                    ]
                },
                "abstract": "With the advent of big data through social media and continuous creation of digital footprints through various mobile devices, special-purpose programming models were developed that would make it easy to write programs to process such data. MapReduce and its Hadoop implementation is one of the most popular platforms for writing such programs. The MapReduce framework involves a \"map\" phase where various tasks work in parallel for intermediate processing of data and a \"reduce\" phase where again various tasks work in parallel to extract information from this processed data. Performance modeling of such systems will need different approaches than are used for traditional multi-threaded multi-core systems supporting Web applications, primarily because the dependencies and synchronization required between various tasks is not easily expressible using standard queuing network models. In this talk we will review work done by researchers to address this modeling problem. The work done encompasses first-principles calculations of execution time completion, queuing network models, and finally, simulation. We will review these efforts as well as highlight opportunities for further work in this area.",
                "call-number": "10.1145/3053600.3053621",
                "collection-title": "ICPE '17 Companion",
                "container-title": "Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering Companion",
                "DOI": "10.1145/3053600.3053621",
                "event-place": "L&apos;Aquila, Italy",
                "ISBN": "9781450348997",
                "number-of-pages": "1",
                "page": "105",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Recent Trends in Performance Modeling of Big Data Systems",
                "URL": "https://doi.org/10.1145/3053600.3053621"
            }
        },
        {
            "10.1109/CCGRID.2017.56": {
                "id": "10.1109/CCGRID.2017.56",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Serrano",
                        "given": "Estefania"
                    },
                    {
                        "family": "Blas",
                        "given": "Javier Garcia"
                    },
                    {
                        "family": "Carretero",
                        "given": "Jesus"
                    },
                    {
                        "family": "Abella",
                        "given": "Monica"
                    },
                    {
                        "family": "Desco",
                        "given": "Manuel"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            5,
                            14
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            5,
                            14
                        ]
                    ]
                },
                "abstract": "The apparition of new paradigms, programming models, and languages that offer better programmability and better performance turns the implementation of current scientific applications into a less time-consuming task than years ago. One significant example of this trend is the MapReduce programming model and its implementation using Apache Spark. Nowadays, this programming model is mainly used for data analysis and machine learning applications, although it has been expanded to its usage in the HPC community. On the side of programming languages, Python has positioned itself as an alternative to other scientific programming languages, such as Matlab or Julia. In this work we explore the capabilities of Python and Apache Spark as partners in the implementation of the backprojection operator of a CT reconstruction application. We present two interesting approaches with two different types of architectures: a heterogeneous architecture including NVidia GPUs and a full performance CPU mode with the compatibility with C/C++ native source code. We experimentally demonstrate that current CPU-based implementations scale with the number of computational units.",
                "call-number": "10.1109/CCGRID.2017.56",
                "collection-title": "CCGrid '17",
                "container-title": "Proceedings of the 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing",
                "DOI": "10.1109/CCGRID.2017.56",
                "event-place": "Madrid, Spain",
                "ISBN": "9781509066100",
                "keyword": "Backprojection, CT, CUDA, Python, Big Data, Apache Spark",
                "number-of-pages": "8",
                "page": "830–837",
                "publisher": "IEEE Press",
                "title": "Medical Imaging Processing on a Big Data platform using Python: Experiences with Heterogeneous and Homogeneous Architectures",
                "URL": "https://doi.org/10.1109/CCGRID.2017.56"
            }
        },
        {
            "10.1145/3366030.3366103": {
                "id": "10.1145/3366030.3366103",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Januzaj",
                        "given": "Eshref"
                    },
                    {
                        "family": "Januzaj",
                        "given": "Visar"
                    },
                    {
                        "family": "Mandl",
                        "given": "Peter"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            12,
                            2
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            12,
                            2
                        ]
                    ]
                },
                "abstract": "When dealing with huge data sets, during the integration process of distributed data into a single data warehouse, one is not only confronted with time and security factors but with the well known problem of low data quality as well. In order to cope with such issues that the integration of distributed data often is faced with, we present in this paper an approach that applies distributed data mining, to facilitate a data quality analysis of the data in their distributed state. Data quality problems are identified by a classifier, which uses the knowledge gained from the clustering (subspace clustering) process performed on the distributed data. Experiments on real data show that the distributed analysis results are comparable to those conducted on the central data warehouse using classical data mining.",
                "call-number": "10.1145/3366030.3366103",
                "collection-title": "iiWAS2019",
                "container-title": "Proceedings of the 21st International Conference on Information Integration and Web-based Applications & Services",
                "DOI": "10.1145/3366030.3366103",
                "event-place": "Munich, Germany",
                "ISBN": "9781450371797",
                "keyword": "Data Mining, Data Quality, Distributed Clustering",
                "number-of-pages": "5",
                "page": "418–422",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "An Application of Distributed Data Mining to Identify Data Quality Problems",
                "URL": "https://doi.org/10.1145/3366030.3366103"
            }
        },
        {
            "10.1145/3379247.3379270": {
                "id": "10.1145/3379247.3379270",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Yang",
                        "given": "Lan"
                    },
                    {
                        "family": "Chiang",
                        "given": "Jason Amaro"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2020,
                            1,
                            4
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2020,
                            1,
                            4
                        ]
                    ]
                },
                "abstract": "In big data analytics the phenomenon of missing data is universal due to reasons such as faulty equipment and nonresponses in surveys. Imputation is the process of replacing missing data with substituted values. Proper imputation could greatly improve the accuracy and effectiveness of big data analytics.In this paper, we analyze a rich set of deletion and imputation methods, focusing on strengths, weaknesses, best use cases, implementation strategies, and error-examination based performance analysis. Our goal is to find the best fitted imputation method(s) for each given use case.",
                "call-number": "10.1145/3379247.3379270",
                "collection-title": "ICCDE 2020",
                "container-title": "Proceedings of 2020 the 6th International Conference on Computing and Data Engineering",
                "DOI": "10.1145/3379247.3379270",
                "event-place": "Sanya, China",
                "ISBN": "9781450376730",
                "keyword": "imputation, missing data, error estimation, Big data analytics",
                "number-of-pages": "5",
                "page": "107–111",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Use Case and Performance Analyses for Missing Data Imputation Methods in Big Data Analytics",
                "URL": "https://doi.org/10.1145/3379247.3379270"
            }
        },
        {
            "10.1145/1864708.1864767": {
                "id": "10.1145/1864708.1864767",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "De Pessemier",
                        "given": "Toon"
                    },
                    {
                        "family": "Dooms",
                        "given": "Simon"
                    },
                    {
                        "family": "Deryckere",
                        "given": "Tom"
                    },
                    {
                        "family": "Martens",
                        "given": "Luc"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2010,
                            9,
                            26
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2010,
                            9,
                            26
                        ]
                    ]
                },
                "abstract": "The efficiency of personal suggestions generated by collaborative filtering techniques is highly dependent on the quality and quantity of the available consumption data. Extending data sets with additional consumption data (from the past) might enrich the user profiles and generally leads to more accurate recommendations. Although if a considerable amount of profile information is already available and detailed personal preferences can be derived, supplementary consumption data may not have any (or a very limited) added value for the recommendation algorithm. These additional consumption data increase the required storage capacity and the computational load to generate the personal recommendations. Moreover, since personal preferences and the relevance of content items may vary over time, older consumption data might be outdated and lead to inaccurate recommendations. Therefore, we investigate which consumption data are (the most) relevant to feed the conventional collaborative filtering algorithms. For provider-generated content systems, we demonstrate that the accuracy of collaborative filtering algorithms increases by extending user profiles with additional older consumption data. In contrast, we witness the opposite effect for user-generated content systems: involving older consumption data has a negative influence on the recommender accuracy. These results are important for website owners who intend to employ a recommendation system at a minimum storage and computation cost.",
                "call-number": "10.1145/1864708.1864767",
                "collection-title": "RecSys '10",
                "container-title": "Proceedings of the fourth ACM conference on Recommender systems",
                "DOI": "10.1145/1864708.1864767",
                "event-place": "Barcelona, Spain",
                "ISBN": "9781605589060",
                "keyword": "recommender systems, data quality, collaborative filtering",
                "number-of-pages": "4",
                "page": "281–284",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Time dependency of data quality for collaborative filtering algorithms",
                "URL": "https://doi.org/10.1145/1864708.1864767"
            }
        },
        {
            "10.5555/2888619.2888701": {
                "id": "10.5555/2888619.2888701",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Hofmann",
                        "given": "Marko A."
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2015,
                            12,
                            6
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2015,
                            12,
                            6
                        ]
                    ]
                },
                "abstract": "p-values of null hypothesis significance testing have long been the standard and decisive measure of deductive statistics. However, for decades, top statistical methodologists have argued that focusing on p-values is not conducive to science, and that these tests are regularly misunderstood. The standard replacement or at least complement proposed for p-values by those critics are confidence intervals and statistical effects sizes. Regrettably, analyzing and comparing huge data sets (from data mining or simulation based data farming) with two measures is awkward. As a single-value measure of first interpretation for the scanning of Big Data this article proposes statistically secured effect sizes either based on exact, mathematically sophisticated confidence intervals for effect sizes or simplified approximations. It is further argued that simplified secured effect sizes are among the most instructive single measures of statistical interpretation completely perspicuous for the layman.",
                "call-number": "10.5555/2888619.2888701",
                "collection-title": "WSC '15",
                "container-title": "Proceedings of the 2015 Winter Simulation Conference",
                "event-place": "Huntington Beach, California",
                "ISBN": "9781467397414",
                "number-of-pages": "12",
                "page": "725–736",
                "publisher": "IEEE Press",
                "title": "Searching for effects in big data: why p-values are not advised and what to use instead"
            }
        },
        {
            "10.1145/2788402.2788406": {
                "id": "10.1145/2788402.2788406",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Rosà",
                        "given": "Andrea"
                    },
                    {
                        "family": "Chen",
                        "given": "Lydia Y."
                    },
                    {
                        "family": "Birke",
                        "given": "Robert"
                    },
                    {
                        "family": "Binder",
                        "given": "Walter"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2015,
                            6,
                            2
                        ]
                    ]
                },
                "abstract": "The ever increasing size and complexity of large-scale datacenters enhance the difficulty of developing efficient scheduling policies for big data systems, where priority scheduling is often employed to guarantee the allocation of system resources to high priority tasks, at the cost of task preemption and resulting resource waste. A large number of related studies focuses on understanding workloads and their performance impact on such systems; nevertheless, existing works pay little attention on evicted tasks, their characteristics, and the resulting impairment on the system performance. In this paper, we base our analysis on Google cluster traces, where tasks can experience three diffierent types of unsuccessful events, namely eviction, kill and fail. We particularly focus on eviction events, i.e., preemption of task execution due to higher priority tasks, and rigorously quantify their performance drawbacks, in terms of wasted machine time and resources, with particular focus on priority. Motivated by the high dependency of eviction on underlying scheduling policies, we also study its statistical patterns and its dependency on other types of unsuccessful events. Moreover, by considering co-executed tasks and system load, we deepen the knowledge on priority scheduling, showing how priority and machine utilization affect the eviction process and related tasks.",
                "call-number": "10.1145/2788402.2788406",
                "container-title": "SIGMETRICS Perform. Eval. Rev.",
                "DOI": "10.1145/2788402.2788406",
                "ISSN": "0163-5999",
                "issue": "4",
                "number-of-pages": "10",
                "page": "12–21",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "March 2015",
                "title": "Demystifying Casualties of Evictions in Big Data Priority Scheduling",
                "URL": "https://doi.org/10.1145/2788402.2788406",
                "volume": "42"
            }
        },
        {
            "10.1145/3297280.3297386": {
                "id": "10.1145/3297280.3297386",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Ianni",
                        "given": "Michele"
                    },
                    {
                        "family": "Masciari",
                        "given": "Elio"
                    },
                    {
                        "family": "Mazzeo",
                        "given": "Giuseppe M."
                    },
                    {
                        "family": "Zaniolo",
                        "given": "Carlo"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            4,
                            8
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            4,
                            8
                        ]
                    ]
                },
                "abstract": "The current era of Big Data [7] has forced both researchers and industries to rethink the computational solutions for analyzing massive data. In fact, a great deal of attention has been devoted to the design of new algorithms for analyzing information available from Twitter, Google, Facebook, and Wikipedia, just to cite a few of the main big data producers. Although this massive volume of data can be quite useful for people and companies, it makes analytical and retrieval operations really time consuming due to their high computational cost. A possible solution relies upon the possibility to cluster big data in a compact but still informative version of the entire data set. Obviously, such clustering techniques should produce clusters (or summaries) having high accuracy. Clustering algorithms could be beneficial in several application scenarios such as cybersecurity, user profiling and recommendation systems, to cite a few.",
                "call-number": "10.1145/3297280.3297386",
                "collection-title": "SAC '19",
                "container-title": "Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing",
                "DOI": "10.1145/3297280.3297386",
                "event-place": "Limassol, Cyprus",
                "ISBN": "9781450359337",
                "number-of-pages": "8",
                "page": "1073–1080",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "How to implement a big data clustering algorithm: a brief report on lesson learned",
                "URL": "https://doi.org/10.1145/3297280.3297386"
            }
        },
        {
            "10.1145/3510858.3511392": {
                "id": "10.1145/3510858.3511392",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Wei",
                        "given": "Dongliang"
                    },
                    {
                        "family": "Wang",
                        "given": "Zhi"
                    },
                    {
                        "family": "Zhou",
                        "given": "Jia"
                    },
                    {
                        "family": "Chen",
                        "given": "Jiangtian"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            12,
                            18
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2021,
                            12,
                            18
                        ]
                    ]
                },
                "abstract": "With the development of the country in information technology, and those technology products are also used in life. Such as capacitive devices in power systems. But capacitive equipment will also have some failures, so it is necessary to monitor it online. Now researchers have found a lot of online monitoring methods, but in most of the current monitoring methods, the monitoring process will produce a huge amount of data, which is very large, so that technicians may miss some important data. In order to solve this problem which can not be discovered early because of the excessive amount of data, this paper adopts some methods based on big data to dig through the data the data obtained by the dig algorithm are statistically analyzed in big data technology. Using the data collected in this paper, through the analysis and research of these data, the results show that the application of big data technology to the on-line monitoring of capacitive equipment is very accurate and practical.",
                "call-number": "10.1145/3510858.3511392",
                "collection-title": "ICASIT 2021",
                "container-title": "2021 International Conference on Aviation Safety and Information Technology",
                "DOI": "10.1145/3510858.3511392",
                "event-place": "Changsha, China",
                "ISBN": "9781450390422",
                "number-of-pages": "5",
                "page": "803–807",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "On-line Monitoring of Capacitive Equipment Based on Big Data",
                "URL": "https://doi.org/10.1145/3510858.3511392"
            }
        },
        {
            "10.5555/3042094.3042329": {
                "id": "10.5555/3042094.3042329",
                "type": "CHAPTER",
                "author": [
                    {
                        "family": "Volovoi",
                        "given": "Vitali"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            12,
                            11
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2016,
                            12,
                            11
                        ]
                    ]
                },
                "abstract": "Maintenance processes of repairable systems have been extensively studied in the past. The resulting simple solutions have proven to be remarkably effective. It requires complex and time-consuming simulations to improve on those simple solutions, and reliable input data is even harder to get. However, new technologies, epitomized by Big Data and the Internet of Things, change the data-availability part of the equation. As a result, there are new exciting possibilities for modeling more subtle effects, and developing processes for easily (and therefore frequently) updated inputs. Modeling decisions can be repeatedly tested on the data, and the models can be quickly adjusted to better reflect reality and even to compensate for missing pieces of the data. In this context, the transparency and simplicity of models becomes a larger virtue. Several examples of the insights based on real-world large-scale applications of predictive analytics using simulation are discussed.",
                "call-number": "10.5555/3042094.3042329",
                "container-title": "Proceedings of the 2016 Winter Simulation Conference",
                "ISBN": "9781509044849",
                "number-of-pages": "12",
                "page": "1872–1883",
                "publisher": "IEEE Press",
                "title": "Simulation of maintenance processes in the big data era"
            }
        }
    ]
}