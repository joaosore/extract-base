{
    "exportedDoiLength": 100,
    "fileName": "acm",
    "style": "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<style xmlns=\"http://purl.org/net/xbiblio/csl\" class=\"in-text\" version=\"1.0\" demote-non-dropping-particle=\"sort-only\" default-locale=\"en-US\">\n    <!-- This style was edited with the Visual CSL Editor (http://editor.citationstyles.org/visualEditor/) -->\n    <info>\n        <title>BibTeX ACM citation style</title>\n        <id>http://www.zotero.org/styles/bibtex-acm-citation-style</id>\n        <link href=\"http://www.zotero.org/styles/bibtex-acm-citation-style\" rel=\"self\"/>\n        <link href=\"http://www.bibtex.org/\" rel=\"documentation\"/>\n        <author>\n            <name>Markus Schaffner</name>\n        </author>\n        <contributor>\n            <name>Richard Karnesky</name>\n            <email>karnesky+zotero@gmail.com</email>\n            <uri>http://arc.nucapt.northwestern.edu/Richard_Karnesky</uri>\n        </contributor>\n        <category citation-format=\"author-date\"/>\n        <category field=\"generic-base\"/>\n        <updated>2018-06-11T10:52:49+00:00</updated>\n        <rights license=\"http://creativecommons.org/licenses/by-sa/3.0/\">This work is licensed under a Creative Commons Attribution-ShareAlike 3.0 License</rights>\n    </info>\n    <macro name=\"zotero2bibtexType\">\n        <choose>\n            <if type=\"BILL BOOK GRAPHIC LEGAL_CASE LEGISLATION MOTION_PICTURE SONG\" match=\"any\">\n                <choose>\n                    <if genre=\"rfc\"  match=\"any\">\n                       <text value=\"rfc\"/>\n                    </if>\n                     <else-if  genre=\"bibliography\" match=\"any\">\n                        <text value=\"bibliography\"/>\n                    </else-if>\n                    <else-if  genre=\"play_drama\" match=\"any\">\n                        <text value=\"playdrama\"/>\n                    </else-if>\n                    <else-if  genre=\"proceeding\" match=\"any\">\n                        <text value=\"proceedings\"/>\n                    </else-if>\n                    <else-if  genre=\"tech_brief\" match=\"any\">\n                        <text value=\"tech-brief\"/>\n                    </else-if>\n                    <else>\n                       <text value=\"book\"/>\n                    </else>\n                </choose>\n            </if>\n            <else-if type=\"CHAPTER\" match=\"any\">\n                <text value=\"inbook\"/>\n            </else-if>\n            <else-if type=\"ARTICLE ARTICLE_JOURNAL ARTICLE_MAGAZINE ARTICLE_NEWSPAPER\" match=\"any\">\n                <text value=\"article\"/>\n            </else-if>\n            <else-if type=\"THESIS\" match=\"any\">\n                <choose>\n                    <if variable=\"genre\">\n                        <text variable=\"genre\" text-case=\"lowercase\" strip-periods=\"true\"/>\n                    </if>\n                </choose>\n                <text value=\"thesis\"/>\n            </else-if>\n            <else-if type=\"PAPER_CONFERENCE\" match=\"any\">\n                <text value=\"inproceedings\"/>\n            </else-if>\n            <else-if type=\"REPORT\" match=\"any\">\n                <text value=\"techreport\"/>\n            </else-if>\n            <else-if type=\"DATASET\" match=\"any\">\n                <choose>\n                    <if genre=\"software\"  match=\"any\">\n                       <text value=\"software\"/>\n                    </if>\n                    <else>\n                       <text value=\"dataset\"/>\n                    </else>\n                </choose>\n            </else-if>\n            <else>\n                <text value=\"misc\"/>\n            </else>\n        </choose>\n    </macro>\n    <macro name=\"citeKey\">\n           <text variable=\"call-number\"/>\n    </macro>\n    <macro name=\"editor-short\">\n        <names variable=\"editor\">\n            <name form=\"short\" delimiter=\":\" delimiter-precedes-last=\"always\"/>\n        </names>\n    </macro>\n    <macro name=\"author-short\">\n        <names variable=\"author\">\n            <name form=\"short\" delimiter=\":\" delimiter-precedes-last=\"always\"/>\n        </names>\n    </macro>\n    <macro name=\"issued-year\">\n        <date variable=\"issued\">\n            <date-part name=\"year\"/>\n        </date>\n    </macro>\n    <macro name=\"issued-month\">\n        <choose>\n            <if type=\"ARTICLE\" match=\"any\">\n                <date variable=\"issued\">\n                    <date-part name=\"month\" form=\"short\" strip-periods=\"true\" text-case=\"lowercase\"/>\n                </date>\n            </if>\n        </choose>\n    </macro>\n    <macro name=\"issue-date\">\n        <choose>\n            <if type=\"ARTICLE ARTICLE_JOURNAL\" match=\"any\">\n                <choose>\n                    <if variable=\"note\" match=\"none\">\n                        <choose>\n                            <if variable=\"source\">\n                                 <text variable=\"source\"/>\n                            </if>\n                               <else>\n                                 <date date-parts=\"year-month\" form=\"text\" variable=\"issued\"/>\n                               </else>\n                        </choose>\n                    </if>\n                </choose>\n            </if>\n        </choose>\n    </macro>\n    <macro name=\"author\">\n        <names variable=\"author\">\n            <name sort-separator=\", \" delimiter=\" and \" delimiter-precedes-last=\"always\" name-as-sort-order=\"all\"/>\n            <label form=\"long\" text-case=\"capitalize-first\"/>\n        </names>\n    </macro>\n    <macro name=\"editor-translator\">\n        <names variable=\"editor translator\" delimiter=\", \">\n            <name sort-separator=\", \" delimiter=\" and \" delimiter-precedes-last=\"always\" name-as-sort-order=\"all\"/>\n            <label form=\"long\" text-case=\"capitalize-first\"/>\n        </names>\n    </macro>\n    <macro name=\"title\">\n        <choose>\n            <if genre=\"proceeding\" match=\"any\">\n                <text variable=\"container-title-short\" suffix=\": \"/>\n                <text variable=\"title\" text-case=\"title\"/>\n            </if>\n            <else-if type=\"ARTICLE_JOURNAL\" match=\"none\">\n                <text variable=\"title\" text-case=\"title\"/>\n            </else-if>\n        </choose>\n    </macro>\n    <macro name=\"volume\">\n        <choose>\n            <if type=\"ARTICLE BOOK ARTICLE_JOURNAL\" match=\"any\">\n                <text variable=\"volume\" prefix=\"volume = {\" suffix=\"}\"/>\n            </if>\n        </choose>\n    </macro>\n    <macro name=\"DOI\">\n        <choose>\n            <if type=\"ARTICLE PAPER_CONFERENCE\" match=\"any\">\n                <text variable=\"DOI\" prefix=\"doi = {\" suffix=\"}\"/>\n            </if>\n        </choose>\n    </macro>\n    <macro name=\"abstract\">\n        <if match=\"any\" variable=\"abstract\">\n            <text variable=\"abstract\"/>\n        </if>\n    </macro>\n    <macro name=\"URL\">\n        <text variable=\"URL\" prefix=\"url = {\" suffix=\"}\"/>\n    </macro>\n    <macro name=\"container-title\">\n        <choose>\n            <if type=\"CHAPTER PAPER_CONFERENCE\" match=\"any\">\n                <text variable=\"container-title\" prefix=\"booktitle = {\" suffix=\"}\" text-case=\"title\"/>\n            </if>\n            <else-if type=\"ARTICLE ARTICLE_JOURNAL\" match=\"any\">\n                <text variable=\"container-title\" prefix=\"journal = {\" suffix=\"}\" text-case=\"title\"/>\n            </else-if>\n        </choose>\n    </macro>\n    <macro name=\"pages\">\n        <group delimiter=\",&#10;\">\n            <choose>\n                <if match=\"any\" variable=\"collection-number\">\n                    <text variable=\"collection-number\" prefix=\"articleno = {\" suffix=\"}\"/>\n                </if>\n                <else>\n                    <text variable=\"page\" prefix=\"pages = {\" suffix=\"}\"/>\n                </else>\n            </choose>\n            <text variable=\"number-of-pages\" prefix=\"numpages = {\" suffix=\"}\"/>\n        </group>\n    </macro>\n    <macro name=\"edition\">\n        <text variable=\"edition\"/>\n    </macro>\n    <macro name=\"editor\">\n        <choose>\n            <if match=\"any\" type=\"THESIS\">\n                <names variable=\"editor\" delimiter=\", \" prefix=\"advisor = {\" suffix=\"}\">\n                    <name delimiter=\" and \" delimiter-precedes-last=\"always\" name-as-sort-order=\"all\"/>\n                </names>\n            </if>\n            <else>\n               <names variable=\"editor\" delimiter=\", \" prefix=\"editor = {\" suffix=\"}\">\n                   <name delimiter=\" and \" delimiter-precedes-last=\"always\" name-as-sort-order=\"all\"/>\n               </names>\n            </else>\n        </choose>\n    </macro>\n    <macro name=\"keyword\">\n        <choose>\n            <if type=\"ARTICLE PAPER_CONFERENCE DATASET\" match=\"any\">\n                <text variable=\"keyword\" prefix=\"keywords = {\" suffix=\"}\"/>\n            </if>\n        </choose>\n    </macro>\n    <citation et-al-min=\"11\" et-al-use-first=\"10\" disambiguate-add-year-suffix=\"true\" disambiguate-add-names=\"false\" disambiguate-add-givenname=\"false\" collapse=\"year\">\n        <layout delimiter=\"_\">\n            <text macro=\"citeKey\"/>\n        </layout>\n    </citation>\n    <bibliography hanging-indent=\"false\">\n        <layout>\n            <group display=\"right-inline\">\n                <text macro=\"zotero2bibtexType\" prefix=\"@\"/>\n                <group prefix=\"{\" suffix=\"&#10;}\" delimiter=\",&#10;\">\n                    <text macro=\"citeKey\"/>\n                    <text macro=\"author\" prefix=\"author = {\" suffix=\"}\"/>\n                    <text macro=\"editor\"/>\n                    <text macro=\"title\" prefix=\"title = {\" suffix=\"}\"/>\n                    <text macro=\"issued-year\" prefix=\"year = {\" suffix=\"}\"/>\n                    <text macro=\"issue-date\" prefix=\"issue_date = {\" suffix=\"}\"/>\n                    <text variable=\"ISBN\" prefix=\"isbn = {\" suffix=\"}\"/>\n                    <text variable=\"publisher\" prefix=\"publisher = {\" suffix=\"}\"/>\n                    <text variable=\"publisher-place\" prefix=\"address = {\" suffix=\"}\"/>\n                    <text variable=\"chapter-number\" prefix=\"chapter = {\" suffix=\"}\"/>\n                    <text macro=\"edition\" prefix=\"edition = {\" suffix=\"}\"/>\n                    <text macro=\"volume\"/>\n                    <text variable=\"issue\" prefix=\"number = {\" suffix=\"}\"/>\n                    <text variable=\"ISSN\" prefix=\"issn = {\" suffix=\"}\"/>\n                    <text variable=\"archive_location\" prefix=\"archiveLocation = {\" suffix=\"}\"/>\n                    <text macro=\"URL\"/>\n                    <text macro=\"DOI\"/>\n                    <text macro=\"abstract\" prefix=\"abstract = {\" suffix=\"}\"/>\n                    <text variable=\"note\" prefix=\"note = {\" suffix=\"}\"/>\n                    <text macro=\"container-title\"/>\n                    <text macro=\"issued-month\" prefix=\"month = {\" suffix=\"}\"/>\n                    <text macro=\"pages\"/>\n                    <text macro=\"keyword\"/>\n                    <text variable=\"event-place\" prefix=\"location = {\" suffix=\"}\"/>\n                    <choose>\n                        <if type=\"PAPER_CONFERENCE\" match=\"any\">\n                            <text variable=\"collection-title\" prefix=\"series = {\" suffix=\"}\"/>\n                        </if>\n                        <else>\n                            <text variable=\"collection-title\" prefix=\"collection = {\" suffix=\"}\"/>\n                        </else>\n                    </choose>\n                </group>\n            </group>\n        </layout>\n    </bibliography>\n</style>\n",
    "suffix": "bib",
    "locale": "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<locale xmlns=\"http://purl.org/net/xbiblio/csl\" version=\"1.0\" xml:lang=\"en-US\">\n  <info>\n    <rights license=\"http://creativecommons.org/licenses/by-sa/3.0/\">This work is licensed under a Creative Commons Attribution-ShareAlike 3.0 License</rights>\n    <updated>2012-07-04T23:31:02+00:00</updated>\n  </info>\n  <style-options punctuation-in-quote=\"true\"\n                 leading-noise-words=\"a,an,the\"\n                 name-as-sort-order=\"ja zh kr my hu vi\"\n                 name-never-short=\"ja zh kr my hu vi\"/>\n  <date form=\"text\">\n    <date-part name=\"month\" suffix=\" \"/>\n    <date-part name=\"day\" suffix=\", \"/>\n    <date-part name=\"year\"/>\n  </date>\n  <date form=\"numeric\">\n    <date-part name=\"month\" form=\"numeric-leading-zeros\" suffix=\"/\"/>\n    <date-part name=\"day\" form=\"numeric-leading-zeros\" suffix=\"/\"/>\n    <date-part name=\"year\"/>\n  </date>\n  <terms>\n    <term name=\"radio-broadcast\">radio broadcast</term>\n    <term name=\"television-broadcast\">television broadcast</term>\n    <term name=\"podcast\">podcast</term>\n    <term name=\"instant-message\">instant message</term>\n    <term name=\"email\">email</term>\n    <term name=\"number-of-volumes\">\n      <single>volume</single>\n      <multiple>volumes</multiple>\n    </term>\n    <term name=\"accessed\">accessed</term>\n    <term name=\"and\">and</term>\n    <term name=\"and\" form=\"symbol\">&amp;</term>\n    <term name=\"and others\">and others</term>\n    <term name=\"anonymous\">anonymous</term>\n    <term name=\"anonymous\" form=\"short\">anon.</term>\n    <term name=\"at\">at</term>\n    <term name=\"available at\">available at</term>\n    <term name=\"by\">by</term>\n    <term name=\"circa\">circa</term>\n    <term name=\"circa\" form=\"short\">c.</term>\n    <term name=\"cited\">cited</term>\n    <term name=\"edition\">\n      <single>edition</single>\n      <multiple>editions</multiple>\n    </term>\n    <term name=\"edition\" form=\"short\">ed.</term>\n    <term name=\"et-al\">et al.</term>\n    <term name=\"forthcoming\">forthcoming</term>\n    <term name=\"from\">from</term>\n    <term name=\"ibid\">ibid.</term>\n    <term name=\"in\">in</term>\n    <term name=\"in press\">in press</term>\n    <term name=\"internet\">internet</term>\n    <term name=\"interview\">interview</term>\n    <term name=\"letter\">letter</term>\n    <term name=\"no date\">no date</term>\n    <term name=\"no date\" form=\"short\">n.d.</term>\n    <term name=\"online\">online</term>\n    <term name=\"presented at\">presented at the</term>\n    <term name=\"reference\">\n      <single>reference</single>\n      <multiple>references</multiple>\n    </term>\n    <term name=\"reference\" form=\"short\">\n      <single>ref.</single>\n      <multiple>refs.</multiple>\n    </term>\n    <term name=\"retrieved\">retrieved</term>\n    <term name=\"scale\">scale</term>\n    <term name=\"version\">version</term>\n\n    <!-- ANNO DOMINI; BEFORE CHRIST -->\n    <term name=\"ad\">AD</term>\n    <term name=\"bc\">BC</term>\n\n    <!-- PUNCTUATION -->\n    <term name=\"open-quote\">“</term>\n    <term name=\"close-quote\">”</term>\n    <term name=\"open-inner-quote\">‘</term>\n    <term name=\"close-inner-quote\">’</term>\n    <term name=\"page-range-delimiter\">–</term>\n\n    <!-- ORDINALS -->\n    <term name=\"ordinal\">th</term>\n    <term name=\"ordinal-01\">st</term>\n    <term name=\"ordinal-02\">nd</term>\n    <term name=\"ordinal-03\">rd</term>\n    <term name=\"ordinal-11\">th</term>\n    <term name=\"ordinal-12\">th</term>\n    <term name=\"ordinal-13\">th</term>\n\n    <!-- LONG ORDINALS -->\n    <term name=\"long-ordinal-01\">first</term>\n    <term name=\"long-ordinal-02\">second</term>\n    <term name=\"long-ordinal-03\">third</term>\n    <term name=\"long-ordinal-04\">fourth</term>\n    <term name=\"long-ordinal-05\">fifth</term>\n    <term name=\"long-ordinal-06\">sixth</term>\n    <term name=\"long-ordinal-07\">seventh</term>\n    <term name=\"long-ordinal-08\">eighth</term>\n    <term name=\"long-ordinal-09\">ninth</term>\n    <term name=\"long-ordinal-10\">tenth</term>\n\n    <!-- LONG LOCATOR FORMS -->\n    <term name=\"book\">\n      <single>book</single>\n      <multiple>books</multiple>\n    </term>\n    <term name=\"chapter\">\n      <single>chapter</single>\n      <multiple>chapters</multiple>\n    </term>\n    <term name=\"column\">\n      <single>column</single>\n      <multiple>columns</multiple>\n    </term>\n    <term name=\"figure\">\n      <single>figure</single>\n      <multiple>figures</multiple>\n    </term>\n    <term name=\"folio\">\n      <single>folio</single>\n      <multiple>folios</multiple>\n    </term>\n    <term name=\"issue\">\n      <single>number</single>\n      <multiple>numbers</multiple>\n    </term>\n    <term name=\"line\">\n      <single>line</single>\n      <multiple>lines</multiple>\n    </term>\n    <term name=\"note\">\n      <single>note</single>\n      <multiple>notes</multiple>\n    </term>\n    <term name=\"opus\">\n      <single>opus</single>\n      <multiple>opera</multiple>\n    </term>\n    <term name=\"page\">\n      <single>page</single>\n      <multiple>pages</multiple>\n    </term>\n    <term name=\"paragraph\">\n      <single>paragraph</single>\n      <multiple>paragraph</multiple>\n    </term>\n    <term name=\"part\">\n      <single>part</single>\n      <multiple>parts</multiple>\n    </term>\n    <term name=\"section\">\n      <single>section</single>\n      <multiple>sections</multiple>\n    </term>\n    <term name=\"sub verbo\">\n      <single>sub verbo</single>\n      <multiple>sub verbis</multiple>\n    </term>\n    <term name=\"verse\">\n      <single>verse</single>\n      <multiple>verses</multiple>\n    </term>\n    <term name=\"volume\">\n      <single>volume</single>\n      <multiple>volumes</multiple>\n    </term>\n\n    <!-- SHORT LOCATOR FORMS -->\n    <term name=\"book\" form=\"short\">bk.</term>\n    <term name=\"chapter\" form=\"short\">chap.</term>\n    <term name=\"column\" form=\"short\">col.</term>\n    <term name=\"figure\" form=\"short\">fig.</term>\n    <term name=\"folio\" form=\"short\">f.</term>\n    <term name=\"issue\" form=\"short\">no.</term>\n    <term name=\"line\" form=\"short\">l.</term>\n    <term name=\"note\" form=\"short\">n.</term>\n    <term name=\"opus\" form=\"short\">op.</term>\n    <term name=\"page\" form=\"short\">\n      <single>p.</single>\n      <multiple>pp.</multiple>\n    </term>\n    <term name=\"paragraph\" form=\"short\">para.</term>\n    <term name=\"part\" form=\"short\">pt.</term>\n    <term name=\"section\" form=\"short\">sec.</term>\n    <term name=\"sub verbo\" form=\"short\">\n      <single>s.v.</single>\n      <multiple>s.vv.</multiple>\n    </term>\n    <term name=\"verse\" form=\"short\">\n      <single>v.</single>\n      <multiple>vv.</multiple>\n    </term>\n    <term name=\"volume\" form=\"short\">\n      <single>vol.</single>\n      <multiple>vols.</multiple>\n    </term>\n\n    <!-- SYMBOL LOCATOR FORMS -->\n    <term name=\"paragraph\" form=\"symbol\">\n      <single>¶</single>\n      <multiple>¶¶</multiple>\n    </term>\n    <term name=\"section\" form=\"symbol\">\n      <single>§</single>\n      <multiple>§§</multiple>\n    </term>\n\n    <!-- LONG ROLE FORMS -->\n    <term name=\"director\">\n      <single>director</single>\n      <multiple>directors</multiple>\n    </term>\n    <term name=\"editor\">\n      <single>editor</single>\n      <multiple>editors</multiple>\n    </term>\n    <term name=\"editorial-director\">\n      <single>editor</single>\n      <multiple>editors</multiple>\n    </term>\n    <term name=\"illustrator\">\n      <single>illustrator</single>\n      <multiple>illustrators</multiple>\n    </term>\n    <term name=\"translator\">\n      <single>translator</single>\n      <multiple>translators</multiple>\n    </term>\n    <term name=\"editortranslator\">\n      <single>editor &amp; translator</single>\n      <multiple>editors &amp; translators</multiple>\n    </term>\n\n    <!-- SHORT ROLE FORMS -->\n    <term name=\"director\" form=\"short\">\n      <single>dir.</single>\n      <multiple>dirs.</multiple>\n    </term>\n    <term name=\"editor\" form=\"short\">\n      <single>ed.</single>\n      <multiple>eds.</multiple>\n    </term>\n    <term name=\"editorial-director\" form=\"short\">\n      <single>ed.</single>\n      <multiple>eds.</multiple>\n    </term>\n    <term name=\"illustrator\" form=\"short\">\n      <single>ill.</single>\n      <multiple>ills.</multiple>\n    </term>\n    <term name=\"translator\" form=\"short\">\n      <single>tran.</single>\n      <multiple>trans.</multiple>\n    </term>\n    <term name=\"editortranslator\" form=\"short\">\n      <single>ed. &amp; tran.</single>\n      <multiple>eds. &amp; trans.</multiple>\n    </term>\n\n    <!-- VERB ROLE FORMS -->\n    <term name=\"director\" form=\"verb\">directed by</term>\n    <term name=\"editor\" form=\"verb\">edited by</term>\n    <term name=\"editorial-director\" form=\"verb\">edited by</term>\n    <term name=\"illustrator\" form=\"verb\">illustrated by</term>\n    <term name=\"interviewer\" form=\"verb\">interview by</term>\n    <term name=\"recipient\" form=\"verb\">to</term>\n    <term name=\"reviewed-author\" form=\"verb\">by</term>\n    <term name=\"translator\" form=\"verb\">translated by</term>\n    <term name=\"editortranslator\" form=\"verb\">edited &amp; translated by</term>\n\n    <!-- SHORT VERB ROLE FORMS -->\n    <term name=\"container-author\" form=\"verb-short\">by</term>\n    <term name=\"director\" form=\"verb-short\">dir.</term>\n    <term name=\"editor\" form=\"verb-short\">ed.</term>\n    <term name=\"editorial-director\" form=\"verb-short\">ed.</term>\n    <term name=\"illustrator\" form=\"verb-short\">illus.</term>\n    <term name=\"translator\" form=\"verb-short\">trans.</term>\n    <term name=\"editortranslator\" form=\"verb-short\">ed. &amp; trans.</term>\n\n    <!-- LONG MONTH FORMS -->\n    <term name=\"month-01\">January</term>\n    <term name=\"month-02\">February</term>\n    <term name=\"month-03\">March</term>\n    <term name=\"month-04\">April</term>\n    <term name=\"month-05\">May</term>\n    <term name=\"month-06\">June</term>\n    <term name=\"month-07\">July</term>\n    <term name=\"month-08\">August</term>\n    <term name=\"month-09\">September</term>\n    <term name=\"month-10\">October</term>\n    <term name=\"month-11\">November</term>\n    <term name=\"month-12\">December</term>\n\n    <!-- SHORT MONTH FORMS -->\n    <term name=\"month-01\" form=\"short\">Jan.</term>\n    <term name=\"month-02\" form=\"short\">Feb.</term>\n    <term name=\"month-03\" form=\"short\">Mar.</term>\n    <term name=\"month-04\" form=\"short\">Apr.</term>\n    <term name=\"month-05\" form=\"short\">May</term>\n    <term name=\"month-06\" form=\"short\">Jun.</term>\n    <term name=\"month-07\" form=\"short\">Jul.</term>\n    <term name=\"month-08\" form=\"short\">Aug.</term>\n    <term name=\"month-09\" form=\"short\">Sep.</term>\n    <term name=\"month-10\" form=\"short\">Oct.</term>\n    <term name=\"month-11\" form=\"short\">Nov.</term>\n    <term name=\"month-12\" form=\"short\">Dec.</term>\n\n    <!-- SEASONS -->\n    <term name=\"season-01\">Spring</term>\n    <term name=\"season-02\">Summer</term>\n    <term name=\"season-03\">Autumn</term>\n    <term name=\"season-04\">Winter</term>\n  </terms>\n</locale>\n",
    "contentType": "Application/x-bibtex",
    "items": [
        {
            "10.1145/3498765.3498810": {
                "id": "10.1145/3498765.3498810",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Jiao",
                        "given": "Ge"
                    },
                    {
                        "family": "Li",
                        "given": "Lang"
                    },
                    {
                        "family": "Deng",
                        "given": "Hongwei"
                    },
                    {
                        "family": "Zheng",
                        "given": "Guangyong"
                    },
                    {
                        "family": "Zou",
                        "given": "Yi"
                    },
                    {
                        "family": "Zhao",
                        "given": "Junxia"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            10,
                            22
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2021,
                            10,
                            22
                        ]
                    ]
                },
                "abstract": "In order to solve the problems in the training of big data talents in colleges and universities, such as unclear training objectives, not close combination of practical teaching and industrial enterprises, and low training quality, this paper proposes a training mode of innovative computer talents oriented to the application of big data. The mode to train applied talents as the goal, USES the OBE's education idea, through the revision of the talent training scheme, optimizing curriculum system, reforming teaching methods, practical teaching system construction, improve teachers' skills, reform the assessment content and ways to improve the quality of talent training, meet the social demand for big data and technical personnel.",
                "call-number": "10.1145/3498765.3498810",
                "collection-title": "ICETC 2021",
                "container-title": "2021 13th International Conference on Education Technology and Computers",
                "DOI": "10.1145/3498765.3498810",
                "event-place": "Wuhan, China",
                "ISBN": "9781450385114",
                "keyword": "talent cultivation, big data, practical teaching, computer professional",
                "number-of-pages": "6",
                "page": "291–296",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Thinking on the Cultivation of Innovative Talents in Computer Majors Facing Big Data Applications",
                "URL": "https://doi.org/10.1145/3498765.3498810"
            }
        },
        {
            "10.1145/3357223.3366029": {
                "id": "10.1145/3357223.3366029",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Patel",
                        "given": "Hiren"
                    },
                    {
                        "family": "Jindal",
                        "given": "Alekh"
                    },
                    {
                        "family": "Szyperski",
                        "given": "Clemens"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            11,
                            20
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            11,
                            20
                        ]
                    ]
                },
                "abstract": "The past decade has seen a tremendous interest in large-scale data processing at Microsoft. Typical scenarios include building business-critical pipelines such as advertiser feedback loop, index builder, and relevance/ranking algorithms for Bing; analyzing user experience telemetry for Office, Windows or Xbox; and gathering recommendations for products like Windows and Xbox. To address these needs a first-party big data analytics platform, referred to as Cosmos, was developed in the early 2010s at Microsoft. Cosmos makes it possible to store data at exabyte scale and process in a serverless form factor, with SCOPE [4] being the query processing workhorse. Over time, however, several newer challenges have emerged, requiring major technical innovations in Cosmos to meet these newer demands. In this abstract, we describe three such challenges from the query processing viewpoint, and our approaches to handling them.Hyper Scale. Cosmos has witnessed a significant growth in usage from its early days, from the number of customers (starting from Bing to almost every single business unit at Microsoft today), to the volume of data processed (from petabytes to exabytes today), to the amount of processing done (from tens of thousands of SCOPE jobs to hundreds of thousands of jobs today, across hundreds of thousands of machines). Even a single job can consume tens of petabytes of data and produce similar volumes of data by running millions of tasks in parallel. Our approach to handle this unprecedented scale is two fold. First, we decoupled and disaggregated the query processor from storage and resource management components, thereby allowing different components in the Cosmos stack to scale independently. Second, we scaled the data movement in the SCOPE query processor with quasilinear complexity [2]. This is crucial since data movement is often the most expensive step, and hence the bottleneck, in massive-scale data processing.Massive Complexity. Cosmos workloads are also highly complex. Thanks to adoption across the whole of Microsoft, Cosmos needs to support workloads that are representative of multiple industry segments, including search engine (Bing), operating system (Windows), workplace productivity (Office), personal computing (Surface), gaming (XBox), etc. To handle such diverse workloads, our approach has been to provide a one-size-fits-all experience. First of all, to make it easy for the customers to express their computations, SCOPE supports different types of queries, from batch to interactive to streaming and machine learning. Second, SCOPE supports both structured and unstructured data processing. Likewise, multiple data formats, including both propriety and open source source such as Parquet, are supported. Third, users can write business logic using a mix of declarative and imperative languages, over even different imperative languages such as C# and Python, in the same job. Furthermore, users can express all of the above in simple data flow style computation for better readability and maintainability. Finally, considering the diverse workload mix inside Microsoft, we have come to realization that it is not possible to fits all scenarios using SCOPE. Therefore, we also support the popular Spark query processing engine. Overall, the one-size-fits-all query processing experience in Cosmos covers very diverse workloads, including data formats, programming languages, and the backend engines.Minimal Cost. While scale and complexity are hard by themselves, the biggest challenge is to achieve all of that at minimal cost. In fact, there is a pressing need to improve Cosmos efficiency and reduce operational costs. This is challenging due to several reasons. First, optimizing a SCOPE job is hard considering that the SCOPE DAGs are super large (up to 1000s of operators in single job!), and the optimization estimates (cardinality, cost, etc.) are often way off from the actuals. Second, SCOPE optimizes a given query, while the operational costs depend on the overall workload. Therefore workload optimization becomes very important. And finally, SCOPE jobs are typically interlinked in data pipelines, i.e., the output of one job is consumed by other jobs. This means that workload optimization needs to be aware of these dependencies. Our approach is to develop a feedback loop to learn from past workloads in order to optimize the future ones. Specifically, we leverage machine learning to learn models for optimizing individual jobs [3], apply multi-query optimizations to optimize the costs of overall workload [1], and build dependency graphs to identify and optimize for the data pipelines.",
                "call-number": "10.1145/3357223.3366029",
                "collection-title": "SoCC '19",
                "container-title": "Proceedings of the ACM Symposium on Cloud Computing",
                "DOI": "10.1145/3357223.3366029",
                "event-place": "Santa Cruz, CA, USA",
                "ISBN": "9781450369732",
                "number-of-pages": "1",
                "page": "490",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Big Data Processing at Microsoft: Hyper Scale, Massive Complexity, and Minimal Cost",
                "URL": "https://doi.org/10.1145/3357223.3366029"
            }
        },
        {
            "10.1109/CCGrid.2016.61": {
                "id": "10.1109/CCGrid.2016.61",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Krish",
                        "given": "K. R."
                    },
                    {
                        "family": "Wadhwa",
                        "given": "Bharti"
                    },
                    {
                        "family": "Iqbal",
                        "given": "M. Safdar"
                    },
                    {
                        "family": "Rafique",
                        "given": "M. Mustafa"
                    },
                    {
                        "family": "Butt",
                        "given": "Ali R."
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            5,
                            16
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2016,
                            5,
                            16
                        ]
                    ]
                },
                "abstract": "A promising trend in storage management for big data frameworks, such as Hadoop and Spark, is the emergence of heterogeneous and hybrid storage systems that employ different types of storage devices, e.g. SSDs, RAMDisks, etc., alongside traditional HDDs. However, scheduling data accesses or requests to an appropriate storage device is non-trivial and depends on several factors such as data locality, device performance, and application compute and storage resources utilization. To this end, we present Dux, an application-attuned dynamic data management system for data processing frameworks, which aims to improve overall application I/O throughput by efficiently using SSDs only for workloads that are expected to benefit from them rather than the extant approach of storing a fraction of the overall workloads in SSDs. The novelty of Dux lies in profiling application performance on SSDs and HDDs, analyzing the resulting I/O behavior, and considering the available SSDs at runtime to dynamically place data in an appropriate storage tier. Evaluation of Dux with trace-driven simulations using synthetic Facebook workloads shows that even when using 5.5× fewer SSDs compared to a SSD-only solution, Dux incurs only a small (5%) performance overhead, and thus offers an affordable and efficient storage tier management.",
                "call-number": "10.1109/CCGrid.2016.61",
                "collection-title": "CCGRID '16",
                "container-title": "Proceedings of the 16th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing",
                "DOI": "10.1109/CCGrid.2016.61",
                "event-place": "Cartagena, Columbia",
                "ISBN": "9781509024520",
                "keyword": "heterogeneous storage, MapReduce, performance prediction, data-intensive computing, tiered storage",
                "number-of-pages": "6",
                "page": "403–408",
                "publisher": "IEEE Press",
                "title": "On efficient hierarchical storage for big data processing",
                "URL": "https://doi.org/10.1109/CCGrid.2016.61"
            }
        },
        {
            "10.1145/1458527.1458538": {
                "id": "10.1145/1458527.1458538",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Askira Gelman",
                        "given": "Irit"
                    },
                    {
                        "family": "Barletta",
                        "given": "Anthony L."
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2008,
                            10,
                            30
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2008,
                            10,
                            30
                        ]
                    ]
                },
                "abstract": "This short paper outlines a research study in progress, which is motivated by the perception that the spelling error rate of a document can serve as a rudimentary proxy for the degree of quality control exercised in its creation, and, subsequently, indicate its quality. One objective of this research is to validate this understanding. Ultimately, the goal of this research is to take advantage of such an association. In particular, we propose a simple, \"quick and dirty\" metric for assisting in the evaluation of the quality of websites. This metric utilizes the reported hit counts of search engine queries on a pre-determined set of commonly misspelled words.",
                "call-number": "10.1145/1458527.1458538",
                "collection-title": "WICOW '08",
                "container-title": "Proceedings of the 2nd ACM workshop on Information credibility on the web",
                "DOI": "10.1145/1458527.1458538",
                "event-place": "Napa Valley, California, USA",
                "ISBN": "9781605582597",
                "keyword": "information credibility, indicator, information quality, web data, data quality indicator, data quality, quick and dirty",
                "number-of-pages": "4",
                "page": "43–46",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "A \"quick and dirty\" website data quality indicator",
                "URL": "https://doi.org/10.1145/1458527.1458538"
            }
        },
        {
            "10.1145/3448823.3448879": {
                "id": "10.1145/3448823.3448879",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Fraccaro",
                        "given": "Paolo"
                    },
                    {
                        "family": "Benatan",
                        "given": "Matt"
                    },
                    {
                        "family": "Reusch",
                        "given": "Katharina"
                    },
                    {
                        "family": "Fare",
                        "given": "Clyde"
                    },
                    {
                        "family": "Edwards",
                        "given": "Blair"
                    },
                    {
                        "family": "Pyzer-Knapp",
                        "given": "Edward"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2020,
                            12,
                            9
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2020,
                            12,
                            9
                        ]
                    ]
                },
                "abstract": "Estimating mobile signal strength accurately is a crucial task for network providers and their customers. However, current methodologies to estimate mobile signal strength present limitations in their practical implementation (i.e. physical based models), portability (i.e. spatial interpolation methods), simplicity and accuracy (i.e. path loss models). In this paper we present a novel approach that takes advantage of geospatial Big Data and advanced Artificial Intelligence to predict mobile signal strength at scale. Particularly, we used open access geo-spatial information about weather, tree coverage, land use, imperviousness, altitude and network infrastructure (i.e. a total of 174 features) to train and test uncertainty-aware artificial neural networks to predict mobile signal strength on data from the NetBravo crowdsourcing platform across all the United Kingdom (UK). Our model scored a best performance of 7.9 (standard deviation of 0.2) dBm for Root Mean Squared Error and 5.7 (standard deviation of 0.4) dBm for the Mean Absolute Error. Feature importance analysis showed that mobile cell tower characteristics and geospatial features showing the distribution of imperviousness and tree cover density over the line of sight between the mobile cell tower and the receiver as well as relative humidity were among the top 20 most important features.",
                "call-number": "10.1145/3448823.3448879",
                "collection-number": "49",
                "collection-title": "ICVISP 2020",
                "container-title": "Proceedings of the 2020 4th International Conference on Vision, Image and Signal Processing",
                "DOI": "10.1145/3448823.3448879",
                "event-place": "Bangkok, Thailand",
                "ISBN": "9781450389532",
                "keyword": "Artificial Neural Networks Learning, Geospatial, Mobile Signal Strength, Big Data",
                "number": "Article 49",
                "number-of-pages": "5",
                "page": "1–5",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Modelling Mobile Signal Strength by Combining Geospatial Big Data and Artificial Intelligence",
                "URL": "https://doi.org/10.1145/3448823.3448879"
            }
        },
        {
            "10.1145/3482632.3483031": {
                "id": "10.1145/3482632.3483031",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Zhu",
                        "given": "Jia"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            9,
                            24
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2021,
                            9,
                            24
                        ]
                    ]
                },
                "abstract": "With the development of society, under the influence of high technology such as big data, cloud computing, network technology, and mobile Internet, education has gradually got rid of the traditional teaching methods and the limitations of traditional teaching methods. This article takes college students as the research theme. In order to obtain more complete survey data, the survey is based on an online questionnaire. In this online questionnaire survey, a total of 11 computer classroom teachers from colleges and universities at the city, county, and town levels were invited to supplement the paper questionnaire survey. After checking and verifying the retrieved questionnaire, the excel software technology and statistical data will be used to effectively analyze the questionnaire. Experimental research shows that 84% of teachers believe that big data education is related to computer teaching. They believe that the use of information technology to collect computer learning behaviors in colleges and universities can help improve the efficiency or quality of computer teaching, while only 16% of teachers are concerned about big data. The impact on computer teaching is unclear. Research confirms that high-data technology is related to computer classroom teaching, and the use of information technology to collect computer-based learning behavior can help improve the effectiveness or quality of computer classroom teaching. Big data technology has great potential in computer classroom teaching and will provide practical theoretical support and help for computer classroom teaching. Although the application of big data related technologies in the field of education in my country is still in the initial stage, it is still of great significance for deepening the reform of education and teaching.",
                "call-number": "10.1145/3482632.3483031",
                "collection-title": "ICISCAE 2021",
                "container-title": "2021 4th International Conference on Information Systems and Computer Aided Education",
                "DOI": "10.1145/3482632.3483031",
                "event-place": "Dalian, China",
                "ISBN": "9781450390255",
                "number-of-pages": "4",
                "page": "836–839",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Application of Big Data Technology in Computer Classroom Teaching",
                "URL": "https://doi.org/10.1145/3482632.3483031"
            }
        },
        {
            "10.14778/3137765.3137829": {
                "id": "10.14778/3137765.3137829",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Giatrakos",
                        "given": "Nikos"
                    },
                    {
                        "family": "Artikis",
                        "given": "Alexander"
                    },
                    {
                        "family": "Deligiannakis",
                        "given": "Antonios"
                    },
                    {
                        "family": "Garofalakis",
                        "given": "Minos"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            8,
                            1
                        ]
                    ]
                },
                "abstract": "The concept of event processing is established as a generic computational paradigm in various application fields, ranging from data processing in Web environments, over maritime and transport, to finance and medicine. Events report on state changes of a system and its environment. Complex Event Recognition (CER) in turn, refers to the identification of complex/composite events of interest, which are collections of simple events that satisfy some pattern, thereby providing the opportunity for reactive and proactive measures. Examples include the recognition of attacks in computer network nodes, human activities on video content, emerging stories and trends on the Social Web, traffic and transport incidents in smart cities, fraud in electronic marketplaces, cardiac arrhythmias, and epidemic spread. In each scenario, CER allows to make sense of Big event Data streams and react accordingly. The goal of this tutorial is to provide a step-by-step guide for realizing CER in the Big Data era. To do so, it elaborates on major challenges and describes algorithmic toolkits for optimized manipulation of event streams characterized by high volume, velocity and/or lack of veracity, placing emphasis on distributed CER over potentially heterogeneous (data variety) event sources. Finally, we highlight future research directions in the field.",
                "call-number": "10.14778/3137765.3137829",
                "container-title": "Proc. VLDB Endow.",
                "DOI": "10.14778/3137765.3137829",
                "ISSN": "2150-8097",
                "issue": "12",
                "number-of-pages": "4",
                "page": "1996–1999",
                "publisher": "VLDB Endowment",
                "source": "August 2017",
                "title": "Complex event recognition in the big data era",
                "URL": "https://doi.org/10.14778/3137765.3137829",
                "volume": "10"
            }
        },
        {
            "10.1007/s00778-019-00557-w": {
                "id": "10.1007/s00778-019-00557-w",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Giatrakos",
                        "given": "Nikos"
                    },
                    {
                        "family": "Alevizos",
                        "given": "Elias"
                    },
                    {
                        "family": "Artikis",
                        "given": "Alexander"
                    },
                    {
                        "family": "Deligiannakis",
                        "given": "Antonios"
                    },
                    {
                        "family": "Garofalakis",
                        "given": "Minos"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2020,
                            1,
                            1
                        ]
                    ]
                },
                "abstract": "The concept of event processing is established as a generic computational paradigm in various application fields. Events report on state changes of a system and its environment. Complex event recognition (CER) refers to the identification of composite events of interest, which are collections of simple, derived events that satisfy some pattern, thereby providing the opportunity for reactive and proactive measures. Examples include the recognition of anomalies in maritime surveillance, electronic fraud, cardiac arrhythmias and epidemic spread. This survey elaborates on the whole pipeline from the time CER queries are expressed in the most prominent languages, to algorithmic toolkits for scaling-out CER to clustered and geo-distributed architectural settings. We also highlight future research directions.",
                "call-number": "10.1007/s00778-019-00557-w",
                "container-title": "The VLDB Journal",
                "DOI": "10.1007/s00778-019-00557-w",
                "ISSN": "1066-8888",
                "issue": "1",
                "keyword": "Complex event recognition, Parallelism, Distributed processing, Big Data, Complex event recognition languages, Elasticity",
                "number-of-pages": "40",
                "page": "313–352",
                "publisher": "Springer-Verlag",
                "publisher-place": "Berlin, Heidelberg",
                "source": "Jan 2020",
                "title": "Complex event recognition in the Big Data era: a survey",
                "URL": "https://doi.org/10.1007/s00778-019-00557-w",
                "volume": "29"
            }
        },
        {
            "10.1145/3383583.3398560": {
                "id": "10.1145/3383583.3398560",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Liao",
                        "given": "Longwen"
                    },
                    {
                        "family": "Li",
                        "given": "Qi"
                    },
                    {
                        "family": "Chen",
                        "given": "Junyan"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2020,
                            8,
                            1
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2020,
                            8,
                            1
                        ]
                    ]
                },
                "abstract": "To explore the key research technology knowledge graphs related to Library and Information Science(LIS), articles between 1998 and 2020 were collected from the \"Web of Science TM\". By using the visualization software CiteSpace, the pivotal literature related to big data in the field of LIS, as well as countries, institutions, and keywords, were visualized and recognized. The results show that the research hot spots in this field mainly include: big data brings influences and challenges to LIS, big data analysis technology, and data management and user privacy.",
                "call-number": "10.1145/3383583.3398560",
                "collection-title": "JCDL '20",
                "container-title": "Proceedings of the ACM/IEEE Joint Conference on Digital Libraries in 2020",
                "DOI": "10.1145/3383583.3398560",
                "event-place": "Virtual Event, China",
                "ISBN": "9781450375856",
                "keyword": "citespace, big data, LIS",
                "number-of-pages": "2",
                "page": "453–454",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Research on the Development of Library and Information Science in the Era of Big Data",
                "URL": "https://doi.org/10.1145/3383583.3398560"
            }
        },
        {
            "10.1145/3477911.3477921": {
                "id": "10.1145/3477911.3477921",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Islam",
                        "given": "Mahmudul"
                    },
                    {
                        "family": "Hasan",
                        "given": "Mahady"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            7,
                            13
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2021,
                            7,
                            13
                        ]
                    ]
                },
                "abstract": "Energy is one of the key factors for a country's economic and social growth. Bangladesh is constantly seeking sustainable energy sources for securing its increasing energy demand as energy security is a national concern. Currently, Bangladesh is producing 64% of electricity using natural gas while 25% is coming from petroleum and the rest from coal, renewable sources [1]. To reduce pressure from the natural gas government is trying to diversify the energy sources. To achieve sustainable diversified energy sources, the Sustainable and Renewable Energy Development Authority (SREDA) has taken many initiatives. Energy Master Plan has been created which targets to improve Primary Energy consumption per GDP by 20% in the year 2030. Many software applications have been developed by SREDA to monitor the progress. We propose to develop another system for energy data analysis and generate reports accordingly. Policymakers can use those reports to make strategic decisions to secure the country's energy consumption, distribution and manage the demand side. In this research work, we discussed how a big data-based solution can be developed and used to forecast the energy balance which ensures the energy security of Bangladesh. We also discuss different aspects of the proposed software system in detail including challenges and possible solutions to overcome those challenges.",
                "call-number": "10.1145/3477911.3477921",
                "collection-title": "ICCTA 2021",
                "container-title": "2021 7th International Conference on Computer Technology Applications",
                "DOI": "10.1145/3477911.3477921",
                "event-place": "Vienna, Austria",
                "ISBN": "9781450390521",
                "keyword": "Energy security, Renewable Energy, Big Data, Data Analysis, Sustainable energy with big data",
                "number-of-pages": "6",
                "page": "60–65",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Big Data Analysis Driven Decision Making System Ensuring Energy Security of a Country",
                "URL": "https://doi.org/10.1145/3477911.3477921"
            }
        },
        {
            "10.1145/2539150.2539154": {
                "id": "10.1145/2539150.2539154",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Pokorny",
                        "given": "Jaroslav"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2013,
                            12,
                            2
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2013,
                            12,
                            2
                        ]
                    ]
                },
                "abstract": "Big Data, big number of users, and cloud computing are driving the adoption of new database architectures, particularly NoSQL databases. Both research and practice indicate that traditional universal DBMS architecture hardly satisfies new trends in data processing in such environment. NoSQL databases enable better application development productivity through a more flexible data model, greater ability to scale dynamically to support more users and data, an ability to develop highly responsive applications and more complex processing of data.On the other hand, NoSQL databases support solving data problems only partially. We will describe their basic features like horizontal scalability and concurrency model, which offer mostly weaker tools for querying and transactions processing than relational SQL-like database systems do. We will also present some data models and querying capabilities of NoSQL databases in more detail as well as an overview of some their representatives.The NoSQL system properties mentioned imply that most of them are unsuitable, e.g., for the DW and BI querying or, in general, for the enterprise data processing. Consequently, new database architectures and various hybrid solutions are developed. We will point out on these actual problems and present some of the current approaches in detail.",
                "call-number": "10.1145/2539150.2539154",
                "collection-title": "IIWAS '13",
                "container-title": "Proceedings of International Conference on Information Integration and Web-based Applications & Services",
                "DOI": "10.1145/2539150.2539154",
                "event-place": "Vienna, Austria",
                "ISBN": "9781450321136",
                "number-of-pages": "1",
                "page": "4",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "NoSQL databases - no panacea for Big Data processing",
                "URL": "https://doi.org/10.1145/2539150.2539154"
            }
        },
        {
            "10.1145/3286606.3286793": {
                "id": "10.1145/3286606.3286793",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "El Haourani",
                        "given": "Lamia"
                    },
                    {
                        "family": "Elkalam",
                        "given": "Anas Abou"
                    },
                    {
                        "family": "Ouahman",
                        "given": "Abdelah Ait"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            10,
                            10
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            10,
                            10
                        ]
                    ]
                },
                "abstract": "The most popular features of Big Data revolve around the so-called \"3V\" criterion: Volume, Variety and Velocity. Big Data is based on the massive collection and in-depth analysis of personal data, with a view to profiling, or even marketing and commercialization, thus violating citizens' privacy and the security of their data.In this article we discuss security and privacy solutions in the context of Big Data. We then focus on access control and present our new model called Knowledge-based Access Control (KBAC); this strengthens the access control already deployed in the target company (e.g., based on \"RBAC\" role or \"ABAC\" attributes for example) by adding a semantic access control layer. KBAC offers thinner access control, tailored to Big Data, with effective protection against intrusion attempts and unauthorized data inferences.",
                "call-number": "10.1145/3286606.3286793",
                "collection-number": "16",
                "collection-title": "SCA '18",
                "container-title": "Proceedings of the 3rd International Conference on Smart City Applications",
                "DOI": "10.1145/3286606.3286793",
                "event-place": "Tetouan, Morocco",
                "ISBN": "9781450365628",
                "keyword": "KBAC, security, Big Data, access control model, privacy",
                "number": "Article 16",
                "number-of-pages": "8",
                "page": "1–8",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Knowledge Based Access Control a model for security and privacy in the Big Data",
                "URL": "https://doi.org/10.1145/3286606.3286793"
            }
        },
        {
            "10.1145/3529836.3529950": {
                "id": "10.1145/3529836.3529950",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Zhai",
                        "given": "Chenggong"
                    },
                    {
                        "family": "Zhang",
                        "given": "Heng"
                    },
                    {
                        "family": "Li",
                        "given": "Xiaoli"
                    },
                    {
                        "family": "Chen",
                        "given": "Shan"
                    },
                    {
                        "family": "Zhou",
                        "given": "Liyong"
                    },
                    {
                        "family": "Wu",
                        "given": "Rangming"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2022,
                            2,
                            18
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2022,
                            2,
                            18
                        ]
                    ]
                },
                "abstract": "The thin air, low oxygen content, harsh environment, sparse population, desolate ground and difficult concealment in the severe cold area of the plateau have a great impact on the food security of personnel going to the severe cold area of the plateau. Researchers went to the food security in the severe cold areas of the plateau, made solid preparations for various security, and achieved results for food security at the first time, which plays an important role in building and completing the food system in China. Focusing on the impact and requirements of plateau cold areas on food security, combined with the difficulties and practical problems faced by personnel going to plateau cold areas and big data technology, this paper analyzes the overview and characteristics of big data, the needs of plateau hypoxia resistance and fatigue resistance, and the advantages of plateau hypoxia resistance and fatigue resistance food, and puts forward the plateau hypoxia resistance and fatigue resistance based on big data The conception of anti fatigue food system, and the plateau food data safety system is constructed.",
                "call-number": "10.1145/3529836.3529950",
                "collection-title": "ICMLC 2022",
                "container-title": "2022 14th International Conference on Machine Learning and Computing (ICMLC)",
                "DOI": "10.1145/3529836.3529950",
                "event-place": "Guangzhou, China",
                "ISBN": "9781450395700",
                "keyword": "Resist fatigue, Plateau, Big data, Hypoxia tolerance",
                "number-of-pages": "7",
                "page": "153–159",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Research on Hypoxia and Fatigue Resistant Food at Plateau Based on Big Data Technology",
                "URL": "https://doi.org/10.1145/3529836.3529950"
            }
        },
        {
            "10.1145/3014812.3014886": {
                "id": "10.1145/3014812.3014886",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Qu",
                        "given": "Youyang"
                    },
                    {
                        "family": "Xu",
                        "given": "Jiyang"
                    },
                    {
                        "family": "Yu",
                        "given": "Shui"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            1,
                            31
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            1,
                            30
                        ]
                    ]
                },
                "abstract": "Big data privacy-preserving has attracted increasing attention of researchers in recent years. But existing models are so complicated and time-consuming that they are not easy to implement. In this paper, we propose a more feasible and efficient model for big data sets privacy-preserving using shuffling multiple attributes(M-Shuffle) to achieve a tradeoff between data utility and privacy. Our strategy is firstly categorize all the records into some groups using K-means algorithm according to the sensitive attributes. Then we choose the columns to be shuffled using entropy. At last we introduce the random shuffle algorithm to our model to break the correlation among the columns of big data sets. Experiments on real-world datasets show that our framework achieves excellent data utility and efficiency while satisfying privacy-preserving.",
                "call-number": "10.1145/3014812.3014886",
                "collection-number": "72",
                "collection-title": "ACSW '17",
                "container-title": "Proceedings of the Australasian Computer Science Week Multiconference",
                "DOI": "10.1145/3014812.3014886",
                "event-place": "Geelong, Australia",
                "ISBN": "9781450347686",
                "keyword": "privacy-preserving, M-shuffle machenism, K-means",
                "number": "Article 72",
                "number-of-pages": "8",
                "page": "1–8",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Privacy preserving in big data sets through multiple shuffle",
                "URL": "https://doi.org/10.1145/3014812.3014886"
            }
        },
        {
            "10.1145/3419604.3419620": {
                "id": "10.1145/3419604.3419620",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Amazal",
                        "given": "Houda"
                    },
                    {
                        "family": "Ramdani",
                        "given": "Mohammed"
                    },
                    {
                        "family": "Kissi",
                        "given": "Mohamed"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2020,
                            9,
                            23
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2020,
                            9,
                            23
                        ]
                    ]
                },
                "abstract": "In big data era, text classification is considered as one of the most important machine learning application domain. However, to build an efficient algorithm for classification, feature selection is a fundamental step to reduce dimensionality, achieve better accuracy and improve time execution. In the literature, most of the feature ranking techniques are document based. The major weakness of this approach is that it favours the terms occurring frequently in the documents and neglects the correlation between the terms and the categories. In this work, unlike the traditional approaches which deal with documents individually, we use mapreduce paradigm to process the documents of each category as a single document. Then, we introduce a parallel frequency-category feature selection method independently of any classifier to select the most relevant features. Experimental results on the 20-Newsgroups dataset showed that our approach improves the classification accuracy to 90.3%. Moreover, the system maintains the simplicity and lower execution time.",
                "call-number": "10.1145/3419604.3419620",
                "collection-number": "14",
                "collection-title": "SITA'20",
                "container-title": "Proceedings of the 13th International Conference on Intelligent Systems: Theories and Applications",
                "DOI": "10.1145/3419604.3419620",
                "event-place": "Rabat, Morocco",
                "ISBN": "9781450377331",
                "keyword": "Machine Learning, Text classification, Naive Bayes, TF-IDF, Big Data, MapReduce",
                "number": "Article 14",
                "number-of-pages": "6",
                "page": "1–6",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "A Frequency-Category Based Feature Selection in Big Data for Text Classification",
                "URL": "https://doi.org/10.1145/3419604.3419620"
            }
        },
        {
            "10.1145/3207677.3278079": {
                "id": "10.1145/3207677.3278079",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Zhao",
                        "given": "Zhuo"
                    },
                    {
                        "family": "Li",
                        "given": "Xingying"
                    },
                    {
                        "family": "Li",
                        "given": "Shanzi"
                    },
                    {
                        "family": "Wu",
                        "given": "Yixuan"
                    },
                    {
                        "family": "Zhao",
                        "given": "Xin"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            10,
                            22
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            10,
                            22
                        ]
                    ]
                },
                "abstract": "High volumes1 of business data is continuously produced by different kinds of information system, which provides big values for the official statistics. However, it is not easy to leverage big volume of business data for the statistical analysis under existing technologies, since they are generally distributed multiple sourced heterogeneous data set. In this paper, we first present the problem scenario and discuss in details the challenges confronting with the problem. Then, we propose an analytical framework for the distributed multiple sourced heterogeneous data set based on the service oriented architecture. Finally, we present a prototype of elementary statistical services for the primary data analysis tasks based on the proposed analytic service framework.",
                "call-number": "10.1145/3207677.3278079",
                "collection-number": "32",
                "collection-title": "CSAE '18",
                "container-title": "Proceedings of the 2nd International Conference on Computer Science and Application Engineering",
                "DOI": "10.1145/3207677.3278079",
                "event-place": "Hohhot, China",
                "ISBN": "9781450365123",
                "keyword": "Big data, official statistics, data mining, web services, distributed computing, multiple sourced dataset",
                "number": "Article 32",
                "number-of-pages": "5",
                "page": "1–5",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Towards the Big Data in Official Statistics: An Analytic Service Framework for Distributed Multiple Sourced Heterogeneous Datasets",
                "URL": "https://doi.org/10.1145/3207677.3278079"
            }
        },
        {
            "10.1145/3155133.3155138": {
                "id": "10.1145/3155133.3155138",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Onizuka",
                        "given": "Makoto"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            12,
                            7
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            12,
                            7
                        ]
                    ]
                },
                "abstract": "As people say \"Data is the new oil,\" Big data is expected to make a large impact on our society and economics by mining hidden knowledge and rules from the data. In particular, the structure of the real world data is changing from traditional relational data model to more generalized graph data model, as the web and social media are getting popular in the world. One of the most important technical challenges here is to efficiently analyze large graph data that express various types of relationship between people, items, and places. In this talk, we overview the trend of Big Data and IoT and then explain our research on distributed query optimization on cloud environment and efficient graph mining algorithms. Finally, we introduce some of our interesting applications of Big Data: 1) social network analysis by employing graph mining algorithms, 2) business data analysis by exploratory data analysis techniques, and 3) Smart route recommendation system empowered by IoT.",
                "call-number": "10.1145/3155133.3155138",
                "collection-title": "SoICT 2017",
                "container-title": "Proceedings of the Eighth International Symposium on Information and Communication Technology",
                "DOI": "10.1145/3155133.3155138",
                "event-place": "Nha Trang City, Viet Nam",
                "ISBN": "9781450353281",
                "number-of-pages": "1",
                "page": "5",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Trend and applications of Big Data and IoT techniques",
                "URL": "https://doi.org/10.1145/3155133.3155138"
            }
        },
        {
            "10.1145/2642769.2642802": {
                "id": "10.1145/2642769.2642802",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Cheptsov",
                        "given": "Alexey"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2014,
                            9,
                            9
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2014,
                            9,
                            9
                        ]
                    ]
                },
                "abstract": "The current IT technologies have a strong need for scaling up the high-performance analysis to large-scale datasets. Tremendously increased over the last few years volume and complexity of data gathered in both public (such as on the web) and enterprise (e.g. digitalized internal document base) domains have posed new challenges to providers of high performance computing (HPC) infrastructures, which is recognised in the community as Big Data problem. On contrast to the typical HPC applications, the Big Data ones are not oriented on reaching the peak performance of the infrastructure and thus offer more opportunities for the \"capacity\" infrastructure model rather than for the \"capability\" one, making the use of Cloud infrastructures preferable over the HPC. However, considering the more and more vanishing difference between these two infrastructure types, i.e. Cloud and HPC, it makes a lot of sense to investigate the abilities of traditional HPC infrastructure to execute Big Data applications as well, despite their relatively poor efficiency as compared with the traditional, very optimized HPC ones. This paper discusses the main state-of-the-art parallelisation techniques utilised in both Cloud and HPC domains and evaluates them on an exemplary text processing application on a testbed HPC cluster.",
                "call-number": "10.1145/2642769.2642802",
                "collection-title": "EuroMPI/ASIA '14",
                "container-title": "Proceedings of the 21st European MPI Users' Group Meeting",
                "DOI": "10.1145/2642769.2642802",
                "event-place": "Kyoto, Japan",
                "ISBN": "9781450328753",
                "keyword": "HPC, MPI, MapReduce, Hadoop, Cloud",
                "number-of-pages": "6",
                "page": "175–180",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "HPC in Big Data Age: An Evaluation Report for Java-Based Data-Intensive Applications Implemented with Hadoop and OpenMPI",
                "URL": "https://doi.org/10.1145/2642769.2642802"
            }
        },
        {
            "10.1145/2983642": {
                "id": "10.1145/2983642",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Wu",
                        "given": "Taotao"
                    },
                    {
                        "family": "Dou",
                        "given": "Wanchun"
                    },
                    {
                        "family": "Wu",
                        "given": "Fan"
                    },
                    {
                        "family": "Tang",
                        "given": "Shaojie"
                    },
                    {
                        "family": "Hu",
                        "given": "Chunhua"
                    },
                    {
                        "family": "Chen",
                        "given": "Jinjun"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            10,
                            12
                        ]
                    ]
                },
                "abstract": "With the prosperity of media streaming applications over the Internet in the past decades, multimedia data has sharply increased (categorized as multimedia big data), which exerts more pressure on the infrastructure, such as networking of the application provider. In order to move this hurdle, an increasing number of traditional media streaming applications have migrated from a private server cluster onto the cloud. With the elastic resource provisioning and centralized management of the cloud, the operational costs of media streaming application providers can decrease dramatically. However, to the best of our knowledge, existing migration solutions do not fully take viewer information such as hardware condition into consideration. In this article, we consider the deployment optimization problem named ODP by leveraging local memories at each viewer. Considering the NP-hardness of calculating the optimal solution, we turn to propose computationally tractable algorithms. Specifically, we unfold the original problem into two interactive subproblems: coarse-grained migration subproblem and fine-grained scheduling subproblem. Then, the corresponding offline approximation algorithms with performance guarantee and computational efficiency are given. The results of extensive evaluation show that compared with the baseline algorithm without leveraging local memories at viewers, our proposed algorithms and their online versions can decrease total bandwidth reservation and enhance the utilization of bandwidth reservation dramatically.",
                "call-number": "10.1145/2983642",
                "collection-number": "73",
                "container-title": "ACM Trans. Multimedia Comput. Commun. Appl.",
                "DOI": "10.1145/2983642",
                "ISSN": "1551-6857",
                "issue": "5s",
                "keyword": "Large-scale media streaming application, cloud, deployment optimization, local memory, multimedia big data",
                "number": "Article 73",
                "number-of-pages": "23",
                "page": "1–23",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "December 2016",
                "title": "A Deployment Optimization Scheme Over Multimedia Big Data for Large-Scale Media Streaming Application",
                "URL": "https://doi.org/10.1145/2983642",
                "volume": "12"
            }
        },
        {
            "10.1109/CCGRID.2017.143": {
                "id": "10.1109/CCGRID.2017.143",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Salaria",
                        "given": "Shweta"
                    },
                    {
                        "family": "Brown",
                        "given": "Kevin"
                    },
                    {
                        "family": "Jitsumoto",
                        "given": "Hideyuki"
                    },
                    {
                        "family": "Matsuoka",
                        "given": "Satoshi"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            5,
                            14
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            5,
                            14
                        ]
                    ]
                },
                "abstract": "The path to HPC-Big Data convergence has resulted in numerous researches that demonstrate the performance trade-off between running applications on supercomputers and cloud platforms. Previous studies typically focus either on scientific HPC benchmarks or previous cloud configurations, failing to consider all the new opportunities offered by current cloud offerings. We present a comparative study of the performance of representative big data benchmarks, or \"Big Data Ogres\", and HPC benchmarks running on supercomputer and cloud. Our work distinguishes itself from previous studies in a way that we explore the latest generation of compute-optimized Amazon Elastic Compute Cloud instances, C4 for our experimentation on cloud. Our results reveal that Amazon C4 instances with increased compute performance and low variability in results make EC2-based cluster feasible for scientific computing and its applications in simulations, modeling and analysis.",
                "call-number": "10.1109/CCGRID.2017.143",
                "collection-title": "CCGrid '17",
                "container-title": "Proceedings of the 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing",
                "DOI": "10.1109/CCGRID.2017.143",
                "event-place": "Madrid, Spain",
                "ISBN": "9781509066100",
                "keyword": "Graph500, Amazon EC2 C4, Supercomputers, Performance evaluation",
                "number-of-pages": "9",
                "page": "1053–1061",
                "publisher": "IEEE Press",
                "title": "Evaluation of HPC-Big Data Applications Using Cloud Platforms",
                "URL": "https://doi.org/10.1109/CCGRID.2017.143"
            }
        },
        {
            "10.1145/1363686.1363915": {
                "id": "10.1145/1363686.1363915",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Cutt",
                        "given": "Bryce"
                    },
                    {
                        "family": "Lawrence",
                        "given": "Ramon"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2008,
                            3,
                            16
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2008,
                            3,
                            16
                        ]
                    ]
                },
                "abstract": "Sensor networks collect vast amounts of real-time information about the environment, business processes, and systems. Archived sensor data is valuable for long-term analysis and decision making, which requires it be suitably archived, indexed, and validated. In this paper, we describe a general approach to managing and improving data quality by the generation and validation of metadata and the logging of workflow events. The approach has been implemented within a system archiving terabytes of U.S. weather radar data. The data quality system has resulted in the detection of data errors while simplifying the administration of the complex archive system.",
                "call-number": "10.1145/1363686.1363915",
                "collection-title": "SAC '08",
                "container-title": "Proceedings of the 2008 ACM symposium on Applied computing",
                "DOI": "10.1145/1363686.1363915",
                "event-place": "Fortaleza, Ceara, Brazil",
                "ISBN": "9781595937537",
                "keyword": "archive, hydrology, sensor network, data quality, real-time warehouse, scientific data",
                "number-of-pages": "5",
                "page": "982–986",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Managing data quality in a terabyte-scale sensor archive",
                "URL": "https://doi.org/10.1145/1363686.1363915"
            }
        },
        {
            "10.14778/2733004.2733069": {
                "id": "10.14778/2733004.2733069",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Suchanek",
                        "given": "Fabian M."
                    },
                    {
                        "family": "Weikum",
                        "given": "Gerhard"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2014,
                            8,
                            1
                        ]
                    ]
                },
                "abstract": "This tutorial gives an overview on state-of-the-art methods for the automatic construction of large knowledge bases and harnessing them for data and text analytics. It covers both big-data methods for building knowledge bases and knowledge bases being assets for big-data applications. The tutorial also points out challenges and research opportunities.",
                "call-number": "10.14778/2733004.2733069",
                "container-title": "Proc. VLDB Endow.",
                "DOI": "10.14778/2733004.2733069",
                "ISSN": "2150-8097",
                "issue": "13",
                "number-of-pages": "2",
                "page": "1713–1714",
                "publisher": "VLDB Endowment",
                "source": "August 2014",
                "title": "Knowledge bases in the age of big data analytics",
                "URL": "https://doi.org/10.14778/2733004.2733069",
                "volume": "7"
            }
        },
        {
            "10.1145/3012258.3012268": {
                "id": "10.1145/3012258.3012268",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Qian",
                        "given": "Xuesheng"
                    },
                    {
                        "family": "Xu",
                        "given": "Yifeng"
                    },
                    {
                        "family": "Zhang",
                        "given": "Jing"
                    },
                    {
                        "family": "Zhao",
                        "given": "Wei"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            11,
                            2
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2016,
                            11,
                            2
                        ]
                    ]
                },
                "abstract": "Organizational evaluation plays an essential role in the decision-making and administrative management. Although there are numbers of conventional evaluation methods, the assessing result is subjective and limited by the application fields of data utilization, let along other drawbacks. With the perspective of big data, this thesis employs lower granularity and promotes the big-data based novel organizational evaluation pattern, in order to overcome the shortage of the conventional evaluation system. Then the thesis applies the big data method to elucidate an arduous assessing analysis case - the typical school evaluation problem with the process of standardizing and normalizing the 200,000 students' activity information in hundreds of k-12 schools of a district in Shanghai. Accordingly, the thesis accomplishes the assessment diagnosis on the school management and shares the effective supplements and risk warnings compared with the conventional ones.",
                "call-number": "10.1145/3012258.3012268",
                "collection-title": "ICIME 2016",
                "container-title": "Proceedings of the 2016 8th International Conference on Information Management and Engineering",
                "DOI": "10.1145/3012258.3012268",
                "event-place": "Istanbul, Turkey",
                "ISBN": "9781450347617",
                "keyword": "Chinese Short Texts De-duplication, Big-Data Assessment, Organizational Evaluation, Data Acquisition, Behavior Record",
                "number-of-pages": "6",
                "page": "30–35",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Novel Method for Organizational Evaluation and Practice Based on Big Data Analysis",
                "URL": "https://doi.org/10.1145/3012258.3012268"
            }
        },
        {
            "10.1145/3488466.3488493": {
                "id": "10.1145/3488466.3488493",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Cai",
                        "given": "Huiying"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            9,
                            15
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2021,
                            9,
                            15
                        ]
                    ]
                },
                "abstract": "With the improvement of network, online teaching has gradually become an important auxiliary teaching mean. In the whole process of teaching and learning, a large amount of educational data is produced. Educational data contain a lot of information to be mined, which is helpful to improve the quality of learning and teaching. This paper will explore how to integrate these educational data that comes from different educational platform to guide customizable education which refers to determine the learning or teaching content independently. To realize the customizable education, the evaluation indicators and detail scheme to make use of the educational big data are proposed. The scheme consists of the acquisition, analysis and visualization of the data for different evaluation indicators. The purpose of this paper is to make these data guide undergraduates to carry out targeted autonomous learning according to their states to promote the learning progress. It can also guide teachers to carry out targeted teaching activities to improve teaching quality. And it is also helpful for teaching managers to have an insight into the learning state of students and the teaching state of the teachers, so as to put forward more reasonable teaching plans and countermeasures. This paper provided a whole framework for the application of the educational big data which can be extended by different educational institutions.",
                "call-number": "10.1145/3488466.3488493",
                "collection-title": "ICDTE 2021",
                "container-title": "2021 5th International Conference on Digital Technology in Education",
                "DOI": "10.1145/3488466.3488493",
                "event-place": "Busan, Republic of Korea",
                "ISBN": "9781450384995",
                "keyword": "Educational big data, Visualization, Mining, Customizable",
                "number-of-pages": "6",
                "page": "61–66",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Discussion on Customizable Education of Colleges Based on Educational Big Data: Customizable Education of Colleges",
                "URL": "https://doi.org/10.1145/3488466.3488493"
            }
        },
        {
            "10.1145/3341069.3341086": {
                "id": "10.1145/3341069.3341086",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Pengxi",
                        "given": "Li"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            6,
                            22
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            6,
                            22
                        ]
                    ]
                },
                "abstract": "Under the background of the rapid development of big data technology, the construction of college informationization teaching service system is the basis of the informationization teaching in colleges and universities. That is very important for the success of the informatization in Universities what is service realization model, business logic, architecture and platform conform to the whole development strategy of universities. Information management organization supports the planning, implementation, operation, maintenance and management of business information system. This paper analyzes the reform mode of college education information service system supported by big data technology. Based on the analysis of the reform mode of college informationization teaching service system supported by big data technology, this paper puts forward the design idea of post system based on big data. At the same time, with the case of \"big data assisted employment\", the post design and adjustment were carried out. The results show that big data assisted employment has greatly improved the efficiency and quality of the school's employment department, providing students with better employment security. Finally, the problems that need to be solved in the informatization teaching service are sorted out.",
                "call-number": "10.1145/3341069.3341086",
                "collection-title": "HPCCT 2019",
                "container-title": "Proceedings of the 2019 3rd High Performance Computing and Cluster Technologies Conference",
                "DOI": "10.1145/3341069.3341086",
                "event-place": "Guangzhou, China",
                "ISBN": "9781450371858",
                "keyword": "Teaching informatization, Service system, Big data",
                "number-of-pages": "5",
                "page": "185–189",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "The Construction Study of College Informationization Teaching Service System under the Background of Big Data",
                "URL": "https://doi.org/10.1145/3341069.3341086"
            }
        },
        {
            "10.1109/TCBB.2014.2351800": {
                "id": "10.1109/TCBB.2014.2351800",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Wang",
                        "given": "Chao"
                    },
                    {
                        "family": "Li",
                        "given": "Xi"
                    },
                    {
                        "family": "Chen",
                        "given": "Peng"
                    },
                    {
                        "family": "Wang",
                        "given": "Aili"
                    },
                    {
                        "family": "Zhou",
                        "given": "Xuehai"
                    },
                    {
                        "family": "Yu",
                        "given": "Hong"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2015,
                            1,
                            1
                        ]
                    ]
                },
                "abstract": "The next generation genome sequencing problem with short (long) reads is an emerging field in numerous scientific and big data research domains. However, data sizes and ease of access for scientific researchers are growing and most current methodologies rely on one acceleration approach and so cannot meet the requirements imposed by explosive data scales and complexities. In this paper, we propose a novel FPGA-based acceleration solution with MapReduce framework on multiple hardware accelerators. The combination of hardware acceleration and MapReduce execution flow could greatly accelerate the task of aligning short length reads to a known reference genome. To evaluate the performance and other metrics, we conducted a theoretical speedup analysis on a MapReduce programming platform, which demonstrates that our proposed architecture have efficient potential to improve the speedup for large scale genome sequencing applications. Also, as a practical study, we have built a hardware prototype on the real Xilinx FPGA chip. Significant metrics on speedup, sensitivity, mapping quality, error rate, and hardware cost are evaluated, respectively. Experimental results demonstrate that the proposed platform could efficiently accelerate the next generation sequencing problem with satisfactory accuracy and acceptable hardware cost.",
                "call-number": "10.1109/TCBB.2014.2351800",
                "container-title": "IEEE/ACM Trans. Comput. Biol. Bioinformatics",
                "DOI": "10.1109/TCBB.2014.2351800",
                "ISSN": "1545-5963",
                "issue": "1",
                "keyword": "short reads, reconfigurable hardware, genome sequencing, FPGA, mapping",
                "number-of-pages": "13",
                "page": "166–178",
                "publisher": "IEEE Computer Society Press",
                "publisher-place": "Washington, DC, USA",
                "source": "January/February 2015",
                "title": "Heterogeneous cloud framework for big data genome sequencing",
                "URL": "https://doi.org/10.1109/TCBB.2014.2351800",
                "volume": "12"
            }
        },
        {
            "10.1145/3356998.3365776": {
                "id": "10.1145/3356998.3365776",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Zhang",
                        "given": "Mingke"
                    },
                    {
                        "family": "Guo",
                        "given": "Danhuai"
                    },
                    {
                        "family": "Hu",
                        "given": "Jinyong"
                    },
                    {
                        "family": "Jin",
                        "given": "Wei"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            11,
                            5
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            11,
                            5
                        ]
                    ]
                },
                "abstract": "In recent years, the outbreak of foodborne diseases has been on an upward trend clearly. It is of great significance for us to predict the outbreak of foodborne diseases accurately and conduct quantitative risk assessment timely. Traditional prediction methods based on a single data source have drawbacks such as complex prediction processes and inaccurate prediction results. In this article, we figure out the scientific issues of how to improve the temporal and spatial accuracy of foodborne disease outbreak risk prediction in Beijing. Firstly, we analyze the different foodborne disease risk factors caused by the spread of water pollution in Beijing, and study the methods of collecting and preprocessing multi-source data. Then, through the comparison of different regression models and parameters tuning, we propose the use of Gradient Boosting Regression (GBR) model as a multi-source data fusion model to predict the outbreak of foodborne disease. Finally, we use the risk map to detect and predict foodborne disease outbreak in different business districts of Beijing based on visualization techniques, aiming to provide prevention and control assessment for decision-makers quickly and precisely.",
                "call-number": "10.1145/3356998.3365776",
                "collection-number": "8",
                "collection-title": "EM-GIS '19",
                "container-title": "Proceedings of the 5th ACM SIGSPATIAL International Workshop on the Use of GIS in Emergency Management",
                "DOI": "10.1145/3356998.3365776",
                "event-place": "Chicago, Illinois",
                "ISBN": "9781450369657",
                "keyword": "foodborne disease, risk assessment, big data, machine learning",
                "number": "Article 8",
                "number-of-pages": "6",
                "page": "1–6",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Risk prediction and assessment of foodborne disease based on big data",
                "URL": "https://doi.org/10.1145/3356998.3365776"
            }
        },
        {
            "10.1145/3075564.3078885": {
                "id": "10.1145/3075564.3078885",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Vermij",
                        "given": "Erik"
                    },
                    {
                        "family": "Fiorin",
                        "given": "Leandro"
                    },
                    {
                        "family": "Hagleitner",
                        "given": "Christoph"
                    },
                    {
                        "family": "Bertels",
                        "given": "Koen"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            5,
                            15
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            5,
                            15
                        ]
                    ]
                },
                "abstract": "Big data workloads assumed recently a relevant importance in many business and scientific applications. Sorting elements efficiently in big data workloads is a key operation. In this work, we analyze the implementation of the mergesort algorithm on heterogeneous systems composed of CPUs and near-data processors located on the system memory channels. For configurations with equal number of active CPU cores and near-data processors, our experiments show a performance speedup of up to 2.5, as well as up to 2.5x energy-per-solution reduction.",
                "call-number": "10.1145/3075564.3078885",
                "collection-title": "CF'17",
                "container-title": "Proceedings of the Computing Frontiers Conference",
                "DOI": "10.1145/3075564.3078885",
                "event-place": "Siena, Italy",
                "ISBN": "9781450344876",
                "number-of-pages": "6",
                "page": "349–354",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Sorting big data on heterogeneous near-data processing systems",
                "URL": "https://doi.org/10.1145/3075564.3078885"
            }
        },
        {
            "10.5555/3344051.3344077": {
                "id": "10.5555/3344051.3344077",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "DePratti",
                        "given": "Roland"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            4,
                            1
                        ]
                    ]
                },
                "abstract": "In a Big Data Programming course, students often need basic instruction in a new programming language, i.e. Python, Scala, R. Traditional programming language instruction involves a textbook and an Interactive Development Environment (IDE). In a course that already included two textbooks and instruction on Big Data frameworks, the author was looking for an effective way to deliver instructional text and the interactive development capabilities of an IDE that would not add additional cost to the student.",
                "call-number": "10.5555/3344051.3344077",
                "container-title": "J. Comput. Sci. Coll.",
                "ISSN": "1937-4771",
                "issue": "6",
                "number-of-pages": "3",
                "page": "157–159",
                "publisher": "Consortium for Computing Sciences in Colleges",
                "publisher-place": "Evansville, IN, USA",
                "source": "April 2019",
                "title": "Using jupyter notebooks in a big data programming course",
                "volume": "34"
            }
        },
        {
            "10.1145/2856059": {
                "id": "10.1145/2856059",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Cassavia",
                        "given": "Nunziato"
                    },
                    {
                        "family": "Masciari",
                        "given": "Elio"
                    },
                    {
                        "family": "Pulice",
                        "given": "Chiara"
                    },
                    {
                        "family": "Saccà",
                        "given": "Domenico"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            7,
                            29
                        ]
                    ]
                },
                "abstract": "Due to the emerging Big Data paradigm, driven by the increasing availability of intelligent services easily accessible by a large number of users (e.g., social networks), traditional data management techniques are inadequate in many real-life scenarios. In particular, the availability of huge amounts of data pertaining to user social interactions, user preferences, and opinions calls for advanced analysis strategies to understand potentially interesting social dynamics. Furthermore, heterogeneity and high speed of user-generated data require suitable data storage and management tools to be designed from scratch. This article presents a framework tailored for analyzing user interactions with intelligent systems while seeking some domain-specific information (e.g., choosing a good restaurant in a visited area). The framework enhances a user's quest for information by exploiting previous knowledge about their social environment, the extent of influence the users are potentially subject to, and the influence they may exert on other users. User influence spread across the network is dynamically computed as well to improve user search strategy by providing specific suggestions, represented as tailored faceted features. Such features are the result of data exchange activity (called data posting) that enriches information sources with additional background information and knowledge derived from experiences and behavioral properties of domain experts and users. The approach is tested in an important application scenario such as tourist recommendation, but it can be profitably exploited in several other contexts, for example, viral marketing and food education.",
                "call-number": "10.1145/2856059",
                "collection-number": "7",
                "container-title": "ACM Trans. Interact. Intell. Syst.",
                "DOI": "10.1145/2856059",
                "ISSN": "2160-6455",
                "issue": "2",
                "keyword": "NoSQL databases, information extraction, user behavior, intelligent recommendation, personal big data",
                "number": "Article 7",
                "number-of-pages": "33",
                "page": "1–33",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "June 2017",
                "title": "Discovering User Behavioral Features to Enhance Information Search on Big Data",
                "URL": "https://doi.org/10.1145/2856059",
                "volume": "7"
            }
        },
        {
            "10.1145/3399205.3399228": {
                "id": "10.1145/3399205.3399228",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Moumen",
                        "given": "Aniss"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2020,
                            3,
                            11
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2020,
                            3,
                            11
                        ]
                    ]
                },
                "abstract": "The integration of new technologies such as big data, cloud computing, and the Internet of Things (IoT), constitute a new challenge for existing information systems of local administrations.In this work, we present the results of interviews conducted with the public administrations in the South-eastern region of Morocco, and as conclusion we expose two proposed models induced by this study.",
                "call-number": "10.1145/3399205.3399228",
                "collection-number": "21",
                "collection-title": "GEOIT4W-2020",
                "container-title": "Proceedings of the 4th Edition of International Conference on Geo-IT and Water Resources 2020, Geo-IT and Water Resources 2020",
                "DOI": "10.1145/3399205.3399228",
                "event-place": "Al-Hoceima, Morocco",
                "ISBN": "9781450375788",
                "keyword": "Big data, IoT, Information System, Cloud Computing",
                "number": "Article 21",
                "number-of-pages": "4",
                "page": "1–4",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Adoption of Big Data, Cloud Computing & IoT in Morocco Perception of Public Administrations Collaborators",
                "URL": "https://doi.org/10.1145/3399205.3399228"
            }
        },
        {
            "10.5555/2735522.2735575": {
                "id": "10.5555/2735522.2735575",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Khalifa",
                        "given": "Shadi"
                    },
                    {
                        "family": "Martin",
                        "given": "Patrick"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2014,
                            11,
                            3
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2014,
                            11,
                            3
                        ]
                    ]
                },
                "abstract": "We propose the Smart Big Data Analytics as a Service framework. A framework to empower in-house business users with intelligent assistance throughout the analytics process. It provides distributed in-memory data processing and an easy-to-learn-and-use analytics query language for data exploration, preprocessing and analytical workflow orchestration. The framework is designed as a service to take advantage of the Cloud's features.",
                "call-number": "10.5555/2735522.2735575",
                "collection-title": "CASCON '14",
                "container-title": "Proceedings of 24th Annual International Conference on Computer Science and Software Engineering",
                "event-place": "Markham, Ontario, Canada",
                "number-of-pages": "4",
                "page": "327–330",
                "publisher": "IBM Corp.",
                "publisher-place": "USA",
                "title": "Smart big data analytics as a service framework: a proposal"
            }
        },
        {
            "10.1145/2783258.2783372": {
                "id": "10.1145/2783258.2783372",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Bifet",
                        "given": "Albert"
                    },
                    {
                        "family": "de Francisci Morales",
                        "given": "Gianmarco"
                    },
                    {
                        "family": "Read",
                        "given": "Jesse"
                    },
                    {
                        "family": "Holmes",
                        "given": "Geoff"
                    },
                    {
                        "family": "Pfahringer",
                        "given": "Bernhard"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2015,
                            8,
                            10
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2015,
                            8,
                            10
                        ]
                    ]
                },
                "abstract": "The evaluation of classifiers in data streams is fundamental so that poorly-performing models can be identified, and either improved or replaced by better-performing models. This is an increasingly relevant and important task as stream data is generated from more sources, in real-time, in large quantities, and is now considered the largest source of big data. Both researchers and practitioners need to be able to effectively evaluate the performance of the methods they employ. However, there are major challenges for evaluation in a stream. Instances arriving in a data stream are usually time-dependent, and the underlying concept that they represent may evolve over time. Furthermore, the massive quantity of data also tends to exacerbate issues such as class imbalance. Current frameworks for evaluating streaming and online algorithms are able to give predictions in real-time, but as they use a prequential setting, they build only one model, and are thus not able to compute the statistical significance of results in real-time. In this paper we propose a new evaluation methodology for big data streams. This methodology addresses unbalanced data streams, data where change occurs on different time scales, and the question of how to split the data between training and testing, over multiple models.",
                "call-number": "10.1145/2783258.2783372",
                "collection-title": "KDD '15",
                "container-title": "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
                "DOI": "10.1145/2783258.2783372",
                "event-place": "Sydney, NSW, Australia",
                "ISBN": "9781450336642",
                "keyword": "classification, evaluation, data streams, online learning",
                "number-of-pages": "10",
                "page": "59–68",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Efficient Online Evaluation of Big Data Stream Classifiers",
                "URL": "https://doi.org/10.1145/2783258.2783372"
            }
        },
        {
            "10.1145/2935882": {
                "id": "10.1145/2935882",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Metcalf",
                        "given": "Jacob"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            6,
                            24
                        ]
                    ]
                },
                "abstract": "Reconsidering traditional research ethics given the emergence of big data analytics.",
                "call-number": "10.1145/2935882",
                "container-title": "Commun. ACM",
                "DOI": "10.1145/2935882",
                "ISSN": "0001-0782",
                "issue": "7",
                "number-of-pages": "3",
                "page": "31–33",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "July 2016",
                "title": "Big data analytics and revision of the common rule",
                "URL": "https://doi.org/10.1145/2935882",
                "volume": "59"
            }
        },
        {
            "10.1145/2987386.2987429": {
                "id": "10.1145/2987386.2987429",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Son",
                        "given": "Jae Gi"
                    },
                    {
                        "family": "Kang",
                        "given": "Ji-Woo"
                    },
                    {
                        "family": "An",
                        "given": "Jae-Hoon"
                    },
                    {
                        "family": "Ahn",
                        "given": "Hyung-Joo"
                    },
                    {
                        "family": "Chun",
                        "given": "Hyo-Jung"
                    },
                    {
                        "family": "Kim",
                        "given": "Jung-Guk"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            10,
                            11
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2016,
                            10,
                            11
                        ]
                    ]
                },
                "abstract": "Since the introduction of big data, numerous researches aiming to improve the accuracy and speed of data processing has been conducted. Many platforms that can process real-time data were developed for this purpose. Most standard data processing platforms used Spark Streaming as data analysis layer. However, its limitation in performance calls for a better alternative. This paper introduces a new data processing framework, Squall. Squall utilizes parallel processing and allows real-time data processing using streaming modules. Go was used for development. Through various experiments, the performance of our newly developed framework on processing real-time data was compared to the performance of the previously existing framework completing the same task. Results show quantitative evidence that Squall excel the platforms that use Spark Streaming. Our future work includes making modifications that will improve Squall's performance.",
                "call-number": "10.1145/2987386.2987429",
                "collection-title": "RACS '16",
                "container-title": "Proceedings of the International Conference on Research in Adaptive and Convergent Systems",
                "DOI": "10.1145/2987386.2987429",
                "event-place": "Odense, Denmark",
                "ISBN": "9781450344555",
                "keyword": "Apache Spark, Real-time Big-Data Processing Framework, Squall, Parallel Job Processing, Realtime Packet Analysis",
                "number-of-pages": "4",
                "page": "226–229",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Parallel Job Processing Technique for Real-time Big-Data Processing Framework",
                "URL": "https://doi.org/10.1145/2987386.2987429"
            }
        },
        {
            "10.1145/3321408.3322865": {
                "id": "10.1145/3321408.3322865",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Zhao",
                        "given": "Junmin"
                    },
                    {
                        "family": "Li",
                        "given": "Dengao"
                    },
                    {
                        "family": "Wang",
                        "given": "Xiaoyu"
                    },
                    {
                        "family": "Bai",
                        "given": "Xiaohong"
                    },
                    {
                        "family": "Zhuang",
                        "given": "Shasha"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            5,
                            17
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            5,
                            17
                        ]
                    ]
                },
                "abstract": "With the development and application of education and big data, big data continues to focus on classroom teaching and learning. In view of the normal application of the teaching platform and the demand of social talents, it is of practical significance to make an empirical study on the educational big data analysis based on the educational platform. The interaction of education platform can be described by \"STM triangle model\", including teacher-media interaction, student-media interaction, teacher-student interaction, student-teacher interaction, student-student interaction and data relationship. The data mining process of educational platform is divided into target understanding, data cleaning, data analysis, data presentation and other steps. Normal response to Education platform Based on the real data, an index analysis model of teacher-student interaction is constructed, and an empirical analysis is carried out to provide a practical example for the analysis and application of education big data.",
                "call-number": "10.1145/3321408.3322865",
                "collection-number": "88",
                "collection-title": "ACM TURC '19",
                "container-title": "Proceedings of the ACM Turing Celebration Conference - China",
                "DOI": "10.1145/3321408.3322865",
                "event-place": "Chengdu, China",
                "ISBN": "9781450371582",
                "keyword": "data mining, STM mode, educational platform, education big data",
                "number": "Article 88",
                "number-of-pages": "2",
                "page": "1–2",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Research on the cultivation of new engineering talents based on educational big data",
                "URL": "https://doi.org/10.1145/3321408.3322865"
            }
        },
        {
            "10.1145/3075564.3078884": {
                "id": "10.1145/3075564.3078884",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Fiore",
                        "given": "Sandro"
                    },
                    {
                        "family": "Palazzo",
                        "given": "Cosimo"
                    },
                    {
                        "family": "D'Anca",
                        "given": "Alessandro"
                    },
                    {
                        "family": "Elia",
                        "given": "Donatello"
                    },
                    {
                        "family": "Londero",
                        "given": "Elisa"
                    },
                    {
                        "family": "Knapic",
                        "given": "Cristina"
                    },
                    {
                        "family": "Monna",
                        "given": "Stephen"
                    },
                    {
                        "family": "Marcucci",
                        "given": "Nicola M."
                    },
                    {
                        "family": "Aguilar",
                        "given": "Fernando"
                    },
                    {
                        "family": "Płóciennik",
                        "given": "Marcin"
                    },
                    {
                        "family": "De Lucas",
                        "given": "Jesús E. Marco"
                    },
                    {
                        "family": "Aloisio",
                        "given": "Giovanni"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            5,
                            15
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            5,
                            15
                        ]
                    ]
                },
                "abstract": "In the context of the EU H2020 INDIGO-DataCloud project several use case on large scale scientific data analysis regarding different research communities have been implemented. All of them require the availability of large amount of data related to either output of simulations or observed data from sensors and need scientific (big) data solutions to run data analysis experiments. More specifically, the paper presents the case studies related to the following research communities: (i) the European Multidisciplinary Seafloor and water column Observatory (INGV-EMSO), (ii) the Large Binocular Telescope, (iii) LifeWatch, and (iv) the European Network for Earth System Modelling (ENES).",
                "call-number": "10.1145/3075564.3078884",
                "collection-title": "CF'17",
                "container-title": "Proceedings of the Computing Frontiers Conference",
                "DOI": "10.1145/3075564.3078884",
                "event-place": "Siena, Italy",
                "ISBN": "9781450344876",
                "keyword": "Workflow, big data, ensemble analysis, scientific use case",
                "number-of-pages": "6",
                "page": "343–348",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Big Data Analytics on Large-Scale Scientific Datasets in the INDIGO-DataCloud Project",
                "URL": "https://doi.org/10.1145/3075564.3078884"
            }
        },
        {
            "10.1145/3483816.3483836": {
                "id": "10.1145/3483816.3483836",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Febiri",
                        "given": "Frank"
                    },
                    {
                        "family": "Yihum Amare",
                        "given": "Meseret"
                    },
                    {
                        "family": "Hub",
                        "given": "Miloslav"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            7,
                            4
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2021,
                            7,
                            4
                        ]
                    ]
                },
                "abstract": "The term “smartness” in the data framework indicates relevancy based on the intended purpose of data. The Internet of Things (IoT) and advancements in technology have resulted in an ever-increasing pool of data available to all institutions to derive meaning and make sound decisions from them. The research presented in this paper explored the role smart data play in information systems quality through a qualitative study of how using the large pool of data (big data) and fusing it to smart data organizations can make sound and intelligent decisions using the available techniques. We use an existing architecture for a public institution to analyze how data ingestion can be achieved with minimum challenges. The findings suggest that even though there is a large pool of data for most organizations, it is becoming more challenging to use this data to make organizational sense due to the challenges posed by such data. The realization of smart data and its benefits in information systems helps improve the quality of information systems, reducing cost and promoting the smartness agenda of today's organization.",
                "call-number": "10.1145/3483816.3483836",
                "collection-title": "ICMECG 2021",
                "container-title": "2021 8th International Conference on Management of e-Commerce and e-Government",
                "DOI": "10.1145/3483816.3483836",
                "event-place": "Jeju, Republic of Korea",
                "ISBN": "9781450390545",
                "keyword": "Quality measures, Information systems, Big Data, Smart data",
                "number-of-pages": "6",
                "page": "112–117",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Fusion from Big Data to Smart Data to enhance quality of Information Systems",
                "URL": "https://doi.org/10.1145/3483816.3483836"
            }
        },
        {
            "10.1145/2232817.2232911": {
                "id": "10.1145/2232817.2232911",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Stebe",
                        "given": "Janez"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2012,
                            6,
                            10
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2012,
                            6,
                            10
                        ]
                    ]
                },
                "abstract": "In the framework of a project aiming to realize a strategy of open research data access in Slovenia in accordance with OECD principles, we conducted a series of interviews with different target audiences in order to assess the initial conditions in the area of data handling. The data creators and data services expressed a high level of awareness about data quality issues, especially in relation to good publication potential. Barriers to ensuring the greater accessibility of data in the future include the little recognition and reputation for doing the related extra work involved in preparing data and documentation, the need for financial rewards for such additional work, and the undeveloped culture of data exchange in general. The motivation to provide open access to such data will involve a combination of requirements prescribed for data delivery, and the provision of support services and financial rewards, in particular changing the views held by the professional scientific community about the benefits of open data for research activities.",
                "call-number": "10.1145/2232817.2232911",
                "collection-title": "JCDL '12",
                "container-title": "Proceedings of the 12th ACM/IEEE-CS joint conference on Digital Libraries",
                "DOI": "10.1145/2232817.2232911",
                "event-place": "Washington, DC, USA",
                "ISBN": "9781450311540",
                "keyword": "culture, open data, stakeholders attitudes, data quality",
                "number-of-pages": "2",
                "page": "401–402",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Responsibility for research data quality in open access: a slovenian case",
                "URL": "https://doi.org/10.1145/2232817.2232911"
            }
        },
        {
            "10.14778/1920841.1921063": {
                "id": "10.14778/1920841.1921063",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Agrawal",
                        "given": "Divyakant"
                    },
                    {
                        "family": "Das",
                        "given": "Sudipto"
                    },
                    {
                        "family": "El Abbadi",
                        "given": "Amr"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2010,
                            9,
                            1
                        ]
                    ]
                },
                "abstract": "Cloud computing is an extremely successful paradigm of service oriented computing and has revolutionized the way computing infrastructure is abstracted and used. Three most popular cloud paradigms include: Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and Software as a Service (SaaS). The concept however can also be extended to Database as a Service and many more. Elasticity, pay-per-use, low upfront investment, low time to market, and transfer of risks are some of the major enabling features that make cloud computing a ubiquitous paradigm for deploying novel applications which were not economically feasible in a traditional enterprise infrastructure settings. This has seen a proliferation in the number of applications which leverage various cloud platforms, resulting in a tremendous increase in the scale of the data generated as well as consumed by such applications. Scalable database management systems (DBMS) -- both for update intensive application workloads, as well as decision support systems for descriptive and deep analytics -- are thus a critical part of cloud infrastructures.",
                "call-number": "10.14778/1920841.1921063",
                "container-title": "Proc. VLDB Endow.",
                "DOI": "10.14778/1920841.1921063",
                "ISSN": "2150-8097",
                "issue": "1-2",
                "number-of-pages": "2",
                "page": "1647–1648",
                "publisher": "VLDB Endowment",
                "source": "September 2010",
                "title": "Big data and cloud computing: new wine or just new bottles?",
                "URL": "https://doi.org/10.14778/1920841.1921063",
                "volume": "3"
            }
        },
        {
            "10.5555/3233397.3233443": {
                "id": "10.5555/3233397.3233443",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Clemente-Castelló",
                        "given": "Francisco J."
                    },
                    {
                        "family": "Nicolae",
                        "given": "Bogdan"
                    },
                    {
                        "family": "Katrinis",
                        "given": "Kostas"
                    },
                    {
                        "family": "Rafique",
                        "given": "M. Mustafa"
                    },
                    {
                        "family": "Mayo",
                        "given": "Rafael"
                    },
                    {
                        "family": "Fernández",
                        "given": "Juan Carlos"
                    },
                    {
                        "family": "Loreti",
                        "given": "Daniela"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2015,
                            12,
                            7
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2015,
                            12,
                            7
                        ]
                    ]
                },
                "abstract": "The cloud computing model has seen tremendous commercial success through its materialization via two prominent models to date, namely public and private cloud. Recently, a third model combining the former two service models as on-/off-premise resources has been receiving significant market traction: hybrid cloud. While state of art techniques that address workload performance prediction and efficient workload execution over hybrid cloud setups exist, how to address data-intensive workloads - including Big Data Analytics - in similar environments is nascent. This paper addresses this gap by taking on the challenge of bursting over hybrid clouds for the benefit of accelerating iterative MapReduce applications. We first specify the challenges associated with data locality and data movement in such setups. Subsequently, we propose a novel technique to address the locality issue, without requiring changes to the MapReduce framework or the underlying storage layer. In addition, we contribute with a performance prediction methodology that combines modeling with micro-benchmarks to estimate completion time for iterative MapReduce applications, which enables users to estimate cost-to-solution before committing extra resources from public clouds. We show through experimentation in a dual-Openstack hybrid cloud setup that our solutions manage to bring substantial improvement at predictable cost-control for two real-life iterative MapReduce applications: large-scale machine learning and text analysis.",
                "call-number": "10.5555/3233397.3233443",
                "collection-title": "UCC '15",
                "container-title": "Proceedings of the 8th International Conference on Utility and Cloud Computing",
                "event-place": "Limassol, Cyprus",
                "ISBN": "9780769556970",
                "keyword": "mapreduce, data locality, big data analytics, hybrid cloud, iterative applications, performance prediction",
                "number-of-pages": "10",
                "page": "290–299",
                "publisher": "IEEE Press",
                "title": "Enabling big data analytics in the hybrid cloud using iterative mapreduce"
            }
        },
        {
            "10.14778/3352063.3352128": {
                "id": "10.14778/3352063.3352128",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Chen",
                        "given": "Zhimin"
                    },
                    {
                        "family": "Wang",
                        "given": "Yue"
                    },
                    {
                        "family": "Narasayya",
                        "given": "Vivek"
                    },
                    {
                        "family": "Chaudhuri",
                        "given": "Surajit"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            8,
                            1
                        ]
                    ]
                },
                "abstract": "Fuzzy join is an important primitive for data cleaning. The ability to customize fuzzy join is crucial to allow applications to address domain-specific data quality issues such as synonyms and abbreviations. While efficient indexing techniques exist for single-node implementations of customizable fuzzy join, the state-of-the-art scale-out techniques do not support customization, and exhibit poor performance and scalability characteristics. We describe the design of a scale-out fuzzy join operator that supports customization. We use a locality-sensitive-hashing (LSH) based signature scheme, and introduce optimizations that result in significant speed up with negligible impact on recall. We evaluate our implementation on the Azure Databricks version of Spark using several real-world and synthetic data sets. We observe speedups exceeding 50X compared to the best-known prior scale-out technique, and close to linear scalability with data size and number of nodes.",
                "call-number": "10.14778/3352063.3352128",
                "container-title": "Proc. VLDB Endow.",
                "DOI": "10.14778/3352063.3352128",
                "ISSN": "2150-8097",
                "issue": "12",
                "number-of-pages": "12",
                "page": "2106–2117",
                "publisher": "VLDB Endowment",
                "source": "August 2019",
                "title": "Customizable and scalable fuzzy join for big data",
                "URL": "https://doi.org/10.14778/3352063.3352128",
                "volume": "12"
            }
        },
        {
            "10.1145/3281375.3281386": {
                "id": "10.1145/3281375.3281386",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Rinaldi",
                        "given": "Antonio M."
                    },
                    {
                        "family": "Russo",
                        "given": "Cristiano"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            9,
                            25
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            9,
                            25
                        ]
                    ]
                },
                "abstract": "The use of formal representation is a key task in the era of big data. In the context of multimedia big data this issue is stressed due to the intrinsic complexity nature of this kind of data. Moreover, the relations among objects should be clearly expressed and formalized to give the right meaning of data correlation. For this reason the design of formal models to represent and manage information is a necessary task to implement intelligent information systems. In this latter some approaches related to the semantic web could be used to improve the data models which underlie the implementation of big data applications. Using these models the visualization of data and information become an intrinsic and strategic task for the analysis and exploration of multimedia BigData. In this paper we propose the use of a semantic approach to formalize the model structure of multimedia BigData. In addition, the recognition of multimodal features to represent concepts and linguistic properties to relate them are an effective way to bridge the gap between the target semantic classes and the available low-level multimedia descriptors. The proposed model has been implemented in a NoSQL graph database populated from different knowledge sources and a visualization of this very large knowledge base has been presented and discussed as a case study.",
                "call-number": "10.1145/3281375.3281386",
                "collection-title": "MEDES '18",
                "container-title": "Proceedings of the 10th International Conference on Management of Digital EcoSystems",
                "DOI": "10.1145/3281375.3281386",
                "event-place": "Tokyo, Japan",
                "ISBN": "9781450356220",
                "keyword": "multimedia ontologies, semantic bigdata, semantics",
                "number-of-pages": "8",
                "page": "31–38",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "A semantic-based model to represent multimedia big data",
                "URL": "https://doi.org/10.1145/3281375.3281386"
            }
        },
        {
            "10.1145/3529570.3529580": {
                "id": "10.1145/3529570.3529580",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Yobsan Bayisa",
                        "given": "Leta"
                    },
                    {
                        "family": "Wang",
                        "given": "Weidong"
                    },
                    {
                        "family": "Wang",
                        "given": "Qing-xian"
                    },
                    {
                        "family": "Meseret Debele",
                        "given": "Gurmu"
                    },
                    {
                        "family": "Bona Debela",
                        "given": "Lamessa"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2022,
                            2,
                            25
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2022,
                            2,
                            25
                        ]
                    ]
                },
                "abstract": "Gaussian process is one of computationally expensive algorithm for large datasets and lack of the flexibility to model different datasets is a common problem for modeling it. We introduce sparse Gaussian regression with the combination of designed kernels to solve the computational complexity of a traditional Gaussian process by taking pseudo input from large datasets and developing a model with better accuracy which enables Gaussian process application. We design a better combination of the kernel that can catch up with most of our data points. We demonstrate the approach on a large weather dataset and sales record dataset. Both are open source big datasets available online. Numerous experiments and comparisons with traditional Gaussian process methods using both large datasets demonstrate the efficiency and accuracy of sparse Gaussian processes.",
                "call-number": "10.1145/3529570.3529580",
                "collection-title": "ICDSP '22",
                "container-title": "Proceedings of the 6th International Conference on Digital Signal Processing",
                "DOI": "10.1145/3529570.3529580",
                "event-place": "Chengdu, China",
                "ISBN": "9781450395809",
                "keyword": "Gaussian Process, Inference, Pseudo-input, Big data, Kernel",
                "number-of-pages": "9",
                "page": "54–62",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Inference and Prediction in Big Data Using Sparse Gaussian Process Method",
                "URL": "https://doi.org/10.1145/3529570.3529580"
            }
        },
        {
            "10.1145/3447548.3469468": {
                "id": "10.1145/3447548.3469468",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Patel",
                        "given": "Hima"
                    },
                    {
                        "family": "Ishikawa",
                        "given": "Fuyuki"
                    },
                    {
                        "family": "Berti-Equille",
                        "given": "Laure"
                    },
                    {
                        "family": "Gupta",
                        "given": "Nitin"
                    },
                    {
                        "family": "Mehta",
                        "given": "Sameep"
                    },
                    {
                        "family": "Masuda",
                        "given": "Satoshi"
                    },
                    {
                        "family": "Mujumdar",
                        "given": "Shashank"
                    },
                    {
                        "family": "Afzal",
                        "given": "Shazia"
                    },
                    {
                        "family": "Bedathur",
                        "given": "Srikanta"
                    },
                    {
                        "family": "Nishi",
                        "given": "Yasuharu"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            8,
                            14
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2021,
                            8,
                            14
                        ]
                    ]
                },
                "abstract": "The 2nd International Workshop on Data Quality Assessment for Machine Learning (DQAML'21) is organized in conjunction with the Special Interest Group on Knowledge Discovery and Data Mining (SIGKDD). This workshop aims to serve as a forum for the presentation of research related to data quality assessment and remediation in AI/ML pipeline. Data quality is a critical issue in the data preparation phase and involves numerous challenging problems related to detection, remediation, visualization and evaluation of data issues. The workshop aims to provide a platform to researchers and practitioners to discuss such challenges across different modalities of data like structured, time series, text and graphical. The aim is to attract perspectives from both industrial and academic circles.",
                "call-number": "10.1145/3447548.3469468",
                "collection-title": "KDD '21",
                "container-title": "Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining",
                "DOI": "10.1145/3447548.3469468",
                "event-place": "Virtual Event, Singapore",
                "ISBN": "9781450383325",
                "keyword": "data assessment, data quality, machine learning",
                "number-of-pages": "2",
                "page": "4147–4148",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "2nd International Workshop on Data Quality Assessment for Machine Learning",
                "URL": "https://doi.org/10.1145/3447548.3469468"
            }
        },
        {
            "10.1145/2608020.2612731": {
                "id": "10.1145/2608020.2612731",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Parashar",
                        "given": "Manish"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2014,
                            6,
                            23
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2014,
                            6,
                            23
                        ]
                    ]
                },
                "abstract": "Data-related challenges are quickly dominating computational and data-enabled sciences, and are limiting the potential impact of scientific applications enabled by current and emerging high-performance distributed computing environments. These data-intensive application workflows involve dynamic coordination, interactions and data coupling between multiple application process that run at scale on different resources, and with services for monitoring, analysis and visualization and archiving. In this talk I will explore data grand challenges in simulation-based science and investigate how solutions based on data sharing abstractions, managed data pipelines, in-memory data-staging, in-situ placement and execution, and in-transit data processing can be used to address these data challenges at extreme scales.",
                "call-number": "10.1145/2608020.2612731",
                "collection-title": "DIDC '14",
                "container-title": "Proceedings of the sixth international workshop on Data intensive distributed computing",
                "DOI": "10.1145/2608020.2612731",
                "event-place": "Vancouver, BC, Canada",
                "ISBN": "9781450329132",
                "keyword": "data-intensive computing, extreme-scale computing, data staging, dataspaces, in-situ data processing",
                "number-of-pages": "2",
                "page": "1–2",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Big data challenges in simulation-based science",
                "URL": "https://doi.org/10.1145/2608020.2612731"
            }
        },
        {
            "10.1145/2534921.2534926": {
                "id": "10.1145/2534921.2534926",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Baumann",
                        "given": "Peter"
                    },
                    {
                        "family": "Dumitru",
                        "given": "Alex"
                    },
                    {
                        "family": "Merticariu",
                        "given": "Vlad"
                    },
                    {
                        "family": "Misev",
                        "given": "Dimitar"
                    },
                    {
                        "family": "Rusu",
                        "given": "Mihaela"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2013,
                            11,
                            4
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2013,
                            11,
                            4
                        ]
                    ]
                },
                "abstract": "Modern sensors, such as hyperspectral cameras, deliver massive amounts of data. On board of satellites, the high volume is paired with low bandwidth and part-time availability, during overpasses. This leads to well-known availability problems and bottlenecks in today's remote sensing.We address this challenge by enhancing the on-board system with flexible filtering and processing capabilities based on the Array Analytics engine, rasdaman. Users then can exact request, which can lead to substantially decreased data traffic. Our project has been accepted for a CubeSat mission for which rasdaman now has been prepared. We present the project setup and core extensions done to rasdaman to this end.",
                "call-number": "10.1145/2534921.2534926",
                "collection-title": "BigSpatial '13",
                "container-title": "Proceedings of the 2nd ACM SIGSPATIAL International Workshop on Analytics for Big Geospatial Data",
                "DOI": "10.1145/2534921.2534926",
                "event-place": "Orlando, Florida",
                "ISBN": "9781450325349",
                "keyword": "array databases, on-board intelligence, rasdaman, big data",
                "number-of-pages": "5",
                "page": "32–36",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Breaking the big data barrier by enhancing on-board sensor flexibility",
                "URL": "https://doi.org/10.1145/2534921.2534926"
            }
        },
        {
            "10.1145/3093338.3104145": {
                "id": "10.1145/3093338.3104145",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Tahmassebi",
                        "given": "Amirhessam"
                    },
                    {
                        "family": "Gandomi",
                        "given": "Amir H."
                    },
                    {
                        "family": "Meyer-Bäse",
                        "given": "Anke"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            7,
                            9
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            7,
                            9
                        ]
                    ]
                },
                "abstract": "We consider resting-state Functional Magnetic Resonance Imaging (fMRI) of two classes of patients: one that took the drug N-acetylcysteine (NAC) and the other one a placebo before and after a smoking cessation treatment. Our goal was to classify the relapse in nicotine-dependent patients as treatment or non-treatment based on their fMRI scans. 80% accuracy was obtained using Independent Component Analysis (ICA) along with Genetic Programming (GP) classifier using High Performance Computing (HPC) which we consider significant enough to suggest that there is a difference in the resting-state fMRI images of a smoker that undergoes this smoking cessation treatment compared to a smoker that receives a placebo.",
                "call-number": "10.1145/3093338.3104145",
                "collection-number": "57",
                "collection-title": "PEARC17",
                "container-title": "Proceedings of the Practice and Experience in Advanced Research Computing 2017 on Sustainability, Success and Impact",
                "DOI": "10.1145/3093338.3104145",
                "event-place": "New Orleans, LA, USA",
                "ISBN": "9781450352727",
                "keyword": "High Performance Computing, Generic Programming, fMRI Big Data, Classification",
                "number": "Article 57",
                "number-of-pages": "4",
                "page": "1–4",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "High Performance GP-Based Approach for fMRI Big Data Classification",
                "URL": "https://doi.org/10.1145/3093338.3104145"
            }
        },
        {
            "10.1145/2685553.2685558": {
                "id": "10.1145/2685553.2685558",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Fiesler",
                        "given": "Casey"
                    },
                    {
                        "family": "Young",
                        "given": "Alyson"
                    },
                    {
                        "family": "Peyton",
                        "given": "Tamara"
                    },
                    {
                        "family": "Bruckman",
                        "given": "Amy S."
                    },
                    {
                        "family": "Gray",
                        "given": "Mary"
                    },
                    {
                        "family": "Hancock",
                        "given": "Jeff"
                    },
                    {
                        "family": "Lutters",
                        "given": "Wayne"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2015,
                            2,
                            28
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2015,
                            2,
                            28
                        ]
                    ]
                },
                "abstract": "The evolution of social technology and research methods present ongoing challenges to studying people online. Recent high-profile cases have prompted discussion among both the research community and the general public about the ethical implications of researching humans, their information, and their activities in large-scale digital contexts. Examples of scientific and market research involving Facebook users and OKCupid clients exemplify the ethical complexities of both studying and manipulating online user behavior. When does data science become human subjects research, and what are our obligations to these subjects as researchers' Drawing from previous work around the ethics of digital research, one goal of this workshop is to work towards a set of guiding principles for CSCW scholars doing research online.",
                "call-number": "10.1145/2685553.2685558",
                "collection-title": "CSCW'15 Companion",
                "container-title": "Proceedings of the 18th ACM Conference Companion on Computer Supported Cooperative Work & Social Computing",
                "DOI": "10.1145/2685553.2685558",
                "event-place": "Vancouver, BC, Canada",
                "ISBN": "9781450329460",
                "keyword": "online communities, sociotechnical systems, research ethics, big data",
                "number-of-pages": "4",
                "page": "289–292",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Ethics for Studying Online Sociotechnical Systems in a Big Data World",
                "URL": "https://doi.org/10.1145/2685553.2685558"
            }
        },
        {
            "10.14778/3229863.3229867": {
                "id": "10.14778/3229863.3229867",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Schelter",
                        "given": "Sebastian"
                    },
                    {
                        "family": "Lange",
                        "given": "Dustin"
                    },
                    {
                        "family": "Schmidt",
                        "given": "Philipp"
                    },
                    {
                        "family": "Celikel",
                        "given": "Meltem"
                    },
                    {
                        "family": "Biessmann",
                        "given": "Felix"
                    },
                    {
                        "family": "Grafberger",
                        "given": "Andreas"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            8,
                            1
                        ]
                    ]
                },
                "abstract": "Modern companies and institutions rely on data to guide every single business process and decision. Missing or incorrect information seriously compromises any decision process downstream. Therefore, a crucial, but tedious task for everyone involved in data processing is to verify the quality of their data. We present a system for automating the verification of data quality at scale, which meets the requirements of production use cases. Our system provides a declarative API, which combines common quality constraints with user-defined validation code, and thereby enables 'unit tests' for data. We efficiently execute the resulting constraint validation workload by translating it to aggregation queries on Apache Spark. Our platform supports the incremental validation of data quality on growing datasets, and leverages machine learning, e.g., for enhancing constraint suggestions, for estimating the 'predictability' of a column, and for detecting anomalies in historic data quality time series. We discuss our design decisions, describe the resulting system architecture, and present an experimental evaluation on various datasets.",
                "call-number": "10.14778/3229863.3229867",
                "container-title": "Proc. VLDB Endow.",
                "DOI": "10.14778/3229863.3229867",
                "ISSN": "2150-8097",
                "issue": "12",
                "number-of-pages": "14",
                "page": "1781–1794",
                "publisher": "VLDB Endowment",
                "source": "August 2018",
                "title": "Automating large-scale data quality verification",
                "URL": "https://doi.org/10.14778/3229863.3229867",
                "volume": "11"
            }
        },
        {
            "10.1145/2744700.2744703": {
                "id": "10.1145/2744700.2744703",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Ali",
                        "given": "Reem Y."
                    },
                    {
                        "family": "Gunturi",
                        "given": "Venkata M. V."
                    },
                    {
                        "family": "Shekhar",
                        "given": "Shashi"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2015,
                            3,
                            10
                        ]
                    ]
                },
                "abstract": "The size, variety, and update rate of spatial datasets are increasingly exceeding the capacity of commonly used spatial computing technologies to learn, manage, and process the data with reasonable effort. We refer to these datasets as Spatial Big Data (SBD). Examples of emerging SBD datasets include temporally detailed (TD) roadmaps that provide speeds every minute for every road-segment, GPS track data from cell-phones, and engine measurements of fuel consumption, greenhouse gas (GHG) emissions, etc. Harnessing SBD has a transformative potential. For example, a 2011 McKinsey Global Institute report estimates savings of \"about $600 billion annually by 2020\" in terms of fuel and time saved by helping vehicles avoid congestion and reduce idling at red lights or left turns. In this paper, we discuss the challenges posed by SBD for a next generation of routing services and we present our work towards addressing these challenges.",
                "call-number": "10.1145/2744700.2744703",
                "container-title": "SIGSPATIAL Special",
                "DOI": "10.1145/2744700.2744703",
                "issue": "2",
                "number-of-pages": "7",
                "page": "19–25",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "July 2014",
                "title": "Spatial big data for eco-routing services: computational challenges and accomplishments",
                "URL": "https://doi.org/10.1145/2744700.2744703",
                "volume": "6"
            }
        },
        {
            "10.1145/3501409.3501630": {
                "id": "10.1145/3501409.3501630",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Chen",
                        "given": "Genjin"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            10,
                            22
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2021,
                            10,
                            22
                        ]
                    ]
                },
                "abstract": "With the development of the times, computer technology is making continuous progress. People's every move is generating data. In the face of such a large amount of data information, the traditional data processing technology has been unable to meet people's needs. Big data analysis and cloud computing technology came into being. Cloud computing and big data technology have far exceeded the traditional data processing technology in terms of accuracy and speed. Now cloud computing and big data analysis have become hot words in the society, and relevant experts and scholars are constantly studying these two technologies. Starting from the elaboration of cloud computing technology and big data technology, this paper explores the advantages and problems of cloud computing, deeply analyzes the application of cloud computing technology in computer big data, imagines the future of cloud computer technology, and provides some references for the future development of cloud computing technology.",
                "call-number": "10.1145/3501409.3501630",
                "collection-title": "EITCE 2021",
                "container-title": "Proceedings of the 2021 5th International Conference on Electronic Information Technology and Computer Engineering",
                "DOI": "10.1145/3501409.3501630",
                "event-place": "Xiamen, China",
                "ISBN": "9781450384322",
                "keyword": "Big data technology, Computer technology, Cloud computing technology, Research",
                "number-of-pages": "6",
                "page": "1257–1262",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Research on the application of cloud computing technology in computer big data analysis",
                "URL": "https://doi.org/10.1145/3501409.3501630"
            }
        },
        {
            "10.1145/3362121": {
                "id": "10.1145/3362121",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Firmani",
                        "given": "Donatella"
                    },
                    {
                        "family": "Tanca",
                        "given": "Letizia"
                    },
                    {
                        "family": "Torlone",
                        "given": "Riccardo"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            12,
                            5
                        ]
                    ]
                },
                "call-number": "10.1145/3362121",
                "collection-number": "2",
                "container-title": "J. Data and Information Quality",
                "DOI": "10.1145/3362121",
                "ISSN": "1936-1955",
                "issue": "1",
                "keyword": "Data integration, knowledge extraction, source selection",
                "number": "Article 2",
                "number-of-pages": "5",
                "page": "1–5",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "March 2020",
                "title": "Ethical Dimensions for Data Quality",
                "URL": "https://doi.org/10.1145/3362121",
                "volume": "12"
            }
        },
        {
            "10.1145/3393527.3393532": {
                "id": "10.1145/3393527.3393532",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Shi",
                        "given": "Bin"
                    },
                    {
                        "family": "YabinXu"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2020,
                            5,
                            22
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2020,
                            5,
                            22
                        ]
                    ]
                },
                "abstract": "Data watermarking technology is an effective means to protect the copyright of big data. In order to embed robust and highly available data watermarks, firstly, based on the game theory, a Nash equilibrium model between watermark robustness and data quality is established to solve the optimal number of data partitioning. Then, the mapping relationship between data partitioning and watermark bit is established by using secure hash algorithm. Finally, under the constraint of data usability, the improved particle swarm optimization algorithm is used to calculate the optimal solution of data change for each data partitioning, and then the data is changed accordingly to complete the embedding of watermark bit. In order to verify the copyright ownership of big data, this paper also gives the corresponding watermark extraction method. Watermark extraction is the inverse process of watermark embedding. First, traverse all partitions and extract the possible embedded bit values in each data partitioning. Then, the actual embedded watermark bit is finally determined by majority voting strategies. The experimental results show that our proposed method can not only detect watermarks under different attack conditions, ensure the robustness of big data watermarks, but also achieve better data quality, and the comprehensive effect of data watermarks is better than the existing methods.",
                "call-number": "10.1145/3393527.3393532",
                "collection-title": "ACM TURC'20",
                "container-title": "Proceedings of the ACM Turing Celebration Conference - China",
                "DOI": "10.1145/3393527.3393532",
                "event-place": "Hefei, China",
                "ISBN": "9781450375344",
                "keyword": "Nash equilibrium, Majority voting strategy, Particle swarm optimization algorithm, Constrained optimization, Big data, Copyright protection, Data watermarking",
                "number-of-pages": "5",
                "page": "21–25",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Research on Copyright Protection Method of Big Data Based on Nash Equilibrium and Constraint Optimization",
                "URL": "https://doi.org/10.1145/3393527.3393532"
            }
        },
        {
            "10.1145/3377817.3377836": {
                "id": "10.1145/3377817.3377836",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Otoo-Arthur",
                        "given": "David"
                    },
                    {
                        "family": "Van Zyl",
                        "given": "Terence"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            8,
                            12
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            8,
                            12
                        ]
                    ]
                },
                "abstract": "The development of Big Data applications in education has drawn much attention in the last few years due to the enormous benefits it brings to improving teaching and learning. The integration of these Big Data applications in education generates massive data that put new demands to available processing technologies of data and extraction of useful information. Primarily, several higher educational institutions depend on the knowledge mined from these vast volumes of data to optimise the teaching and learning environment. However, Big Data in the higher education context has relied on traditional data techniques and platforms that are less efficient. This paper, therefore, conducts a Systematic Literature Review (SLR) that examines Big Data framework technologies in higher education outlining gaps that need a solution in Big Educational Data Analytics. We achieved this by summarising the current knowledge on the topic and recommend areas where educational institutions could focus on exploring the potential of Big Data Analytics. To this end, we reviewed 55 related articles out of 1543 selected from Six (6) accessible Computer Science databases between the period of 2007 and 2018, focusing on the development of the Big Data framework and its applicability in education for academic purposes. Our results show that very few researchers have tried to address the integrative use of Big Data framework and learning analytics in higher education. The review further suggests that there is an emerging best practice in applying Big Data Analytics to improve teaching and learning. However, this information does not appear to have been thoroughly examined in higher education. Hence, there is the need for a complete investigation to come up with comprehensive Big Data frameworks that build effective learning systems for instructors, learners, course designers and educational administrators.",
                "call-number": "10.1145/3377817.3377836",
                "collection-number": "15",
                "collection-title": "EBIMCS '19",
                "container-title": "Proceedings of the 2019 2nd International Conference on E-Business, Information Management and Computer Science",
                "DOI": "10.1145/3377817.3377836",
                "event-place": "Kuala Lumpur, Malaysia",
                "ISBN": "9781450366496",
                "keyword": "Data Mining, Big Data, Higher Education, MS, Big Educational Data, Learning Analytics",
                "number": "Article 15",
                "number-of-pages": "9",
                "page": "1–9",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "A Systematic Review on Big Data Analytics Frameworks for Higher Education - Tools and Algorithms",
                "URL": "https://doi.org/10.1145/3377817.3377836"
            }
        },
        {
            "10.1145/2851613.2851881": {
                "id": "10.1145/2851613.2851881",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Jlassi",
                        "given": "Aymen"
                    },
                    {
                        "family": "Martineau",
                        "given": "Patrick"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            4,
                            4
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2016,
                            4,
                            4
                        ]
                    ]
                },
                "abstract": "Today, consumers request virtual resources like CPU, RAM, disk (etc.) supplied by the service providers (like Amazon) and they pay on a \"pay-as-you-go\" basis. Generally, the supervisors adopt virtualization technologies, which optimize resources usage and limit the operating cost. The virtualization technologies are classified in two categories. The first one concerns the heavy virtualization. Each virtual machines (VM) emulates hardware and embeds its own operating system (OS) that is completely isolated from the host OS. The second one concerns the light virtualization, which is based on the management of containers. The containers share the host OS kernel [5] while ensuring isolation. In this paper, we benchmark the performance and the energy consumption of an infrastructure that is based on the software Hadoop regarding the two technologies of virtualization. At first, we will identify the points to be improved concerning Hadoop performances and then we will reduce the deployment cost on the cloud. Second, the Hadoop community finds an in-depth study of the resources consumption depending on the environment of deployment. Our experiments are based on the comparison of the Docker technology (light virtualization) and VMware technology® (heavy virtualization). We come to the point that in most experiments the light technology offers better performances in completion time of workloads and it is more adapted to be used with the Hadoop software.",
                "call-number": "10.1145/2851613.2851881",
                "collection-title": "SAC '16",
                "container-title": "Proceedings of the 31st Annual ACM Symposium on Applied Computing",
                "DOI": "10.1145/2851613.2851881",
                "event-place": "Pisa, Italy",
                "ISBN": "9781450337397",
                "keyword": "Hadoop, virtualization, benchmarks, resources consumption",
                "number-of-pages": "4",
                "page": "542–545",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Virtualization technologies for the big data environment",
                "URL": "https://doi.org/10.1145/2851613.2851881"
            }
        },
        {
            "10.1145/3297720": {
                "id": "10.1145/3297720",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Müller",
                        "given": "Daniel"
                    },
                    {
                        "family": "Jain",
                        "given": "Pratiksha"
                    },
                    {
                        "family": "Te",
                        "given": "Yieh-Funk"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            5,
                            7
                        ]
                    ]
                },
                "abstract": "Mappings of first name to gender have been widely recognized as a critical tool for the completion, study, and validation of data records in a range of areas. In this study, we investigate how organizations with large databases of existing entities can create their own mappings between first names and gender and how these mappings can be improved and utilized. Therefore, we first explore a dataset with demographic information on more than 4 million people, which was provided by a car insurance company. Then, we study how naming conventions have changed over time and how they differ by nationality. Next, we build a probabilistic first-name-to-gender mapping and augment the mapping by adding nationality and decade of birth to improve the mapping's performance. We test our mapping in two-label and three-label settings and further validate our mapping by categorizing patent filings by gender of the inventor. We compare the results with previous studies’ outcomes and find that our mapping produces high-precision results. We validate that the additional information of nationality and year of birth improve the precision scores of name-to-gender mappings. Therefore, the proposed approach constitutes an efficient process for improving the data quality of organizations’ records, if the gender attribute is missing or unreliable.",
                "call-number": "10.1145/3297720",
                "collection-number": "8",
                "container-title": "J. Data and Information Quality",
                "DOI": "10.1145/3297720",
                "ISSN": "1936-1955",
                "issue": "2",
                "keyword": "gender name mapping, patenting, record completion, Data quality improvement",
                "number": "Article 8",
                "number-of-pages": "18",
                "page": "1–18",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "June 2019",
                "title": "Augmenting Data Quality through High-Precision Gender Categorization",
                "URL": "https://doi.org/10.1145/3297720",
                "volume": "11"
            }
        },
        {
            "10.1145/3378936.3378950": {
                "id": "10.1145/3378936.3378950",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Tosson",
                        "given": "A."
                    },
                    {
                        "family": "Shokr",
                        "given": "M."
                    },
                    {
                        "family": "Pietsch",
                        "given": "U."
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2020,
                            1,
                            12
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2020,
                            1,
                            12
                        ]
                    ]
                },
                "abstract": "The X-ray crystallography community has recently been affected by a significant increase in data volume caused by the use of advanced detector technologies and the new generation of high brilliance light sources. The fact that forced the decision makers to implement Big Data analytics, aiming to achieve a suitable environment for scientists at experimental and post-experimental phases. This paper demonstrates an extension of our approach towards a compact platform which provides the scientists with the digital ecosystem for the systematic harvest of data. It introduces an innovative solution to use warehousing and cloud computing to manage datasets collected by 2D energy-dispersive detectors, for an example. Moreover, it suggests that, deploying a Software as a Service (SaaS) cloud model, a public cloud data center, and cloud-based in-memory warehousing architecture, it is possible to dramatically reduce both hardware and processing costs.",
                "call-number": "10.1145/3378936.3378950",
                "collection-title": "ICSIM '20",
                "container-title": "Proceedings of the 3rd International Conference on Software Engineering and Information Management",
                "DOI": "10.1145/3378936.3378950",
                "event-place": "Sydney, NSW, Australia",
                "ISBN": "9781450376907",
                "keyword": "Cloud computing, Big Data, Crystallography, In-Memory warehousing",
                "number-of-pages": "4",
                "page": "1–4",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Application of Cloud Computing for Big Data in the X-Ray Crystallography Community",
                "URL": "https://doi.org/10.1145/3378936.3378950"
            }
        },
        {
            "10.1145/2568088.2576096": {
                "id": "10.1145/2568088.2576096",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Suzumura",
                        "given": "Toyotaro"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2014,
                            3,
                            22
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2014,
                            3,
                            22
                        ]
                    ]
                },
                "abstract": "This paper introduces some of the example applications handling extremely big data with supercomputers such as large-scale network analysis, X10-based large-scale graph analytics library, Graph500 benchmark, and billion-scale social simulation.",
                "call-number": "10.1145/2568088.2576096",
                "collection-title": "ICPE '14",
                "container-title": "Proceedings of the 5th ACM/SPEC international conference on Performance engineering",
                "DOI": "10.1145/2568088.2576096",
                "event-place": "Dublin, Ireland",
                "ISBN": "9781450327336",
                "keyword": "graph, social simulation, big data, supercomputer",
                "number-of-pages": "2",
                "page": "1–2",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Extreme big data processing in large-scale graph analytics and billion-scale social simulation",
                "URL": "https://doi.org/10.1145/2568088.2576096"
            }
        },
        {
            "10.1145/2487575.2487677": {
                "id": "10.1145/2487575.2487677",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Canny",
                        "given": "John"
                    },
                    {
                        "family": "Zhao",
                        "given": "Huasha"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2013,
                            8,
                            11
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2013,
                            8,
                            11
                        ]
                    ]
                },
                "abstract": "This paper describes the BID Data Suite, a collection of hardware, software and design patterns that enable fast, large-scale data mining at very low cost. By co-designing all of these elements we achieve single-machine performance levels that equal or exceed reported cluster implementations for common benchmark problems. A key design criterion is rapid exploration of models, hence the system is interactive and primarily single-user. The elements of the suite are: (i) the data engine, a hardware design pattern that balances storage, CPU and GPU acceleration for typical data mining workloads, (ii) BIDMat, an interactive matrix library that integrates CPU and GPU acceleration and novel computational kernels (iii), BIDMach, a machine learning system that includes very efficient model optimizers, (iv) Butterfly mixing, a communication strategy that hides the latency of frequent model updates needed by fast optimizers and (v) Design patterns to improve performance of iterative update algorithms. We present several benchmark problems to show how the above elements combine to yield multiple orders-of-magnitude improvements for each problem.",
                "call-number": "10.1145/2487575.2487677",
                "collection-title": "KDD '13",
                "container-title": "Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining",
                "DOI": "10.1145/2487575.2487677",
                "event-place": "Chicago, Illinois, USA",
                "ISBN": "9781450321747",
                "keyword": "data mining, machine learning, cluster, toolkit, gpu",
                "number-of-pages": "9",
                "page": "95–103",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Big data analytics with small footprint: squaring the cloud",
                "URL": "https://doi.org/10.1145/2487575.2487677"
            }
        },
        {
            "10.1145/2588555.2618215": {
                "id": "10.1145/2588555.2618215",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Özcan",
                        "given": "Fatma"
                    },
                    {
                        "family": "Tatbul",
                        "given": "Nesime"
                    },
                    {
                        "family": "Abadi",
                        "given": "Daniel J."
                    },
                    {
                        "family": "Kornacker",
                        "given": "Marcel"
                    },
                    {
                        "family": "Mohan",
                        "given": "C."
                    },
                    {
                        "family": "Ramasamy",
                        "given": "Karthik"
                    },
                    {
                        "family": "Wiener",
                        "given": "Janet"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2014,
                            6,
                            18
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2014,
                            6,
                            18
                        ]
                    ]
                },
                "call-number": "10.1145/2588555.2618215",
                "collection-title": "SIGMOD '14",
                "container-title": "Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data",
                "DOI": "10.1145/2588555.2618215",
                "event-place": "Snowbird, Utah, USA",
                "ISBN": "9781450323765",
                "keyword": "NewSQL, NoSQL, SQL-on-HADOOP",
                "number-of-pages": "2",
                "page": "1407–1408",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Are we experiencing a big data bubble?",
                "URL": "https://doi.org/10.1145/2588555.2618215"
            }
        },
        {
            "10.1145/3093338.3093372": {
                "id": "10.1145/3093338.3093372",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Jimenez-Ruiz",
                        "given": "Ivan"
                    },
                    {
                        "family": "Gonzalez-Mendez",
                        "given": "Ricardo"
                    },
                    {
                        "family": "Ropelewski",
                        "given": "Alexander"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            7,
                            9
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            7,
                            9
                        ]
                    ]
                },
                "abstract": "Appropriate optimization of bioinformatics workflows is vital to improve the timely discovery of variants implicated in cancer genomics. Sequenced human brain tumor data was assembled to optimize tool implementations and run various components of RNA sequence (RNA-seq) workflows. The measurable information produced by these tools account for the success rate and overall efficiency of a standardized and simultaneous analysis. We used the National Center for Biotechnology Information) Sequence Read Archive (NCBI-SRA) database to retrieve two transcriptomic datasets containing over 104 million reads as input data. We used these datasets to benchmark various file systems on the Bridges supercomputer to improve overall workflow throughput. Based on program and job timings, we report critical recommendations on selections of appropriate file systems and node types to efficiently execute these workflows.",
                "call-number": "10.1145/3093338.3093372",
                "collection-number": "45",
                "collection-title": "PEARC17",
                "container-title": "Proceedings of the Practice and Experience in Advanced Research Computing 2017 on Sustainability, Success and Impact",
                "DOI": "10.1145/3093338.3093372",
                "event-place": "New Orleans, LA, USA",
                "ISBN": "9781450352727",
                "keyword": "Performance, Supercomputing, Timings, Genome, Bioinformatics, Memory, Transcriptome, Workflows, File Systems, ACM proceedings",
                "number": "Article 45",
                "number-of-pages": "4",
                "page": "1–4",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Optimizing High Performance Big Data Cancer Workflows",
                "URL": "https://doi.org/10.1145/3093338.3093372"
            }
        },
        {
            "10.1145/3297280.3297474": {
                "id": "10.1145/3297280.3297474",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Gonzalez-Aparicio",
                        "given": "Maria Teresa"
                    },
                    {
                        "family": "Younas",
                        "given": "Muhammad"
                    },
                    {
                        "family": "Tuya",
                        "given": "Javier"
                    },
                    {
                        "family": "Casado",
                        "given": "Rubén"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            4,
                            8
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            4,
                            8
                        ]
                    ]
                },
                "abstract": "Traditional SQL and NoSQL big data systems are the backbone for managing data in cloud, fog and edge computing. This paper develops a new system and adopts the TPC-DS industry standard benchmark in order to evaluate three key properties, availability, consistency and efficiency (ACE) of SQL and NoSQL systems. The contributions of this work are manifold. It evaluates and analyses the tradeoff between the ACE properties. It provides insight into the NoSQL systems and how they can be improved to be sustainable for a more wide range of applications. The evaluation shows that SQL provides stronger consistency, but at the expense of low efficiency and availability. NoSQL provides better efficiency and availability but lacks support for stronger consistency. In order for NoSQL systems to be more sustainable they need to implement transactional schemes that enforce stronger consistency as well as better efficiency and availability.",
                "call-number": "10.1145/3297280.3297474",
                "collection-title": "SAC '19",
                "container-title": "Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing",
                "DOI": "10.1145/3297280.3297474",
                "event-place": "Limassol, Cyprus",
                "ISBN": "9781450359337",
                "keyword": "TPC-DS, big data, SQL, NoSQL, data consistency, Riak",
                "number-of-pages": "8",
                "page": "1988–1995",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Evaluation of ACE properties of traditional SQL and NoSQL big data systems",
                "URL": "https://doi.org/10.1145/3297280.3297474"
            }
        },
        {
            "10.1145/2939672.2939859": {
                "id": "10.1145/2939672.2939859",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Li",
                        "given": "Qingyang"
                    },
                    {
                        "family": "Qiu",
                        "given": "Shuang"
                    },
                    {
                        "family": "Ji",
                        "given": "Shuiwang"
                    },
                    {
                        "family": "Thompson",
                        "given": "Paul M."
                    },
                    {
                        "family": "Ye",
                        "given": "Jieping"
                    },
                    {
                        "family": "Wang",
                        "given": "Jie"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            8,
                            13
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2016,
                            8,
                            13
                        ]
                    ]
                },
                "abstract": "Lasso regression is a widely used technique in data mining for model selection and feature extraction. In many applications, it remains challenging to apply the regression model to large-scale problems that have massive data samples with high-dimensional features. One popular and promising strategy is to solve the Lasso problem in parallel. Parallel solvers run multiple cores in parallel on a shared memory system to speedup the computation, while the practical usage is limited by the huge dimension in the feature space. Screening is a promising method to solve the problem of high dimensionality by discarding the inactive features and removing them from optimization. However, when integrating screening methods with parallel solvers, most of solvers cannot guarantee the convergence on the reduced feature matrix. In this paper, we propose a novel parallel framework by parallelizing screening methods and integrating it with our proposed parallel solver. We propose two parallel screening algorithms: Parallel Strong Rule (PSR) and Parallel Dual Polytope Projection (PDPP). For the parallel solver, we proposed an Asynchronous Grouped Coordinate Descent method (AGCD) to optimize the regression problem in parallel on the reduced feature matrix. AGCD is based on a grouped selection strategy to select the coordinate that has the maximum descent for the objective function in a group of candidates. Empirical studies on the real-world datasets demonstrate that the proposed parallel framework has a superior performance compared to the state-of-the-art parallel solvers.",
                "call-number": "10.1145/2939672.2939859",
                "collection-title": "KDD '16",
                "container-title": "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
                "DOI": "10.1145/2939672.2939859",
                "event-place": "San Francisco, California, USA",
                "ISBN": "9781450342322",
                "keyword": "screening rules, parallel computing, lasso regression, coordinate descent, aynchronized coordinate descent",
                "number-of-pages": "10",
                "page": "1705–1714",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Parallel Lasso Screening for Big Data Optimization",
                "URL": "https://doi.org/10.1145/2939672.2939859"
            }
        },
        {
            "10.1145/3178461.3178464": {
                "id": "10.1145/3178461.3178464",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Tahat",
                        "given": "Ashraf"
                    },
                    {
                        "family": "Aburub",
                        "given": "Ruba"
                    },
                    {
                        "family": "Al-Zyoude",
                        "given": "Aseel"
                    },
                    {
                        "family": "Talhi",
                        "given": "Chamseddine"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            1,
                            4
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            1,
                            4
                        ]
                    ]
                },
                "abstract": "A new integrated environmental monitoring system to carry-out real-time measurements on board a moving vehicle is presented. It is composed of an arbitrary number of Electronic Measurements Units (EMU), a smart phone application to relay collected data, and a cloud Central Processing Platform (CPP) to perform analysis utilizing big data techniques and algorithms. Each EMU consists of an electric circuit that incorporates an ultra violet (UV) sensor, an air particles concentration sensor, a temperature sensor and a humidity sensor that all interface to a microcontroller. Bluetooth is employed for communication between the EMU and the smart phone application, while a 3G/4G cellular communications network furnishes the wireless connectivity to the remote CPP. When the collected data reaches the designated cloud server (CPP), it is immediately stored for subsequent analysis. Finally, big data statistical analysis (clustering and classification), mapping and plotting are performed to deduce correlations and to facilitate inferencing. Moreover, the scalability and low-cost of selected components of this realistic system makes it very feasible for large scale deployments in the context of smart cities initiatives, ad-hoc designs, or educational projects.",
                "call-number": "10.1145/3178461.3178464",
                "collection-title": "ICSIM2018",
                "container-title": "Proceedings of the 2018 International Conference on Software Engineering and Information Management",
                "DOI": "10.1145/3178461.3178464",
                "event-place": "Casablanca, Morocco",
                "ISBN": "9781450354387",
                "keyword": "telemetry, Big data, air particles, smart phone, temperature sensor, environment, UV index",
                "number-of-pages": "5",
                "page": "82–86",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "A Smart City Environmental Monitoring Network and Analysis Relying on Big Data Techniques",
                "URL": "https://doi.org/10.1145/3178461.3178464"
            }
        },
        {
            "10.14778/2752939.2752945": {
                "id": "10.14778/2752939.2752945",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Loghin",
                        "given": "Dumitrel"
                    },
                    {
                        "family": "Tudor",
                        "given": "Bogdan Marius"
                    },
                    {
                        "family": "Zhang",
                        "given": "Hao"
                    },
                    {
                        "family": "Ooi",
                        "given": "Beng Chin"
                    },
                    {
                        "family": "Teo",
                        "given": "Yong Meng"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2015,
                            2,
                            1
                        ]
                    ]
                },
                "abstract": "The continuous increase in volume, variety and velocity of Big Data exposes datacenter resource scaling to an energy utilization problem. Traditionally, datacenters employ x86-64 (big) server nodes with power usage of tens to hundreds of Watts. But lately, low-power (small) systems originally developed for mobile devices have seen significant improvements in performance. These improvements could lead to the adoption of such small systems in servers, as announced by major industry players. In this context, we systematically conduct a performance study of Big Data execution on small nodes in comparison with traditional big nodes, and present insights that would be useful for future development. We run Hadoop MapReduce, MySQL and in-memory Shark workloads on clusters of ARM big. LITTLE boards and Intel Xeon server systems. We evaluate execution time, energy usage and total cost of running the workloads on self-hosted ARM and Xeon nodes. Our study shows that there is no one size fits all rule for judging the efficiency of executing Big Data workloads on small and big nodes. But small memory size, low memory and I/O bandwidths, and software immaturity concur in canceling the lower-power advantage of ARM servers. We show that I/O-intensive MapReduce workloads are more energy-efficient to run on Xeon nodes. In contrast, database query processing is always more energy-efficient on ARM servers, at the cost of slightly lower throughput. With minor software modifications, CPU-intensive MapReduce workloads are almost four times cheaper to execute on ARM servers.",
                "call-number": "10.14778/2752939.2752945",
                "container-title": "Proc. VLDB Endow.",
                "DOI": "10.14778/2752939.2752945",
                "ISSN": "2150-8097",
                "issue": "7",
                "number-of-pages": "12",
                "page": "762–773",
                "publisher": "VLDB Endowment",
                "source": "February 2015",
                "title": "A performance study of big data on small nodes",
                "URL": "https://doi.org/10.14778/2752939.2752945",
                "volume": "8"
            }
        },
        {
            "10.1145/3383455.3422529": {
                "id": "10.1145/3383455.3422529",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Chen",
                        "given": "Qian"
                    },
                    {
                        "family": "Liu",
                        "given": "Xiao-Yang"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2020,
                            10,
                            15
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2020,
                            10,
                            15
                        ]
                    ]
                },
                "abstract": "ESG (Environmental, social and governance) alpha strategy that makes sustainable investment has gained popularity among investors. The ESG fields of study in scholar big data is a valuable alternative data that reflects a company's long-term ESG commitment. However, it is considered a difficulty to quantitatively measure a company's ESG premium and its impact to the company's stock price using scholar big data. In this paper, we utilize ESG scholar data as alternative data to develop an automatic trading strategy and propose a practical machine learning approach to quantify the ESG premium of a company and capture the ESG alpha. First, we construct our ESG investment universe and apply feature engineering on the companies' ESG scholar data from the Microsoft Academic Graph database. Then, we train six complementary machine learning models using a combination of financial indicators and ESG scholar data features and employ an ensemble method to predict stock prices and automatically set up portfolio allocation. Finally, we manage our portfolio, trade and rebalance the portfolio allocation monthly using predicted stock prices. We backtest our ESG alpha strategy and compare its performance with benchmarks. The proposed ESG alpha strategy achieves a cumulative return of 2,154.4% during the backtesting period of ten years, which significantly outperforms the NASDAQ-100 index's 397.4% and S&P 500's 226.9%. The traditional financial indicators results in only 1,443.7%, thus our scholar data-based ESG alpha strategy is better at capturing ESG premium than traditional financial indicators.",
                "call-number": "10.1145/3383455.3422529",
                "collection-number": "41",
                "collection-title": "ICAIF '20",
                "container-title": "Proceedings of the First ACM International Conference on AI in Finance",
                "DOI": "10.1145/3383455.3422529",
                "event-place": "New York, New York",
                "ISBN": "9781450375849",
                "keyword": "ESG alpha, quantitative investment, AI in finance, alternative data, scholar data",
                "number": "Article 41",
                "number-of-pages": "8",
                "page": "1–8",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Quantifying ESG alpha using scholar big data: an automated machine learning approach",
                "URL": "https://doi.org/10.1145/3383455.3422529"
            }
        },
        {
            "10.1145/3093241.3093265": {
                "id": "10.1145/3093241.3093265",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Madani",
                        "given": "Youness"
                    },
                    {
                        "family": "Bengourram",
                        "given": "Jemaa"
                    },
                    {
                        "family": "Erritali",
                        "given": "Mohammed"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            5,
                            19
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            5,
                            19
                        ]
                    ]
                },
                "abstract": "Studies have shown that the registration forms on Websites are ineffective because many people give false data, forget their login information to the site or just refuse to register, to overcome these problems a new type of authentication is born is the social authentication or social login which is a type of SSO(Single Sign-On),due to this type of authentication enrollment increases to a platform because the user registered to the platform with a simple click of a button authentication without passing by the step of filling a form, choose a username and a secure password. In this article, we will study the social authentication how it works, and how after the authorization of the user we can Retrieve personal data to complete registration, we can also use its social authorization on our facebook application to register its data on HDFS in a Big data system to analyze them and personalize its member space in the platform, using the Hadoop framework based on the MapReduce programming.",
                "call-number": "10.1145/3093241.3093265",
                "collection-title": "ICCDA '17",
                "container-title": "Proceedings of the International Conference on Compute and Data Analysis",
                "DOI": "10.1145/3093241.3093265",
                "event-place": "Lakeland, FL, USA",
                "ISBN": "9781450352413",
                "keyword": "SSO(Single Sign-On), Hadoop, social authentication, MapReduce, Big Data, authorization, HDFS, Authentication",
                "number-of-pages": "7",
                "page": "91–97",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Social Login and Data Storage in the Big Data File System HDFS",
                "URL": "https://doi.org/10.1145/3093241.3093265"
            }
        },
        {
            "10.1145/2786752": {
                "id": "10.1145/2786752",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Shim",
                        "given": "J. P."
                    },
                    {
                        "family": "Koh",
                        "given": "J."
                    },
                    {
                        "family": "Fister",
                        "given": "S."
                    },
                    {
                        "family": "Seo",
                        "given": "H. Y."
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            1,
                            25
                        ]
                    ]
                },
                "abstract": "Data from phone interactions can help address customers' complaints, and predict their future purchasing behavior.",
                "call-number": "10.1145/2786752",
                "container-title": "Commun. ACM",
                "DOI": "10.1145/2786752",
                "ISSN": "0001-0782",
                "issue": "2",
                "number-of-pages": "7",
                "page": "84–90",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "February 2016",
                "title": "Phonetic analytics technology and big data: real-world cases",
                "URL": "https://doi.org/10.1145/2786752",
                "volume": "59"
            }
        },
        {
            "10.1145/3511211": {
                "id": "10.1145/3511211",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Chen",
                        "given": "Lei"
                    },
                    {
                        "family": "Zhao",
                        "given": "Jiacheng"
                    },
                    {
                        "family": "Wang",
                        "given": "Chenxi"
                    },
                    {
                        "family": "Cao",
                        "given": "Ting"
                    },
                    {
                        "family": "Zigman",
                        "given": "John"
                    },
                    {
                        "family": "Volos",
                        "given": "Haris"
                    },
                    {
                        "family": "Mutlu",
                        "given": "Onur"
                    },
                    {
                        "family": "Lv",
                        "given": "Fang"
                    },
                    {
                        "family": "Feng",
                        "given": "Xiaobing"
                    },
                    {
                        "family": "Xu",
                        "given": "Guoqing Harry"
                    },
                    {
                        "family": "Cui",
                        "given": "Huimin"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2022,
                            7,
                            5
                        ]
                    ]
                },
                "abstract": "To process real-world datasets, modern data-parallel systems often require extremely large amounts of memory, which are both costly and energy inefficient. Emerging non-volatile memory (NVM) technologies offer high capacity compared to DRAM and low energy compared to SSDs. Hence, NVMs have the potential to fundamentally change the dichotomy between DRAM and durable storage in Big Data processing. However, most Big Data applications are written in managed languages and executed on top of a managed runtime that already performs various dimensions of memory management. Supporting hybrid physical memories adds a new dimension, creating unique challenges in data replacement. This article proposes Panthera, a semantics-aware, fully automated memory management technique for Big Data processing over hybrid memories. Panthera analyzes user programs on a Big Data system to infer their coarse-grained access patterns, which are then passed to the Panthera runtime for efficient data placement and migration. For Big Data applications, the coarse-grained data division information is accurate enough to guide the GC for data layout, which hardly incurs overhead in data monitoring and moving. We implemented Panthera in OpenJDK and Apache Spark. Based on Big Data applications’ memory access pattern, we also implemented a new profiling-guided optimization strategy, which is transparent to applications. With this optimization, our extensive evaluation demonstrates that Panthera reduces energy by 32–53% at less than 1% time overhead on average. To show Panthera’s applicability, we extend it to QuickCached, a pure Java implementation of Memcached. Our evaluation results show that Panthera reduces energy by 28.7% at 5.2% time overhead on average.",
                "call-number": "10.1145/3511211",
                "collection-number": "2",
                "container-title": "ACM Trans. Comput. Syst.",
                "DOI": "10.1145/3511211",
                "ISSN": "0734-2071",
                "issue": "1-4",
                "keyword": "memory management, garbage collection, Hybrid memories, Big Data systems",
                "number": "Article 2",
                "number-of-pages": "38",
                "page": "1–38",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "November 2021",
                "title": "Unified Holistic Memory Management Supporting Multiple Big Data Processing Frameworks over Hybrid Memories",
                "URL": "https://doi.org/10.1145/3511211",
                "volume": "39"
            }
        },
        {
            "10.1145/2983323.2983841": {
                "id": "10.1145/2983323.2983841",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Zhang",
                        "given": "Xuyun"
                    },
                    {
                        "family": "Leckie",
                        "given": "Christopher"
                    },
                    {
                        "family": "Dou",
                        "given": "Wanchun"
                    },
                    {
                        "family": "Chen",
                        "given": "Jinjun"
                    },
                    {
                        "family": "Kotagiri",
                        "given": "Ramamohanarao"
                    },
                    {
                        "family": "Salcic",
                        "given": "Zoran"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            10,
                            24
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2016,
                            10,
                            24
                        ]
                    ]
                },
                "abstract": "While cloud computing has become an attractive platform for supporting data intensive applications, a major obstacle to the adoption of cloud computing in sectors such as health and defense is the privacy risk associated with releasing datasets to third-parties in the cloud for analysis. A widely-adopted technique for data privacy preservation is to anonymize data via local recoding. However, most existing local-recoding techniques are either serial or distributed without directly optimizing scalability, thus rendering them unsuitable for big data applications. In this paper, we propose a highly scalable approach to local-recoding anonymization in cloud computing, based on Locality Sensitive Hashing (LSH). Specifically, a novel semantic distance metric is presented for use with LSH to measure the similarity between two data records. Then, LSH with the MinHash function family can be employed to divide datasets into multiple partitions for use with MapReduce to parallelize computation while preserving similarity. By using our efficient LSH-based scheme, we can anonymize each partition through the use of a recursive agglomerative $k$-member clustering algorithm. Extensive experiments on real-life datasets show that our approach significantly improves the scalability and time-efficiency of local-recoding anonymization by orders of magnitude over existing approaches.",
                "call-number": "10.1145/2983323.2983841",
                "collection-title": "CIKM '16",
                "container-title": "Proceedings of the 25th ACM International on Conference on Information and Knowledge Management",
                "DOI": "10.1145/2983323.2983841",
                "event-place": "Indianapolis, Indiana, USA",
                "ISBN": "9781450340731",
                "keyword": "mapreduce, big data, privacy preservation, cloud, LSH",
                "number-of-pages": "10",
                "page": "1793–1802",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Scalable Local-Recoding Anonymization using Locality Sensitive Hashing for Big Data Privacy Preservation",
                "URL": "https://doi.org/10.1145/2983323.2983841"
            }
        },
        {
            "10.1145/2771299": {
                "id": "10.1145/2771299",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Goth",
                        "given": "Gregory"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2015,
                            6,
                            25
                        ]
                    ]
                },
                "abstract": "Open source tools assist data science.",
                "call-number": "10.1145/2771299",
                "container-title": "Commun. ACM",
                "DOI": "10.1145/2771299",
                "ISSN": "0001-0782",
                "issue": "7",
                "number-of-pages": "3",
                "page": "17–19",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "July 2015",
                "title": "Bringing big data to the big tent",
                "URL": "https://doi.org/10.1145/2771299",
                "volume": "58"
            }
        },
        {
            "10.1145/3098593.3098597": {
                "id": "10.1145/3098593.3098597",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Blenk",
                        "given": "Andreas"
                    },
                    {
                        "family": "Kalmbach",
                        "given": "Patrick"
                    },
                    {
                        "family": "Kellerer",
                        "given": "Wolfgang"
                    },
                    {
                        "family": "Schmid",
                        "given": "Stefan"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            8,
                            7
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            8,
                            7
                        ]
                    ]
                },
                "abstract": "At the heart of many computer network planning, deployment, and operational tasks lie hard algorithmic problems. Accordingly, over the last decades, we have witnessed a continuous pursuit for ever more accurate and faster algorithms. We propose an approach to design network algorithms which is radically different from most existing algorithms. Our approach is motivated by the observation that most existing algorithms to solve a given hard computer networking problem overlook a simple yet very powerful optimization opportunity in practice: many network algorithms are executed repeatedly (e.g., for each virtual network request or in reaction to user mobility), and hence with each execution, generate interesting data: (problem,solution)-pairs. We make the case for leveraging the potentially big data of an algorithm's past executions to improve and speed up future, similar solutions, by reducing the algorithm's search space. We study the applicability of machine learning to network algorithm design, identify challenges and discuss limitations. We empirically demonstrate the potential of machine learning network algorithms in two case studies, namely the embedding of virtual networks (a packing optimization problem) and k-center facility location (a covering optimization problem), using a prototype implementation.",
                "call-number": "10.1145/3098593.3098597",
                "collection-title": "Big-DAMA '17",
                "container-title": "Proceedings of the Workshop on Big Data Analytics and Machine Learning for Data Communication Networks",
                "DOI": "10.1145/3098593.3098597",
                "event-place": "Los Angeles, CA, USA",
                "ISBN": "9781450350549",
                "keyword": "Machine Learning, Computer Networks, Big Data, Algorithms",
                "number-of-pages": "6",
                "page": "19–24",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "o'zapft is: Tap Your Network Algorithm's Big Data!",
                "URL": "https://doi.org/10.1145/3098593.3098597"
            }
        },
        {
            "10.1145/3474880.3474882": {
                "id": "10.1145/3474880.3474882",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Yin",
                        "given": "Pengzhi"
                    },
                    {
                        "family": "Huang",
                        "given": "Hao"
                    },
                    {
                        "family": "Zhao",
                        "given": "Mengxuan"
                    },
                    {
                        "family": "Zhu",
                        "given": "Ying"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            6,
                            18
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2021,
                            6,
                            18
                        ]
                    ]
                },
                "abstract": "Policy decisions and marketing models have a great impact on the specific application of customer relationship management. In various fields of marketing, big data marketing is gradually popularized as a combined product. The so-called big data marketing refers to collecting a large amount of behavior data through the Internet, primarily helping advertisers find out the target audience to analyze the content, time and form of advertising and finally complete the marketing process of advertising. The big data marketing can also be used for effective customer relationship management, which not only effectively enhance customer stickiness and reduce enterprise operating costs, but also has great significance for deeply mining the potential value of existing customers, predicting the future demand trend of customers, discovering new market growth points and developing new customer groups. Taking the large FMCG company A as the research subject, its sales is grim and stagnant: the purchase quantity of products is low; the price is lower than the cost; simple reproduction is difficult to continue. In order to change the current situation of company A, first of all, we cooperate with the marketing team to determine the business needs, using SQL and python to conduct exploratory data analysis (EDA) on 110k + e-commerce transaction data, and generate Data-Driven Insights on purchasing behavior and product sales; secondly, we use Python to establish RFM model (recent degree, frequency, currency), classify customers and calculate. Finally, according to the above customer analysis, a feasible listing strategy is proposed, which is expected to increase the retention rate by 3.5% and the monthly sales by 6%. Through big data marketing calculation, we come to a solution for the current situation of company A, which will have a good influence on many other companies.",
                "call-number": "10.1145/3474880.3474882",
                "collection-title": "ICEBT '21",
                "container-title": "Proceedings of the 2021 5th International Conference on E-Education, E-Business and E-Technology",
                "DOI": "10.1145/3474880.3474882",
                "event-place": "Beijing, China",
                "ISBN": "9781450389600",
                "page": "1–0",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Application of Big Data Marketing in Customer Relationship Management",
                "URL": "https://doi.org/10.1145/3474880.3474882"
            }
        },
        {
            "10.1145/3194452.3194458": {
                "id": "10.1145/3194452.3194458",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Aboagye",
                        "given": "Emelia Opoku"
                    },
                    {
                        "family": "James",
                        "given": "Gee C."
                    },
                    {
                        "family": "Jianbin",
                        "given": "Gao"
                    },
                    {
                        "family": "Kumar",
                        "given": "Rajesh"
                    },
                    {
                        "family": "Khan",
                        "given": "Riaz Ullah"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            3,
                            12
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            3,
                            12
                        ]
                    ]
                },
                "abstract": "A parallel scheme based on Probabilistic Tensor Factorization which addresses the scalability problem of Collaborative Filtering (CF) is proposed for big data processing. Parallel algorithms for large scale recommendation problems have witnessed advancements in the big data era in recent times. Matrix Factorization models have been enormously used to tackle such constraints, which we see as not scalable and does not converge easily unless numerous iterations making it computationally expensive. This study proposes a novel coordinate descent based probabilistic Tensor factorization method; Scalable Probabilistic Time Context Tensor Factorization (SPTTF) for collaborative recommendation. Our experiments with natural datasets show its efficiency.",
                "call-number": "10.1145/3194452.3194458",
                "collection-title": "ICCAI 2018",
                "container-title": "Proceedings of the 2018 International Conference on Computing and Artificial Intelligence",
                "DOI": "10.1145/3194452.3194458",
                "event-place": "Chengdu, China",
                "ISBN": "9781450364195",
                "keyword": "Time contest, algorithm integration, tensor, SPTTF",
                "number-of-pages": "4",
                "page": "118–121",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Probabilistic Time Context Framework for Big Data Collaborative Recommendation",
                "URL": "https://doi.org/10.1145/3194452.3194458"
            }
        },
        {
            "10.1145/3093338.3104155": {
                "id": "10.1145/3093338.3104155",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Rivera",
                        "given": "Sergio"
                    },
                    {
                        "family": "Hayashida",
                        "given": "Mami"
                    },
                    {
                        "family": "Griffioen",
                        "given": "James"
                    },
                    {
                        "family": "Fei",
                        "given": "Zongming"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            7,
                            9
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            7,
                            9
                        ]
                    ]
                },
                "abstract": "Existing campus network infrastructure is not designed to effectively handle the transmission of big data sets. Performance degradation in these networks is often caused by middleboxes -- appliances that enforce campus-wide policies by deeply inspecting all traffic going through the network (including big data transmissions). We are developing a Software-Defined Networking (SDN) solution for our campus network that grants privilege to science flows by dynamically calculating routes that bypass certain middleboxes to avoid the bottlenecks they create. Using the global network information provided by an SDN controller, we are developing graph databases approaches to compute custom paths that not only bypass middleboxes to achieve certain requirements (e.g., latency, bandwidth, hop-count) but also insert rules that modify packets hop-by-hop to create the illusion of standard routing/forward despite the fact that packets are being rerouted. In some cases, additional functionality needs to be added to the path using network function virtualization (NFV) techniques (e.g., NAT). To ensure that path computations are run on an up-to-date snapshot of the topology, we introduce a versioning mechanism that allows for lazy topology updates that occur only when \"important\" network changes take place and are requested by big data flows.",
                "call-number": "10.1145/3093338.3104155",
                "collection-number": "59",
                "collection-title": "PEARC17",
                "container-title": "Proceedings of the Practice and Experience in Advanced Research Computing 2017 on Sustainability, Success and Impact",
                "DOI": "10.1145/3093338.3104155",
                "event-place": "New Orleans, LA, USA",
                "ISBN": "9781450352727",
                "keyword": "Path Calculation, Software-Defined Networks, Big Data Flows",
                "number": "Article 59",
                "number-of-pages": "4",
                "page": "1–4",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Dynamically Creating Custom SDN High-Speed Network Paths for Big Data Science Flows",
                "URL": "https://doi.org/10.1145/3093338.3104155"
            }
        },
        {
            "10.1145/3344948.3344986": {
                "id": "10.1145/3344948.3344986",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Castellanos",
                        "given": "Camilo"
                    },
                    {
                        "family": "Varela",
                        "given": "Carlos A."
                    },
                    {
                        "family": "Correal",
                        "given": "Dario"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            9,
                            9
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            9,
                            9
                        ]
                    ]
                },
                "abstract": "Big data analytics (BDA) applications use advanced analysis algorithms to extract valuable insights from large, fast, and heterogeneous data sources. These complex BDA applications require software design, development, and deployment strategies to deal with volume, velocity, and variety (3vs) while sustaining expected performance levels. BDA software complexity frequently leads to delayed deployments, longer development cycles and challenging performance monitoring. This paper proposes a DevOps and Domain Specific Model (DSM) approach to design, deploy, and monitor performance Quality Scenarios (QS) in BDA applications. This approach uses high-level abstractions to describe deployment strategies and QS enabling performance monitoring. Our experimentation compares the effort of development, deployment and QS monitoring of BDA applications with two use cases of near mid-air collisions (NMAC) detection. The use cases include different performance QS, processing models, and deployment strategies. Our results show shorter (re)deployment cycles and the fulfillment of latency and deadline QS for micro-batch and batch processing.",
                "call-number": "10.1145/3344948.3344986",
                "collection-title": "ECSA '19",
                "container-title": "Proceedings of the 13th European Conference on Software Architecture - Volume 2",
                "DOI": "10.1145/3344948.3344986",
                "event-place": "Paris, France",
                "ISBN": "9781450371421",
                "keyword": "DevOps, software architecture, performance quality scenarios, big data analytics, domain specific model",
                "number-of-pages": "8",
                "page": "165–172",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Measuring performance quality scenarios in big data analytics applications: a DevOps and domain-specific model approach",
                "URL": "https://doi.org/10.1145/3344948.3344986"
            }
        },
        {
            "10.1145/2818869.2818881": {
                "id": "10.1145/2818869.2818881",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Yongmei",
                        "given": "Wei"
                    },
                    {
                        "family": "Fengmin",
                        "given": "Chen"
                    },
                    {
                        "family": "Cher",
                        "given": "Lim Khai"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2015,
                            10,
                            7
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2015,
                            10,
                            7
                        ]
                    ]
                },
                "abstract": "Current distributed storage systems mainly rely on data replication to ensure certain level of data availability and reliability. But in scenarios, like data archiving, replication is not cost effective and does not provides a robust solution to prevent data loss. A recent trend is to introduce erasure codes into the distributed storage. Inspired by the RAID system, early attempts have been focused on designing Reed-Solomon (R-S) based solutions with small block sizes. This paper investigates in details about repair traffic to apply Low Density Parity Check (LDPC) codes with relatively large block sizes. It has been demonstrated that the LDPC codes have unique advantages over R-S based solutions including low repair traffic for multiple erasures and parity erasures. The LDPC-based method is integrated with the Hadoop system with various configurations. Both theoretical analysis and simulations show that significant improvement in reliability can be achieved through using large LDPC codes without increasing the repair latency and network traffic especially for multiple erasures. Simulations also show great improvement in terms repairing latencies compared with Reed-Solomon codes. The latency is further improved through parallelism by engaging map-reduce processes from Hadoop.",
                "call-number": "10.1145/2818869.2818881",
                "collection-number": "1",
                "collection-title": "ASE BD&amp;SI '15",
                "container-title": "Proceedings of the ASE BigData & SocialInformatics 2015",
                "DOI": "10.1145/2818869.2818881",
                "event-place": "Kaohsiung, Taiwan",
                "ISBN": "9781450337359",
                "keyword": "Reliability, Reed-Solomon codes, Redundancy, Erasure codes, LDPC codes, Distributed storage",
                "number": "Article 1",
                "number-of-pages": "6",
                "page": "1–6",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Large LDPC Codes for Big Data Storage",
                "URL": "https://doi.org/10.1145/2818869.2818881"
            }
        },
        {
            "10.1145/2641225": {
                "id": "10.1145/2641225",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Greengard",
                        "given": "Samuel"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2014,
                            9,
                            1
                        ]
                    ]
                },
                "abstract": "Increased computing power combined with new and more advanced models are changing weather forecasting.",
                "call-number": "10.1145/2641225",
                "container-title": "Commun. ACM",
                "DOI": "10.1145/2641225",
                "ISSN": "0001-0782",
                "issue": "9",
                "number-of-pages": "3",
                "page": "12–14",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "September 2014",
                "title": "Weathering a new era of big data",
                "URL": "https://doi.org/10.1145/2641225",
                "volume": "57"
            }
        },
        {
            "10.14778/2733004.2733071": {
                "id": "10.14778/2733004.2733071",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Li",
                        "given": "Yunyao"
                    },
                    {
                        "family": "Liu",
                        "given": "Ziyang"
                    },
                    {
                        "family": "Zhu",
                        "given": "Huaiyu"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2014,
                            8,
                            1
                        ]
                    ]
                },
                "abstract": "Enterprise search allows users in an enterprise to retrieve desired information through a simple search interface. It is widely viewed as an important productivity tool within an enterprise. While Internet search engines have been highly successful, enterprise search remains notoriously challenging due to a variety of unique challenges, and is being made more so by the increasing heterogeneity and volume of enterprise data. On the other hand, enterprise search also presents opportunities to succeed in ways beyond current Internet search capabilities. This tutorial presents an organized overview of these challenges and opportunities, and reviews the state-of-the-art techniques for building a reliable and high quality enterprise search engine, in the context of the rise of big data.",
                "call-number": "10.14778/2733004.2733071",
                "container-title": "Proc. VLDB Endow.",
                "DOI": "10.14778/2733004.2733071",
                "ISSN": "2150-8097",
                "issue": "13",
                "number-of-pages": "2",
                "page": "1717–1718",
                "publisher": "VLDB Endowment",
                "source": "August 2014",
                "title": "Enterprise search in the big data era: recent developments and open challenges",
                "URL": "https://doi.org/10.14778/2733004.2733071",
                "volume": "7"
            }
        },
        {
            "10.1145/3380688.3380705": {
                "id": "10.1145/3380688.3380705",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Ngo",
                        "given": "Vuong M."
                    },
                    {
                        "family": "Kechadi",
                        "given": "M-Tahar"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2020,
                            1,
                            17
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2020,
                            1,
                            17
                        ]
                    ]
                },
                "abstract": "Nowadays, the agricultural data can be generated through various sources, such as: Internet of Thing (IoT), sensors, satellites, weather stations, robots, farm equipment, agricultural laboratories, farmers, government agencies and agribusinesses. The analysis of this big data enables farmers, companies and agronomists to extract high business and scientific knowledge, improving their operational processes and product quality. However, before analysing this data, different data sources need to be normalised, homogenised and integrated into a unified data representation. In this paper, we propose an agricultural data integration method using a constellation schema which is designed to be flexible enough to incorporate other datasets and big data models. We also apply some methods to extract knowledge with the view to improve crop yield; these include finding suitable quantities of soil properties, herbicides and insecticides for both increasing crop yield and protecting the environment.",
                "call-number": "10.1145/3380688.3380705",
                "collection-title": "ICMLSC 2020",
                "container-title": "Proceedings of the 4th International Conference on Machine Learning and Soft Computing",
                "DOI": "10.1145/3380688.3380705",
                "event-place": "Haiphong City, Viet Nam",
                "ISBN": "9781450376310",
                "keyword": "insecticides, Decision support, herbicides, soil properties, crop yield",
                "number-of-pages": "5",
                "page": "46–50",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Crop Knowledge Discovery Based on Agricultural Big Data Integration",
                "URL": "https://doi.org/10.1145/3380688.3380705"
            }
        },
        {
            "10.1145/3537693.3537710": {
                "id": "10.1145/3537693.3537710",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Nguyen Thi Thu",
                        "given": "Ha"
                    },
                    {
                        "family": "Nguyen",
                        "given": "Binh Giang"
                    },
                    {
                        "family": "Trung",
                        "given": "Nguyen Xuan"
                    },
                    {
                        "family": "Ho Ngoc",
                        "given": "Vinh"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2022,
                            4,
                            27
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2022,
                            4,
                            27
                        ]
                    ]
                },
                "abstract": "The rapid growth of online booking websites has created a new trend in hotel star rating based on customer reviews. Therefore, there is a discrepancy between hotel ratings by traveler on the Internet and hotel ratings according to national standards, especially for 4–5 stars hotels. In recent years, a number of hotel rating organizations on the world have incorporated Internet star rating standards to update their hotel star rating standards. In Vietnam, the hotel star rating standards have been updated since 2015 and have not yet approached online hotel star ratings. In this study, a new hotel rating method is proposed using Internet traveler reviews for rating. Data was collected from TripAdvisor about hotels in Vietnam from 4-5 stars of 5 major cities. Deep neural network model is used to classify hotels from 3 to 5 stars. The results shown that, the deviation between online rating and actual star rating is 0.6. This is also a suggestion for hotel managers to understand about their customers and improve the quality of their hotels to match the common standards of many different customers around the world.",
                "call-number": "10.1145/3537693.3537710",
                "collection-title": "ICEEG '22",
                "container-title": "Proceedings of the 6th International Conference on E-Commerce, E-Business and E-Government",
                "DOI": "10.1145/3537693.3537710",
                "event-place": "Plymouth, United Kingdom",
                "ISBN": "9781450396523",
                "keyword": "hotel management, online review, data analysis, neural network, Vietnamese hotel, overall rating",
                "number-of-pages": "7",
                "page": "108–114",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "A method for Vietnamese Hotel Online Rating based on Big Data Analysis: Vietnames Hotel Rating based on Big Data analysis",
                "URL": "https://doi.org/10.1145/3537693.3537710"
            }
        },
        {
            "10.1145/3468264.3468532": {
                "id": "10.1145/3468264.3468532",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Yang",
                        "given": "Chengxu"
                    },
                    {
                        "family": "Li",
                        "given": "Yuanchun"
                    },
                    {
                        "family": "Xu",
                        "given": "Mengwei"
                    },
                    {
                        "family": "Chen",
                        "given": "Zhenpeng"
                    },
                    {
                        "family": "Liu",
                        "given": "Yunxin"
                    },
                    {
                        "family": "Huang",
                        "given": "Gang"
                    },
                    {
                        "family": "Liu",
                        "given": "Xuanzhe"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            8,
                            20
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2021,
                            8,
                            20
                        ]
                    ]
                },
                "abstract": "Big data has become valuable property for enterprises and enabled various intelligent applications. Today, it is common to host data in big data platforms (e.g., Spark), where developers can submit scripts to process the original and intermediate data tables. Meanwhile, it is highly desirable to manage the data to comply with various privacy requirements. To enable flexible and automated privacy policy enforcement, we propose TaintStream, a fine-grained taint tracking framework for Spark-like big data platforms. TaintStream works by automatically injecting taint tracking logic into the data processing scripts, and the injected scripts are dynamically translated to maintain a taint tag for each cell during execution. The dynamic translation rules are carefully designed to guarantee non-interference in the original data operation. By defining different semantics of taint tags, TaintStream can enable various data management applications such as access control, data retention, and user data erasure. Our experiments on a self-crafted benchmarksuite show that TaintStream is able to achieve accurate cell-level taint tracking with a precision of 93.0% and less than 15% overhead. We also demonstrate the usefulness of TaintStream through several real-world use cases of privacy policy enforcement.",
                "call-number": "10.1145/3468264.3468532",
                "collection-title": "ESEC/FSE 2021",
                "container-title": "Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
                "DOI": "10.1145/3468264.3468532",
                "event-place": "Athens, Greece",
                "ISBN": "9781450385626",
                "keyword": "Taint tracking, privacy compliance, GDPR, big data platform",
                "number-of-pages": "12",
                "page": "806–817",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "TaintStream: fine-grained taint tracking for big data platforms through dynamic code translation",
                "URL": "https://doi.org/10.1145/3468264.3468532"
            }
        },
        {
            "10.1145/3377672.3378054": {
                "id": "10.1145/3377672.3378054",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Liqiang",
                        "given": "Hao"
                    },
                    {
                        "family": "Quan",
                        "given": "Liu"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            12,
                            8
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            12,
                            8
                        ]
                    ]
                },
                "abstract": "This paper proposes a personalized learning resource recommendation model based on big data. The design of the model consists of data storage, data analysis, resource matching, and the resource recommendation. In order to provide a suitable resource, data analysis is a more critical procedure that involves the analyses of basic information, learning style, learning status, learning behavior, and learning interest, which can be successfully analyzed by means of kafka and flume. Through an experiment, it shows that personalized resource recommendation platform really plays a positive role in improving students learning.",
                "call-number": "10.1145/3377672.3378054",
                "collection-title": "AMME 2019",
                "container-title": "Proceedings of the 2019 Annual Meeting on Management Engineering",
                "DOI": "10.1145/3377672.3378054",
                "event-place": "Kuala Lumpur, Malaysia",
                "ISBN": "9781450362481",
                "keyword": "personalized learning, recommendation model, big data",
                "number-of-pages": "7",
                "page": "181–187",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Design of Resource Recommendation Model for Personalized Learning in the Era of Big Data",
                "URL": "https://doi.org/10.1145/3377672.3378054"
            }
        },
        {
            "10.1145/3178442.3178447": {
                "id": "10.1145/3178442.3178447",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Ertel",
                        "given": "Sebastian"
                    },
                    {
                        "family": "Adam",
                        "given": "Justus"
                    },
                    {
                        "family": "Castrillon",
                        "given": "Jeronimo"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            2,
                            24
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            2,
                            24
                        ]
                    ]
                },
                "abstract": "Big data systems scale with the number of cores in a cluster for the parts of an application that can be executed in data parallel fashion. It has been recently reported, however, that these systems fail to translate hardware improvements, such as increased network bandwidth, into a higher throughput. This is particularly the case for applications that have inherent sequential, computationally intensive phases. In this paper, we analyze the data processing cores of state-of-the-art big data systems to find the cause for these scalability problems. We identify design patterns in the code that are suitable for pipeline and task-level parallelism, potentially increasing application performance. As a proof of concept, we rewrite parts of the Hadoop MapReduce framework in an implicit parallel language that exploits this parallelism without adding code complexity. Our experiments on a data analytics workload show throughput speedups of up to 3.5x.",
                "call-number": "10.1145/3178442.3178447",
                "collection-title": "PMAM'18",
                "container-title": "Proceedings of the 9th International Workshop on Programming Models and Applications for Multicores and Manycores",
                "DOI": "10.1145/3178442.3178447",
                "event-place": "Vienna, Austria",
                "ISBN": "9781450356459",
                "number-of-pages": "10",
                "page": "41–50",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Supporting Fine-grained Dataflow Parallelism in Big Data Systems",
                "URL": "https://doi.org/10.1145/3178442.3178447"
            }
        },
        {
            "10.1145/3407703.3407711": {
                "id": "10.1145/3407703.3407711",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Wenxia",
                        "given": "Ding"
                    },
                    {
                        "family": "Heping",
                        "given": "Li"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2020,
                            8,
                            20
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2020,
                            8,
                            20
                        ]
                    ]
                },
                "abstract": "Once the bridge is put into use, in addition to its own material aging, it will also receive damage from human factors such as vehicles, wind, earthquakes, fatigue, and overload. This damage has more or less reduced the service life of the bridge or even destroyed the safety performance of the bridge, causing huge losses to people's lives and property. This requires us to pay attention to the safety, reliability and durability of the bridge at all times during the construction of the bridge and in the later maintenance process. The traditional bridge monitoring work and operation and maintenance have a low degree of automation, bridge condition assessment, and bridge real-time monitoring and comprehensive information management is difficult. This requires real-time monitoring of this information in the context of big data to ensure the healthy use of the bridge. In this paper, a finite element analysis model is established to monitor the sensor network of the bridge and the data detected by the sensor in the context of big data. The optimization analysis results in an optimized layout plan of the identifiable static sensors, taking into account both the economic and structural operating conditions of the bridge.",
                "call-number": "10.1145/3407703.3407711",
                "collection-title": "AICSconf '20",
                "container-title": "Proceedings of the 2020 Artificial Intelligence and Complex Systems Conference",
                "DOI": "10.1145/3407703.3407711",
                "event-place": "Wuhan, China",
                "ISBN": "9781450377270",
                "keyword": "condition evaluation, Big data, real-time monitoring, finite element analysis, bridge health, reliability",
                "number-of-pages": "5",
                "page": "34–38",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Correlation analysis of factors affecting bridge health under the background of big data",
                "URL": "https://doi.org/10.1145/3407703.3407711"
            }
        },
        {
            "10.1145/3219104.3229276": {
                "id": "10.1145/3219104.3229276",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Rivera",
                        "given": "Sergio"
                    },
                    {
                        "family": "Griffioen",
                        "given": "James"
                    },
                    {
                        "family": "Fei",
                        "given": "Zongming"
                    },
                    {
                        "family": "Hayashida",
                        "given": "Mami"
                    },
                    {
                        "family": "Shi",
                        "given": "Pinyi"
                    },
                    {
                        "family": "Chitre",
                        "given": "Bhushan"
                    },
                    {
                        "family": "Chappell",
                        "given": "Jacob"
                    },
                    {
                        "family": "Song",
                        "given": "Yongwook"
                    },
                    {
                        "family": "Pike",
                        "given": "Lowell"
                    },
                    {
                        "family": "Carpenter",
                        "given": "Charles"
                    },
                    {
                        "family": "Nasir",
                        "given": "Hussamuddin"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            7,
                            22
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            7,
                            22
                        ]
                    ]
                },
                "abstract": "The emergence of big data has created new challenges for researchers transmitting big data sets across campus networks to local (HPC) cloud resources, or over wide area networks to public cloud services. Unlike conventional HPC systems where the network is carefully architected (e.g., a high speed local interconnect, or a wide area connection between Data Transfer Nodes), today's big data communication often occurs over shared network infrastructures with many external and uncontrolled factors influencing performance.This paper describes our efforts to understand and characterize the performance of various big data transfer tools such as rclone, cyberduck, and other provider-specific CLI tools when moving data to/from public and private cloud resources. We analyze the various parameter settings available on each of these tools and their impact on performance. Our experimental results give insights into the performance of cloud providers and transfer tools, and provide guidance for parameter settings when using cloud transfer tools. We also explore performance when coming from HPC DTN nodes as well as researcher machines located deep in the campus network, and show that emerging SDN approaches such as the VIP Lanes system can deliver excellent performance even from researchers' machines.",
                "call-number": "10.1145/3219104.3229276",
                "collection-number": "22",
                "collection-title": "PEARC '18",
                "container-title": "Proceedings of the Practice and Experience on Advanced Research Computing",
                "DOI": "10.1145/3219104.3229276",
                "event-place": "Pittsburgh, PA, USA",
                "ISBN": "9781450364461",
                "keyword": "Big Data Flows, Data Transfer Tools, Software-Defined Networks",
                "number": "Article 22",
                "number-of-pages": "8",
                "page": "1–8",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Navigating the Unexpected Realities of Big Data Transfers in a Cloud-based World",
                "URL": "https://doi.org/10.1145/3219104.3229276"
            }
        },
        {
            "10.1145/3528416.3530868": {
                "id": "10.1145/3528416.3530868",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Slooff",
                        "given": "Tom"
                    },
                    {
                        "family": "Regazzoni",
                        "given": "Francesco"
                    },
                    {
                        "family": "Brocheton",
                        "given": "Fabien"
                    },
                    {
                        "family": "Parodi",
                        "given": "Antonio"
                    },
                    {
                        "family": "Cmar",
                        "given": "Radim"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2022,
                            5,
                            17
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2022,
                            5,
                            17
                        ]
                    ]
                },
                "abstract": "Big data analytics largely rely on data. Because of their central role, it is fundamental to ensure the security and correctness of data used in these applications. Anomaly detection could help to increase the security of big data analytics applications. However, these applications are very diverse both for the properties of the data analyzed and for the computations to be carried out on them. As a result, the selection of the most appropriate anomaly detection method is a challenging and time consuming task for designers. Hierarchical Temporal Memory (HTM) is as an anomaly detection technique sufficiently generic to achieve satisfactory performance on a wide range of applications, thus suitable to ease the burden of selecting the anomaly detection method. To confirm this, in this paper we explore the performance of HTM on a dataset used for air quality prediction. Our preliminary results show that HTM achieves excellent performance when compared to other popular anomaly detection methods.",
                "call-number": "10.1145/3528416.3530868",
                "collection-title": "CF '22",
                "container-title": "Proceedings of the 19th ACM International Conference on Computing Frontiers",
                "DOI": "10.1145/3528416.3530868",
                "event-place": "Turin, Italy",
                "ISBN": "9781450393386",
                "number-of-pages": "2",
                "page": "205–206",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Anomaly detection to improve security of big data analytics",
                "URL": "https://doi.org/10.1145/3528416.3530868"
            }
        },
        {
            "10.5555/2857070.2857072": {
                "id": "10.5555/2857070.2857072",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Chen",
                        "given": "Hsin-liang"
                    },
                    {
                        "family": "Doty",
                        "given": "Philip"
                    },
                    {
                        "family": "Mollman",
                        "given": "Carol"
                    },
                    {
                        "family": "Niu",
                        "given": "Xi"
                    },
                    {
                        "family": "Yu",
                        "given": "Jen-chien"
                    },
                    {
                        "family": "Zhang",
                        "given": "Tao"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2015,
                            11,
                            6
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2015,
                            11,
                            6
                        ]
                    ]
                },
                "abstract": "Emerging technologies have offered libraries and librarians new ways and methods to collect and analyze data in the era of accountability to justify their value and contributions. For example, Gallagher, Bauer and Dollar (2005) analyzed the paper and online journal usage from all possible data sources and discovered that users at the Yale Medical Library preferred the electronic format of articles to the print version. After this discovery, they were able to take necessary steps to adjust their journal subscriptions. Many library professionals advocate such data-driven library management to strengthen and specify library budget proposals.",
                "call-number": "10.5555/2857070.2857072",
                "collection-number": "2",
                "collection-title": "ASIST '15",
                "container-title": "Proceedings of the 78th ASIS&T Annual Meeting: Information Science with Impact: Research in and for the Community",
                "event-place": "St. Louis, Missouri",
                "ISBN": "087715547X",
                "keyword": "big data, information privacy, library assessment, data analytics, information policy",
                "number": "Article 2",
                "number-of-pages": "4",
                "page": "1–4",
                "publisher": "American Society for Information Science",
                "publisher-place": "USA",
                "title": "Library assessment and data analytics in the big data era: practice and policies"
            }
        },
        {
            "10.1145/3394486.3406477": {
                "id": "10.1145/3394486.3406477",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Jain",
                        "given": "Abhinav"
                    },
                    {
                        "family": "Patel",
                        "given": "Hima"
                    },
                    {
                        "family": "Nagalapatti",
                        "given": "Lokesh"
                    },
                    {
                        "family": "Gupta",
                        "given": "Nitin"
                    },
                    {
                        "family": "Mehta",
                        "given": "Sameep"
                    },
                    {
                        "family": "Guttula",
                        "given": "Shanmukha"
                    },
                    {
                        "family": "Mujumdar",
                        "given": "Shashank"
                    },
                    {
                        "family": "Afzal",
                        "given": "Shazia"
                    },
                    {
                        "family": "Sharma Mittal",
                        "given": "Ruhi"
                    },
                    {
                        "family": "Munigala",
                        "given": "Vitobha"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2020,
                            8,
                            23
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2020,
                            8,
                            23
                        ]
                    ]
                },
                "abstract": "It is well understood from literature that the performance of a machine learning (ML) model is upper bounded by the quality of the data. While researchers and practitioners have focused on improving the quality of models (such as neural architecture search and automated feature selection), there are limited efforts towards improving the data quality. One of the crucial requirements before consuming datasets for any application is to understand the dataset at hand and failure to do so can result in inaccurate analytics and unreliable decisions. Assessing the quality of the data across intelligently designed metrics and developing corresponding transformation operations to address the quality gaps helps to reduce the effort of a data scientist for iterative debugging of the ML pipeline to improve model performance. This tutorial highlights the importance of analysing data quality in terms of its value for machine learning applications. This tutorial surveys all the important data quality related approaches discussed in literature, focusing on the intuition behind them, highlighting their strengths and similarities, and illustrates their applicability to real-world problems. Finally we will discuss the interesting work IBM Research is doing in this space.",
                "call-number": "10.1145/3394486.3406477",
                "collection-title": "KDD '20",
                "container-title": "Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining",
                "DOI": "10.1145/3394486.3406477",
                "event-place": "Virtual Event, CA, USA",
                "ISBN": "9781450379984",
                "keyword": "data quality, machine learning, quality metrics",
                "number-of-pages": "2",
                "page": "3561–3562",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Overview and Importance of Data Quality for Machine Learning Tasks",
                "URL": "https://doi.org/10.1145/3394486.3406477"
            }
        },
        {
            "10.1145/3404512.3404527": {
                "id": "10.1145/3404512.3404527",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Deng",
                        "given": "Jianzhi"
                    },
                    {
                        "family": "Zhou",
                        "given": "Yuehan"
                    },
                    {
                        "family": "Tang",
                        "given": "Weixian"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2020,
                            5,
                            29
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2020,
                            5,
                            29
                        ]
                    ]
                },
                "abstract": "In this paper, we try to find the differentially expressed lncRNA and differentially expressed miRNA of Liver cancer, especially between the different gender's patients. The differentially expressed genes were screened from TCGA liver data. Based on the extracted differentially expressed lncRNAs and differentially expressed miRNAs, we reveal an 8-lncRNA (TTTY14, UCA1, LINC00162, TTTY10, XIST, ERVH48-1, ZFY-AS1 and TTTY15) to 3-miRNA (hsa-mir-506, hsa-mir-508, hsa-mir-205) regulatory network of 13 pairs inter-regulatory between male and female. The 8 differentially expressed lncRNAs in the lncRNA-miRNA regulatory network were analyzed by the multivariable COX regression model, and LINC00162 and TTTY10 were found as the co-expression differentially expressed lncRNAs. After survival kmplot analysis and receiver operating characteristic analysis of the co-expression differentially expressed lncRNAs, TTTY10 was selected and proved as the potential biomarker of liver cancer for the diagnose and therapy.",
                "call-number": "10.1145/3404512.3404527",
                "collection-title": "BDE 2020",
                "container-title": "Proceedings of the 2020 2nd International Conference on Big Data Engineering",
                "DOI": "10.1145/3404512.3404527",
                "event-place": "Shanghai, China",
                "ISBN": "9781450377225",
                "keyword": "liver cancer, TCGA, TTTY10, COX model, regulatory network, gender",
                "number-of-pages": "5",
                "page": "24–28",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Gene Big Data Analysis of Differentially Expressed lncRNA and MiRNA in Liver Cancer with Different Gender",
                "URL": "https://doi.org/10.1145/3404512.3404527"
            }
        },
        {
            "10.1145/3349341.3349431": {
                "id": "10.1145/3349341.3349431",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Wang",
                        "given": "Xiaodong"
                    },
                    {
                        "family": "Wang",
                        "given": "Qing"
                    },
                    {
                        "family": "Tao",
                        "given": "Ye"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            7,
                            12
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            7,
                            12
                        ]
                    ]
                },
                "abstract": "In recent years, big data has become the new focus of attention from all walks of life. The valuable information contained in big data becomes the driving force for people to process and analyze big data. Big data analytics helps enterprises to take better decisions to improve business output. As a user description tool, user profile is widely used in various fields. However, it is difficult to deal with large-scale datasets using traditional methods since the established processes was not designed to handle large volumes of data. In this paper, we propose a user profile analysis framework using machine learning approach which apply advanced machine learning programs to solve industrial scale problems. And this approach can be effective to speculate real and potential needs of various groups of users and precisely extract individual characteristics and group generality. By introducing high-level data parallel framework, the process of large-scale data processing can be executed efficiently. We use real-world data to validate the effectiveness of the proposed framework.",
                "call-number": "10.1145/3349341.3349431",
                "collection-title": "AICS 2019",
                "container-title": "Proceedings of the 2019 International Conference on Artificial Intelligence and Computer Science",
                "DOI": "10.1145/3349341.3349431",
                "event-place": "Wuhan, Hubei, China",
                "ISBN": "9781450371506",
                "keyword": "Distributed Machine Learning, Computing Framework, User Profile, Big Data Analysis",
                "number-of-pages": "6",
                "page": "358–363",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "A User Profile Analysis Framework Driven by Distributed Machine Learning for Big Data",
                "URL": "https://doi.org/10.1145/3349341.3349431"
            }
        },
        {
            "10.1145/3206157.3206164": {
                "id": "10.1145/3206157.3206164",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Kyo",
                        "given": "Koki"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            3,
                            9
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            3,
                            9
                        ]
                    ]
                },
                "abstract": "In this paper, we propose an approach for isolating the effects of economic and social events on stock prices. Using a newly-proposed Bayesian modeling technique, we decompose the daily time series of stock price data into three components: a trend component, a cyclical component, and an irregular component. We can then analyze the behavior of each estimated component in relation to economic and social events. As an empirical example, we analyze the daily time series for closing values of the Nikkei Stock Average (NSA) from January 4, 2000 to November 28, 2017, and examine relationships between the estimated components of NSA and significant events together with variations in the economic and social.",
                "call-number": "10.1145/3206157.3206164",
                "collection-title": "ICBDE '18",
                "container-title": "Proceedings of the 2018 International Conference on Big Data and Education",
                "DOI": "10.1145/3206157.3206164",
                "event-place": "Honolulu, HI, USA",
                "ISBN": "9781450363587",
                "keyword": "Bayesian modeling, Nikkei Stock Average, daily stock price data, state space model",
                "number-of-pages": "6",
                "page": "5–10",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "A Method for Big Data Analysis of the impact of Economic and Social Events on Japanese Stock Prices",
                "URL": "https://doi.org/10.1145/3206157.3206164"
            }
        },
        {
            "10.1145/3485190.3485204": {
                "id": "10.1145/3485190.3485204",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Yuan Lv",
                        "given": "Zhi"
                    },
                    {
                        "family": "Mu",
                        "given": "Zhang"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            8,
                            27
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2021,
                            8,
                            27
                        ]
                    ]
                },
                "abstract": "In order to investigate the mechanism of financial technology on big data industry, this paper selected the relevant data of 30 provinces (autonomous regions and municipalities) in China from 2013 to 2018 to establish a dynamic panel data model to measure the impact of financial technology on big data industry; and then used the mediating effect test method to test the action path of financial technology on big data industry through big data enterprise financing constraints. The results show that the regression coefficient of financial technology to big data industry is significantly positive at the significance level of 5%, it indicates that the financial technology can directly promote the development of big data industry. In the control variables, the regional technological innovation ability and the degree of opening to the outside world play a significant role in promoting the big data industry. In addition, the big data enterprise financing constraints have a partial mediating effect, the mediating effect account for 12.27% of the total effect, it indicates that the financial technology can indirectly promote the development of big data industry by alleviating the big data enterprise financing constraints.",
                "call-number": "10.1145/3485190.3485204",
                "collection-title": "IMMS 2021",
                "container-title": "2021 4th International Conference on Information Management and Management Science",
                "DOI": "10.1145/3485190.3485204",
                "event-place": "Chengdu, China",
                "ISBN": "9781450384278",
                "keyword": "financial technology, enterprise financing constraints, dynamic panel data model, big data industry, mediating effect",
                "number-of-pages": "7",
                "page": "82–88",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "The Effect Mechanism of Financial Technology on Big Data Industry Development in China",
                "URL": "https://doi.org/10.1145/3485190.3485204"
            }
        },
        {
            "10.1145/2396556.2396578": {
                "id": "10.1145/2396556.2396578",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Zahavi",
                        "given": "Eitan"
                    },
                    {
                        "family": "Keslassy",
                        "given": "Isaac"
                    },
                    {
                        "family": "Kolodny",
                        "given": "Avinoam"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2012,
                            10,
                            29
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2012,
                            10,
                            29
                        ]
                    ]
                },
                "abstract": "With the growing popularity of big-data applications, Data Center Networks increasingly carry larger and longer traffic flows. As a result of this increased flow granularity, static routing cannot efficiently load-balance traffic, resulting in an increased network contention and a reduced throughput. Unfortunately, while adaptive routing can solve this load-balancing problem, network designers refrain from using it, because it also creates out-of-order packet delivery that can significantly degrade the reliable transport performance of the longer flows. In this paper, we show that by throttling each flow bandwidth to half of the network link capacity, a distributed-adaptive-routing algorithm is able to converge to a non-blocking routing assignment within a few iterations, causing minimal out-of-order packet delivery. We present a Markov chain model for distributed-adaptive-routing in the context of Clos networks that provides an approximation for the expected convergence time. This model predicts that for full-link-bandwidth traffic, the convergence time is exponential with the network size, so out-of-order packet delivery is unavoidable for long messages. However, with half-rate traffic, the algorithm converges within a few iterations and exhibits weak dependency on the network size. Therefore, we show that distributed-adaptive-routing may be used to provide a scalable and non-blocking routing even for long flows on a rearrangeably-non-blocking Clos network under half-rate conditions. The proposed model is evaluated and approximately fits the abstract system simulation model. Hardware implementation guidelines are provided and evaluated using a detailed flit-level InfiniBand simulation model. These results directly apply to adaptive-routing systems designed and deployed in various fields.",
                "call-number": "10.1145/2396556.2396578",
                "collection-title": "ANCS '12",
                "container-title": "Proceedings of the eighth ACM/IEEE symposium on Architectures for networking and communications systems",
                "DOI": "10.1145/2396556.2396578",
                "event-place": "Austin, Texas, USA",
                "ISBN": "9781450316859",
                "keyword": "data center networks, big-data, adaptive routing",
                "number-of-pages": "12",
                "page": "99–110",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Distributed adaptive routing for big-data applications running on data center networks",
                "URL": "https://doi.org/10.1145/2396556.2396578"
            }
        },
        {
            "10.1145/2538862.2544296": {
                "id": "10.1145/2538862.2544296",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Martínez-Arocho",
                        "given": "Allison G."
                    },
                    {
                        "family": "Buffum",
                        "given": "Philip Sheridan"
                    },
                    {
                        "family": "Boyer",
                        "given": "Kristy Elizabeth"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2014,
                            3,
                            5
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2014,
                            3,
                            5
                        ]
                    ]
                },
                "abstract": "Exposing students early to computer science may influence their choice of career, and there is increasing recognition that even for students who do not pursue computer science careers, computational literacy is important. This poster reports on a project targeting the development of a new middle school computer science curriculum. This research aims to highlight the role of computation in Big Data in the context of middle school computer science education, which serves as a catalyst to keep students engaged in computer science through middle school via the ENGAGE narrative game-based learning environment. This poster discusses steps taken to validate one activity meant to highlight the role of computation in the context of Big Data: skip list manipulation. While we found that most of the middle school students performed poorly in assessments after the skip list activities, several students showed they were capable of completing the activity successfully, implying that a repetition of the revised skip list study and additional pilot studies for other Big Data activities are needed to pave the way for the development of this Big Data curriculum. This activity will be just one part of a broader curriculum designed to showcase the social relevance and power of Big Data.",
                "call-number": "10.1145/2538862.2544296",
                "collection-title": "SIGCSE '14",
                "container-title": "Proceedings of the 45th ACM technical symposium on Computer science education",
                "DOI": "10.1145/2538862.2544296",
                "event-place": "Atlanta, Georgia, USA",
                "ISBN": "9781450326056",
                "keyword": "engage, middle school, big data, game-based learning",
                "number-of-pages": "1",
                "page": "712",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Developing a game-based learning curriculum for \"Big Data\" in middle school (abstract only)",
                "URL": "https://doi.org/10.1145/2538862.2544296"
            }
        },
        {
            "10.1145/3183519.3183528": {
                "id": "10.1145/3183519.3183528",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Selakovic",
                        "given": "Marija"
                    },
                    {
                        "family": "Barnett",
                        "given": "Michael"
                    },
                    {
                        "family": "Musuvathi",
                        "given": "Madan"
                    },
                    {
                        "family": "Mytkowicz",
                        "given": "Todd"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            5,
                            27
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            5,
                            27
                        ]
                    ]
                },
                "abstract": "Building scalable big data programs currently requires programmers to combine relational (SQL) with non-relational code (Java, C#, Scala). Relational code is declarative - a program describes what the computation is and the compiler decides how to distribute the program. SQL query optimization has enjoyed a rich and fruitful history, however, most research and commercial optimization engines treat non-relational code as a black-box and thus are unable to optimize it.This paper empirically studies over 3 million SCOPE programs across five data centers within Microsoft and finds programs with non-relational code take between 45-70% of data center CPU time. We further explore the potential for SCOPE optimization by generating more native code from the non-relational part. Finally, we present 6 case studies showing that triggering more generation of native code in these jobs yields significant performance improvement: optimizing just one portion resulted in as much as 25% improvement for an entire program.",
                "call-number": "10.1145/3183519.3183528",
                "collection-title": "ICSE-SEIP '18",
                "container-title": "Proceedings of the 40th International Conference on Software Engineering: Software Engineering in Practice",
                "DOI": "10.1145/3183519.3183528",
                "event-place": "Gothenburg, Sweden",
                "ISBN": "9781450356596",
                "number-of-pages": "10",
                "page": "45–54",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Cross-language optimizations in big data systems: a case study of SCOPE",
                "URL": "https://doi.org/10.1145/3183519.3183528"
            }
        },
        {
            "10.1145/2808797.2809372": {
                "id": "10.1145/2808797.2809372",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Wang",
                        "given": "Kun"
                    },
                    {
                        "family": "Sun",
                        "given": "Duoyong"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2015,
                            8,
                            25
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2015,
                            8,
                            25
                        ]
                    ]
                },
                "abstract": "The Shanghai Cooperation Organization (SCO) is playing an increasingly important role in many respects, such as the economic, political and security cooperation. Research results aiming at this regional organization have gained lots of attentions for a long time. The Social Network Analysis (SNA) is considered to be an effective method in the studies of international relation, especially from the Big Data perspective. Our research is mainly based on the economic and trade data among the member states of SCO in 2012. All the results have significantly shown that both Russia and China have occupied the central roles, no matter in the tables or in the figures. Due to the limitation of essential data, the importance of Russia has not been fully reflected. Based on the results, we suggests that China should pay more attention to the affairs of the region. And then the Chinese government may enhance her influence in the SCO.",
                "call-number": "10.1145/2808797.2809372",
                "collection-title": "ASONAM '15",
                "container-title": "Proceedings of the 2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2015",
                "DOI": "10.1145/2808797.2809372",
                "event-place": "Paris, France",
                "ISBN": "9781450338547",
                "keyword": "Shanghai Cooperation Organization, Networks Architecture, Social Networks Analysis, Big Data",
                "number-of-pages": "4",
                "page": "1208–1211",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Research on the Shanghai Cooperation Organization Network Architecture from the Big Data Perspective",
                "URL": "https://doi.org/10.1145/2808797.2809372"
            }
        },
        {
            "10.1145/3456415.3456416": {
                "id": "10.1145/3456415.3456416",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Chen",
                        "given": "Ken"
                    },
                    {
                        "family": "He",
                        "given": "Jiabei"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            2,
                            25
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2021,
                            2,
                            25
                        ]
                    ]
                },
                "abstract": "With the continuous development and wide application of big data and artificial intelligence technology, how to efficiently use and mine the whole process data of university hydropower models, perception, business and flows, and realize the transformation of informationization of hydropower management to intelligentialize and wisdom, it has become one of the main tasks in the construction of universitiy informatization under the strategy of advocating energy conservation, lowcarbon sustainable development. Combining with the actual demand of university hydropower management, managing and serving the whole process of hydropower data collection, storage, analysis, monitoring and decision-making assistance, this paper proposes the architecture of an intelligent decision-making service platform for university hydropower on big data, and sorts out the core and key technologies in the platform development process and the current mainstream development frameworks and tools to provide technical references for the realization of intelligent hydropower management and application services in universities, and promote the overall planning and step-by-step implementation of smart campuses.",
                "call-number": "10.1145/3456415.3456416",
                "collection-title": "ICCBN 2021",
                "container-title": "2021 9th International Conference on Communications and Broadband Networking",
                "DOI": "10.1145/3456415.3456416",
                "event-place": "Shanghai, China",
                "ISBN": "9781450389174",
                "keyword": "hydropower information, big data, intelligentization",
                "number-of-pages": "5",
                "page": "1–5",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Big-Data-Based Research on the Architecture Design of University Hydropower Intelligent Decision Service Platform",
                "URL": "https://doi.org/10.1145/3456415.3456416"
            }
        },
        {
            "10.1145/3299869.3319898": {
                "id": "10.1145/3299869.3319898",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Ge",
                        "given": "Chang"
                    },
                    {
                        "family": "Li",
                        "given": "Yinan"
                    },
                    {
                        "family": "Eilebrecht",
                        "given": "Eric"
                    },
                    {
                        "family": "Chandramouli",
                        "given": "Badrish"
                    },
                    {
                        "family": "Kossmann",
                        "given": "Donald"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            6,
                            25
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            6,
                            25
                        ]
                    ]
                },
                "abstract": "There has been a recent flurry of interest in providing query capability on raw data in today's big data systems. These raw data must be parsed before processing or use in analytics. Thus, a fundamental challenge in distributed big data systems is that of efficient parallel parsing of raw data. The difficulties come from the inherent ambiguity while independently parsing chunks of raw data without knowing the context of these chunks. Specifically, it can be difficult to find the beginnings and ends of fields and records in these chunks of raw data. To parallelize parsing, this paper proposes a speculation-based approach for the CSV format, arguably the most commonly used raw data format. Due to the syntactic and statistical properties of the format, speculative parsing rarely fails and therefore parsing is efficiently parallelized in a distributed setting. Our speculative approach is also robust, meaning that it can reliably detect syntax errors in CSV data. We experimentally evaluate the speculative, distributed parsing approach in Apache Spark using more than 11,000 real-world datasets, and show that our parser produces significant performance benefits over existing methods.",
                "call-number": "10.1145/3299869.3319898",
                "collection-title": "SIGMOD '19",
                "container-title": "Proceedings of the 2019 International Conference on Management of Data",
                "DOI": "10.1145/3299869.3319898",
                "event-place": "Amsterdam, Netherlands",
                "ISBN": "9781450356435",
                "keyword": "parsing, parallel, distributed, csv",
                "number-of-pages": "17",
                "page": "883–899",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Speculative Distributed CSV Data Parsing for Big Data Analytics",
                "URL": "https://doi.org/10.1145/3299869.3319898"
            }
        }
    ]
}