{
    "exportedDoiLength": 101,
    "fileName": "acm",
    "style": "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<style xmlns=\"http://purl.org/net/xbiblio/csl\" class=\"in-text\" version=\"1.0\" demote-non-dropping-particle=\"sort-only\" default-locale=\"en-US\">\n    <!-- This style was edited with the Visual CSL Editor (http://editor.citationstyles.org/visualEditor/) -->\n    <info>\n        <title>BibTeX ACM citation style</title>\n        <id>http://www.zotero.org/styles/bibtex-acm-citation-style</id>\n        <link href=\"http://www.zotero.org/styles/bibtex-acm-citation-style\" rel=\"self\"/>\n        <link href=\"http://www.bibtex.org/\" rel=\"documentation\"/>\n        <author>\n            <name>Markus Schaffner</name>\n        </author>\n        <contributor>\n            <name>Richard Karnesky</name>\n            <email>karnesky+zotero@gmail.com</email>\n            <uri>http://arc.nucapt.northwestern.edu/Richard_Karnesky</uri>\n        </contributor>\n        <category citation-format=\"author-date\"/>\n        <category field=\"generic-base\"/>\n        <updated>2018-06-11T10:52:49+00:00</updated>\n        <rights license=\"http://creativecommons.org/licenses/by-sa/3.0/\">This work is licensed under a Creative Commons Attribution-ShareAlike 3.0 License</rights>\n    </info>\n    <macro name=\"zotero2bibtexType\">\n        <choose>\n            <if type=\"BILL BOOK GRAPHIC LEGAL_CASE LEGISLATION MOTION_PICTURE SONG\" match=\"any\">\n                <choose>\n                    <if genre=\"rfc\"  match=\"any\">\n                       <text value=\"rfc\"/>\n                    </if>\n                     <else-if  genre=\"bibliography\" match=\"any\">\n                        <text value=\"bibliography\"/>\n                    </else-if>\n                    <else-if  genre=\"play_drama\" match=\"any\">\n                        <text value=\"playdrama\"/>\n                    </else-if>\n                    <else-if  genre=\"proceeding\" match=\"any\">\n                        <text value=\"proceedings\"/>\n                    </else-if>\n                    <else-if  genre=\"tech_brief\" match=\"any\">\n                        <text value=\"tech-brief\"/>\n                    </else-if>\n                    <else>\n                       <text value=\"book\"/>\n                    </else>\n                </choose>\n            </if>\n            <else-if type=\"CHAPTER\" match=\"any\">\n                <text value=\"inbook\"/>\n            </else-if>\n            <else-if type=\"ARTICLE ARTICLE_JOURNAL ARTICLE_MAGAZINE ARTICLE_NEWSPAPER\" match=\"any\">\n                <text value=\"article\"/>\n            </else-if>\n            <else-if type=\"THESIS\" match=\"any\">\n                <choose>\n                    <if variable=\"genre\">\n                        <text variable=\"genre\" text-case=\"lowercase\" strip-periods=\"true\"/>\n                    </if>\n                </choose>\n                <text value=\"thesis\"/>\n            </else-if>\n            <else-if type=\"PAPER_CONFERENCE\" match=\"any\">\n                <text value=\"inproceedings\"/>\n            </else-if>\n            <else-if type=\"REPORT\" match=\"any\">\n                <text value=\"techreport\"/>\n            </else-if>\n            <else-if type=\"DATASET\" match=\"any\">\n                <choose>\n                    <if genre=\"software\"  match=\"any\">\n                       <text value=\"software\"/>\n                    </if>\n                    <else>\n                       <text value=\"dataset\"/>\n                    </else>\n                </choose>\n            </else-if>\n            <else>\n                <text value=\"misc\"/>\n            </else>\n        </choose>\n    </macro>\n    <macro name=\"citeKey\">\n           <text variable=\"call-number\"/>\n    </macro>\n    <macro name=\"editor-short\">\n        <names variable=\"editor\">\n            <name form=\"short\" delimiter=\":\" delimiter-precedes-last=\"always\"/>\n        </names>\n    </macro>\n    <macro name=\"author-short\">\n        <names variable=\"author\">\n            <name form=\"short\" delimiter=\":\" delimiter-precedes-last=\"always\"/>\n        </names>\n    </macro>\n    <macro name=\"issued-year\">\n        <date variable=\"issued\">\n            <date-part name=\"year\"/>\n        </date>\n    </macro>\n    <macro name=\"issued-month\">\n        <choose>\n            <if type=\"ARTICLE\" match=\"any\">\n                <date variable=\"issued\">\n                    <date-part name=\"month\" form=\"short\" strip-periods=\"true\" text-case=\"lowercase\"/>\n                </date>\n            </if>\n        </choose>\n    </macro>\n    <macro name=\"issue-date\">\n        <choose>\n            <if type=\"ARTICLE ARTICLE_JOURNAL\" match=\"any\">\n                <choose>\n                    <if variable=\"note\" match=\"none\">\n                        <choose>\n                            <if variable=\"source\">\n                                 <text variable=\"source\"/>\n                            </if>\n                               <else>\n                                 <date date-parts=\"year-month\" form=\"text\" variable=\"issued\"/>\n                               </else>\n                        </choose>\n                    </if>\n                </choose>\n            </if>\n        </choose>\n    </macro>\n    <macro name=\"author\">\n        <names variable=\"author\">\n            <name sort-separator=\", \" delimiter=\" and \" delimiter-precedes-last=\"always\" name-as-sort-order=\"all\"/>\n            <label form=\"long\" text-case=\"capitalize-first\"/>\n        </names>\n    </macro>\n    <macro name=\"editor-translator\">\n        <names variable=\"editor translator\" delimiter=\", \">\n            <name sort-separator=\", \" delimiter=\" and \" delimiter-precedes-last=\"always\" name-as-sort-order=\"all\"/>\n            <label form=\"long\" text-case=\"capitalize-first\"/>\n        </names>\n    </macro>\n    <macro name=\"title\">\n        <choose>\n            <if genre=\"proceeding\" match=\"any\">\n                <text variable=\"container-title-short\" suffix=\": \"/>\n                <text variable=\"title\" text-case=\"title\"/>\n            </if>\n            <else-if type=\"ARTICLE_JOURNAL\" match=\"none\">\n                <text variable=\"title\" text-case=\"title\"/>\n            </else-if>\n        </choose>\n    </macro>\n    <macro name=\"volume\">\n        <choose>\n            <if type=\"ARTICLE BOOK ARTICLE_JOURNAL\" match=\"any\">\n                <text variable=\"volume\" prefix=\"volume = {\" suffix=\"}\"/>\n            </if>\n        </choose>\n    </macro>\n    <macro name=\"DOI\">\n        <choose>\n            <if type=\"ARTICLE PAPER_CONFERENCE\" match=\"any\">\n                <text variable=\"DOI\" prefix=\"doi = {\" suffix=\"}\"/>\n            </if>\n        </choose>\n    </macro>\n    <macro name=\"abstract\">\n        <if match=\"any\" variable=\"abstract\">\n            <text variable=\"abstract\"/>\n        </if>\n    </macro>\n    <macro name=\"URL\">\n        <text variable=\"URL\" prefix=\"url = {\" suffix=\"}\"/>\n    </macro>\n    <macro name=\"container-title\">\n        <choose>\n            <if type=\"CHAPTER PAPER_CONFERENCE\" match=\"any\">\n                <text variable=\"container-title\" prefix=\"booktitle = {\" suffix=\"}\" text-case=\"title\"/>\n            </if>\n            <else-if type=\"ARTICLE ARTICLE_JOURNAL\" match=\"any\">\n                <text variable=\"container-title\" prefix=\"journal = {\" suffix=\"}\" text-case=\"title\"/>\n            </else-if>\n        </choose>\n    </macro>\n    <macro name=\"pages\">\n        <group delimiter=\",&#10;\">\n            <choose>\n                <if match=\"any\" variable=\"collection-number\">\n                    <text variable=\"collection-number\" prefix=\"articleno = {\" suffix=\"}\"/>\n                </if>\n                <else>\n                    <text variable=\"page\" prefix=\"pages = {\" suffix=\"}\"/>\n                </else>\n            </choose>\n            <text variable=\"number-of-pages\" prefix=\"numpages = {\" suffix=\"}\"/>\n        </group>\n    </macro>\n    <macro name=\"edition\">\n        <text variable=\"edition\"/>\n    </macro>\n    <macro name=\"editor\">\n        <choose>\n            <if match=\"any\" type=\"THESIS\">\n                <names variable=\"editor\" delimiter=\", \" prefix=\"advisor = {\" suffix=\"}\">\n                    <name delimiter=\" and \" delimiter-precedes-last=\"always\" name-as-sort-order=\"all\"/>\n                </names>\n            </if>\n            <else>\n               <names variable=\"editor\" delimiter=\", \" prefix=\"editor = {\" suffix=\"}\">\n                   <name delimiter=\" and \" delimiter-precedes-last=\"always\" name-as-sort-order=\"all\"/>\n               </names>\n            </else>\n        </choose>\n    </macro>\n    <macro name=\"keyword\">\n        <choose>\n            <if type=\"ARTICLE PAPER_CONFERENCE DATASET\" match=\"any\">\n                <text variable=\"keyword\" prefix=\"keywords = {\" suffix=\"}\"/>\n            </if>\n        </choose>\n    </macro>\n    <citation et-al-min=\"11\" et-al-use-first=\"10\" disambiguate-add-year-suffix=\"true\" disambiguate-add-names=\"false\" disambiguate-add-givenname=\"false\" collapse=\"year\">\n        <layout delimiter=\"_\">\n            <text macro=\"citeKey\"/>\n        </layout>\n    </citation>\n    <bibliography hanging-indent=\"false\">\n        <layout>\n            <group display=\"right-inline\">\n                <text macro=\"zotero2bibtexType\" prefix=\"@\"/>\n                <group prefix=\"{\" suffix=\"&#10;}\" delimiter=\",&#10;\">\n                    <text macro=\"citeKey\"/>\n                    <text macro=\"author\" prefix=\"author = {\" suffix=\"}\"/>\n                    <text macro=\"editor\"/>\n                    <text macro=\"title\" prefix=\"title = {\" suffix=\"}\"/>\n                    <text macro=\"issued-year\" prefix=\"year = {\" suffix=\"}\"/>\n                    <text macro=\"issue-date\" prefix=\"issue_date = {\" suffix=\"}\"/>\n                    <text variable=\"ISBN\" prefix=\"isbn = {\" suffix=\"}\"/>\n                    <text variable=\"publisher\" prefix=\"publisher = {\" suffix=\"}\"/>\n                    <text variable=\"publisher-place\" prefix=\"address = {\" suffix=\"}\"/>\n                    <text variable=\"chapter-number\" prefix=\"chapter = {\" suffix=\"}\"/>\n                    <text macro=\"edition\" prefix=\"edition = {\" suffix=\"}\"/>\n                    <text macro=\"volume\"/>\n                    <text variable=\"issue\" prefix=\"number = {\" suffix=\"}\"/>\n                    <text variable=\"ISSN\" prefix=\"issn = {\" suffix=\"}\"/>\n                    <text variable=\"archive_location\" prefix=\"archiveLocation = {\" suffix=\"}\"/>\n                    <text macro=\"URL\"/>\n                    <text macro=\"DOI\"/>\n                    <text macro=\"abstract\" prefix=\"abstract = {\" suffix=\"}\"/>\n                    <text variable=\"note\" prefix=\"note = {\" suffix=\"}\"/>\n                    <text macro=\"container-title\"/>\n                    <text macro=\"issued-month\" prefix=\"month = {\" suffix=\"}\"/>\n                    <text macro=\"pages\"/>\n                    <text macro=\"keyword\"/>\n                    <text variable=\"event-place\" prefix=\"location = {\" suffix=\"}\"/>\n                    <choose>\n                        <if type=\"PAPER_CONFERENCE\" match=\"any\">\n                            <text variable=\"collection-title\" prefix=\"series = {\" suffix=\"}\"/>\n                        </if>\n                        <else>\n                            <text variable=\"collection-title\" prefix=\"collection = {\" suffix=\"}\"/>\n                        </else>\n                    </choose>\n                </group>\n            </group>\n        </layout>\n    </bibliography>\n</style>\n",
    "suffix": "bib",
    "locale": "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<locale xmlns=\"http://purl.org/net/xbiblio/csl\" version=\"1.0\" xml:lang=\"en-US\">\n  <info>\n    <rights license=\"http://creativecommons.org/licenses/by-sa/3.0/\">This work is licensed under a Creative Commons Attribution-ShareAlike 3.0 License</rights>\n    <updated>2012-07-04T23:31:02+00:00</updated>\n  </info>\n  <style-options punctuation-in-quote=\"true\"\n                 leading-noise-words=\"a,an,the\"\n                 name-as-sort-order=\"ja zh kr my hu vi\"\n                 name-never-short=\"ja zh kr my hu vi\"/>\n  <date form=\"text\">\n    <date-part name=\"month\" suffix=\" \"/>\n    <date-part name=\"day\" suffix=\", \"/>\n    <date-part name=\"year\"/>\n  </date>\n  <date form=\"numeric\">\n    <date-part name=\"month\" form=\"numeric-leading-zeros\" suffix=\"/\"/>\n    <date-part name=\"day\" form=\"numeric-leading-zeros\" suffix=\"/\"/>\n    <date-part name=\"year\"/>\n  </date>\n  <terms>\n    <term name=\"radio-broadcast\">radio broadcast</term>\n    <term name=\"television-broadcast\">television broadcast</term>\n    <term name=\"podcast\">podcast</term>\n    <term name=\"instant-message\">instant message</term>\n    <term name=\"email\">email</term>\n    <term name=\"number-of-volumes\">\n      <single>volume</single>\n      <multiple>volumes</multiple>\n    </term>\n    <term name=\"accessed\">accessed</term>\n    <term name=\"and\">and</term>\n    <term name=\"and\" form=\"symbol\">&amp;</term>\n    <term name=\"and others\">and others</term>\n    <term name=\"anonymous\">anonymous</term>\n    <term name=\"anonymous\" form=\"short\">anon.</term>\n    <term name=\"at\">at</term>\n    <term name=\"available at\">available at</term>\n    <term name=\"by\">by</term>\n    <term name=\"circa\">circa</term>\n    <term name=\"circa\" form=\"short\">c.</term>\n    <term name=\"cited\">cited</term>\n    <term name=\"edition\">\n      <single>edition</single>\n      <multiple>editions</multiple>\n    </term>\n    <term name=\"edition\" form=\"short\">ed.</term>\n    <term name=\"et-al\">et al.</term>\n    <term name=\"forthcoming\">forthcoming</term>\n    <term name=\"from\">from</term>\n    <term name=\"ibid\">ibid.</term>\n    <term name=\"in\">in</term>\n    <term name=\"in press\">in press</term>\n    <term name=\"internet\">internet</term>\n    <term name=\"interview\">interview</term>\n    <term name=\"letter\">letter</term>\n    <term name=\"no date\">no date</term>\n    <term name=\"no date\" form=\"short\">n.d.</term>\n    <term name=\"online\">online</term>\n    <term name=\"presented at\">presented at the</term>\n    <term name=\"reference\">\n      <single>reference</single>\n      <multiple>references</multiple>\n    </term>\n    <term name=\"reference\" form=\"short\">\n      <single>ref.</single>\n      <multiple>refs.</multiple>\n    </term>\n    <term name=\"retrieved\">retrieved</term>\n    <term name=\"scale\">scale</term>\n    <term name=\"version\">version</term>\n\n    <!-- ANNO DOMINI; BEFORE CHRIST -->\n    <term name=\"ad\">AD</term>\n    <term name=\"bc\">BC</term>\n\n    <!-- PUNCTUATION -->\n    <term name=\"open-quote\">“</term>\n    <term name=\"close-quote\">”</term>\n    <term name=\"open-inner-quote\">‘</term>\n    <term name=\"close-inner-quote\">’</term>\n    <term name=\"page-range-delimiter\">–</term>\n\n    <!-- ORDINALS -->\n    <term name=\"ordinal\">th</term>\n    <term name=\"ordinal-01\">st</term>\n    <term name=\"ordinal-02\">nd</term>\n    <term name=\"ordinal-03\">rd</term>\n    <term name=\"ordinal-11\">th</term>\n    <term name=\"ordinal-12\">th</term>\n    <term name=\"ordinal-13\">th</term>\n\n    <!-- LONG ORDINALS -->\n    <term name=\"long-ordinal-01\">first</term>\n    <term name=\"long-ordinal-02\">second</term>\n    <term name=\"long-ordinal-03\">third</term>\n    <term name=\"long-ordinal-04\">fourth</term>\n    <term name=\"long-ordinal-05\">fifth</term>\n    <term name=\"long-ordinal-06\">sixth</term>\n    <term name=\"long-ordinal-07\">seventh</term>\n    <term name=\"long-ordinal-08\">eighth</term>\n    <term name=\"long-ordinal-09\">ninth</term>\n    <term name=\"long-ordinal-10\">tenth</term>\n\n    <!-- LONG LOCATOR FORMS -->\n    <term name=\"book\">\n      <single>book</single>\n      <multiple>books</multiple>\n    </term>\n    <term name=\"chapter\">\n      <single>chapter</single>\n      <multiple>chapters</multiple>\n    </term>\n    <term name=\"column\">\n      <single>column</single>\n      <multiple>columns</multiple>\n    </term>\n    <term name=\"figure\">\n      <single>figure</single>\n      <multiple>figures</multiple>\n    </term>\n    <term name=\"folio\">\n      <single>folio</single>\n      <multiple>folios</multiple>\n    </term>\n    <term name=\"issue\">\n      <single>number</single>\n      <multiple>numbers</multiple>\n    </term>\n    <term name=\"line\">\n      <single>line</single>\n      <multiple>lines</multiple>\n    </term>\n    <term name=\"note\">\n      <single>note</single>\n      <multiple>notes</multiple>\n    </term>\n    <term name=\"opus\">\n      <single>opus</single>\n      <multiple>opera</multiple>\n    </term>\n    <term name=\"page\">\n      <single>page</single>\n      <multiple>pages</multiple>\n    </term>\n    <term name=\"paragraph\">\n      <single>paragraph</single>\n      <multiple>paragraph</multiple>\n    </term>\n    <term name=\"part\">\n      <single>part</single>\n      <multiple>parts</multiple>\n    </term>\n    <term name=\"section\">\n      <single>section</single>\n      <multiple>sections</multiple>\n    </term>\n    <term name=\"sub verbo\">\n      <single>sub verbo</single>\n      <multiple>sub verbis</multiple>\n    </term>\n    <term name=\"verse\">\n      <single>verse</single>\n      <multiple>verses</multiple>\n    </term>\n    <term name=\"volume\">\n      <single>volume</single>\n      <multiple>volumes</multiple>\n    </term>\n\n    <!-- SHORT LOCATOR FORMS -->\n    <term name=\"book\" form=\"short\">bk.</term>\n    <term name=\"chapter\" form=\"short\">chap.</term>\n    <term name=\"column\" form=\"short\">col.</term>\n    <term name=\"figure\" form=\"short\">fig.</term>\n    <term name=\"folio\" form=\"short\">f.</term>\n    <term name=\"issue\" form=\"short\">no.</term>\n    <term name=\"line\" form=\"short\">l.</term>\n    <term name=\"note\" form=\"short\">n.</term>\n    <term name=\"opus\" form=\"short\">op.</term>\n    <term name=\"page\" form=\"short\">\n      <single>p.</single>\n      <multiple>pp.</multiple>\n    </term>\n    <term name=\"paragraph\" form=\"short\">para.</term>\n    <term name=\"part\" form=\"short\">pt.</term>\n    <term name=\"section\" form=\"short\">sec.</term>\n    <term name=\"sub verbo\" form=\"short\">\n      <single>s.v.</single>\n      <multiple>s.vv.</multiple>\n    </term>\n    <term name=\"verse\" form=\"short\">\n      <single>v.</single>\n      <multiple>vv.</multiple>\n    </term>\n    <term name=\"volume\" form=\"short\">\n      <single>vol.</single>\n      <multiple>vols.</multiple>\n    </term>\n\n    <!-- SYMBOL LOCATOR FORMS -->\n    <term name=\"paragraph\" form=\"symbol\">\n      <single>¶</single>\n      <multiple>¶¶</multiple>\n    </term>\n    <term name=\"section\" form=\"symbol\">\n      <single>§</single>\n      <multiple>§§</multiple>\n    </term>\n\n    <!-- LONG ROLE FORMS -->\n    <term name=\"director\">\n      <single>director</single>\n      <multiple>directors</multiple>\n    </term>\n    <term name=\"editor\">\n      <single>editor</single>\n      <multiple>editors</multiple>\n    </term>\n    <term name=\"editorial-director\">\n      <single>editor</single>\n      <multiple>editors</multiple>\n    </term>\n    <term name=\"illustrator\">\n      <single>illustrator</single>\n      <multiple>illustrators</multiple>\n    </term>\n    <term name=\"translator\">\n      <single>translator</single>\n      <multiple>translators</multiple>\n    </term>\n    <term name=\"editortranslator\">\n      <single>editor &amp; translator</single>\n      <multiple>editors &amp; translators</multiple>\n    </term>\n\n    <!-- SHORT ROLE FORMS -->\n    <term name=\"director\" form=\"short\">\n      <single>dir.</single>\n      <multiple>dirs.</multiple>\n    </term>\n    <term name=\"editor\" form=\"short\">\n      <single>ed.</single>\n      <multiple>eds.</multiple>\n    </term>\n    <term name=\"editorial-director\" form=\"short\">\n      <single>ed.</single>\n      <multiple>eds.</multiple>\n    </term>\n    <term name=\"illustrator\" form=\"short\">\n      <single>ill.</single>\n      <multiple>ills.</multiple>\n    </term>\n    <term name=\"translator\" form=\"short\">\n      <single>tran.</single>\n      <multiple>trans.</multiple>\n    </term>\n    <term name=\"editortranslator\" form=\"short\">\n      <single>ed. &amp; tran.</single>\n      <multiple>eds. &amp; trans.</multiple>\n    </term>\n\n    <!-- VERB ROLE FORMS -->\n    <term name=\"director\" form=\"verb\">directed by</term>\n    <term name=\"editor\" form=\"verb\">edited by</term>\n    <term name=\"editorial-director\" form=\"verb\">edited by</term>\n    <term name=\"illustrator\" form=\"verb\">illustrated by</term>\n    <term name=\"interviewer\" form=\"verb\">interview by</term>\n    <term name=\"recipient\" form=\"verb\">to</term>\n    <term name=\"reviewed-author\" form=\"verb\">by</term>\n    <term name=\"translator\" form=\"verb\">translated by</term>\n    <term name=\"editortranslator\" form=\"verb\">edited &amp; translated by</term>\n\n    <!-- SHORT VERB ROLE FORMS -->\n    <term name=\"container-author\" form=\"verb-short\">by</term>\n    <term name=\"director\" form=\"verb-short\">dir.</term>\n    <term name=\"editor\" form=\"verb-short\">ed.</term>\n    <term name=\"editorial-director\" form=\"verb-short\">ed.</term>\n    <term name=\"illustrator\" form=\"verb-short\">illus.</term>\n    <term name=\"translator\" form=\"verb-short\">trans.</term>\n    <term name=\"editortranslator\" form=\"verb-short\">ed. &amp; trans.</term>\n\n    <!-- LONG MONTH FORMS -->\n    <term name=\"month-01\">January</term>\n    <term name=\"month-02\">February</term>\n    <term name=\"month-03\">March</term>\n    <term name=\"month-04\">April</term>\n    <term name=\"month-05\">May</term>\n    <term name=\"month-06\">June</term>\n    <term name=\"month-07\">July</term>\n    <term name=\"month-08\">August</term>\n    <term name=\"month-09\">September</term>\n    <term name=\"month-10\">October</term>\n    <term name=\"month-11\">November</term>\n    <term name=\"month-12\">December</term>\n\n    <!-- SHORT MONTH FORMS -->\n    <term name=\"month-01\" form=\"short\">Jan.</term>\n    <term name=\"month-02\" form=\"short\">Feb.</term>\n    <term name=\"month-03\" form=\"short\">Mar.</term>\n    <term name=\"month-04\" form=\"short\">Apr.</term>\n    <term name=\"month-05\" form=\"short\">May</term>\n    <term name=\"month-06\" form=\"short\">Jun.</term>\n    <term name=\"month-07\" form=\"short\">Jul.</term>\n    <term name=\"month-08\" form=\"short\">Aug.</term>\n    <term name=\"month-09\" form=\"short\">Sep.</term>\n    <term name=\"month-10\" form=\"short\">Oct.</term>\n    <term name=\"month-11\" form=\"short\">Nov.</term>\n    <term name=\"month-12\" form=\"short\">Dec.</term>\n\n    <!-- SEASONS -->\n    <term name=\"season-01\">Spring</term>\n    <term name=\"season-02\">Summer</term>\n    <term name=\"season-03\">Autumn</term>\n    <term name=\"season-04\">Winter</term>\n  </terms>\n</locale>\n",
    "contentType": "Application/x-bibtex",
    "items": [
        {
            "10.1145/3482632.3487482": {
                "id": "10.1145/3482632.3487482",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Luan",
                        "given": "Shaohong"
                    },
                    {
                        "family": "Wu",
                        "given": "Yunze"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            9,
                            24
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2021,
                            9,
                            24
                        ]
                    ]
                },
                "abstract": "As big data technology penetrates all aspects of our lives; more and more industries are beginning to apply big data technology. Its robust data analysis, resource acquisition, and predictive capabilities have brought tremendous business activities as a brand-new data management technology. This article analyzes the big data technology application in business activities and introduces the principles of two essential algorithms in big data technology, clustering algorithm, and principal component analysis. Moreover, it introduces applying the mean shift clustering and principal component analysis in clustering algorithm to analyze the commercial data. Finally, a comprehensive analysis of the advantages and disadvantages of big data technology is made.",
                "call-number": "10.1145/3482632.3487482",
                "collection-title": "ICISCAE 2021",
                "container-title": "2021 4th International Conference on Information Systems and Computer Aided Education",
                "DOI": "10.1145/3482632.3487482",
                "event-place": "Dalian, China",
                "ISBN": "9781450390255",
                "number-of-pages": "5",
                "page": "2614–2618",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Application of big data technology and clustering algorithm in business activities",
                "URL": "https://doi.org/10.1145/3482632.3487482"
            }
        },
        {
            "10.1145/3306500.3306552": {
                "id": "10.1145/3306500.3306552",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Lee",
                        "given": "Kuan-Yin"
                    },
                    {
                        "family": "Hsu",
                        "given": "Yin-Chiech"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            1,
                            10
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            1,
                            10
                        ]
                    ]
                },
                "abstract": "Loyalty programs lead to a natural split of a firm's customer base into members and nonmembers. To manage both groups effectively, it is essential to know how they concern about, such as services or promotions. This article, set in context of the second hypermarket density in Asia, examines the impact of satisfaction on store patronage and explores moderator roles of employee interaction and price sensitivity between members and nonmembers. Therefore, a survey was performed among 317 hypermarket members and nonmembers from top three settings in Taiwan. The study demonstrates that the satisfaction and store patronage behavior relationship of members stronger than nonmembers. And moderator of employee interaction and price sensitivity of members has stronger effect between satisfaction and store patronage than nonmembers. According to inconsistent relation between satisfaction and store patronage in past studies, the study extend existing theories of retention to incorporate contingency relationships, especially among members and nonmembers to manage retailer-both customer relationship better.",
                "call-number": "10.1145/3306500.3306552",
                "collection-title": "IC4E '19",
                "container-title": "Proceedings of the 10th International Conference on E-Education, E-Business, E-Management and E-Learning",
                "DOI": "10.1145/3306500.3306552",
                "event-place": "Tokyo, Japan",
                "ISBN": "9781450366021",
                "keyword": "shopping characteristics, satisfaction, loyalty program, store patronage",
                "number-of-pages": "5",
                "page": "363–367",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Big data for loyalty program management in hypermarket",
                "URL": "https://doi.org/10.1145/3306500.3306552"
            }
        },
        {
            "10.1145/2938503.2938517": {
                "id": "10.1145/2938503.2938517",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Cuzzocrea",
                        "given": "Alfredo"
                    },
                    {
                        "family": "Psaila",
                        "given": "Giuseppe"
                    },
                    {
                        "family": "Toccu",
                        "given": "Maurizio"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            7,
                            11
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2016,
                            7,
                            11
                        ]
                    ]
                },
                "abstract": "Mobile Social Media are gaining momentum in the broader context of Big Data Analytics, where the main issue is represented by the problem of extracting interesting and actionable knowledge from big data repositories. Mobile social media sources like Twitter and Instagram are indeed producing massive amounts of data (namely, posts) that represent a very rich source of knowledge for predictive analytics. In line with this emerging trend, this paper proposes an innovative approach for effectively and efficiently supporting big data analytics over geo-localized mobile social media, with particular emphasis with the context of modern tourist information systems. In this context, the innovative FollowMe suite, which implements the proposed methodology, is also described in details. We complement our analytical contribution with a real-life case study focusing on the EXPO 2015 event in Milan, Italy which clearly shows benefits and potentialities of our proposed big data analytics framework.",
                "call-number": "10.1145/2938503.2938517",
                "collection-title": "IDEAS '16",
                "container-title": "Proceedings of the 20th International Database Engineering & Applications Symposium",
                "DOI": "10.1145/2938503.2938517",
                "event-place": "Montreal, QC, Canada",
                "ISBN": "9781450341189",
                "keyword": "Mobile Social Media, Big Data Analytics, Big Data Frameworks",
                "number-of-pages": "8",
                "page": "62–69",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "An Innovative Framework for Effectively and Efficiently Supporting Big Data Analytics over Geo-Located Mobile Social Media",
                "URL": "https://doi.org/10.1145/2938503.2938517"
            }
        },
        {
            "10.1145/3203217.3205863": {
                "id": "10.1145/3203217.3205863",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Bartolini",
                        "given": "Andrea"
                    },
                    {
                        "family": "Borghesi",
                        "given": "Andrea"
                    },
                    {
                        "family": "Libri",
                        "given": "Antonio"
                    },
                    {
                        "family": "Beneventi",
                        "given": "Francesco"
                    },
                    {
                        "family": "Gregori",
                        "given": "Daniele"
                    },
                    {
                        "family": "Tinti",
                        "given": "Simone"
                    },
                    {
                        "family": "Gianfreda",
                        "given": "Cosimo"
                    },
                    {
                        "family": "Altoè",
                        "given": "Piero"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            5,
                            8
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            5,
                            8
                        ]
                    ]
                },
                "abstract": "On the race toward exascale supercomputing systems are facing important challenges which limit the efficiency of the system. Among all, power and energy consumption fueled by the end of Dennard's scaling start to show their impact on limiting supercomputers peak performance and cost effectiveness.In this paper we present and describe a new methodology based on a set of HW and SW extensions for fine-grain monitoring of power and aggregation of them for fast analysis and visualization. We propose a turn-key system which uses MQTT communication layer, NoSQL database, fine grain monitoring and in future AI technology to measure and control power and performance. This methodology is shown as an integrated feature of the D.A.V.I.D.E. supercomputing machine.",
                "call-number": "10.1145/3203217.3205863",
                "collection-title": "CF '18",
                "container-title": "Proceedings of the 15th ACM International Conference on Computing Frontiers",
                "DOI": "10.1145/3203217.3205863",
                "event-place": "Ischia, Italy",
                "ISBN": "9781450357616",
                "keyword": "big data, beaglebone black, high performance computing, AMESTER, fine-grain power and performance monitoring",
                "number-of-pages": "6",
                "page": "303–308",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "The D.A.V.I.D.E. big-data-powered fine-grain power and performance monitoring support",
                "URL": "https://doi.org/10.1145/3203217.3205863"
            }
        },
        {
            "10.1145/3363459": {
                "id": "10.1145/3363459",
                "type": "BOOK",
                "issued": {
                    "date-parts": [
                        [
                            2019
                        ]
                    ]
                },
                "abstract": "The advancements and availability of low-cost, low-energy sensors have improved energy and environmental sensing exponentially. Besides the millions of sensors used for monitoring, the improved accuracy of these sensors offer greater resolution for modeling 'what-if' scenarios in near real-time harnessing the vast computational power. Similarly, big data analysis has enabled city-scale modeling of energy and environmental impact using, among others, energy-efficient 'smaller' machine learning algorithms and/or physics-based modeling approaches. Coupled with interactive data visualization including Virtual Reality (VR), urban-scale energy and environmental systems modeling has become an exciting niche at the intersection of computer science and urban / architecture / mechanical engineering disciplines. The 1st International Urban Building Energy Sensing, Controls, Big Data Analysis, and Visualization (UrbSys) Workshop intends to capture recent exciting work by research experts, from U.S. universities and U.S. national laboratories, at this nexus that supports sustainable urban systems' design and engineering through state-of-the-art sensing, controls, modeling, and visualization.",
                "call-number": "10.1145/3363459",
                "container-title-short": "UrbSys'19",
                "event-place": "New York, NY, USA",
                "genre": "proceeding",
                "ISBN": "9781450370141",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Proceedings of the 1st ACM International Workshop on Urban Building Energy Sensing, Controls, Big Data Analysis, and Visualization"
            }
        },
        {
            "10.1145/3396452.3396465": {
                "id": "10.1145/3396452.3396465",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Xu",
                        "given": "Wei"
                    },
                    {
                        "family": "Chen",
                        "given": "Chongyang"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2020,
                            4,
                            1
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2020,
                            4,
                            1
                        ]
                    ]
                },
                "abstract": "Exposed in environment of big data, college students come easily into contact with massive data presentation. When crisis events occur, college students will be affected not only by crisis events but also by human psychological crisis. The greater psychological threat, faced by college students in the crisis environment, is the loss of sense of security. Through literature review, the hypotheses in crisis events are as followed: crisis events, government and media response, university coping measures, group coping behavior are the four main factors that affect college students' sense of security in crisis events. The outbreak of COVID-19 in Wuhan, Hubei affecting all the people, all colleges and universities across the country delayed the opening time. Among the affected universities, take the University of Electronic Science and Technology as an example, 600 samples were randomly selected to collect data. Through the exploratory factor analysis test, the influence hypotheses are verified. Through the structural equation model test, the four kinds of factors can explain the loss of college students' sense of security in the crisis, but show differences in explanatory power. Based on the elements of college students' sense of security, this paper puts forward an further explanation on the action path of the four factors on the public sense of security. According to the conclusion, we come to the conclusion that improving the coping ability of colleges and universities, enhancing the sense of crisis determination and the efficiency of control are the key to improve college students' sense of security and ensure the effectiveness of crisis management in colleges and universities.",
                "call-number": "10.1145/3396452.3396465",
                "collection-title": "ICBDE '20",
                "container-title": "Proceedings of the 2020 The 3rd International Conference on Big Data and Education",
                "DOI": "10.1145/3396452.3396465",
                "event-place": "London, United Kingdom",
                "ISBN": "9781450374989",
                "keyword": "college students' sense of security, big data, crisis events, government and media's response, coping measures of colleges, dealing with emergency in groups, structural equation model",
                "number-of-pages": "5",
                "page": "21–25",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Research on the Influencing Factors and Management Countermeasures of College Students' Sense of Security under the Environment of Big Data-an Empirical Analysis based on the Event of COVID-19",
                "URL": "https://doi.org/10.1145/3396452.3396465"
            }
        },
        {
            "10.1145/3497749": {
                "id": "10.1145/3497749",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Li",
                        "given": "Zixuan"
                    },
                    {
                        "family": "Li",
                        "given": "Hao"
                    },
                    {
                        "family": "Li",
                        "given": "Kenli"
                    },
                    {
                        "family": "Wu",
                        "given": "Fan"
                    },
                    {
                        "family": "Chen",
                        "given": "Lydia"
                    },
                    {
                        "family": "Li",
                        "given": "Keqin"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2022,
                            3,
                            25
                        ]
                    ]
                },
                "abstract": "Matrix factorization (MF) can extract the low-rank features and integrate the information of the data manifold distribution from high-dimensional data, which can consider the nonlinear neighborhood information. Thus, MF has drawn wide attention for low-rank analysis of sparse big data, e.g., Collaborative Filtering (CF) Recommender Systems, Social Networks, and Quality of Service. However, the following two problems exist: (1) huge computational overhead for the construction of the Graph Similarity Matrix (GSM) and (2) huge memory overhead for the intermediate GSM. Therefore, GSM-based MF, e.g., kernel MF, graph regularized MF, and so on, cannot be directly applied to the low-rank analysis of sparse big data on cloud and edge platforms. To solve this intractable problem for sparse big data analysis, we propose Locality Sensitive Hashing (LSH) aggregated MF (LSH-MF), which can solve the following problems: (1) The proposed probabilistic projection strategy of LSH-MF can avoid the construction of the GSM. Furthermore, LSH-MF can satisfy the requirement for the accurate projection of sparse big data. (2) To run LSH-MF for fine-grained parallelization and online learning on GPUs, we also propose CULSH-MF, which works on CUDA parallelization. Experimental results show that CULSH-MF can not only reduce the computational time and memory overhead but also obtain higher accuracy. Compared with deep learning models, CULSH-MF can not only save training time but also achieve the same accuracy performance.",
                "call-number": "10.1145/3497749",
                "collection-number": "37",
                "container-title": "ACM/IMS Trans. Data Sci.",
                "DOI": "10.1145/3497749",
                "ISSN": "2691-1922",
                "issue": "4",
                "keyword": "Graph Similarity Matrix (GSM), Locality Sensitive Hash (LSH), Matrix Factorization (MF), online learning for sparse big data, CUDA parallelization on gpu and multiple GPUs, Top-K nearest neighboors.",
                "number": "Article 37",
                "number-of-pages": "27",
                "page": "1–27",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "November 2021",
                "title": "Locality Sensitive Hash Aggregated Nonlinear Neighborhood Matrix Factorization for Online Sparse Big Data Analysis",
                "URL": "https://doi.org/10.1145/3497749",
                "volume": "2"
            }
        },
        {
            "10.1145/2766196.2766199": {
                "id": "10.1145/2766196.2766199",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Wang",
                        "given": "Fusheng"
                    },
                    {
                        "family": "Aji",
                        "given": "Ablimit"
                    },
                    {
                        "family": "Vo",
                        "given": "Hoang"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2015,
                            4,
                            22
                        ]
                    ]
                },
                "abstract": "Support of high performance queries on large volumes of spatial data has become increasingly important in many application domains, including geospatial problems in numerous disciplines, location based services, and emerging medical imaging applications. There are two major challenges for managing massive spatial data to support spatial queries: the explosion of spatial data, and the high computational complexity of spatial queries. Our goal is to develop a general framework to support high performance spatial queries and analytics for spatial big data on MapReduce and CPU-GPU hybrid platforms. In this paper, we introduce Hadoop-GIS -- a scalable and high performance spatial data warehousing system for running large scale spatial queries on Hadoop. Hadoop-GIS supports multiple types of spatial queries on MapReduce through skew-aware spatial partitioning, on-demand indexing, customizable spatial query engine RESQUE, implicit parallel spatial query execution on MapReduce, and effective methods for amending query results through handling boundary objects. To accelerate compute-intensive geometric operations, GPU based geometric computation algorithms are integrated into MapReduce pipelines. Our experiments have demonstrated that Hadoop-GIS is highly efficient and scalable, and outperforms parallel spatial DBMS for compute-intensive spatial queries.",
                "call-number": "10.1145/2766196.2766199",
                "container-title": "SIGSPATIAL Special",
                "DOI": "10.1145/2766196.2766199",
                "issue": "3",
                "number-of-pages": "8",
                "page": "11–18",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "November 2014",
                "title": "High performance spatial queries for spatial big data: from medical imaging to GIS",
                "URL": "https://doi.org/10.1145/2766196.2766199",
                "volume": "6"
            }
        },
        {
            "10.1145/3076113.3076115": {
                "id": "10.1145/3076113.3076115",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Karnagel",
                        "given": "Tomas"
                    },
                    {
                        "family": "Ben-Nun",
                        "given": "Tal"
                    },
                    {
                        "family": "Werner",
                        "given": "Matthias"
                    },
                    {
                        "family": "Habich",
                        "given": "Dirk"
                    },
                    {
                        "family": "Lehner",
                        "given": "Wolfgang"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            5,
                            14
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            5,
                            14
                        ]
                    ]
                },
                "abstract": "GPUs are increasingly adopted for large-scale database processing, where data accesses represent the major part of the computation. If the data accesses are irregular, like hash table accesses or random sampling, the GPU performance can suffer. Especially when scaling such accesses beyond 2GB of data, a performance decrease of an order of magnitude is encountered. This paper analyzes the source of the slowdown through extensive micro-benchmarking, attributing the root cause to the Translation Lookaside Buffer (TLB). Using the micro-benchmarks, the TLB hierarchy and structure are fully analyzed on two different GPU architectures, identifying never-before-published TLB sizes that can be used for efficient large-scale application tuning. Based on the gained knowledge, we propose a TLB-conscious approach to mitigate the slowdown for algorithms with irregular memory access. The proposed approach is applied to two fundamental database operations - random sampling and hash-based grouping - showing that the slowdown can be dramatically reduced, and resulting in a performance increase of up to 13×.",
                "call-number": "10.1145/3076113.3076115",
                "collection-number": "6",
                "collection-title": "DAMON '17",
                "container-title": "Proceedings of the 13th International Workshop on Data Management on New Hardware",
                "DOI": "10.1145/3076113.3076115",
                "event-place": "Chicago, Illinois",
                "ISBN": "9781450350259",
                "keyword": "virtual memory, random memory access, grouping, TLB, GPU",
                "number": "Article 6",
                "number-of-pages": "10",
                "page": "1–10",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Big data causing big (TLB) problems: taming random memory accesses on the GPU",
                "URL": "https://doi.org/10.1145/3076113.3076115"
            }
        },
        {
            "10.1145/2757384.2757390": {
                "id": "10.1145/2757384.2757390",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Yin",
                        "given": "Bo"
                    },
                    {
                        "family": "Shen",
                        "given": "Wenlong"
                    },
                    {
                        "family": "Cai",
                        "given": "Lin X."
                    },
                    {
                        "family": "Cheng",
                        "given": "Yu"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2015,
                            6,
                            21
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2015,
                            6,
                            21
                        ]
                    ]
                },
                "abstract": "Recent years have witnessed an explosive growth of mobile applications. Thanks to improved network connectivity, it becomes a promising enabling solution to offload computation-intensive applications to the resource abundant public cloud to further augment the capacity of resource-constrained devices. As mobile applications usually have QoS requirements, it is critical to provide low latency services to the mobile users while maintain low leasing cost of cloud resources. However, the resources offered by cloud vendors are usually charged based on a time quanta while the offloading demand for heavy-lifting computation may occur infrequently on mobile devices. This mismatch would demotivate users to resort to public cloud for computation offloading. In this paper, we design a computation offloading middleware which bridges the aforementioned gap between cloud vendors and mobile clients, providing offloading service to multiple users with low cost and delay. The proposed middleware has two key components: Task Scheduler and Instance Manager. The Task Scheduler dispatches the received offloading tasks to execute in the instances reserved by the Instance Manager. Based on the arrival pattern of offloading tasks, the Instance Manager dynamically changes the number of instances to ensure certain service grade of mobile users. Our proposed mechanisms are validated through numerical results. It is shown that a lower average delay can be achieved through proposed scheduling heuristic, and the number of reserved instances well adapts to the offloading demands.",
                "call-number": "10.1145/2757384.2757390",
                "collection-title": "Mobidata '15",
                "container-title": "Proceedings of the 2015 Workshop on Mobile Big Data",
                "DOI": "10.1145/2757384.2757390",
                "event-place": "Hangzhou, China",
                "ISBN": "9781450335249",
                "keyword": "computation offloading, mobile cloud computing",
                "number-of-pages": "5",
                "page": "31–35",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "A Mobile Cloud Computing Middleware for Low Latency Offloading of Big Data",
                "URL": "https://doi.org/10.1145/2757384.2757390"
            }
        },
        {
            "10.1145/2481528.2481537": {
                "id": "10.1145/2481528.2481537",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Stonebraker",
                        "given": "Michael"
                    },
                    {
                        "family": "Madden",
                        "given": "Sam"
                    },
                    {
                        "family": "Dubey",
                        "given": "Pradeep"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2013,
                            5,
                            1
                        ]
                    ]
                },
                "abstract": "Intel has moved to a collaboration model with universities consisting of \"Science and Technology Centers\" (ISTCs). These are located at a \"hub\" university with participation from other universities, contain embedded Intel personnel, and are focused on some research theme. Intel held a national competition for a 5th Science and Technology center in 2012 and selected a proposal from M.I.T. with a theme of \"Big Data\". This paper presents the big data vision of this technology center and the execution plan for the first few years.",
                "call-number": "10.1145/2481528.2481537",
                "container-title": "SIGMOD Rec.",
                "DOI": "10.1145/2481528.2481537",
                "ISSN": "0163-5808",
                "issue": "1",
                "number-of-pages": "6",
                "page": "44–49",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "March 2013",
                "title": "Intel \"big data\" science and technology center vision and execution plan",
                "URL": "https://doi.org/10.1145/2481528.2481537",
                "volume": "42"
            }
        },
        {
            "10.1145/3209415.3209427": {
                "id": "10.1145/3209415.3209427",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Androutsopoulou",
                        "given": "Aggeliki"
                    },
                    {
                        "family": "Charalabidis",
                        "given": "Yannis"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            4,
                            4
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            4,
                            4
                        ]
                    ]
                },
                "abstract": "Governments and policy makers are striving to respond to contemporary socio-economic challenges, however, often neglecting the human factor and the multidimensionality of policy implications. In this chapter, a framework for evidence based policy making is proposed, which integrates the usage of open big data coming from a multiplicity of sources with policy simulations. It encompasses the application of dynamic modelling methodologies and data mining techniques to extract knowledge from two types of data. On the one hand, objective data such as governmental and statistical data, are used to capture the interlinked policy domains and their underlying casual mechanisms. On the other hand, behavioural patterns and citizens' opinions are extracted from Web 2.0 sources, social media posts, polls and statistical surveys. To combine this multimodal information, our approach suggests a modelling methodology that bases on big data acquisition and processing for the identification of significant factors and counterintuitive interrelations between them, which can be applied in any policy domain. Then, to allow the practical application of the framework an ICT architecture is designed, with the aim to overcome challenges related with big data management and processing. Finally, validation of the approach for driving policy design and implementation in the future in diverse policy domains, is suggested.",
                "call-number": "10.1145/3209415.3209427",
                "collection-title": "ICEGOV '18",
                "container-title": "Proceedings of the 11th International Conference on Theory and Practice of Electronic Governance",
                "DOI": "10.1145/3209415.3209427",
                "event-place": "Galway, Ireland",
                "ISBN": "9781450354219",
                "keyword": "impact assessment, evidence based policy making, dynamic simulation, behavioural patterns, Big data, data mining, policy Modelling",
                "number-of-pages": "9",
                "page": "575–583",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "A framework for evidence based policy making combining big data, dynamic modelling and machine intelligence",
                "URL": "https://doi.org/10.1145/3209415.3209427"
            }
        },
        {
            "10.5555/2888619.2888700": {
                "id": "10.5555/2888619.2888700",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Dong",
                        "given": "Wen"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2015,
                            12,
                            6
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2015,
                            12,
                            6
                        ]
                    ]
                },
                "abstract": "In this paper, we develop a stochastic process tool to tell the stories behind big data with agent-based models. Specifically, we identify an agent-based model as a stochastic process that generates the big data, and make inferences by solving the agent-based model under the constraint of the data. We hope to use this tool to create a bridge between those who have access to big data and those who use agent-based simulators to convey their insight about these data.",
                "call-number": "10.5555/2888619.2888700",
                "collection-title": "WSC '15",
                "container-title": "Proceedings of the 2015 Winter Simulation Conference",
                "event-place": "Huntington Beach, California",
                "ISBN": "9781467397414",
                "number-of-pages": "12",
                "page": "713–724",
                "publisher": "IEEE Press",
                "title": "Weaving multi-agent modeling and big data for stochastic process inference"
            }
        },
        {
            "10.1145/1595808.1595830": {
                "id": "10.1145/1595808.1595830",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Bachmann",
                        "given": "Adrian"
                    },
                    {
                        "family": "Bernstein",
                        "given": "Abraham"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2009,
                            8,
                            24
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2009,
                            8,
                            24
                        ]
                    ]
                },
                "abstract": "Software process data gathered from bug tracking databases and version control system log files are a very valuable source to analyze the evolution and history of a project or predict its future. These data are used for instance to predict defects, gather insight into a project's life-cycle, and additional tasks. In this paper we survey five open source projects and one closed source project in order to provide a deeper insight into the quality and characteristics of these often-used process data. Specifically, we first define quality and characteristics measures, which allow us to compare the quality and characteristics of the data gathered for different projects. We then compute the measures and discuss the issues arising from these observation. We show that there are vast differences between the projects, particularly with respect to the quality in the link rate between bugs and commits.",
                "call-number": "10.1145/1595808.1595830",
                "collection-title": "IWPSE-Evol '09",
                "container-title": "Proceedings of the joint international and annual ERCIM workshops on Principles of software evolution (IWPSE) and software evolution (Evol) workshops",
                "DOI": "10.1145/1595808.1595830",
                "event-place": "Amsterdam, The Netherlands",
                "ISBN": "9781605586786",
                "keyword": "open source, data quality, closed source, data characteristics, case study, bug tracker, version control system",
                "number-of-pages": "10",
                "page": "119–128",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Software process data quality and characteristics: a historical view on open and closed source projects",
                "URL": "https://doi.org/10.1145/1595808.1595830"
            }
        },
        {
            "10.1145/3510858.3511373": {
                "id": "10.1145/3510858.3511373",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Zhan",
                        "given": "Shaohui"
                    },
                    {
                        "family": "Tang",
                        "given": "Keqian"
                    },
                    {
                        "family": "Chang",
                        "given": "Kaixuan"
                    },
                    {
                        "family": "Yuan",
                        "given": "Liang"
                    },
                    {
                        "family": "Liu",
                        "given": "Shuo"
                    },
                    {
                        "family": "Li",
                        "given": "Zhaoming"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            12,
                            18
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2021,
                            12,
                            18
                        ]
                    ]
                },
                "abstract": "With the development of Internet finance industry, there are more and more fraud means in credit business. How to effectively prevent credit fraud and reduce credit risk has become a key research topic for financial institutions. Based on the big data of electric power, the evaluation index of enterprise production and operation is established, and the real and objective enterprise production and operation situation is provided for financial units according to the scoring standard. This can assist the decision-making financial units to evaluate the risk of enterprises in the pre loan link, effectively reduce the financing risk of enterprises, and avoid bad debts and non-performing assets.",
                "call-number": "10.1145/3510858.3511373",
                "collection-title": "ICASIT 2021",
                "container-title": "2021 International Conference on Aviation Safety and Information Technology",
                "DOI": "10.1145/3510858.3511373",
                "event-place": "Changsha, China",
                "ISBN": "9781450390422",
                "number-of-pages": "7",
                "page": "735–741",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Credit Anti Fraud Identification Method Based on Power Big Data",
                "URL": "https://doi.org/10.1145/3510858.3511373"
            }
        },
        {
            "10.1145/3327962.3331455": {
                "id": "10.1145/3327962.3331455",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Chow",
                        "given": "Sherman S. M."
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            7,
                            2
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            7,
                            2
                        ]
                    ]
                },
                "abstract": "Advances in cryptography such as secure multiparty computation (SMC) and fully-/somewhat-homomorphic encryption (FHE/SHE) have already provided a generic solution to the problem of processing encrypted data; however, they are still not that efficient if one directly applies them for big data analytics.Many cryptographers have recently designed specialized privacy-preserving frameworks for neural networks. While promising, they are still not entirely satisfactory. Gazelle (Usenix Security 2018) supports inference but not training. SecureNN (PoPETS 2019), with the help of non-colluding servers, is still orders of magnitudes slower than plaintext training/inferencing.To narrow the gap between theory and practice, we put forward a new paradigm for privacy-preserving big data analytics which leverages both trusted processor such as Intel SGX (Software Guard Extensions) and (untrusted) GPU (Graphics Processing Unit). Note that SGX is not a silver bullet in this scenario. In general, SGX is subject to a memory constraint which can be easily exceeded by a single layer of the (evergrowing) neural networks. Relying on the generic solution such as paging mechanism is, again, inefficient. GPU is an ideal platform for deep learning, yet, we do not want to assume it is trusted. We thus still need cryptographic techniques.In this keynote, we will briefly survey the research landscape of privacy-preserving machine learning, point out the obstacles brought by seemingly slight changes of requirements (e.g., a single query from different data sources, multiple model owners, outsourcing a trained model to an untrusted cloud), and highlight a number of settings which aids in ensuring privacy without heavyweight cryptography. We will also discuss two notable recent works, Graviton (OSDI 2018) and Slalom (ICLR 2019), and our ongoing research.",
                "call-number": "10.1145/3327962.3331455",
                "collection-title": "SCC '19",
                "container-title": "Proceedings of the Seventh International Workshop on Security in Cloud Computing",
                "DOI": "10.1145/3327962.3331455",
                "event-place": "Auckland, New Zealand",
                "ISBN": "9781450367882",
                "keyword": "neural networks, applied cryptography, homomorphic encryption",
                "number-of-pages": "1",
                "page": "1",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Can We Securely Outsource Big Data Analytics with Lightweight Cryptography?",
                "URL": "https://doi.org/10.1145/3327962.3331455"
            }
        },
        {
            "10.1145/2808719.2816981": {
                "id": "10.1145/2808719.2816981",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Deng",
                        "given": "Xin"
                    },
                    {
                        "family": "Wu",
                        "given": "Donghui"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2015,
                            9,
                            9
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2015,
                            9,
                            9
                        ]
                    ]
                },
                "abstract": "In 10 years, one third of the population in developed countries will be 60-years or older. 90% of seniors want to age in their own homes, not a facility. Families want to know that their aging parents are healthy and safe. It is extremely challenging for seniors with chronic conditions to manage their day-to-day medical needs and prevent medical emergencies, e.g. stroke, fall, etc. With the rapid growth of Internet of Things, increasing popularity of wearable medical devices or monitor devices, and other sensors at home, the opportunity for senior health management and prevention of medical emergency at home has never been better with big data platform, integration of massive and diverse data sources, e.g. medical charts from doctor's office, prescription information from pharmacies, claims data from insurance companies, more importantly, real-time data stream from Internet-of-things, wearable devices, and other vital and sensor data at home; and even more critically, real-time alerts of various risks from predictive analytics connected with providers, care givers and family members. In this paper, we will analyze the scope of the problem, and present current status and challenges in this area. Secondly we will propose a prototype schema to address this problem through Internet of things and real-time big data predictive analytics. Finally we will discuss some technical and non-technical challenges observed.",
                "call-number": "10.1145/2808719.2816981",
                "collection-title": "BCB '15",
                "container-title": "Proceedings of the 6th ACM Conference on Bioinformatics, Computational Biology and Health Informatics",
                "DOI": "10.1145/2808719.2816981",
                "event-place": "Atlanta, Georgia",
                "ISBN": "9781450338530",
                "keyword": "real-time data collection, predictive analytics, big data platform, health informatics, real-time alerts, data integration",
                "number-of-pages": "1",
                "page": "674",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Senior health management through internet of things and real-time big data analytics",
                "URL": "https://doi.org/10.1145/2808719.2816981"
            }
        },
        {
            "10.1145/3488377": {
                "id": "10.1145/3488377",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Zhang",
                        "given": "Jiaru"
                    },
                    {
                        "family": "Ma",
                        "given": "Ruhui"
                    },
                    {
                        "family": "Song",
                        "given": "Tao"
                    },
                    {
                        "family": "Hua",
                        "given": "Yang"
                    },
                    {
                        "family": "Xue",
                        "given": "Zhengui"
                    },
                    {
                        "family": "Guan",
                        "given": "Chenyang"
                    },
                    {
                        "family": "Guan",
                        "given": "Haibing"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2022,
                            2,
                            3
                        ]
                    ]
                },
                "abstract": "Approximate nearest neighbor search is a classical problem in data science, which is widely applied in many fields. With the rapid growth of data in the real world, it becomes more and more important to speed up the nearest neighbor search process. Satellite System Graph (SSG) is one of the state-of-the-art methods to solve the problem. However, with the further increase of the data scale of problems, SSG still needs a considerable amount of time to finish the search due to the limitation of step length and start point locations. To solve the problem, we propose Hierarchical Satellite System Graph (HSSG) and present its index algorithm and search algorithm. The index process can be distributed deployed due to the good parallelism of our designed hierarchical structure. The theoretical analysis reveals that HSSG decreases the search steps and reduces the computational cost and reduces the search time by searching on the hierarchical structure with a similar indexing time compared with SSG, hence reaches a better search efficiency. The experiments on multiple datasets present that HSSG reduces the distance computations, accelerates the search process, and increases the search precision in the real tasks, especially under the tasks with large scale and crowded distributions, which presents a good application prospect of HSSG.",
                "call-number": "10.1145/3488377",
                "collection-number": "32",
                "container-title": "ACM/IMS Trans. Data Sci.",
                "DOI": "10.1145/3488377",
                "ISSN": "2691-1922",
                "issue": "4",
                "keyword": "hierarchical structure, big data, approximate nearest neighbor search, Nearest neighbor search, data science",
                "number": "Article 32",
                "number-of-pages": "15",
                "page": "1–15",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "November 2021",
                "title": "Hierarchical Satellite System Graph for Approximate Nearest Neighbor Search on Big Data",
                "URL": "https://doi.org/10.1145/3488377",
                "volume": "2"
            }
        },
        {
            "10.1145/2538542.2538565": {
                "id": "10.1145/2538542.2538565",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Brim",
                        "given": "Michael J."
                    },
                    {
                        "family": "Dillow",
                        "given": "David A."
                    },
                    {
                        "family": "Oral",
                        "given": "Sarp"
                    },
                    {
                        "family": "Settlemyer",
                        "given": "Bradley W."
                    },
                    {
                        "family": "Wang",
                        "given": "Feiyi"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2013,
                            11,
                            17
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2013,
                            11,
                            17
                        ]
                    ]
                },
                "abstract": "This paper presents our design for an asynchronous object storage system intended for use in scientific and commercial big data workloads. Use cases from the target workload domains are used to motivate the key abstractions used in the application programming interface (API). The architecture of the Scalable Object Store (SOS), a prototype object storage system that supports the API's facilities, is presented. The SOS serves as a vehicle for future research into scalable and resilient big data object storage. We briefly review our research into providing efficient storage servers capable of providing quality of service (QoS) contracts relevant for big data use cases.",
                "call-number": "10.1145/2538542.2538565",
                "collection-title": "PDSW '13",
                "container-title": "Proceedings of the 8th Parallel Data Storage Workshop",
                "DOI": "10.1145/2538542.2538565",
                "event-place": "Denver, Colorado",
                "ISBN": "9781450325059",
                "keyword": "HPC storage, object storage, cloud storage, storage QoS",
                "number-of-pages": "7",
                "page": "7–13",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Asynchronous object storage with QoS for scientific and commercial big data",
                "URL": "https://doi.org/10.1145/2538542.2538565"
            }
        },
        {
            "10.1145/3265639.3265680": {
                "id": "10.1145/3265639.3265680",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Guo",
                        "given": "Aizhang"
                    },
                    {
                        "family": "Liu",
                        "given": "Xiuyuan"
                    },
                    {
                        "family": "Sun",
                        "given": "Tao"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            8,
                            11
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            8,
                            11
                        ]
                    ]
                },
                "abstract": "At present, the modern manufacturing and management concepts such as digitalization, networking and intellectualization have been popularized in the industry, and the degree of industrial automation and information has been improved unprecedentedly. Industrial products are everywhere in the world. They are involved in design, manufacture, operation, maintenance and recycling. The whole life cycle involves huge amounts of data. Improving data quality is very important for data mining and data analysis. To solve the problem of data inconsistency is a very important part of improving data quality.",
                "call-number": "10.1145/3265639.3265680",
                "collection-title": "ICRCA '18",
                "container-title": "Proceedings of the 3rd International Conference on Robotics, Control and Automation",
                "DOI": "10.1145/3265639.3265680",
                "event-place": "Chengdu, China",
                "ISBN": "9781450365307",
                "keyword": "Data Cleaning, Data Quality, Data Inconsistency",
                "number-of-pages": "4",
                "page": "245–248",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Research on Key Problems of Data Quality in Large Industrial Data Environment",
                "URL": "https://doi.org/10.1145/3265639.3265680"
            }
        },
        {
            "10.1145/3377672.3378052": {
                "id": "10.1145/3377672.3378052",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Quanli",
                        "given": "Wang"
                    },
                    {
                        "family": "Chu-jian",
                        "given": "Guo"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            12,
                            8
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            12,
                            8
                        ]
                    ]
                },
                "abstract": "The arrival of the era of big data has had a profound impact on the education system. As far as local university entrepreneurship education is concerned, in the context of big data, the evaluation model can be better applied to analyze the basic features of entrepreneurship education. Based on the profound influence of big data on entrepreneurship education, this paper analyzes the combination of entrepreneurship education and big data with the sample of entrepreneurship education in local colleges and universities, and analyzes the construction of scientific and reasonable evaluation system of entrepreneurship education.",
                "call-number": "10.1145/3377672.3378052",
                "collection-title": "AMME 2019",
                "container-title": "Proceedings of the 2019 Annual Meeting on Management Engineering",
                "DOI": "10.1145/3377672.3378052",
                "event-place": "Kuala Lumpur, Malaysia",
                "ISBN": "9781450362481",
                "keyword": "Evaluation of entrepreneurship education, Big data, Local universities",
                "number-of-pages": "5",
                "page": "169–173",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Research on Evaluation of Innovation and Entrepreneurship Education in Local Universities under the Background of Big Data",
                "URL": "https://doi.org/10.1145/3377672.3378052"
            }
        },
        {
            "10.1145/1839379.1839396": {
                "id": "10.1145/1839379.1839396",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Pham Thi",
                        "given": "Thanh Thoa"
                    },
                    {
                        "family": "Helfert",
                        "given": "Markus"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2010,
                            6,
                            17
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2010,
                            6,
                            17
                        ]
                    ]
                },
                "abstract": "Rules based approaches for data quality solutions often use business rules or integrity rules for data monitoring purpose. Integrity rules are constraints on data derived from business rules into a formal form in order to allow computerization. One of challenges of these approaches is rules discovering, which is usually manually made by business experts or system analysts based on experiences. In this paper, we present our rule-based approach for data quality analyzing, in which we discuss a comprehensive method for discovering dynamic integrity rules.",
                "call-number": "10.1145/1839379.1839396",
                "collection-title": "CompSysTech '10",
                "container-title": "Proceedings of the 11th International Conference on Computer Systems and Technologies and Workshop for PhD Students in Computing on International Conference on Computer Systems and Technologies",
                "DOI": "10.1145/1839379.1839396",
                "event-place": "Sofia, Bulgaria",
                "ISBN": "9781450302432",
                "keyword": "data quality analyzing, business rules, integrity rules, data quality",
                "number-of-pages": "6",
                "page": "89–94",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Discovering dynamic integrity rules with a rules-based tool for data quality analyzing",
                "URL": "https://doi.org/10.1145/1839379.1839396"
            }
        },
        {
            "10.5555/2591305.2591323": {
                "id": "10.5555/2591305.2591323",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Birke",
                        "given": "Robert"
                    },
                    {
                        "family": "Björkqvist",
                        "given": "Mathias"
                    },
                    {
                        "family": "Chen",
                        "given": "Lydia Y."
                    },
                    {
                        "family": "Smirni",
                        "given": "Evgenia"
                    },
                    {
                        "family": "Engbersen",
                        "given": "Ton"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2014,
                            2,
                            17
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2014,
                            2,
                            17
                        ]
                    ]
                },
                "abstract": "Virtualization is the ubiquitous way to provide computation and storage services to datacenter end-users. Guaranteeing sufficient data storage and efficient data access is central to all datacenter operations, yet little is known of the effects of virtualization on storage workloads. In this study, we collect and analyze field data from production datacenters that operate within the private cloud paradigm, during a period of three years. The datacenters of our study consist of 8,000 physical boxes, hosting over 90,000 VMs, which in turn use over 22 PB of storage. Storage data is analyzed from the perspectives of volume, velocity, and variety of storage demands on virtual machines and of their dependency on other resources. In addition to the growth rate and churn rate of allocated and used storage volume, the trace data illustrates the impact of virtualization and consolidation on the velocity of IO reads and writes, including IO deduplication ratios and peak load analysis of co-located VMs. We focus on a variety of applications which are roughly classified as app, web, database, file, mail, and print, and correlate their storage and IO demands with CPU, memory, and network usage. This study provides critical storage workload characterization by showing usage trends and how application types create storage traffic in large datacenters.",
                "call-number": "10.5555/2591305.2591323",
                "collection-title": "FAST'14",
                "container-title": "Proceedings of the 12th USENIX conference on File and Storage Technologies",
                "event-place": "Santa Clara, CA",
                "ISBN": "9781931971089",
                "number-of-pages": "13",
                "page": "177–189",
                "publisher": "USENIX Association",
                "publisher-place": "USA",
                "title": "(Big)data in a virtualized world: volume, velocity, and variety in cloud datacenters"
            }
        },
        {
            "10.1145/3092944": {
                "id": "10.1145/3092944",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Kulkarni",
                        "given": "Amey"
                    },
                    {
                        "family": "Shea",
                        "given": "Colin"
                    },
                    {
                        "family": "Abtahi",
                        "given": "Tahmid"
                    },
                    {
                        "family": "Homayoun",
                        "given": "Houman"
                    },
                    {
                        "family": "Mohsenin",
                        "given": "Tinoosh"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            12,
                            6
                        ]
                    ]
                },
                "abstract": "Big data processing on hardware gained immense interest among the hardware research community to take advantage of fast processing and reconfigurability. Though the computation latency can be reduced using hardware, big data processing cost is dominated by data transfers. In this article, we propose a low overhead framework based on compressive sensing (CS) to reduce data transfers up to 67% without affecting signal quality. CS has two important kernels: “sensing” and “reconstruction.” In this article, we focus on CS reconstruction is using orthogonal matching pursuit (OMP) algorithm. We implement the OMP CS reconstruction algorithm on a domain-specific PENC many-core platform and a low-power Jetson TK1 platform consisting of an ARM CPU and a K1 GPU. Detailed performance analysis of OMP algorithm on each platform suggests that the PENC many-core platform has 15× and 18× less energy consumption and 16× and 8× faster reconstruction time as compared to the low-power ARM CPU and K1 GPU, respectively. Furthermore, we implement the proposed CS-based framework on heterogeneous architecture, in which the PENC many-core architecture is used as an “accelerator” and processing is performed on the ARM CPU platform. For demonstration, we integrate the proposed CS-based framework with a hadoop MapReduce platform for a face detection application. The results show that the proposed CS-based framework with the PENC many-core as an accelerator achieves a 26.15% data storage/transfer reduction, with an execution time and energy consumption overhead of 3.7% and 0.002%, respectively, for 5,000 image transfers. Compared to the CS-based framework implementation on the low-power Jetson TK1 ARM CPU+GPU platform, the PENC many-core implementation is 2.3× faster for the image reconstruction part, while achieving 29% higher performance and 34% better energy efficiency for the complete face detection application on the Hadoop MapReduce platform.",
                "call-number": "10.1145/3092944",
                "collection-number": "25",
                "container-title": "ACM Trans. Embed. Comput. Syst.",
                "DOI": "10.1145/3092944",
                "ISSN": "1539-9087",
                "issue": "1",
                "keyword": "Compressive sensing, heterogeneous architecture, many-core",
                "number": "Article 25",
                "number-of-pages": "25",
                "page": "1–25",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "January 2018",
                "title": "Low Overhead CS-Based Heterogeneous Framework for Big Data Acceleration",
                "URL": "https://doi.org/10.1145/3092944",
                "volume": "17"
            }
        },
        {
            "10.1145/2980258.2980319": {
                "id": "10.1145/2980258.2980319",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Kamaruddin",
                        "given": "Sk."
                    },
                    {
                        "family": "Ravi",
                        "given": "Vadlamani"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            8,
                            25
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2016,
                            8,
                            25
                        ]
                    ]
                },
                "abstract": "Banking and financial industries are facing severe challenges in the form of fraudulent transactions. Credit card fraud is one example of them. In order to detect credit card fraud, we employed one-class classification approach in big data paradigm. We implemented a hybrid architecture of Particle Swarm Optimization and Auto-Associative Neural Network for one-class classification in Spark computational framework. In this paper, we implemented parallelization of the auto-associative neural network in the hybrid architecture.",
                "call-number": "10.1145/2980258.2980319",
                "collection-number": "33",
                "collection-title": "ICIA-16",
                "container-title": "Proceedings of the International Conference on Informatics and Analytics",
                "DOI": "10.1145/2980258.2980319",
                "event-place": "Pondicherry, India",
                "ISBN": "9781450347563",
                "keyword": "Auto-encoder, Single class classification, Auto-associative neural network, Particle swarm optimization, One-class classification",
                "number": "Article 33",
                "number-of-pages": "8",
                "page": "1–8",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Credit Card Fraud Detection using Big Data Analytics: Use of PSOAANN based One-Class Classification",
                "URL": "https://doi.org/10.1145/2980258.2980319"
            }
        },
        {
            "10.1145/3310205.3310211": {
                "id": "10.1145/3310205.3310211",
                "type": "CHAPTER",
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            7,
                            9
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            7,
                            9
                        ]
                    ]
                },
                "abstract": "Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.",
                "call-number": "10.1145/3310205.3310211",
                "container-title": "Data Cleaning",
                "DOI": "10.1145/3310205.3310211",
                "ISBN": "9781450371520",
                "page": "",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Data quality rule definition and discovery",
                "URL": "https://doi.org/10.1145/3310205.3310211"
            }
        },
        {
            "10.1145/3331453.3360976": {
                "id": "10.1145/3331453.3360976",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Zhou",
                        "given": "Guixian"
                    },
                    {
                        "family": "Chen",
                        "given": "Kaijian"
                    },
                    {
                        "family": "Tu",
                        "given": "Jingmei"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            10,
                            22
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            10,
                            22
                        ]
                    ]
                },
                "abstract": "Government affairs open service platform to integrate distributed and heterogeneous system of information resources, eliminate the \"information island\" phenomenon, through the open service platform, service background, e-government service management platform, monitoring platform four subsystems to achieve between different institutions, different application system and database based on the different transmission protocol of data exchange, information sharing and business collaboration, thus to provide good data to support the government and the enterprise information construction environment.",
                "call-number": "10.1145/3331453.3360976",
                "collection-number": "25",
                "collection-title": "CSAE 2019",
                "container-title": "Proceedings of the 3rd International Conference on Computer Science and Application Engineering",
                "DOI": "10.1145/3331453.3360976",
                "event-place": "Sanya, China",
                "ISBN": "9781450362948",
                "keyword": "E-government service, Open intelligent platform, Big data",
                "number": "Article 25",
                "number-of-pages": "5",
                "page": "1–5",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Research on Big Data Open Intelligent Platform of Guizhou Province E-government Service",
                "URL": "https://doi.org/10.1145/3331453.3360976"
            }
        },
        {
            "10.1145/2463676.2465290": {
                "id": "10.1145/2463676.2465290",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Mishne",
                        "given": "Gilad"
                    },
                    {
                        "family": "Dalton",
                        "given": "Jeff"
                    },
                    {
                        "family": "Li",
                        "given": "Zhenghua"
                    },
                    {
                        "family": "Sharma",
                        "given": "Aneesh"
                    },
                    {
                        "family": "Lin",
                        "given": "Jimmy"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2013,
                            6,
                            22
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2013,
                            6,
                            22
                        ]
                    ]
                },
                "abstract": "We present the architecture behind Twitter's real-time related query suggestion and spelling correction service. Although these tasks have received much attention in the web search literature, the Twitter context introduces a real-time \"twist\": after significant breaking news events, we aim to provide relevant results within minutes. This paper provides a case study illustrating the challenges of real-time data processing in the era of \"big data\". We tell the story of how our system was built twice: our first implementation was built on a typical Hadoop-based analytics stack, but was later replaced because it did not meet the latency requirements necessary to generate meaningful real-time results. The second implementation, which is the system deployed in production today, is a custom in-memory processing engine specifically designed for the task. This experience taught us that the current typical usage of Hadoop as a \"big data\" platform, while great for experimentation, is not well suited to low-latency processing, and points the way to future work on data analytics platforms that can handle \"big\" as well as \"fast\" data.",
                "call-number": "10.1145/2463676.2465290",
                "collection-title": "SIGMOD '13",
                "container-title": "Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data",
                "DOI": "10.1145/2463676.2465290",
                "event-place": "New York, New York, USA",
                "ISBN": "9781450320375",
                "keyword": "hadoop, log analysis, mapreduce",
                "number-of-pages": "12",
                "page": "1147–1158",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Fast data in the era of big data: Twitter's real-time related query suggestion architecture",
                "URL": "https://doi.org/10.1145/2463676.2465290"
            }
        },
        {
            "10.1145/3549843.3549851": {
                "id": "10.1145/3549843.3549851",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Yang",
                        "given": "Yun"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2022,
                            6,
                            25
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2022,
                            6,
                            25
                        ]
                    ]
                },
                "abstract": "Base on literature research, this study defined the concepts of learner portrait and academic prediction and constructed the label system of learner portrait. By applying portrait technology to realize learning situation analysis, it proposed learning situation analysis based on five dimensions of portrait, to grasp learners' learning situation information. Base on the results of learning situation analysis of five dimensions, association rules, sequence analysis model and other technical means were used to construct four-dimensional academic prediction, and provide different academic prediction strategies according to learning situation analysis. Based on the design of academic prediction mechanism based on big data of learning situation, a learning situation analysis system with Hadoop as the core was designed, and an implementation scheme of data mining parallel algorithm processing platform was proposed to mine more valuable data information and recommend more scientific, reasonable and useful learning schemes for learners. It also helped teachers to selectively provide suitable personalized teaching for individual and group learners of different types of learners",
                "call-number": "10.1145/3549843.3549851",
                "collection-title": "ICEBT '22",
                "container-title": "Proceedings of the 2022 6th International Conference on E-Education, E-Business and E-Technology",
                "DOI": "10.1145/3549843.3549851",
                "event-place": "Beijing, China",
                "ISBN": "9781450397216",
                "keyword": "learning Situation Analysis, Education Big Data, Academic Prediction",
                "number-of-pages": "7",
                "page": "52–58",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Research on the Learning Situation Analysis and Academic Prediction based on Education Big Data",
                "URL": "https://doi.org/10.1145/3549843.3549851"
            }
        },
        {
            "10.1145/3368756.3369046": {
                "id": "10.1145/3368756.3369046",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Kaderi",
                        "given": "Farah Al"
                    },
                    {
                        "family": "Koulali",
                        "given": "Rim"
                    },
                    {
                        "family": "Rida",
                        "given": "Mohamed"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            10,
                            2
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            10,
                            2
                        ]
                    ]
                },
                "abstract": "Global trade is growing steadily and the management of the transport and logistics fields is facing many challenges day after day. Container shipping is one of those areas whose management has become a crucial task because of its major role in ensuring the quality of freight transport service. In this paper, we present and describe a real-time monitoring and management system dedicated to the automatization of the maritime terminal containers management. Our proposed solution is based on using both Internet of Things (IoT) and Big Data technologies in order to build a system that provides advanced features to cover different aspects of container logistics management. As the implementation of the proposed system implies using various tools and technologies, we conducted a comparative study on several technologies and tools related to IoT and Big Data in order to choose the appropriate ones. We also adopted a multi-layer architecture for the proposed system and through this paper, we will present the scope, role and tools set for every tier.",
                "call-number": "10.1145/3368756.3369046",
                "collection-number": "65",
                "collection-title": "SCA '19",
                "container-title": "Proceedings of the 4th International Conference on Smart City Applications",
                "DOI": "10.1145/3368756.3369046",
                "event-place": "Casablanca, Morocco",
                "ISBN": "9781450362894",
                "keyword": "Apache hadoop, big data, internet of things, maritime container terminal, HDFS, MQTT, Apache spark, Arduino Uno, multi-layer architecture, MapReduce, RFID, RaspBerry pi 3 b+",
                "number": "Article 65",
                "number-of-pages": "6",
                "page": "1–6",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Automated management of maritime container terminals using internet of things and big data technologies",
                "URL": "https://doi.org/10.1145/3368756.3369046"
            }
        },
        {
            "10.1145/3510249.3510262": {
                "id": "10.1145/3510249.3510262",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Eltweri",
                        "given": "Ahmed"
                    },
                    {
                        "family": "Faccia",
                        "given": "Alessio"
                    },
                    {
                        "family": "KHASSAWNEH",
                        "given": "OSAMA"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            12,
                            17
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2021,
                            12,
                            17
                        ]
                    ]
                },
                "abstract": "Big Data within the world of finance is about large, complex, and diverse (unstructured and structured) data sets that may be employed in providing solutions across the world for business challenges within banking companies and financial services that are long-standing. Big Data helps in enhancing the significance of FinTech in offering numerous financial services for users, facilitating the distribution of new payment, financing, and exchange services within an increasingly large proportion of the population. Technological developments have changed our lives profoundly, particularly over the last two decades. All fields of the economy have been changed, so it is hardly surprising that the world of real estate has been impacted by technological advances. Indeed, great technological strides have been made within the financial world that has allowed both professionals and amateurs to employ technical, innovative solutions that may lead to improved performance both within personally used commercial activity and for the purposes of commerce. Various applications of Big Data have been very beneficial for the world of finance because of new innovations in various technologies. The focus of this research has been upon the undertaking of a systematic analysis related to the technologies that are considered most important and that currently allow great progress to be made in fraud detection and risk management within the real estate industry by analysing the data collected. The particular focus of the research has been to highlight 3 particular interest areas, namely: i) FinTech and Big Data, ii) risk management, iii) fraud detection. A recent case study related to scandals that have arisen in the FinTech industry has provided further help in support of the research hypotheses and the conclusions are drawn.",
                "call-number": "10.1145/3510249.3510262",
                "collection-title": "EBEE 2021",
                "container-title": "2021 3rd International Conference on E-Business and E-commerce Engineering",
                "DOI": "10.1145/3510249.3510262",
                "event-place": "Sanya, China",
                "ISBN": "9781450387392",
                "keyword": "Big Data, Risk management, PropTech, Real Estate Industry, Fraud Detection, Fintech",
                "number-of-pages": "7",
                "page": "67–73",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Applications of Big Data within Finance: Fraud Detection and Risk Management within the Real Estate Industry",
                "URL": "https://doi.org/10.1145/3510249.3510262"
            }
        },
        {
            "10.1145/3482632.3483101": {
                "id": "10.1145/3482632.3483101",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Qi",
                        "given": "Xiaoying"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            9,
                            24
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2021,
                            9,
                            24
                        ]
                    ]
                },
                "abstract": "Under the background of the current big data era, it is particularly important to establish and apply the big data guarantee mechanism for quantitative research to carry out psychological education, Based on the results of Freshmen's psychological survey in the era of big data, this paper discusses the theory and practice methods of psychological education in the era of big data, aiming to put forward constructive suggestions for the development of psychological education in Colleges and universities, give full play to the full participation of the whole staff in student management, the maintenance and improvement of students' psychological quality, and strengthen the positive work of the whole staff in major crisis events In order to promote the healthy development of psychological education in Colleges and universities and the happy growth of students, it is necessary to improve the enthusiasm, initiative and sense of acquisition of all staff in Colleges and universities.",
                "call-number": "10.1145/3482632.3483101",
                "collection-title": "ICISCAE 2021",
                "container-title": "2021 4th International Conference on Information Systems and Computer Aided Education",
                "DOI": "10.1145/3482632.3483101",
                "event-place": "Dalian, China",
                "ISBN": "9781450390255",
                "number-of-pages": "5",
                "page": "1139–1143",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Quantitative Research on Psychological Education Based on Big Data Analysis Technology",
                "URL": "https://doi.org/10.1145/3482632.3483101"
            }
        },
        {
            "10.1145/966389.966395": {
                "id": "10.1145/966389.966395",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Pierce",
                        "given": "Elizabeth M."
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2004,
                            2,
                            1
                        ]
                    ]
                },
                "abstract": "The control matrix, long used by IS auditors to evaluate information integrity, can be modified to assess the reliability of information products.",
                "call-number": "10.1145/966389.966395",
                "container-title": "Commun. ACM",
                "DOI": "10.1145/966389.966395",
                "ISSN": "0001-0782",
                "issue": "2",
                "number-of-pages": "5",
                "page": "82–86",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "February 2004",
                "title": "Assessing data quality with control matrices",
                "URL": "https://doi.org/10.1145/966389.966395",
                "volume": "47"
            }
        },
        {
            "10.1145/3495018.3495292": {
                "id": "10.1145/3495018.3495292",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Niu",
                        "given": "Nana"
                    },
                    {
                        "family": "Che",
                        "given": "Di"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            10,
                            23
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2021,
                            10,
                            23
                        ]
                    ]
                },
                "abstract": "Since the issuance of the Plan for Furthering the Standardization Reforms in March 2015, in the more than five years of cultivating and developing social organization standards, government departments and social organizations across the country have actively promoted social organization standardization work, and social organization standards have developed rapidly in China. In order to better study the standardization work of various local organizations, this paper selected Beijing for research based on big data analysis methods. In the process of actively implementing the standardization reform policy, cultivating and developing social organization standards, Beijing's social organization standardization work has achieved remarkable results, but there are also some problems. Based on the data disclosed by the National Social Organization Standard Information Platform, this paper sorts out and summarizes the situation of Beijing's social organizations carrying out social organization standardization work, analyzes existing problems and makes suggestions.",
                "call-number": "10.1145/3495018.3495292",
                "collection-title": "AIAM2021",
                "container-title": "2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture",
                "DOI": "10.1145/3495018.3495292",
                "event-place": "Manchester, United Kingdom",
                "ISBN": "9781450385046",
                "number-of-pages": "6",
                "page": "862–867",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Analysis on Regional Social Organization Standardization Based on Big Data",
                "URL": "https://doi.org/10.1145/3495018.3495292"
            }
        },
        {
            "10.1145/2818869.2818934": {
                "id": "10.1145/2818869.2818934",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Lee",
                        "given": "Chung-Hong"
                    },
                    {
                        "family": "Yang",
                        "given": "Hsin-Chang"
                    },
                    {
                        "family": "Cheng",
                        "given": "Shou-Chen"
                    },
                    {
                        "family": "Tsai",
                        "given": "Sheng-Wen"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2015,
                            10,
                            7
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2015,
                            10,
                            7
                        ]
                    ]
                },
                "abstract": "In the manufacturing of semiconductor encapsulation, the production yield is one of critical issues concerned by all foundries. It is because that yield rate can directly affects the quality of the final product and the profitability. In this work we take the defect-products as samples and use machine learning techniques to classify the samples and verify the accuracy and feasibility of the experiment. We use Support Vector Machines (SVM) model to perform classification and compare the resulting accuracy with the results of Back Propagation Neural Network (BPN) model. Furthermore, we employ a statistical method namely Pearson product-moment correlation coefficient to identify the influential factors for production quality. The experimental result demonstrates that our hybrid method has great potentials for yield improvement in semiconductor manufacturing.",
                "call-number": "10.1145/2818869.2818934",
                "collection-number": "9",
                "collection-title": "ASE BD&amp;SI '15",
                "container-title": "Proceedings of the ASE BigData & SocialInformatics 2015",
                "DOI": "10.1145/2818869.2818934",
                "event-place": "Kaohsiung, Taiwan",
                "ISBN": "9781450337359",
                "keyword": "Big Data Analysis, Support Vector Machines, Machine Learning",
                "number": "Article 9",
                "number-of-pages": "4",
                "page": "1–4",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "A Hybrid Big Data Analytics Method for Yield Improvement in Semiconductor Manufacturing",
                "URL": "https://doi.org/10.1145/2818869.2818934"
            }
        },
        {
            "10.1145/3255772": {
                "id": "10.1145/3255772",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Finkelstein",
                        "given": "Shel"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2014,
                            6,
                            18
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2014,
                            6,
                            18
                        ]
                    ]
                },
                "call-number": "10.1145/3255772",
                "collection-title": "SIGMOD '14",
                "container-title": "Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data",
                "DOI": "10.1145/3255772",
                "event-place": "Snowbird, Utah, USA",
                "ISBN": "9781450323765",
                "page": "",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Session details: Industry session 4: big data systems",
                "URL": "https://doi.org/10.1145/3255772"
            }
        },
        {
            "10.1145/3480571.3480580": {
                "id": "10.1145/3480571.3480580",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Zeli",
                        "given": "Cen"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            7,
                            29
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2021,
                            7,
                            29
                        ]
                    ]
                },
                "abstract": "Based on the background of big data in the Internet age, and ecological linguistics as the theoretical foundation, this article uses the annual \"Top Ten Media Buzzwords\" and the annual \"Top Ten Internet Terms\" as the research objects to explore the synchronicity of the internal semantics of these annual buzzwords The diachronic development of change and its life trend and the study of its communication mechanism. The study found that both the media's annual buzzwords and the Internet's annual buzzwords have the characteristics of time, high frequency and history, but they are different in style, communication path, semantic expression connotation and production path, grammatical level, etc. They are constantly in the process of dynamic and balanced development. The annual Internet buzzwords are mainly spread dynamically in the way of semantic generalization and form derivation. Taking the media annual buzzwords as an example to conduct a dynamic investigation, it is found that their life trends are mainly \"protruding\". \"Trends\", \"Growth\" trends, \"Recession\" trends, \"Stable\" trends and other categories.",
                "call-number": "10.1145/3480571.3480580",
                "collection-title": "ICIIP 2021",
                "container-title": "2021 6th International Conference on Intelligent Information Processing",
                "DOI": "10.1145/3480571.3480580",
                "event-place": "Bucharest, Romania",
                "ISBN": "9781450390637",
                "keyword": "Regular algorithms, Buzzwords, Dynamics, Big data",
                "number-of-pages": "5",
                "page": "53–57",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Research on language dynamics development based on rule-based methods in the context of big data",
                "URL": "https://doi.org/10.1145/3480571.3480580"
            }
        },
        {
            "10.1145/3361525.3361547": {
                "id": "10.1145/3361525.3361547",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Birke",
                        "given": "Robert"
                    },
                    {
                        "family": "Rocha",
                        "given": "Isabelly"
                    },
                    {
                        "family": "Perez",
                        "given": "Juan"
                    },
                    {
                        "family": "Schiavoni",
                        "given": "Valerio"
                    },
                    {
                        "family": "Felber",
                        "given": "Pascal"
                    },
                    {
                        "family": "Chen",
                        "given": "Lydia Y."
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            12,
                            9
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            12,
                            9
                        ]
                    ]
                },
                "abstract": "Today's big data clusters based on the MapReduce paradigm are capable of executing analysis jobs with multiple priorities, providing differential latency guarantees. Traces from production systems show that the latency advantage of high-priority jobs comes at the cost of severe latency degradation of low-priority jobs as well as daunting resource waste caused by repetitive eviction and re-execution of low-priority jobs. We advocate a new resource management design that exploits the idea of differential approximation and sprinting. The unique combination of approximation and sprinting avoids the eviction of low-priority jobs and its consequent latency degradation and resource waste. To this end, we designed, implemented and evaluated DiAS, an extension of the Spark processing engine to support deflate jobs by dropping tasks and to sprint jobs. Our experiments on scenarios with two and three priority classes indicate that DiAS achieves up to 90% and 60% latency reduction for low- and high-priority jobs, respectively. DiAS not only eliminates resource waste but also (surprisingly) lowers energy consumption up to 30% at only a marginal accuracy loss for low-priority jobs.",
                "call-number": "10.1145/3361525.3361547",
                "collection-title": "Middleware '19",
                "container-title": "Proceedings of the 20th International Middleware Conference",
                "DOI": "10.1145/3361525.3361547",
                "event-place": "Davis, CA, USA",
                "ISBN": "9781450370097",
                "keyword": "priorities, sprinting, differential approximation, energy savings, Spark",
                "number-of-pages": "13",
                "page": "202–214",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Differential Approximation and Sprinting for Multi-Priority Big Data Engines",
                "URL": "https://doi.org/10.1145/3361525.3361547"
            }
        },
        {
            "10.1145/3219819.3220015": {
                "id": "10.1145/3219819.3220015",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Nguyen",
                        "given": "Khanh"
                    },
                    {
                        "family": "Le",
                        "given": "Trung"
                    },
                    {
                        "family": "Nguyen",
                        "given": "Tu Dinh"
                    },
                    {
                        "family": "Phung",
                        "given": "Dinh"
                    },
                    {
                        "family": "Webb",
                        "given": "Geoffrey I."
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            7,
                            19
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            7,
                            19
                        ]
                    ]
                },
                "abstract": "Kernel methods are powerful supervised machine learning models for their strong generalization ability, especially on limited data to effectively generalize on unseen data. However, most kernel methods, including the state-of-the-art LIBSVM, are vulnerable to the curse of kernelization, making them infeasible to apply to large-scale datasets. This issue is exacerbated when kernel methods are used in conjunction with a grid search to tune their kernel parameters and hyperparameters which brings in the question of model robustness when applied to real datasets. In this paper, we propose a robust Bayesian Kernel Machine (BKM) - a Bayesian kernel machine that exploits the strengths of both the Bayesian modelling and kernel methods. A key challenge for such a formulation is the need for an efficient learning algorithm. To this end, we successfully extended the recent Stein variational theory for Bayesian inference for our proposed model, resulting in fast and efficient learning and prediction algorithms. Importantly our proposed BKM is resilient to the curse of kernelization, hence making it applicable to large-scale datasets and robust to parameter tuning, avoiding the associated expense and potential pitfalls with current practice of parameter tuning. Our extensive experimental results on 12 benchmark datasets show that our BKM without tuning any parameter can achieve comparable predictive performance with the state-of-the-art LIBSVM and significantly outperforms other baselines, while obtaining significantly speedup in terms of the total training time compared with its rivals",
                "call-number": "10.1145/3219819.3220015",
                "collection-title": "KDD '18",
                "container-title": "Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining",
                "DOI": "10.1145/3219819.3220015",
                "event-place": "London, United Kingdom",
                "ISBN": "9781450355520",
                "keyword": "variational method, multiclass supervised learning, stein divergence, big data, kernel methods, bayesian inference, random feature",
                "number-of-pages": "9",
                "page": "2003–2011",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Robust Bayesian Kernel Machine via Stein Variational Gradient Descent for Big Data",
                "URL": "https://doi.org/10.1145/3219819.3220015"
            }
        },
        {
            "10.1145/3494885.3494939": {
                "id": "10.1145/3494885.3494939",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Wang",
                        "given": "Yan"
                    },
                    {
                        "family": "Zhao",
                        "given": "Zi-hao"
                    },
                    {
                        "family": "Li",
                        "given": "Hang"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            10,
                            22
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2021,
                            10,
                            22
                        ]
                    ]
                },
                "abstract": "In the era of data explosion, the data is growing exponentially. Information visualization maps abstract data into visual effects, and helps people to understand the market and make efficient decisions in an intuitive way, which has attracted more and more attention.This paper focuses on the field of information visualization, studies the theoretical basis of visualization models, research methods and design principles, expounds commercial application cases of information visualization, and presents the challenges and opportunities of the information visualization.",
                "call-number": "10.1145/3494885.3494939",
                "collection-title": "CSSE 2021",
                "container-title": "2021 4th International Conference on Computer Science and Software Engineering (CSSE 2021)",
                "DOI": "10.1145/3494885.3494939",
                "event-place": "Singapore, Singapore",
                "ISBN": "9781450390675",
                "keyword": "Information Visualization, Model, Business Applications, Big Data",
                "number-of-pages": "5",
                "page": "295–299",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Overview of information visualization for business under the background of big data: Overview of information visualization",
                "URL": "https://doi.org/10.1145/3494885.3494939"
            }
        },
        {
            "10.1145/2847263.2847294": {
                "id": "10.1145/2847263.2847294",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Ghasemi",
                        "given": "Ehsan"
                    },
                    {
                        "family": "Chow",
                        "given": "Paul"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            2,
                            21
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2016,
                            2,
                            21
                        ]
                    ]
                },
                "abstract": "Due to rapidly expanding data size, there is increasing need for scalable, high-performance, and low-energy frameworks for large- scale data computation. We build a dataflow architecture that harnesses FPGA resources within a distributed analytics platform creating a heterogeneous data analytics framework. This approach leverages the scalability of existing distributed processing environments and provides easy access to custom hardware accelerators for large-scale data analysis. We prototype our framework within the Apache Spark analytics tool running on a CPU-FPGA heterogeneous cluster. As a specific application case study, we have chosen the MapReduce paradigm to implement a multi-purpose, scalable, and customizable RTL accelerator inside the FPGA, capable of incorporating custom High-Level Synthesis (HLS) MapReduce kernels. We demonstrate how a typical MapReduce application can be simply adapted to our distributed framework while retaining the scalability of the Spark platform.",
                "call-number": "10.1145/2847263.2847294",
                "collection-title": "FPGA '16",
                "container-title": "Proceedings of the 2016 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays",
                "DOI": "10.1145/2847263.2847294",
                "event-place": "Monterey, California, USA",
                "ISBN": "9781450338561",
                "keyword": "big data, fpga, mapreduce, apache spark",
                "number-of-pages": "1",
                "page": "274",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "A Scalable Heterogeneous Dataflow Architecture For Big Data Analytics Using FPGAs (Abstract Only)",
                "URL": "https://doi.org/10.1145/2847263.2847294"
            }
        },
        {
            "10.1145/3495018.3495374": {
                "id": "10.1145/3495018.3495374",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Hu",
                        "given": "Na"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            10,
                            23
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2021,
                            10,
                            23
                        ]
                    ]
                },
                "abstract": "Fashion design belongs to the category of art and craft, which is an art form combining practicality and artistry. With the development of science and technology, network technology has been fully integrated into social life. The emergence of big data has changed the traditional economic and social form, and put forward new requirements for fashion design profession in the field of education. Based on big data, this paper studies the concept of Chinese clothing material design through hash algorithm of big data. Therefore, in order to adapt to the requirements of the era of big data, this paper believes that the fashion design major of higher professional universities should start with the education reform, strengthen the education mode in line with the times, improve the market demand, and provide professional fashion design talents for the society. Recently, China's garment industry needs a lot of experts. Many enterprises can't find talents suitable for many positions every year. Graduates are still facing employment problems. The imbalance of demand and supply is reflected in the talent training methods can not meet the requirements of the clothing industry. In the context of big data today, the innovation of fashion design should be the main task to develop, massive information can be easily presented in front of people. Therefore, we need to cater to the pace of fashion elements to meet the existing needs. In the source of fashion elements, the innovation of fashion design and the pursuit of fashion is no longer a lot of designers. Massive data resources use traditional fashion prediction methods to predict the results of its development trend, but the actual embodiment of life and culture. The trend of fashion design has been unable to achieve. In the context of the new era, the development of the Internet has led to information innovation, which is not limited to the structure of style and color, but in the development of clothing data.",
                "call-number": "10.1145/3495018.3495374",
                "collection-title": "AIAM2021",
                "container-title": "2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture",
                "DOI": "10.1145/3495018.3495374",
                "event-place": "Manchester, United Kingdom",
                "ISBN": "9781450385046",
                "number-of-pages": "4",
                "page": "1244–1247",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Clothing Material Design Concept Based on Big Data and Information Technology",
                "URL": "https://doi.org/10.1145/3495018.3495374"
            }
        },
        {
            "10.1145/3495018.3495415": {
                "id": "10.1145/3495018.3495415",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Wang",
                        "given": "Xiaoxiao"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            10,
                            23
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2021,
                            10,
                            23
                        ]
                    ]
                },
                "abstract": "In recent years, with the rapid development of e-commerce, the technology of online shopping platform is becoming more and more mature, followed by the emergence of a large number of e-commerce platforms, such as Taobao, JD, pinduoduo, etc. Most people live a fast-paced life and have little free time. Most people prefer online shopping and then distribute the purchased goods through third-party logistics. This method is simple, convenient and fast. In recent years, the rise of modern information technologies such as Internet of things, cloud computing and big data has had a significant impact on the development and innovation of logistics industry. By building e-commerce and other information platforms based on big data, manufacturers, distributors, consumers, logistics providers and financial service providers in the supply chain can be promoted to eliminate space barriers for communication and exchange. The application of big data technology can effectively integrate e-commerce platform and urban and rural logistics system, so that their data can be shared, which is conducive to reducing their dual uncertainty and improving operation efficiency. Based on the above analysis, this paper discusses the development and planning of logistics points under the background of big data from the perspective of big data. Based on the analysis of the research situation at home and abroad, this paper makes a theoretical and systematic research on the cost design of urban logistics system. Firstly, taking Hanzhong City as an example, this paper demonstrates the applicability of the prediction method, and then puts forward the principles and principles of urban logistics design. Secondly, combined with the characteristics of the city, this paper studies the prediction method and urban logistics demand prediction model, and the overall planning route of logistics city design. Finally, the layout and design method of urban logistics service function are put forward. Experiments show that the performance index E0 obtained by this method can reach 0.007%, which is in line with the expected results of the experiment.",
                "call-number": "10.1145/3495018.3495415",
                "collection-title": "AIAM2021",
                "container-title": "2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture",
                "DOI": "10.1145/3495018.3495415",
                "event-place": "Manchester, United Kingdom",
                "ISBN": "9781450385046",
                "number-of-pages": "5",
                "page": "1432–1436",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Logistics Network Deployment Planning under the Background of Big Data Technology",
                "URL": "https://doi.org/10.1145/3495018.3495415"
            }
        },
        {
            "10.1145/3252678": {
                "id": "10.1145/3252678",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Mishne",
                        "given": "Gilad"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            7,
                            7
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2016,
                            7,
                            7
                        ]
                    ]
                },
                "call-number": "10.1145/3252678",
                "collection-title": "SIGIR '16",
                "container-title": "Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval",
                "DOI": "10.1145/3252678",
                "event-place": "Pisa, Italy",
                "ISBN": "9781450340694",
                "page": "",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Session details: SIRIP I: Big companies, big data",
                "URL": "https://doi.org/10.1145/3252678"
            }
        },
        {
            "10.1145/3257971": {
                "id": "10.1145/3257971",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Parashar",
                        "given": "Manish"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            5,
                            31
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2016,
                            5,
                            31
                        ]
                    ]
                },
                "call-number": "10.1145/3257971",
                "collection-title": "HPDC '16",
                "container-title": "Proceedings of the 25th ACM International Symposium on High-Performance Parallel and Distributed Computing",
                "DOI": "10.1145/3257971",
                "event-place": "Kyoto, Japan",
                "ISBN": "9781450343145",
                "page": "",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Session details: Big Data Processing and I/O",
                "URL": "https://doi.org/10.1145/3257971"
            }
        },
        {
            "10.1145/2896387.2896423": {
                "id": "10.1145/2896387.2896423",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Pise",
                        "given": "Priya Dudhale"
                    },
                    {
                        "family": "Uke",
                        "given": "Nilesh J."
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            3,
                            22
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2016,
                            3,
                            22
                        ]
                    ]
                },
                "abstract": "Now day's use of big data platforms is increasing for storing large amount of end user's data remotely on big data servers. Cloud computing storage was widely used for storing user's data, but cloud computing only providing the tasks of data storage but not supporting the important functionalities like computation and database operations. These operations are supported by big data systems and hence currently use of big data platform for storage in increases worldwide by enterprises. Sharing sensitive information and data resulted into big reduction in costs of enterprises for users to provide value added data and personalized services. As enterprises are sharing their important and sensitive information on big data platforms from different and many domains, it becomes necessary to provide the security and privacy in big data platform. Data security and privacy is gaining significant attentions of researchers. There are many security methods already proposed for cloud computing platform, now same methods slowly adopted on big data platform. For Big Data platforms, secure sharing of sensitive data is challenging research problem. In this paper, first we are introducing the different security and privacy preserving methods of cloud computing and big data platforms with their limitations, and then presenting the novel hybrid framework for secure sensitive data sharing and privacy preserving public auditing for shared data over big data systems including functionalities such as privacy preserving, public auditing, data security, storage, data access, deletion or secure data destruction using cloud services.",
                "call-number": "10.1145/2896387.2896423",
                "collection-number": "38",
                "collection-title": "ICC '16",
                "container-title": "Proceedings of the International Conference on Internet of things and Cloud Computing",
                "DOI": "10.1145/2896387.2896423",
                "event-place": "Cambridge, United Kingdom",
                "ISBN": "9781450340632",
                "keyword": "Ring Search, Sensitive Data, Privacy Preserving, Data Sharing, Data Security, Public Auditing, Big Data, Proxy Re-encryption",
                "number": "Article 38",
                "number-of-pages": "5",
                "page": "1–5",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Efficient Security Framework for Sensitive Data Sharing and Privacy Preserving on Big-Data and Cloud Platforms",
                "URL": "https://doi.org/10.1145/2896387.2896423"
            }
        },
        {
            "10.1145/3377812.3390811": {
                "id": "10.1145/3377812.3390811",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Khalajzadeh",
                        "given": "Hourieh"
                    },
                    {
                        "family": "Simmons",
                        "given": "Andrew"
                    },
                    {
                        "family": "Abdelrazek",
                        "given": "Mohamed"
                    },
                    {
                        "family": "Grundy",
                        "given": "John"
                    },
                    {
                        "family": "Hosking",
                        "given": "John"
                    },
                    {
                        "family": "He",
                        "given": "Qiang"
                    },
                    {
                        "family": "Ratnakanthan",
                        "given": "Prasanna"
                    },
                    {
                        "family": "Zia",
                        "given": "Adil"
                    },
                    {
                        "family": "Law",
                        "given": "Meng"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2020,
                            6,
                            27
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2020,
                            6,
                            27
                        ]
                    ]
                },
                "abstract": "Data analytics application development introduces many challenges including: new roles not in traditional software engineering practices - e.g. data scientists and data engineers; use of sophisticated machine learning (ML) model-based approaches; uncertainty inherent in the models; interfacing with models to fulfill software functionalities; deploying models at scale and rapid evolution of business goals and data sources. We describe our Big Data Analytics Modeling Languages (BiDaML) toolset to bring all stakeholders around one tool to specify, model and document big data applications. We report on our experience applying BiDaML to three real-world large-scale applications. Our approach successfully supports complex data analytics application development in industrial settings.",
                "call-number": "10.1145/3377812.3390811",
                "collection-title": "ICSE '20",
                "container-title": "Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Companion Proceedings",
                "DOI": "10.1145/3377812.3390811",
                "event-place": "Seoul, South Korea",
                "ISBN": "9781450371223",
                "number-of-pages": "2",
                "page": "256–257",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "A practical, collaborative approach for modeling big data analytics application requirements",
                "URL": "https://doi.org/10.1145/3377812.3390811"
            }
        },
        {
            "10.5555/352925.352970": {
                "id": "10.5555/352925.352970",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Parssian",
                        "given": "Amir"
                    },
                    {
                        "family": "Sarkar",
                        "given": "Sumit"
                    },
                    {
                        "family": "Jacob",
                        "given": "Varghese S."
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            1999,
                            1,
                            1
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            1999,
                            1,
                            1
                        ]
                    ]
                },
                "call-number": "10.5555/352925.352970",
                "collection-title": "ICIS '99",
                "container-title": "Proceedings of the 20th international conference on Information Systems",
                "event-place": "Charlotte, North Carolina, USA",
                "number-of-pages": "6",
                "page": "428–433",
                "publisher": "Association for Information Systems",
                "publisher-place": "USA",
                "title": "Assessing data quality for information products"
            }
        },
        {
            "10.1145/3180457.3180463": {
                "id": "10.1145/3180457.3180463",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Gupta",
                        "given": "Maanak"
                    },
                    {
                        "family": "Patwa",
                        "given": "Farhan"
                    },
                    {
                        "family": "Sandhu",
                        "given": "Ravi"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            3,
                            14
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            3,
                            14
                        ]
                    ]
                },
                "abstract": "Apache Hadoop is a predominant software framework for distributed compute and storage with capability to handle huge amounts of data, usually referred to as Big Data. This data collected from different enterprises and government agencies often includes private and sensitive information, which needs to be secured from unauthorized access. This paper proposes extensions to the current authorization capabilities offered by Hadoop core and other ecosystem projects, specifically Apache Ranger and Apache Sentry. We present a fine-grained attribute-based access control model, referred as HeABAC, catering to the security and privacy needs of multi-tenant Hadoop ecosystem. The paper reviews the current multi-layered access control model used primarily in Hadoop core (2.x), Apache Ranger (version 0.6) and Sentry (version 1.7.0), as well as a previously proposed RBAC extension (OT-RBAC). It then presents a formal attribute-based access control model for Hadoop ecosystem, including the novel concept of cross Hadoop services trust. It further highlights different trust scenarios, presents an implementation approach for HeABAC using Apache Ranger and, discusses the administration requirements of HeABAC operational model. Some comprehensive, real-world use cases are also discussed to reflect the application and enforcement of the proposed HeABAC model in Hadoop ecosystem.",
                "call-number": "10.1145/3180457.3180463",
                "collection-title": "ABAC'18",
                "container-title": "Proceedings of the Third ACM Workshop on Attribute-Based Access Control",
                "DOI": "10.1145/3180457.3180463",
                "event-place": "Tempe, AZ, USA",
                "ISBN": "9781450356336",
                "keyword": "trust, big data, role based, attributes based, data lake, authorization, access control, hadoop ecosystem",
                "number-of-pages": "12",
                "page": "13–24",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "An Attribute-Based Access Control Model for Secure Big Data Processing in Hadoop Ecosystem",
                "URL": "https://doi.org/10.1145/3180457.3180463"
            }
        },
        {
            "10.1145/3495018.3495406": {
                "id": "10.1145/3495018.3495406",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Hao",
                        "given": "Jia"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            10,
                            23
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2021,
                            10,
                            23
                        ]
                    ]
                },
                "abstract": "The main task of educational administration is educational administration. Nowadays, many schools have thousands or even thousands of teachers, leading to the increase of a large amount of data such as various systems and databases operated by the school, such as management of school conditions and management performance analysis. How to use these data to transform existing management data into useful knowledge, improve school management decision-making, and improve school management level and quality is an urgent problem for schools to solve. This paper studies the intelligent educational administration management system based on big data technology. After understanding the relevant theories of big data technology and intelligent educational administration management system based on literature data, the intelligent educational administration management system based on big data technology is designed, and the designed system is tested, and the test results show that when the number of users in the system is 40, the average response time of the system is 0.31s, and the average response time of student score query is 0.63s, which basically meets the design requirements of the system.",
                "call-number": "10.1145/3495018.3495406",
                "collection-title": "AIAM2021",
                "container-title": "2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture",
                "DOI": "10.1145/3495018.3495406",
                "event-place": "Manchester, United Kingdom",
                "ISBN": "9781450385046",
                "number-of-pages": "5",
                "page": "1390–1394",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Design of Intelligent Educational Administration System Based on Big Data Technology",
                "URL": "https://doi.org/10.1145/3495018.3495406"
            }
        },
        {
            "10.1145/1966883.1966892": {
                "id": "10.1145/1966883.1966892",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Guerra-García",
                        "given": "César"
                    },
                    {
                        "family": "Caballero",
                        "given": "Ismael"
                    },
                    {
                        "family": "Piattini",
                        "given": "Mario"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2011,
                            3,
                            25
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2011,
                            3,
                            25
                        ]
                    ]
                },
                "abstract": "The number and complexity of Web applications which are part of Business Intelligence (BI) applications had grown exponentially in recent years. The amount of data used in these applications has consequently also grown. Managing data with an acceptable level of quality is paramount to success in any organizational business process. In order to raise and maintain the adequate levels of Data Quality (DQ) it is indispensable for Web applications to be able to satisfy specific DQ requirements. In order to achieve this goal, DQ requirements should be captured and introduced into the development process together with the other software requirements needed in the applications. However, in the field of Web application development, and to the best of our knowledge, no proposals exist with regard to the way in which to manage specific DQ software requirements. This paper considers the MDA (Model Driven Architecture) approach and, principally, the benefits provided by Model Driven Web Engineering (MDWE) in order to put forward a proposal for two artifacts. These two artifacts are a metamodel and a UML profile for the management of Data Quality Software Requirements for Web Applications (DQ_WebRE).",
                "call-number": "10.1145/1966883.1966892",
                "collection-title": "BEWEB '11",
                "container-title": "Proceedings of the 2nd International Workshop on Business intelligencE and the WEB",
                "DOI": "10.1145/1966883.1966892",
                "event-place": "Uppsala, Sweden",
                "ISBN": "9781450306102",
                "keyword": "model driven Web engineering, Web engineering, requirements engineering, data quality, requirements modeling",
                "number-of-pages": "8",
                "page": "28–35",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Capturing data quality requirements for web applications by means of DQ_WebRE",
                "URL": "https://doi.org/10.1145/1966883.1966892"
            }
        },
        {
            "10.1145/3210752": {
                "id": "10.1145/3210752",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Orenga-Roglá",
                        "given": "Sergio"
                    },
                    {
                        "family": "Chalmeta",
                        "given": "Ricardo"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            12,
                            19
                        ]
                    ]
                },
                "abstract": "Featuring the various dimensions of data management, it guides organizations through implementation fundamentals.",
                "call-number": "10.1145/3210752",
                "container-title": "Commun. ACM",
                "DOI": "10.1145/3210752",
                "ISSN": "0001-0782",
                "issue": "1",
                "number-of-pages": "8",
                "page": "58–65",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "January 2019",
                "title": "Framework for implementing a big data ecosystem in organizations",
                "URL": "https://doi.org/10.1145/3210752",
                "volume": "62"
            }
        },
        {
            "10.1145/3358331.3358336": {
                "id": "10.1145/3358331.3358336",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Jia",
                        "given": "Dong-Ming"
                    },
                    {
                        "family": "Yuan",
                        "given": "Cun-Feng"
                    },
                    {
                        "family": "Guo",
                        "given": "Song"
                    },
                    {
                        "family": "Jiang",
                        "given": "Zu-Zhen"
                    },
                    {
                        "family": "Xu",
                        "given": "Ding"
                    },
                    {
                        "family": "Wang",
                        "given": "Da-An"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            10,
                            17
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            10,
                            17
                        ]
                    ]
                },
                "abstract": "Under the background of \"Wisdom Drug Rehabilitation\", we introduced \"Artificial Intelligence and Big Data\" into \"exercise rehabilitation\" work of drug addicts in judicial administrative system. It is a practical innovation of drug treatment in China. This article will elaborate this innovation of the construction and application of \"Exercise Rehabilitation\" intelligence platform system. This system will improve mental status, alleviate physical and psychological symptoms, ensure safety in places, lighten the burden of professional police officers, make rapid analysis, make accurate decisions and improve the integrity rate of the addict.",
                "call-number": "10.1145/3358331.3358336",
                "collection-number": "5",
                "collection-title": "AIAM 2019",
                "container-title": "Proceedings of the 2019 International Conference on Artificial Intelligence and Advanced Manufacturing",
                "DOI": "10.1145/3358331.3358336",
                "event-place": "Dublin, Ireland",
                "ISBN": "9781450372022",
                "keyword": "judicial administrative, big data, rehabilitation training, Sports rehabilitation, artificial intelligence",
                "number": "Article 5",
                "number-of-pages": "5",
                "page": "1–5",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Application of \"Artificial Intelligence and Big Data\" in Sports Rehabilitation for Chinese Judicial Administrative Drug Addicts",
                "URL": "https://doi.org/10.1145/3358331.3358336"
            }
        },
        {
            "10.1145/3260511": {
                "id": "10.1145/3260511",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Newton",
                        "given": "Glen"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2015,
                            6,
                            21
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2015,
                            6,
                            21
                        ]
                    ]
                },
                "call-number": "10.1145/3260511",
                "collection-title": "JCDL '15",
                "container-title": "Proceedings of the 15th ACM/IEEE-CS Joint Conference on Digital Libraries",
                "DOI": "10.1145/3260511",
                "event-place": "Knoxville, Tennessee, USA",
                "ISBN": "9781450335942",
                "page": "",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Session details: Session 3 - Big Data, Big Resources",
                "URL": "https://doi.org/10.1145/3260511"
            }
        },
        {
            "10.1145/3229049": {
                "id": "10.1145/3229049",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Malik",
                        "given": "Maria"
                    },
                    {
                        "family": "Rafatirad",
                        "given": "Setareh"
                    },
                    {
                        "family": "Homayoun",
                        "given": "Houman"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            7,
                            23
                        ]
                    ]
                },
                "abstract": "The rapid growth in data yields challenges to process data efficiently using current high-performance server architectures such as big Xeon cores. Furthermore, physical design constraints, such as power and density, have become the dominant limiting factor for scaling out servers. Low-power embedded cores in servers such as little Atom have emerged as a promising solution to enhance energy-efficiency to address these challenges. Therefore, the question of whether to process the big data applications on big Xeon- or Little Atom-based servers becomes important. In this work, through methodical investigation of power and performance measurements, and comprehensive application-level, system-level, and micro-architectural level analysis, we characterize dominant big data applications on big Xeon- and little Atom-based server architectures. The characterization results across a wide range of real-world big data applications, and various software stacks demonstrate how the choice of big- versus little-core-based server for energy-efficiency is significantly influenced by the size of data, performance constraints, and presence of accelerator. In addition, we analyze processor resource utilization of this important class of applications, such as memory footprints, CPU utilization, and disk bandwidth, to understand their run-time behavior. Furthermore, we perform micro-architecture-level analysis to highlight where improvement is needed in big- and little-core microarchitectures to address their performance bottlenecks.",
                "call-number": "10.1145/3229049",
                "collection-number": "14",
                "container-title": "ACM Trans. Model. Perform. Eval. Comput. Syst.",
                "DOI": "10.1145/3229049",
                "ISSN": "2376-3639",
                "issue": "3",
                "keyword": "power, big data, low-power server, accelerator, Performance, high-performance server, characterization",
                "number": "Article 14",
                "number-of-pages": "32",
                "page": "1–32",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "September 2018",
                "title": "System and Architecture Level Characterization of Big Data Applications on Big and Little Core Server Architectures",
                "URL": "https://doi.org/10.1145/3229049",
                "volume": "3"
            }
        },
        {
            "10.1145/3287324.3287494": {
                "id": "10.1145/3287324.3287494",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Deb",
                        "given": "Debzani"
                    },
                    {
                        "family": "Fuad",
                        "given": "Muztaba"
                    },
                    {
                        "family": "Irwin",
                        "given": "Keith"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            2,
                            22
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            2,
                            22
                        ]
                    ]
                },
                "abstract": "Big data and cloud computing collectively offer a paradigm shift in the way businesses are now acquiring, using and managing information technology. This creates the need for every CS student to be equipped with foundational knowledge in this collective paradigm and to possess some hands-on experience in deploying and managing big data applications in the cloud. We argue that, for substantial coverage of big data and cloud computing concepts and skills, the relevant topics need to be integrated into multiple core courses across the undergraduate CS curriculum rather than creating additional standalone core or elective courses and performing a major overhaul of the curriculum. Our approach to including these topics is to develop autonomous learning modules for specific core courses in which their coverage might find an appropriate context. In this paper, three such modules are discussed and our classroom experiences during these interventions are documented. So far, we have achieved reasonable success in attaining student learning outcomes, enhanced engagement, and interests. Our objective is to share our experience with the academics who aim at incorporating similar pedagogy and to receive feedback about our approach.",
                "call-number": "10.1145/3287324.3287494",
                "collection-title": "SIGCSE '19",
                "container-title": "Proceedings of the 50th ACM Technical Symposium on Computer Science Education",
                "DOI": "10.1145/3287324.3287494",
                "event-place": "Minneapolis, MN, USA",
                "ISBN": "9781450358903",
                "keyword": "big data, cloud computing, curriculum, mapreduce, apache spark",
                "number-of-pages": "7",
                "page": "2–8",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "A Module-based Approach to Teaching Big data and Cloud Computing Topics at CS Undergraduate Level",
                "URL": "https://doi.org/10.1145/3287324.3287494"
            }
        },
        {
            "10.5555/2399776.2399802": {
                "id": "10.5555/2399776.2399802",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Petrazickis",
                        "given": "Leons"
                    },
                    {
                        "family": "Butuc",
                        "given": "Marius"
                    },
                    {
                        "family": "Steinfeld",
                        "given": "Bradley"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2012,
                            11,
                            5
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2012,
                            11,
                            5
                        ]
                    ]
                },
                "abstract": "There is an ongoing information explosion in every field of human endeavour. Enormous, unstructured, immensely valuable data sets are being accumulated. Every device logs numbers, audio, video and text; data is aggregated and stored somewhere.Unfortunately, traditional techniques cannot analyze these data sets. There's too much data to query -- volume! And it's all different -- variety! And it's arriving too fast -- velocity! Finance firms need to analyze transactions to detect fraud and model risk; Energy firms need to analyze old rig performance and wind speeds; IT needs to analyze logs of every type; Service providers need to analyze the prices of various services worldwide; Healthcare providers need to analyze patient data and measurements.New techniques are needed to deal with this Big Data. Fortunately, cloud computing is allowing the emergence of technologies that rely on clusters of commodity hardware to crunch data. Google is one example of a company that had to face and solve a Big Data problem before it could revolutionize internet search and consign countless other early search engines to the dust heap of history. The page-rank algorithm that it uses to rank results is based on something called Map-Reduce. In the Map phase, the data set (all of the internet) is split into microscopic chunks, which are transformed from unstructured data (information about a web page) to useful data (the value of web page). In the Reduce phase, the transformed microscopic chunks are reassembled back together into a Google results page.Because the chunks are microscopic, the mapping could run on many off-the-shelf computers rather than one big server. As a result, Google is able to use cheap commodity hardware and put competitors that relied on expensive servers out of business. The hardware side of this approach created Cloud Computing, which is a way of organizing vast amounts of cheap hardware on demand. The software side of this created a lot of useful data analytics applications.Apache Hadoop is one useful data analytics application that's native to the cloud. Hadoop is an open source project led by Yahoo. It makes it straightforward to apply the idea of Map-Reduce to any data set. Many vendors such as Cloudera. HortonWorks, and IBM have their own distribution of Hadoop. The Hadoop ecosystem includes many other open source technologies. The Java libraries are enhanced by the Pig high level language, the HBase database, the Hive data warehouse system, and the Flume log aggregation service. Each of these makes Hadoop more powerful at dealing with larger volumes of data, greater varieties of data, and quicker velocities of data.IBM InfoSphere BigInsights is a distribution of Hadoop. It integrates an IBM-created open source query language called JAQL (Jackal) with the usual components such as Hive, HBase, and Pig. JAQL allows the user to query through large sets of data in JSON (JavaScript Object Notation) form, which is the native data format of Hadoop. The Basic edition of BigInsights is available for download at no charge. It can also be easily deployed on Amazon Elastic Compute Cloud or IBM SmartCloud Enterprise.",
                "call-number": "10.5555/2399776.2399802",
                "collection-title": "CASCON '12",
                "container-title": "Proceedings of the 2012 Conference of the Center for Advanced Studies on Collaborative Research",
                "event-place": "Toronto, Ontario, Canada",
                "number-of-pages": "2",
                "page": "241–242",
                "publisher": "IBM Corp.",
                "publisher-place": "USA",
                "title": "Crunching big data with Hadoop and BigInsights in the cloud"
            }
        },
        {
            "10.1145/3057148.3057149": {
                "id": "10.1145/3057148.3057149",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Mesbah",
                        "given": "Sepideh"
                    },
                    {
                        "family": "Bozzon",
                        "given": "Alessandro"
                    },
                    {
                        "family": "Lofi",
                        "given": "Christoph"
                    },
                    {
                        "family": "Houben",
                        "given": "Geert-Jan"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            2,
                            10
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            2,
                            10
                        ]
                    ]
                },
                "abstract": "The rise of Big Data analytics has been a disruptive game changer for many application domains, allowing the integration into domain-specific applications and systems of insights and knowledge extracted from external big data sets. The effective \"injection\" of external Big Data demands an understanding of the properties of available data sets, and expertise on the available and most suitable methods for data collection, enrichment and analysis. A prominent knowledge source is scientific literature, where data processing pipelines are described, discussed, and evaluated. Such knowledge is however not readily accessible, due to its distributed and unstructured nature. In this paper, we propose a novel ontology aimed at modeling properties of data processing pipelines, and their related artifacts, as described in scientific publications. The ontology is the result of a requirement analysis that involved experts from both academia and industry. We showcase the effectiveness of our ontology by manually applying it to a collection of publications describing data processing methods.",
                "call-number": "10.1145/3057148.3057149",
                "collection-title": "SWM '17",
                "container-title": "Proceedings of the 1st Workshop on Scholarly Web Mining",
                "DOI": "10.1145/3057148.3057149",
                "event-place": "Cambridge, United Kingdom",
                "ISBN": "9781450352406",
                "number-of-pages": "8",
                "page": "1–8",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Describing Data Processing Pipelines in Scientific Publications for Big Data Injection",
                "URL": "https://doi.org/10.1145/3057148.3057149"
            }
        },
        {
            "10.14778/3407790.3407848": {
                "id": "10.14778/3407790.3407848",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Rong",
                        "given": "Kexin"
                    },
                    {
                        "family": "Lu",
                        "given": "Yao"
                    },
                    {
                        "family": "Bailis",
                        "given": "Peter"
                    },
                    {
                        "family": "Kandula",
                        "given": "Srikanth"
                    },
                    {
                        "family": "Levis",
                        "given": "Philip"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2020,
                            7,
                            1
                        ]
                    ]
                },
                "abstract": "Many big-data clusters store data in large partitions that support access at a coarse, partition-level granularity. As a result, approximate query processing via row-level sampling is inefficient, often requiring reads of many partitions. In this work, we seek to answer queries quickly and approximately by reading a subset of the data partitions and combining partial answers in a weighted manner without modifying the data layout. We illustrate how to efficiently perform this query processing using a set of pre-computed summary statistics, which inform the choice of partitions and weights. We develop novel means of using the statistics to assess the similarity and importance of partitions. Our experiments on several datasets and data layouts demonstrate that to achieve the same relative error compared to uniform partition sampling, our techniques offer from 2.7x to 70x reduction in the number of partitions read, and the statistics stored per partition require fewer than 100KB.",
                "call-number": "10.14778/3407790.3407848",
                "container-title": "Proc. VLDB Endow.",
                "DOI": "10.14778/3407790.3407848",
                "ISSN": "2150-8097",
                "issue": "12",
                "number-of-pages": "14",
                "page": "2606–2619",
                "publisher": "VLDB Endowment",
                "source": "August 2020",
                "title": "Approximate partition selection for big-data workloads using summary statistics",
                "URL": "https://doi.org/10.14778/3407790.3407848",
                "volume": "13"
            }
        },
        {
            "10.1145/2485922.2485944": {
                "id": "10.1145/2485922.2485944",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Wu",
                        "given": "Lisa"
                    },
                    {
                        "family": "Barker",
                        "given": "Raymond J."
                    },
                    {
                        "family": "Kim",
                        "given": "Martha A."
                    },
                    {
                        "family": "Ross",
                        "given": "Kenneth A."
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2013,
                            6,
                            23
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2013,
                            6,
                            23
                        ]
                    ]
                },
                "abstract": "The global pool of data is growing at 2.5 quintillion bytes per day, with 90% of it produced in the last two years alone [24]. There is no doubt the era of big data has arrived. This paper explores targeted deployment of hardware accelerators to improve the throughput and energy efficiency of large-scale data processing. In particular, data partitioning is a critical operation for manipulating large data sets. It is often the limiting factor in database performance and represents a significant fraction of the overall runtime of large data queries.To accelerate partitioning, this paper describes a hardware accelerator for range partitioning, or HARP, and a hardware-software data streaming framework. The streaming framework offers a seamless execution environment for streaming accelerators such as HARP. Together, HARP and the streaming framework provide an order of magnitude improvement in partitioning performance and energy. A detailed analysis of a 32nm physical design shows 7.8 times the throughput of a highly optimized and optimistic software implementation, while consuming just 6.9% of the area and 4.3% of the power of a single Xeon core in the same technology generation.",
                "call-number": "10.1145/2485922.2485944",
                "collection-title": "ISCA '13",
                "container-title": "Proceedings of the 40th Annual International Symposium on Computer Architecture",
                "DOI": "10.1145/2485922.2485944",
                "event-place": "Tel-Aviv, Israel",
                "ISBN": "9781450320795",
                "keyword": "specialized functional unit, streaming data, accelerator, microarchitecture, data partitioning",
                "number-of-pages": "12",
                "page": "249–260",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Navigating big data with high-throughput, energy-efficient data partitioning",
                "URL": "https://doi.org/10.1145/2485922.2485944"
            }
        },
        {
            "10.1145/2508148.2485944": {
                "id": "10.1145/2508148.2485944",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Wu",
                        "given": "Lisa"
                    },
                    {
                        "family": "Barker",
                        "given": "Raymond J."
                    },
                    {
                        "family": "Kim",
                        "given": "Martha A."
                    },
                    {
                        "family": "Ross",
                        "given": "Kenneth A."
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2013,
                            6,
                            23
                        ]
                    ]
                },
                "abstract": "The global pool of data is growing at 2.5 quintillion bytes per day, with 90% of it produced in the last two years alone [24]. There is no doubt the era of big data has arrived. This paper explores targeted deployment of hardware accelerators to improve the throughput and energy efficiency of large-scale data processing. In particular, data partitioning is a critical operation for manipulating large data sets. It is often the limiting factor in database performance and represents a significant fraction of the overall runtime of large data queries.To accelerate partitioning, this paper describes a hardware accelerator for range partitioning, or HARP, and a hardware-software data streaming framework. The streaming framework offers a seamless execution environment for streaming accelerators such as HARP. Together, HARP and the streaming framework provide an order of magnitude improvement in partitioning performance and energy. A detailed analysis of a 32nm physical design shows 7.8 times the throughput of a highly optimized and optimistic software implementation, while consuming just 6.9% of the area and 4.3% of the power of a single Xeon core in the same technology generation.",
                "call-number": "10.1145/2508148.2485944",
                "container-title": "SIGARCH Comput. Archit. News",
                "DOI": "10.1145/2508148.2485944",
                "ISSN": "0163-5964",
                "issue": "3",
                "keyword": "streaming data, specialized functional unit, microarchitecture, data partitioning, accelerator",
                "number-of-pages": "12",
                "page": "249–260",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "June 2013",
                "title": "Navigating big data with high-throughput, energy-efficient data partitioning",
                "URL": "https://doi.org/10.1145/2508148.2485944",
                "volume": "41"
            }
        },
        {
            "10.1145/3077839.3081670": {
                "id": "10.1145/3077839.3081670",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Lee",
                        "given": "Junghoon"
                    },
                    {
                        "family": "Park",
                        "given": "Gyung-Leen"
                    },
                    {
                        "family": "Han",
                        "given": "Yeonju"
                    },
                    {
                        "family": "Yoo",
                        "given": "Seunghee"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            5,
                            16
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            5,
                            16
                        ]
                    ]
                },
                "abstract": "This paper describes a big data analysis strategy for electric vehicle charging infrastructure, mainly built upon open data sets and open software components. The data acquisition module periodically retrieves the real-time status information of each charger from the public data portal, while the downloaded XML files are parsed to extract fields of interest. At this stage, we present the distribution of charging facilities in Jeju City based on our own map viewer implementation, the city-wide dynamics of the number of chargers in operation based on MySQL queries, and the visualization of regional occupancy rates based on the R GISTools library. After combining a variety of statistical and machine learning techniques to understand the demand pattern of electric vehicle charging, we will integrate renewable energy to charging-intensive power grids as much as possible.",
                "call-number": "10.1145/3077839.3081670",
                "collection-title": "e-Energy '17",
                "container-title": "Proceedings of the Eighth International Conference on Future Energy Systems",
                "DOI": "10.1145/3077839.3081670",
                "event-place": "Shatin, Hong Kong",
                "ISBN": "9781450350365",
                "keyword": "open software, data acquisition, Charging infrastructure, occupancy rate, big data analysis",
                "number-of-pages": "2",
                "page": "252–253",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Big data analysis for an electric vehicle charging infrastructure using open data and software",
                "URL": "https://doi.org/10.1145/3077839.3081670"
            }
        },
        {
            "10.5555/2093889.2093935": {
                "id": "10.5555/2093889.2093935",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Petrazickis",
                        "given": "Leons"
                    },
                    {
                        "family": "Steinfeld",
                        "given": "Bradley"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2011,
                            11,
                            7
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2011,
                            11,
                            7
                        ]
                    ]
                },
                "abstract": "There is an ongoing information explosion in every field of human endeavour. Enormous, unstructured, immensely valuable data sets are being accumulated. Every device logs numbers and audio and video and text, and then this data is aggregated and stored somewhere. Unfortunately, traditional techniques cannot analyze these data sets. There's too much data to query -- volume! And it's all different -- variety! And it's arriving too fast -- velocity!Finance firms need to analyze transactions to detect fraud and model risk. Energy firms need to analyze old rig performance and wind speeds. IT needs to analyze logs of every type. Service providers need to analyze the prices of various services worldwide. Healthcare providers need to analyze patient data and measurements.New techniques are needed to deal with this Big Data. Fortunately, cloud computing is allowing the emergence of technologies that rely on clusters of commodity hardware to crunch data.Google is one example of a company that had to face and solve a Big Data problem before it could revolutionize internet search and consign countless other early search engines to the dust heap of history. Its page-rank algorithm that it uses to rank results is based on something called Map-Reduce. In the Map phase, the data set (all of the internet) is split into itsy-bitsy chunks, which are transformed from unstructured data (information about a web page) to useful data (the value of web page). In the Reduce phase, the transformed itsy-bitsy chunks are reassembled back together into a Google results page.Because the chunks are itsy-bitsy, the mapping could run on many off-the-shelf computers rather than one big server. This allowed Google to use cheap commodity hardware and put competitors that relied on expensive servers out of business. The hardware side of this approach created Cloud Computing, which is a way of organizing vast amounts of cheap hardware on demand. The software side of this created a lot of useful data analytics applications.Apache Hadoop is one useful data analytics application that's native to the cloud. Hadoop is an open source project led by Yahoo. It makes it straightforward to apply the idea of Map-Reduce to any data set. Many vendors such as Cloudera, HortonWorks, and IBM have their own distribution of Hadoop.The Hadoop ecosystem includes many other open source technologies. The Java libraries are enhanced by the Pig high level language, the HBase database, the Hive data warehouse system, and the Flume log aggregation service. Each of these makes Hadoop more powerful at dealing with larger volumes of data, greater varieties of data, and quicker velocities of data.IBM InfoSphere BigInsights is a distribution of Hadoop. It integrates an IBM-created open source query language called JAQL (Jackal) with the usual components such as Hive, HBase, and Pig. JAQL allows the user to query through large sets of data in JSON (JavaScript Object Notation) form, which is the native data format of Hadoop.The Basic edition of BigInsights is available for download at no charge. It can also be easily deployed on Amazon Elastic Compute Cloud or IBM SmartCloud Enterprise.",
                "call-number": "10.5555/2093889.2093935",
                "collection-title": "CASCON '11",
                "container-title": "Proceedings of the 2011 Conference of the Center for Advanced Studies on Collaborative Research",
                "event-place": "Toronto, Ontario, Canada",
                "number-of-pages": "2",
                "page": "334–335",
                "publisher": "IBM Corp.",
                "publisher-place": "USA",
                "title": "Crunching big data in the cloud with Hadoop and BigInsights"
            }
        },
        {
            "10.1145/2815782.2815791": {
                "id": "10.1145/2815782.2815791",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Botha",
                        "given": "Marna"
                    },
                    {
                        "family": "Botha",
                        "given": "Adele"
                    },
                    {
                        "family": "Herselman",
                        "given": "Marlien"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2015,
                            9,
                            28
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2015,
                            9,
                            28
                        ]
                    ]
                },
                "abstract": "Data quality is one of many challenges experienced in e-health. The collection of data with substandard data quality leads to inappropriate information for health and management purposes. Given evidence of challenges with regards to data quality in electronic health systems, the purpose of the study is to prioritise data quality challenges as experienced by data users of electronic healthcare systems in South Africa. The study adopted a sequential QUAL-quan mixed method research design towards the realisation of the research purpose. After carrying out a literature review on the background of e-health and the current status of research on data quality challenges, a qualitative study was conducted to verify and extend the theoretical list of data quality challenges. A quantitative study followed to prioritise data quality challenges as experienced by data users of electronic healthcare systems. Data users of electronic healthcare systems in South Africa served as the unit of analysis in the study. The data collection process included the conducting of interviews with four data quality experts to verify and extend the theoretical list of data quality challenges. This was followed by a survey targeting 100 data users of electronic healthcare systems in South Africa for which 82 responses were received.From the results of the study, a prioritised list of data quality challenges has been developed which can be applied to assist data users of electronic health care systems in South Africa to improve the quality of data in electronic healthcare systems. The most important data challenge is training. The prioritised list of data quality challenges allowed for evidence-based recommendations which can assist health institutions in South Africa to ensure future data quality.",
                "call-number": "10.1145/2815782.2815791",
                "collection-number": "5",
                "collection-title": "SAICSIT '15",
                "container-title": "Proceedings of the 2015 Annual Research Conference on South African Institute of Computer Scientists and Information Technologists",
                "DOI": "10.1145/2815782.2815791",
                "event-place": "Stellenbosch, South Africa",
                "ISBN": "9781450336833",
                "keyword": "data quality challenges, E-health, data quality, IT",
                "number": "Article 5",
                "number-of-pages": "10",
                "page": "1–10",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "On the prioritization of data quality challenges in e-health systems in South Africa",
                "URL": "https://doi.org/10.1145/2815782.2815791"
            }
        },
        {
            "10.1145/2742854.2747282": {
                "id": "10.1145/2742854.2747282",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Fiore",
                        "given": "Sandro"
                    },
                    {
                        "family": "Mancini",
                        "given": "Marco"
                    },
                    {
                        "family": "Elia",
                        "given": "Donatello"
                    },
                    {
                        "family": "Nassisi",
                        "given": "Paola"
                    },
                    {
                        "family": "Brasileiro",
                        "given": "Francisco Vilar"
                    },
                    {
                        "family": "Blanquer",
                        "given": "Ignacio"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2015,
                            5,
                            6
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2015,
                            5,
                            6
                        ]
                    ]
                },
                "abstract": "The analysis of large volumes of data is key for knowledge discovery in several scientific domains such as climate, astrophysics, life sciences among others. It requires a large set of computational and storage resources, as well as flexible and efficient software solutions able to dynamically exploit the available infrastructure and address issues related to data volume, distribution, velocity and heterogeneity. This paper presents a data-driven and cloud-based use case implemented in the context of the EUBrazilCC project for the analysis of climate change and biodiversity data. The use case architecture and main components, as well as a Platform as a Service (PaaS) framework for big data analytics named PDAS, together with its elastic deployment in the EUBrazilCC federated cloud infrastructure are presented and discussed in detail.",
                "call-number": "10.1145/2742854.2747282",
                "collection-number": "52",
                "collection-title": "CF '15",
                "container-title": "Proceedings of the 12th ACM International Conference on Computing Frontiers",
                "DOI": "10.1145/2742854.2747282",
                "event-place": "Ischia, Italy",
                "ISBN": "9781450333580",
                "keyword": "scientific data management, big data analytics, federated clouds, cloud computing",
                "number": "Article 52",
                "number-of-pages": "8",
                "page": "1–8",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Big data analytics for climate change and biodiversity in the EUBrazilCC federated cloud infrastructure",
                "URL": "https://doi.org/10.1145/2742854.2747282"
            }
        },
        {
            "10.1145/3250292": {
                "id": "10.1145/3250292",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Bacardit",
                        "given": "Jaume"
                    },
                    {
                        "family": "Arnaldo",
                        "given": "Ignacio"
                    },
                    {
                        "family": "Veeramachaneni",
                        "given": "Kalyan"
                    },
                    {
                        "family": "O'Reilly",
                        "given": "Una-May"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2014,
                            7,
                            12
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2014,
                            7,
                            12
                        ]
                    ]
                },
                "abstract": "It is our great pleasure to welcome you to the first Workshop on Evolutionary Computation for Big Data and Big Learning ECBDL'14We live in a time of unprecedented access to cheap and vast amounts of computational resources, which is producing a big leap forward in the fields of machine learning and data mining. We can tackle datasets of a scale (be it instances, attributes, classes, etc.) that was unimaginable some years ago, in what is well known as big data. On the other hand we can also use all these vast computational resources with the aim of understanding better our machine learning methods, by performing large scale evaluations, parameter sweeps, etc. We refer to the overall use massive on-demand computation (cloud or GPUs) for machine learning as Big Learning. Evolutionary Machine Learning techniques are perfect candidates for big learning tasks due to their flexibility in knowledge representations, learning paradigms and their innate parallelism.In this workshop we have accepted two papers representing very different scenarios of big data and big learning. The first paper, \"Evolving Relational Hierarchical Classification Rules for Predicting Gene Ontology-Based Protein Functions\" by Ricardo Cerri, Rodrigo C. Barros, Alex A. Freitas and André C. P. L. F. Carvalho, focuses on an extremely complex and heterogeneous classification task, where instances have multiple classes organized hierarchically. The method is tested on real-world biological data, and explores the use of new rule representations to enhance knowledge discovery.The second paper, \"On the Application of GP to Streaming Data Classification Tasks with Label Budgets\" by Ali Vahdat, Aaron Atwater, Andrew R. McIntyre, Malcolm I. Heywood, focuses on a very important topic within big data/learning, streaming data classification, in the particular scenario where access to the real annotation (classes) of data is costly, and budgets need to be specified.",
                "call-number": "10.1145/3250292",
                "collection-title": "GECCO Comp '14",
                "container-title": "Proceedings of the Companion Publication of the 2014 Annual Conference on Genetic and Evolutionary Computation",
                "DOI": "10.1145/3250292",
                "event-place": "Vancouver, BC, Canada",
                "ISBN": "9781450328814",
                "page": "",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Session details: Workshop: evolutionary computation for big data and big learning",
                "URL": "https://doi.org/10.1145/3250292"
            }
        },
        {
            "10.1145/3219104.3229279": {
                "id": "10.1145/3219104.3229279",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Li",
                        "given": "Yu"
                    },
                    {
                        "family": "Zhang",
                        "given": "Xiaohong"
                    },
                    {
                        "family": "Srinath",
                        "given": "Ashwin"
                    },
                    {
                        "family": "Getman",
                        "given": "Rachel B."
                    },
                    {
                        "family": "Ngo",
                        "given": "Linh B."
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            7,
                            22
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            7,
                            22
                        ]
                    ]
                },
                "abstract": "Advances in scientific software and computing infrastructure have enabled researchers across disciplines to simulate and model highly complex systems. At the same time, these increases in simulation duration and scale have led to significant growths in the sizes of output data, which can be as much as hundreds of gigabytes or more. While there exist solutions to assist with most standard post-simulation analytics, researchers must develop their own code to support customized analytical tasks. Given the nature of these output data, most naive in-house sequential codes end up being inefficient, and in most cases, time-consuming. In this paper, we propose a solution to this issue by transparently combining the strengths of a high-performance computing cluster and a big data infrastructure to support an end-to-end scientific workflow. More specifically, we present a case study around the design of a research computing environment at Clemson University where these two computing systems are integrated and accessible from one another. This environment allows simulation data to be automatically transferred across systems and complex analytical tasks on these data to be developed using the Hadoop/Spark frameworks. Results show that a hybrid workflow for molecular dynamics simulation can provide significant performance improvements over a traditional workflow. Furthermore, code complexity of Hadoop/Spark solutions is shown to be less than that of a traditional solution.",
                "call-number": "10.1145/3219104.3229279",
                "collection-number": "41",
                "collection-title": "PEARC '18",
                "container-title": "Proceedings of the Practice and Experience on Advanced Research Computing",
                "DOI": "10.1145/3219104.3229279",
                "event-place": "Pittsburgh, PA, USA",
                "ISBN": "9781450364461",
                "keyword": "Big Data, Molecular Dynamics Simulation, Apache Hadoop, Apache Spark, HPC",
                "number": "Article 41",
                "number-of-pages": "7",
                "page": "1–7",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Combining HPC and Big Data Infrastructures in Large-Scale Post-Processing of Simulation Data: A Case Study",
                "URL": "https://doi.org/10.1145/3219104.3229279"
            }
        },
        {
            "10.1145/3530050.3532928": {
                "id": "10.1145/3530050.3532928",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Paasche",
                        "given": "Simon"
                    },
                    {
                        "family": "Groppe",
                        "given": "Sven"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2022,
                            6,
                            12
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2022,
                            6,
                            12
                        ]
                    ]
                },
                "abstract": "An essential component of today's industry is data, which is generated during manufacturing. The goal of industry 4.0 is efficient collection, processing and analysis of this data. In our work, we address these three tasks and present an extensible system to solve them. To the best of our knowledge, the combination of a consistency checker (CC) for data preparation and a digital twin (DT) for analysis activities represents a novel approach. Consistency checking in combination with a DT leads to increased data quality, which in turn has a positive effect on analyses, like reducing errors to decrease costs, identifying relevant parameters to increase the productivity, and determining the bottleneck of a manufacturing line for enhanced production planning.",
                "call-number": "10.1145/3530050.3532928",
                "collection-number": "9",
                "collection-title": "BiDEDE '22",
                "container-title": "Proceedings of The International Workshop on Big Data in Emergent Distributed Environments",
                "DOI": "10.1145/3530050.3532928",
                "event-place": "Philadelphia, Pennsylvania",
                "ISBN": "9781450393461",
                "keyword": "consistency checking, industry 4.0, digital twin",
                "number": "Article 9",
                "number-of-pages": "7",
                "page": "1–7",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Enhancing data quality and process optimization for smart manufacturing lines in industry 4.0 scenarios",
                "URL": "https://doi.org/10.1145/3530050.3532928"
            }
        },
        {
            "10.1145/2737909.2737912": {
                "id": "10.1145/2737909.2737912",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Fox",
                        "given": "Geoffrey C."
                    },
                    {
                        "family": "Jha",
                        "given": "Shantenu"
                    },
                    {
                        "family": "Qiu",
                        "given": "Judy"
                    },
                    {
                        "family": "Luckow",
                        "given": "Andre"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2014,
                            10,
                            13
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2014,
                            10,
                            13
                        ]
                    ]
                },
                "abstract": "We study many Big Data applications from a variety of research and commercial areas and suggest a set of characteristic features and possible kernel benchmarks that stress those features for data analytics. We draw conclusions for the hardware and software architectures that are suggested by this analysis.",
                "call-number": "10.1145/2737909.2737912",
                "collection-title": "Beowulf '14",
                "container-title": "Proceedings of the 20 Years of Beowulf Workshop on Honor of Thomas Sterling's 65th Birthday",
                "DOI": "10.1145/2737909.2737912",
                "event-place": "Annapolis, MD, USA",
                "ISBN": "9781450330312",
                "number-of-pages": "10",
                "page": "7–16",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Towards an Understanding of Facets and Exemplars of Big Data Applications",
                "URL": "https://doi.org/10.1145/2737909.2737912"
            }
        },
        {
            "10.1145/2949550": {
                "id": "10.1145/2949550",
                "type": "BOOK",
                "issued": {
                    "date-parts": [
                        [
                            2016
                        ]
                    ]
                },
                "call-number": "10.1145/2949550",
                "container-title-short": "XSEDE16",
                "event-place": "Miami, USA",
                "genre": "proceeding",
                "ISBN": "9781450347556",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Proceedings of the XSEDE16 Conference on Diversity, Big Data, and Science at Scale"
            }
        },
        {
            "10.5555/3382225.3382363": {
                "id": "10.5555/3382225.3382363",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Zhao",
                        "given": "Ying"
                    },
                    {
                        "family": "Zhou",
                        "given": "Charles C."
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            8,
                            28
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            8,
                            28
                        ]
                    ]
                },
                "abstract": "We demonstrate a machine learning and artificial intelligence method, i.e., lexical link analysis (LLA) to discover high-value information from big data. In this paper, high-value information refers to the information that has the potential to grow its value over time. LLA is a unsupervised learning method that does not require manually labeled training data. New value metrics are defined based on a game-theoretic framework for LLA. In this paper, we show the value metrics generated from LLA in a use case of analyzing business news. We show the results from LLA are validated and correlated with the ground truth. We show that by using game theory, the high-value information selected by LLA reaches a Nash equilibrium by superpositioning popular and anomalous information, and at the same time generates high social welfare, therefore, contains higher intrinsic value.",
                "call-number": "10.5555/3382225.3382363",
                "collection-title": "ASONAM '18",
                "container-title": "Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
                "event-place": "Barcelona, Spain",
                "ISBN": "9781538660515",
                "keyword": "pareto superior, high-value, social welfare, big data, lexical link analysis, unsupervised learning, pareto efficient, nash equilibrium, game theory",
                "number-of-pages": "5",
                "page": "621–625",
                "publisher": "IEEE Press",
                "title": "A game-theoretic lexical link analysis for discovering high-value information from big data"
            }
        },
        {
            "10.1145/3429889.3429920": {
                "id": "10.1145/3429889.3429920",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Wu",
                        "given": "Ning"
                    },
                    {
                        "family": "Cao",
                        "given": "Yanping"
                    },
                    {
                        "family": "Chen",
                        "given": "Zhuo"
                    },
                    {
                        "family": "Zhu",
                        "given": "Yifan"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2020,
                            9,
                            11
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2020,
                            9,
                            11
                        ]
                    ]
                },
                "abstract": "Intelligent clinical decision is an important utility of artificial intelligence. At present, most of its algorithm is based on big data or nature language processing. The limitation of such algorithm is discussed and summarized. That clinical decision artificial intelligence should meet the requirements of clinical medicine and artificial intelligence technique is proposed.",
                "call-number": "10.1145/3429889.3429920",
                "collection-title": "ISAIMS 2020",
                "container-title": "Proceedings of the 2020 International Symposium on Artificial Intelligence in Medical Sciences",
                "DOI": "10.1145/3429889.3429920",
                "event-place": "Beijing, China",
                "ISBN": "9781450388603",
                "keyword": "Nature language processing, Big data, Artificial intelligence, Clinical decision, Algorithm t",
                "number-of-pages": "5",
                "page": "161–165",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Limitation of on Big Data or Nature Language Processing based algorithm for Clinical Decision Artificial Intelligence",
                "URL": "https://doi.org/10.1145/3429889.3429920"
            }
        },
        {
            "10.1145/2383276.2383302": {
                "id": "10.1145/2383276.2383302",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Petkov",
                        "given": "Plamen"
                    },
                    {
                        "family": "Helfert",
                        "given": "Markus"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2012,
                            6,
                            22
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2012,
                            6,
                            22
                        ]
                    ]
                },
                "abstract": "Service oriented composition is a prospective approach, which enables flexible and loose composition of applications. Data, on the other hand, is an integral part of service. Despite the huge number of studies that have been done on service oriented environments, very little has been investigated about the data quality. In this paper we examine some problems in service-oriented architecture and in particular problem concerned with data quality. In addition, issues associated with data incorporation and how data consistency is realized through data modelling concepts will be investigated.",
                "call-number": "10.1145/2383276.2383302",
                "collection-title": "CompSysTech '12",
                "container-title": "Proceedings of the 13th International Conference on Computer Systems and Technologies",
                "DOI": "10.1145/2383276.2383302",
                "event-place": "Ruse, Bulgaria",
                "ISBN": "9781450311939",
                "keyword": "service oriented architectures, SoA concepts, data quality issues data inconsistency and data incorporation",
                "number-of-pages": "8",
                "page": "163–170",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Data oriented challenges of service architectures a data quality perspective",
                "URL": "https://doi.org/10.1145/2383276.2383302"
            }
        },
        {
            "10.1145/1449814.1449820": {
                "id": "10.1145/1449814.1449820",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Lowry",
                        "given": "Sonya J."
                    },
                    {
                        "family": "Warner",
                        "given": "Phillip B."
                    },
                    {
                        "family": "Deaubl",
                        "given": "Evan"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2008,
                            10,
                            19
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2008,
                            10,
                            19
                        ]
                    ]
                },
                "abstract": "Due to the natures of legacy astronomical imaging meta data acquisition and storage technologies and techniques at the National Optical Astronomy Observatory, the challenge of methodically improving the quality of such information has been insurmountable until now. The diversity of the sources and lack of cohesive effort along with technologies that enable silo style efforts have contributed to the current, disorganized state of the collection of astronomical imaging information. By meeting these issues with a solution that combines policy and technology support for implementing master data management, use of transformations that enable continued improvements and a history of changes made, and a modular architecture based upon services that are federated over a flexible, robust communications architecture, the NOAO Archive System can assure improvements in the quality of the imaging meta data now and into the future.",
                "call-number": "10.1145/1449814.1449820",
                "collection-title": "OOPSLA Companion '08",
                "container-title": "Companion to the 23rd ACM SIGPLAN conference on Object-oriented programming systems languages and applications",
                "DOI": "10.1145/1449814.1449820",
                "event-place": "Nashville, TN, USA",
                "ISBN": "9781605582207",
                "keyword": "meta data, astronomy, integration, virtual observatory, data quality, soa, transformation, service-oriented architecture, service platform",
                "number-of-pages": "10",
                "page": "675–684",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "NOAO imaging meta data quality improvement: a case study of the evolution of a service oriented system",
                "URL": "https://doi.org/10.1145/1449814.1449820"
            }
        },
        {
            "10.1109/CCGRID.2017.147": {
                "id": "10.1109/CCGRID.2017.147",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Runsewe",
                        "given": "Olubisi"
                    },
                    {
                        "family": "Samaan",
                        "given": "Nancy"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            5,
                            14
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            5,
                            14
                        ]
                    ]
                },
                "abstract": "Recent advancements in technology have led to a deluge of data that require real-time analysis with strict latency constraints. A major challenge, however, is determining the amount of resources required by big data stream processing applications in response to heterogeneous data sources, streaming events, unpredictable data volume and velocity changes. Over-provisioning of resources for peak loads can be wasteful while under-provisioning can have a huge impact on the performance of the streaming applications. The majority of research efforts on resource scaling in the cloud are investigated from the cloud provider's perspective, they focus on web applications and do not consider multiple resource bottlenecks. We aim at analyzing the resource scaling problem from a big data streaming application provider's point of view such that efficient scaling decisions can be made for future resource utilization. This paper proposes a Layered Multi-dimensional Hidden Markov Model (LMD-HMM) for facilitating the management of resource auto-scaling for big data streaming applications in the cloud. Our detailed experimental evaluation shows that LMD-HMM performs best with an accuracy of 98%, outperforming the single-layer hidden markov model.",
                "call-number": "10.1109/CCGRID.2017.147",
                "collection-title": "CCGrid '17",
                "container-title": "Proceedings of the 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing",
                "DOI": "10.1109/CCGRID.2017.147",
                "event-place": "Madrid, Spain",
                "ISBN": "9781509066100",
                "keyword": "Resource Scaling, Stream Processing, Layered Hidden Markov Model, Resource Prediction, Big Data, Cloud Computing",
                "number-of-pages": "10",
                "page": "848–857",
                "publisher": "IEEE Press",
                "title": "Cloud Resource Scaling for Big Data Streaming Applications Using A Layered Multi-dimensional Hidden Markov Model",
                "URL": "https://doi.org/10.1109/CCGRID.2017.147"
            }
        },
        {
            "10.1145/3376897.3379162": {
                "id": "10.1145/3376897.3379162",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Shi",
                        "given": "Laixi"
                    },
                    {
                        "family": "Zhang",
                        "given": "Yue"
                    },
                    {
                        "family": "Pan",
                        "given": "Shijia"
                    },
                    {
                        "family": "Chi",
                        "given": "Yuejie"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2020,
                            3,
                            3
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2020,
                            3,
                            3
                        ]
                    ]
                },
                "abstract": "Floor vibration-based sensing provides an alternative approach for multiple occupant localization, enabling various smart building applications such as elderly care. Prior work mainly focuses on detecting onsets of individual footstep from overlapping signals for localization. However, the error rate is higher than that of single footsteps. In this work, we present a data quality-informed time-sequence approach for accurate multi-people localization. The intuition is that when signals overlap, the overlapping part of the signal has a lower SNR, which can be quantified and used as estimation confidence. We conducted real-world experiments to validate our data quality-informed approach for location estimation.",
                "call-number": "10.1145/3376897.3379162",
                "collection-title": "HotMobile '20",
                "container-title": "Proceedings of the 21st International Workshop on Mobile Computing Systems and Applications",
                "DOI": "10.1145/3376897.3379162",
                "event-place": "Austin, TX, USA",
                "ISBN": "9781450371162",
                "keyword": "tdoa, ambient vibration sensing, data quality quantification, indoor localization, multiple pedestrian localization, signal overlapping",
                "number-of-pages": "1",
                "page": "98",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Data Quality-Informed Multiple Occupant Localization using Floor Vibration Sensing",
                "URL": "https://doi.org/10.1145/3376897.3379162"
            }
        },
        {
            "10.1145/3368640.3368658": {
                "id": "10.1145/3368640.3368658",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Gerakidis",
                        "given": "Sergios"
                    },
                    {
                        "family": "Mamalis",
                        "given": "Basilis"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            11,
                            28
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            11,
                            28
                        ]
                    ]
                },
                "abstract": "Clustering is an efficient data mining as well as machine-learning method when we need to get an insight of the objects of a dataset that could be grouped together. The K-Means algorithm and the Hierarchical Agglomerative Clustering (HAC) algorithm are two of the most known and commonly used methods of clustering; the former due to its low time cost and the latter due to its accuracy. However, even the use of K-Means in document clustering over large-scale collections can lead to unpredictable time costs. In this paper, towards the direction of the efficient handling of big text data, we present a hybrid clustering approach based on a customized version of the Buckshot algorithm, which first applies a hierarchical clustering procedure on a sample of the input dataset and then uses the results as the initial centers for a K-Means based assignment of the remaining documents, with very few iterations. We also give a highly efficient adaptation of the proposed Buckshot-based approach in the MapReduce model which is then experimentally tested using Apache Hadoop over a real cluster environment. As it comes out of the experiments, it leads to acceptable clustering quality as well as to significant execution time improvements. Preliminary results drawn from relevant experiments using the Spark framework are also presented.",
                "call-number": "10.1145/3368640.3368658",
                "collection-title": "PCI '19",
                "container-title": "Proceedings of the 23rd Pan-Hellenic Conference on Informatics",
                "DOI": "10.1145/3368640.3368658",
                "event-place": "Nicosia, Cyprus",
                "ISBN": "9781450372923",
                "keyword": "buckshot algorithm, K-Means, MapReduce, big data, spark, hierarchical agglomerative clustering",
                "number-of-pages": "6",
                "page": "112–117",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Utilizing the buckshot algorithm for efficient big data clustering in the MapReduce model",
                "URL": "https://doi.org/10.1145/3368640.3368658"
            }
        },
        {
            "10.1145/3314527.3314537": {
                "id": "10.1145/3314527.3314537",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Cabanban-Casem",
                        "given": "Christianne Lynnette"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            1,
                            25
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            1,
                            25
                        ]
                    ]
                },
                "abstract": "Education is an important element towards learning and human development, thus, it is the key towards identifying competencies and better productivity for the workforce. As part of the Commission on Higher Education's (CHED) thrust for improving efficiency and effectiveness by simplifying the collection process for all the stakeholders, the developed system will drastically improve the availability of data for making informed decisions and efficient generation of reports.This research outlines opportunities and challenges associated with the implementation and governance of Big Data in higher education through development and implementation of data analytics tool.",
                "call-number": "10.1145/3314527.3314537",
                "collection-title": "APIT 2019",
                "container-title": "Proceedings of the 2019 Asia Pacific Information Technology Conference",
                "DOI": "10.1145/3314527.3314537",
                "event-place": "Jeju Island, Republic of Korea",
                "ISBN": "9781450366212",
                "keyword": "Higher Education Data, Data Science, Knowledge Management",
                "number-of-pages": "4",
                "page": "61–64",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Analytical Visualization of Higher Education Institutions' Big Data for Decision Making",
                "URL": "https://doi.org/10.1145/3314527.3314537"
            }
        },
        {
            "10.1145/3510249.3510256": {
                "id": "10.1145/3510249.3510256",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "ZHOU",
                        "given": "JIANG"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            12,
                            17
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2021,
                            12,
                            17
                        ]
                    ]
                },
                "abstract": "This paper analyzes the industrial operation mode of e-commerce, summarizes the industrial development characteristics under the background of big data, and realizes the issues existing in the industry operation. In addition, through the analysis of various limiting factors, this paper elaborates the e-commerce industrial development mode and strengthen the industrial competitiveness in order to meet the industrial economic development requirements.",
                "call-number": "10.1145/3510249.3510256",
                "collection-title": "EBEE 2021",
                "container-title": "2021 3rd International Conference on E-Business and E-commerce Engineering",
                "DOI": "10.1145/3510249.3510256",
                "event-place": "Sanya, China",
                "ISBN": "9781450387392",
                "number-of-pages": "3",
                "page": "37–39",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "E-commerce Industrial Development Analysis from the Perspective of Big Data",
                "URL": "https://doi.org/10.1145/3510249.3510256"
            }
        },
        {
            "10.1145/3156346.3156694": {
                "id": "10.1145/3156346.3156694",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Bandeira",
                        "given": "Nuno"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            12,
                            7
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            12,
                            7
                        ]
                    ]
                },
                "abstract": "Translating the growing volumes of proteomics mass spectrometry data into reusable evidence of the occurrence and provenance of proteomics events requires the development of novel algorithms and community-scale computational workflows. MassIVE (http://massive.ucsd.edu) proposes to address this challenge in three stages.First, systematic annotation of human proteomics big data requires automated reanalysis of all public data using open source workflows with detailed records of search parameters and of individual Peptide Spectrum Matches (PSMs). As such, our large-scale reanalysis of tens of terabytes of human data has now increased the total number of proper public PSMs by over 10-fold to over 320 million PSMs whose coverage includes over 95Second, proper synthesis of community-scale search results into a reusable knowledge base (KB) requires scalable workflows imposing strict statistical controls. Our MassIVE-KB spectral library has thus properly assembled 2+ million precursors from over 1.5 million peptides covering over 6.2 million amino acids in the human proteome, all of which at least double the numbers covered by the popular NIST spectral libraries. Moreover, MassIVE-KB detects 723 novel proteins (PE 2-5) for a total of 16,852 proteins observed in non-synthetic LCMS runs and 19,610 total proteins when including the recent ProteomeTools data.Third, we show how advanced identification algorithms combine with public data to reveal dozens of unexpected putative modifications supported by multiple highly-correlated spectra. These show that protein regions can be observed in over 100 different variants with various combinations of post-translational modifications and cleavage events, thus suggesting that current coverage of proteome diversity (at 1.3 variants per protein region) is far below what is observable in experimental data.",
                "call-number": "10.1145/3156346.3156694",
                "collection-title": "CSBio '17",
                "container-title": "Proceedings of the 8th International Conference on Computational Systems-Biology and Bioinformatics",
                "DOI": "10.1145/3156346.3156694",
                "event-place": "Nha Trang City, Viet Nam",
                "ISBN": "9781450353502",
                "number-of-pages": "1",
                "page": "2",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Revealing deep proteome diversity with community-scale proteomics big data",
                "URL": "https://doi.org/10.1145/3156346.3156694"
            }
        },
        {
            "10.1145/3482632.3482684": {
                "id": "10.1145/3482632.3482684",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Cai",
                        "given": "Lei"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            9,
                            24
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2021,
                            9,
                            24
                        ]
                    ]
                },
                "abstract": "The English education platform overcomes the limitations of traditional education on both sides of English teaching in terms of time, space and instant interaction, making learners more flexible and free in terms of learning content, learning form, learning time, and learning location. Fully mobilize the enthusiasm of learners. The research purpose of this article is to build and apply English education platform based on big data, mainly introduces the application research of data mining technology in assisting teachers to make decisions and analyze learner information. In view of the problem of the scattered information of the English education platform and it is difficult to unify, firstly, it is proposed to use the statistical analysis module to comprehensively grasp the learner's activities, which is conducive to the teacher's overall grasp of the learner's learning situation, and also helps the teacher understand the learning situation of this course. The application process of association rules in the analysis of learners' basic information is described in detail, and some potential associations hidden in students' basic information and student activities are discovered through association rules mining, so as to facilitate teachers to grasp the learning situation of learners and make appropriate Formative evaluation. On the basis of the system design, the main technology of the development platform of the English education platform based on big data is analyzed and selected, and the system implementation of the big data infrastructure module is introduced with emphasis, and the test results are analyzed. The experimental results show that under the condition of 800G data volume, the current system data loss rate is within 0.3%, which can ensure the accuracy of data analysis and meet the stability requirements.",
                "call-number": "10.1145/3482632.3482684",
                "collection-title": "ICISCAE 2021",
                "container-title": "2021 4th International Conference on Information Systems and Computer Aided Education",
                "DOI": "10.1145/3482632.3482684",
                "event-place": "Dalian, China",
                "ISBN": "9781450390255",
                "number-of-pages": "4",
                "page": "250–253",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Construction and Application of English Education Platform Based on Big Data",
                "URL": "https://doi.org/10.1145/3482632.3482684"
            }
        },
        {
            "10.1145/2939672.2939730": {
                "id": "10.1145/2939672.2939730",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Li",
                        "given": "Xiang"
                    },
                    {
                        "family": "Makkie",
                        "given": "Milad"
                    },
                    {
                        "family": "Lin",
                        "given": "Binbin"
                    },
                    {
                        "family": "Sedigh Fazli",
                        "given": "Mojtaba"
                    },
                    {
                        "family": "Davidson",
                        "given": "Ian"
                    },
                    {
                        "family": "Ye",
                        "given": "Jieping"
                    },
                    {
                        "family": "Liu",
                        "given": "Tianming"
                    },
                    {
                        "family": "Quinn",
                        "given": "Shannon"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            8,
                            13
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2016,
                            8,
                            13
                        ]
                    ]
                },
                "abstract": "It has been shown from various functional neuroimaging studies that sparsity-regularized dictionary learning could achieve superior performance in decomposing comprehensive and neuroscientifically meaningful functional networks from massive fMRI signals. However, the computational cost for solving the dictionary learning problem has been known to be very demanding, especially when dealing with large-scale data sets. Thus in this work, we propose a novel distributed rank-1 dictionary learning (D-r1DL) model and apply it for fMRI big data analysis. The model estimates one rank-1 basis vector with sparsity constraint on its loading coefficient from the input data at each learning step through alternating least squares updates. By iteratively learning the rank-1 basis and deflating the input data at each step, the model is then capable of decomposing the whole set of functional networks. We implement and parallelize the rank-1 dictionary learning algorithm using Spark engine and deployed the resilient distributed dataset (RDDs) abstracts for the data distribution and operations. Experimental results from applying the model on the Human Connectome Project (HCP) data show that the proposed D-r1DL model is efficient and scalable towards fMRI big data analytics, thus enabling data-driven neuroscientific discovery from massive fMRI big data in the future.",
                "call-number": "10.1145/2939672.2939730",
                "collection-title": "KDD '16",
                "container-title": "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
                "DOI": "10.1145/2939672.2939730",
                "event-place": "San Francisco, California, USA",
                "ISBN": "9781450342322",
                "keyword": "sparse coding, algorithm parallelization, distributed computation, fMRI",
                "number-of-pages": "9",
                "page": "511–519",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Scalable Fast Rank-1 Dictionary Learning for fMRI Big Data Analysis",
                "URL": "https://doi.org/10.1145/2939672.2939730"
            }
        },
        {
            "10.1145/3430665.3456325": {
                "id": "10.1145/3430665.3456325",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Graux",
                        "given": "Damien"
                    },
                    {
                        "family": "Janev",
                        "given": "Valentina"
                    },
                    {
                        "family": "Jabeen",
                        "given": "Hajira"
                    },
                    {
                        "family": "Sallinger",
                        "given": "Emanuel"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            6,
                            26
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2021,
                            6,
                            26
                        ]
                    ]
                },
                "abstract": "Big Data Analytics is a crucial component of the Big Data paradigm and deals with the extraction of knowledge from the enormous amount of data. As the number of Big Data related methods, tools, frameworks, and solutions is growing, there is a need to systematize the knowledge about the domain. Moreover, as this domain is rapidly involving, it is difficult to keep up to date with its latest and most efficient technologies; and this is thereby especially challenging for countries which have so far suffered from a lack of infrastructure in Big Data.In this article, we report on the deployment of a strategy we design, fostering the knowledge and awareness around the challenges of Big Data analytics in the West Balkan region. In particular, we describe how we joined forces across multiple European institutions in order to design bespoke actions (from, e.g. classes, to student exchanges) to be applied in Serbia and the West Balkan region over several years.",
                "call-number": "10.1145/3430665.3456325",
                "collection-title": "ITiCSE '21",
                "container-title": "Proceedings of the 26th ACM Conference on Innovation and Technology in Computer Science Education V. 1",
                "DOI": "10.1145/3430665.3456325",
                "event-place": "Virtual Event, Germany",
                "ISBN": "9781450382144",
                "keyword": "teaching big data analytics, European collaboration, west Balkan region, summer school",
                "number-of-pages": "7",
                "page": "491–497",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Deploying a Strategy to Unlock Big Data Research and Teaching Activities in the West Balkan Region",
                "URL": "https://doi.org/10.1145/3430665.3456325"
            }
        },
        {
            "10.1145/3448016.3457568": {
                "id": "10.1145/3448016.3457568",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Negi",
                        "given": "Parimarjan"
                    },
                    {
                        "family": "Interlandi",
                        "given": "Matteo"
                    },
                    {
                        "family": "Marcus",
                        "given": "Ryan"
                    },
                    {
                        "family": "Alizadeh",
                        "given": "Mohammad"
                    },
                    {
                        "family": "Kraska",
                        "given": "Tim"
                    },
                    {
                        "family": "Friedman",
                        "given": "Marc"
                    },
                    {
                        "family": "Jindal",
                        "given": "Alekh"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            6,
                            9
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2021,
                            6,
                            9
                        ]
                    ]
                },
                "abstract": "In recent years, there has been tremendous interest in research that applies machine learning to database systems. Being one of the most complex components of a DBMS, query optimizers could benefit from adaptive policies that are learned systematically from the data and the query workload. Recent research has brought up novel ideas towards a learned query optimizer, however these ideas have not been evaluated on a commercial query processor or on large scale, real-world workloads. In this paper, we take the approach used by Marcus et al. in Bao and adapt it to SCOPE, a big data system used internally at Microsoft. Along the way, we solve multiple new challenges: we define how optimizer rules affect final query plans by introducing the concept of a rule signature, we devise a pipeline computing interesting rule configurations for recurring jobs, and we define a new learning problem allowing us to apply such interesting rule configurations to previously unseen jobs. We evaluate the efficacy of the approach on production workloads that include 150K daily jobs. Our results show that alternative rule configurations can generate plans with lower costs, and this can translate to runtime latency savings of 7-30% on average and up to 90% for a non trivial subset of the workload.",
                "call-number": "10.1145/3448016.3457568",
                "collection-title": "SIGMOD '21",
                "container-title": "Proceedings of the 2021 International Conference on Management of Data",
                "DOI": "10.1145/3448016.3457568",
                "event-place": "Virtual Event, China",
                "ISBN": "9781450383431",
                "keyword": "distributed query optimization, learning for query optimization",
                "number-of-pages": "13",
                "page": "2557–2569",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Steering Query Optimizers: A Practical Take on Big Data Workloads",
                "URL": "https://doi.org/10.1145/3448016.3457568"
            }
        },
        {
            "10.1145/1014052.1016916": {
                "id": "10.1145/1014052.1016916",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Davidson",
                        "given": "Ian"
                    },
                    {
                        "family": "Grover",
                        "given": "Ashish"
                    },
                    {
                        "family": "Satyanarayana",
                        "given": "Ashwin"
                    },
                    {
                        "family": "Tayi",
                        "given": "Giri K."
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2004,
                            8,
                            22
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2004,
                            8,
                            22
                        ]
                    ]
                },
                "abstract": "Data quality is a central issue for many information-oriented organizations. Recent advances in the data quality field reflect the view that a database is the product of a manufacturing process. While routine errors, such as non-existent zip codes, can be detected and corrected using traditional data cleansing tools, many errors systemic to the manufacturing process cannot be addressed. Therefore, the product of the data manufacturing process is an imprecise recording of information about the entities of interest (i.e. customers, transactions or assets). In this way, the database is only one (flawed) version of the entities it is supposed to represent. Quality assurance systems such as Motorola's Six-Sigma and other continuous improvement methods document the data manufacturing process's shortcomings. A widespread method of documentation is quality matrices. In this paper, we explore the use of the readily available data quality matrices for the data mining classification task. We first illustrate that if we do not factor in these quality matrices, then our results for prediction are sub-optimal. We then suggest a general-purpose ensemble approach that perturbs the data according to these quality matrices to improve the predictive accuracy and show the improvement is due to a reduction in variance.",
                "call-number": "10.1145/1014052.1016916",
                "collection-title": "KDD '04",
                "container-title": "Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining",
                "DOI": "10.1145/1014052.1016916",
                "event-place": "Seattle, WA, USA",
                "ISBN": "1581138881",
                "keyword": "decision trees, data quality, six-sigma, ensemble approaches, classification",
                "number-of-pages": "5",
                "page": "794–798",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "A general approach to incorporate data quality matrices into data mining algorithms",
                "URL": "https://doi.org/10.1145/1014052.1016916"
            }
        },
        {
            "10.1145/3148055.3148072": {
                "id": "10.1145/3148055.3148072",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Abdullah",
                        "given": "Tariq"
                    },
                    {
                        "family": "Ahmet",
                        "given": "Ahmed"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            12,
                            5
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            12,
                            5
                        ]
                    ]
                },
                "abstract": "Abstract Genomics data is unstructured and mostly stored on hard disks. It is both technically and culturally residing in big data domain due to the challenges of volume, velocity and variety. Huge volumes of data are generated from diverse sources in different formats and at a high frequency. Appropriate data models are required to accommodate these data formats for analysing and producing required results with a quick response time. Genomics data can be analysed for a variety of purposes. Existing genomics data analysis pipelines are disk I/O intensive and focus on optimizing data processing for individual analysis tasks. Intensive disk I/O operations and focus on optimizing individual analysis tasks are the biggest bottleneck of existing genomics analysis pipelines. Making any updates in genomics data require reading the whole data set again. In this paper, we present a genomics data analysis framework that addresses both the issues of existing genomics analysis pipelines. It reads unstructured genomics data from sources, transforms it in a structured format and stores this data into a NoSQL database. In this way, genomics data can be queried like any other data and an update in the genomics data does not require reading the whole data set. The framework also presents an efficient analysis pipeline for analysing the genomics data for a variety of purposes like genotype clustering, gene expression microarrays, chromosome variations or gene linkage analysis. A case study of genotype clustering is presented to demonstrate and evaluate the effectiveness of the presented framework. Our results show that the framework improves overall performance of the genomics data analysis pipeline by 49% from existing genomics data analysis pipelines. Furthermore, our approach is robust and is able sustain high performance with high system workloads.",
                "call-number": "10.1145/3148055.3148072",
                "collection-title": "BDCAT '17",
                "container-title": "Proceedings of the Fourth IEEE/ACM International Conference on Big Data Computing, Applications and Technologies",
                "DOI": "10.1145/3148055.3148072",
                "event-place": "Austin, Texas, USA",
                "ISBN": "9781450355490",
                "keyword": "in-memory computing, machine learning, population scale clustering, resource management, big data, algorithms, compute cluster, data analysis",
                "number-of-pages": "9",
                "page": "189–197",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Genomics Analyser: A Big Data Framework for Analysing Genomics Data",
                "URL": "https://doi.org/10.1145/3148055.3148072"
            }
        },
        {
            "10.1145/3012004": {
                "id": "10.1145/3012004",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Becker",
                        "given": "Christoph"
                    },
                    {
                        "family": "Duretec",
                        "given": "Kresimir"
                    },
                    {
                        "family": "Rauber",
                        "given": "Andreas"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            1,
                            4
                        ]
                    ]
                },
                "call-number": "10.1145/3012004",
                "collection-number": "7",
                "container-title": "J. Data and Information Quality",
                "DOI": "10.1145/3012004",
                "ISSN": "1936-1955",
                "issue": "2",
                "keyword": "Benchmarking, test oracle, data formats, model-based testing, quality model, ground truth, digital curation, digital preservation, data processing, data quality, test data",
                "number": "Article 7",
                "number-of-pages": "4",
                "page": "1–4",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "February 2017",
                "title": "The Challenge of Test Data Quality in Data Processing",
                "URL": "https://doi.org/10.1145/3012004",
                "volume": "8"
            }
        },
        {
            "10.1145/3495018.3495311": {
                "id": "10.1145/3495018.3495311",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Jin",
                        "given": "Ziheng"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            10,
                            23
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2021,
                            10,
                            23
                        ]
                    ]
                },
                "abstract": "NSAWS is an intelligent and real-time large database management system. By analyzing the user identity data and access rights contained in the collected information, the NSAWS finds out the potential risks and issues an alarm notice in a timely manner. This paper mainly studies how to strengthen the prevention of network attacks in BD environment from the following aspects. This paper first introduces the common technology on the Internet in our country and its application status; secondly, it expounds the architecture, deployment mode and operation mode of the large database system based on the basic security facilities such as cloud computing platform and firewall. Then the traditional NSAWS is analyzed and the simulation platform is tested. The results show that the platform has high accuracy and stability in alerting network security hazards and can effectively protect network security.",
                "call-number": "10.1145/3495018.3495311",
                "collection-title": "AIAM2021",
                "container-title": "2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture",
                "DOI": "10.1145/3495018.3495311",
                "event-place": "Manchester, United Kingdom",
                "ISBN": "9781450385046",
                "number-of-pages": "5",
                "page": "954–958",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Analysis on NSAW Reminder Based on Big Data Technology",
                "URL": "https://doi.org/10.1145/3495018.3495311"
            }
        },
        {
            "10.1145/3018896.3018950": {
                "id": "10.1145/3018896.3018950",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Cheraghchi",
                        "given": "Fatemeh"
                    },
                    {
                        "family": "Iranzad",
                        "given": "Arash"
                    },
                    {
                        "family": "Raahemi",
                        "given": "Bijan"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            3,
                            22
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            3,
                            22
                        ]
                    ]
                },
                "abstract": "In high-dimensional space with large amounts of data, distances between data points tend to become relatively uniform. The notion of the nearest neighbours of a data point thus becomes meaningless, a phenomenon known as \"curse of dimensionality.\" Identifying outliers (data points with statistical characteristics significantly different than the majority of the data) in such a high-dimensional space can be a significant challenge. Mining for outliers in subspaces with relevant attributes is one of approaches for this problem, and identifying these attributes is the main objective of this work. In this paper, we scale a grid-based solution to search for subspaces that are candidates for outlier detection with regard to the subset of features in the subspace. We specify a population and a fitness function for a distributed genetic algorithm to heuristically search the subspaces within the high dimensional data, and find the subspace with maximal sparsity. We designed and implemented our proposed subspace selection algorithm in Apache Spark, a fast in-memory engine for large-scale data processing. The initial experimental results on a large dataset (77,000 records and 1,379 attributes) confirm that our proposed method can identify the most relevant subspaces for outlier detection.",
                "call-number": "10.1145/3018896.3018950",
                "collection-number": "54",
                "collection-title": "ICC '17",
                "container-title": "Proceedings of the Second International Conference on Internet of things, Data and Cloud Computing",
                "DOI": "10.1145/3018896.3018950",
                "event-place": "Cambridge, United Kingdom",
                "ISBN": "9781450347747",
                "keyword": "subspace selection, outlier detection, high-dimensional, big data, apache spark, genetic algorithm",
                "number": "Article 54",
                "number-of-pages": "7",
                "page": "1–7",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Subspace selection in high-dimensional big data using genetic algorithm in apache spark",
                "URL": "https://doi.org/10.1145/3018896.3018950"
            }
        },
        {
            "10.1145/2934466.2934476": {
                "id": "10.1145/2934466.2934476",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Eichelberger",
                        "given": "Holger"
                    },
                    {
                        "family": "Qin",
                        "given": "Cui"
                    },
                    {
                        "family": "Sizonenko",
                        "given": "Roman"
                    },
                    {
                        "family": "Schmid",
                        "given": "Klaus"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            9,
                            16
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2016,
                            9,
                            16
                        ]
                    ]
                },
                "abstract": "Creating product lines of Big Data stream processing applications introduces a number of novel challenges to variability modeling. In this paper, we discuss these challenges and demonstrate how advanced variability modeling capabilities can be used to directly model the topology of processing pipelines as well as their variability. We also show how such processing pipelines can be modeled, configured and validated using the Integrated Variability Modeling Language (IVML).",
                "call-number": "10.1145/2934466.2934476",
                "collection-title": "SPLC '16",
                "container-title": "Proceedings of the 20th International Systems and Software Product Line Conference",
                "DOI": "10.1145/2934466.2934476",
                "event-place": "Beijing, China",
                "ISBN": "9781450340502",
                "keyword": "topologies, variability modeling, software product lines",
                "number-of-pages": "5",
                "page": "204–208",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Using IVML to model the topology of big data processing pipelines",
                "URL": "https://doi.org/10.1145/2934466.2934476"
            }
        },
        {
            "10.1145/3102254.3102272": {
                "id": "10.1145/3102254.3102272",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Weichselbraun",
                        "given": "Albert"
                    },
                    {
                        "family": "Kuntschik",
                        "given": "Philipp"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            6,
                            19
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            6,
                            19
                        ]
                    ]
                },
                "abstract": "Advances in research areas such as named entity linking and sentiment analysis have triggered the emergence of knowledge-intensive information extraction methods that combine classical information extraction with background knowledge from the Web. Despite data quality concerns, linked data sources such as DBpedia, GeoNames and Wikidata which encode facts in a standardized structured format are particularly attractive for such applications.This paper addresses the problem of data quality by introducing a framework that elaborates on linked data quality issues relevant to different stages of the background knowledge acquisition process, their impact on information extraction performance and applicable mitigation strategies. Applying this framework to named entity linking and data enrichment demonstrates the potential of the introduced mitigation strategies to lessen the impact of different kinds of data quality problems. An industrial use case that aims at the automatic generation of image metadata from image descriptions illustrates the successful deployment of knowledge-intensive information extraction in real-world applications and constraints introduced by data quality concerns.",
                "call-number": "10.1145/3102254.3102272",
                "collection-number": "17",
                "collection-title": "WIMS '17",
                "container-title": "Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics",
                "DOI": "10.1145/3102254.3102272",
                "event-place": "Amantea, Italy",
                "ISBN": "9781450352253",
                "keyword": "mitigation strategies, information extraction, applications, linked data quality, named entity linking, semantic technologies",
                "number": "Article 17",
                "number-of-pages": "12",
                "page": "1–12",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Mitigating linked data quality issues in knowledge-intense information extraction methods",
                "URL": "https://doi.org/10.1145/3102254.3102272"
            }
        },
        {
            "10.1145/2614512.2614518": {
                "id": "10.1145/2614512.2614518",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Topi",
                        "given": "Heikki"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2014,
                            6,
                            1
                        ]
                    ]
                },
                "call-number": "10.1145/2614512.2614518",
                "container-title": "ACM Inroads",
                "DOI": "10.1145/2614512.2614518",
                "ISSN": "2153-2184",
                "issue": "2",
                "number-of-pages": "2",
                "page": "24–25",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "June 2014",
                "title": "Learning to think about broader implications of big data",
                "URL": "https://doi.org/10.1145/2614512.2614518",
                "volume": "5"
            }
        },
        {
            "10.1145/3209281.3209300": {
                "id": "10.1145/3209281.3209300",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Chotvijit",
                        "given": "Sarunkorn"
                    },
                    {
                        "family": "Thiarai",
                        "given": "Malkiat"
                    },
                    {
                        "family": "Jarvis",
                        "given": "Stephen"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            5,
                            30
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            5,
                            30
                        ]
                    ]
                },
                "abstract": "There is significant national interest in tackling issues surrounding the needs of vulnerable children and adults. At the same time, UK local authorities face severe financial challenges as a result of decreasing financial settlements and increasing demands from growing urban populations. This research employs state-of-the-art data analytics and visualisation techniques to analyse six years of local government social care data for the city of Birmingham, the UK's second most populated city. This analysis identifies: (i) service cost profiles over time; (ii) geographical dimensions to service demand and delivery; (iii) patterns in the provision of services, and (iv) the extent to which data value and data protection interact. The research accesses data held by the local authority to discover patterns and insights that may assist in the understanding of service demand, support decision making and resource management, whilst protecting and safeguarding its most vulnerable citizens. The use of data in this manner could also inform the approach a local authority has to its data, its capture and use, and the potential for supporting data-led management and service improvements.",
                "call-number": "10.1145/3209281.3209300",
                "collection-number": "5",
                "collection-title": "dg.o '18",
                "container-title": "Proceedings of the 19th Annual International Conference on Digital Government Research: Governance in the Data Age",
                "DOI": "10.1145/3209281.3209300",
                "event-place": "Delft, The Netherlands",
                "ISBN": "9781450365260",
                "keyword": "spatio-temporal analysis, social care, service provision, data analytics, local authority, Birmingham",
                "number": "Article 5",
                "number-of-pages": "8",
                "page": "1–8",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Big data analytics in social care provision: spatial and temporal evidence from Birmingham",
                "URL": "https://doi.org/10.1145/3209281.3209300"
            }
        },
        {
            "10.1145/2345316.2345327": {
                "id": "10.1145/2345316.2345327",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Percivall",
                        "given": "George"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2012,
                            7,
                            1
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2012,
                            7,
                            1
                        ]
                    ]
                },
                "abstract": "What happens when you have connected sensors in everyone's pockets, homes, vehicles, workspaces, street corners, shopping areas, and more? With the convergence of Mobile Computing, the Internet of Things (IoT), and the ability to gather and analyze this Big Data, the availability of massive amounts of information will continue to be gathered and you can expect the unexpected to happen.The themes of this panel are driving development in information technology, but what is the intersection with geospatial? Location determination and use of location for context are core capabilities of Mobile and IoT. Knowing your location along with nearby Points of Interest (PoIs) and Indoor maps provide a new level of spatial awareness and decision making. This information will be used and viewed in new ways including Augmented Reality (AR). Social computing with geospatial checkins provides a rich picture of the social environment. With embedded computing becoming even more ubiquitous, Sensor Webs will provide opportunistic sensing of the physical environment. Geospatial filtering is one of the most effective methods to extracting information from these big data streams. These streams will continue to grow, e.g., mobile 3D video at incredibly high resolution. Data Fusion to combine multiple data sources will create new capabilities many based on geospatial processing.How can we realize the full potential of these technological capabilities in regards to geospatial? We can envision a lot of upside with the technology, but at what cost to privacy and rights? How should policy, privacy and rights be included in the conversations and deployments of these technologies and the resultant data? What role will ambient and participatory crowdsourcing play? A goal of our technology development must be to reduce the apparent tradeoff between surveillance for public safety vs. interests and rights of people. Technology development will continue to be a social activity based on geospatial APIs and standards for mobile platforms from organizations like W3C, OGC, IETF, and OMA. Development of these technologies are a basis for the critical outcomes, e.g, in creating Smart Cities including Smart Energy. Crowdsourcing from mobile platforms and M2M-based sensors webs will provide a basis for humanity to better understand our world and make critical decisions about the livability of our future..",
                "call-number": "10.1145/2345316.2345327",
                "collection-number": "8",
                "collection-title": "COM.Geo '12",
                "container-title": "Proceedings of the 3rd International Conference on Computing for Geospatial Research and Applications",
                "DOI": "10.1145/2345316.2345327",
                "event-place": "Washington, D.C., USA",
                "ISBN": "9781450311137",
                "number": "Article 8",
                "number-of-pages": "1",
                "page": "1",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Realizing the geospatial potential of mobile, IoT and big data",
                "URL": "https://doi.org/10.1145/2345316.2345327"
            }
        },
        {
            "10.1145/3465631.3465969": {
                "id": "10.1145/3465631.3465969",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Wang",
                        "given": "Xuan-Yao"
                    },
                    {
                        "family": "Xiao",
                        "given": "Wei"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            8,
                            19
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2021,
                            8,
                            19
                        ]
                    ]
                },
                "abstract": "NOTICE OF RETRACTION: While investigating potential publication-related misconduct in connection with the ICIMTech 2021 Conference Proceedings, serious concerns were raised that cast doubt on the integrity of the peer-review process and all papers published in the Proceedings of this Conference. The integrity of the entire Conference has been called into question. As a result, of its investigation, ACM has decided to retract the Entire Conference Proceedings and all related papers from the ACM Digital Library.None of the papers from this Proceeding should be cited in the literature because of the questionable integrity of the peer review process for this Conference.",
                "call-number": "10.1145/3465631.3465969",
                "collection-number": "296",
                "collection-title": "ICIMTECH 21",
                "container-title": "The Sixth International Conference on Information Management and Technology",
                "DOI": "10.1145/3465631.3465969",
                "event-place": "Jakarta, Indonesia",
                "ISBN": "9781450385015",
                "number": "Article 296",
                "number-of-pages": "4",
                "page": "1–4",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Application of Video Big Data in Technical Art Training",
                "URL": "https://doi.org/10.1145/3465631.3465969"
            }
        },
        {
            "10.1145/2768830": {
                "id": "10.1145/2768830",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Cui",
                        "given": "Licong"
                    },
                    {
                        "family": "Tao",
                        "given": "Shiqiang"
                    },
                    {
                        "family": "Zhang",
                        "given": "Guo-Qiang"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            5,
                            24
                        ]
                    ]
                },
                "abstract": "This article presents recent progresses made in using scalable cloud computing environment, Hadoop and MapReduce, to perform ontology quality assurance (OQA), and points to areas of future opportunity. The standard sequential approach used for implementing OQA methods can take weeks if not months for exhaustive analyses for large biomedical ontological systems. With OQA methods newly implemented using massively parallel algorithms in the MapReduce framework, several orders of magnitude in speed-up can be achieved (e.g., from three months to three hours). Such dramatically reduced time makes it feasible not only to perform exhaustive structural analysis of large ontological hierarchies, but also to systematically track structural changes between versions for evolutional analysis. As an exemplar, progress is reported in using MapReduce to perform evolutional analysis and visualization on the Systemized Nomenclature of Medicine—Clinical Terms (SNOMED CT), a prominent clinical terminology system. Future opportunities in three areas are described: one is to extend the scope of MapReduce-based approach to existing OQA methods, especially for automated exhaustive structural analysis. The second is to apply our proposed MapReduce Pipeline for Lattice-based Evaluation (MaPLE) approach, demonstrated as an exemplar method for SNOMED CT, to other biomedical ontologies. The third area is to develop interfaces for reviewing results obtained by OQA methods and for visualizing ontological alignment and evolution, which can also take advantage of cloud computing technology to systematically pre-compute computationally intensive jobs in order to increase performance during user interactions with the visualization interface. Advances in these directions are expected to better support the ontological engineering lifecycle.",
                "call-number": "10.1145/2768830",
                "collection-number": "41",
                "container-title": "ACM Trans. Knowl. Discov. Data",
                "DOI": "10.1145/2768830",
                "ISSN": "1556-4681",
                "issue": "4",
                "keyword": "SNOMED CT, Hadoop, terminology quality assurance, lattice",
                "number": "Article 41",
                "number-of-pages": "28",
                "page": "1–28",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "July 2016",
                "title": "Biomedical Ontology Quality Assurance Using a Big Data Approach",
                "URL": "https://doi.org/10.1145/2768830",
                "volume": "10"
            }
        },
        {
            "10.5555/2627817.2627920": {
                "id": "10.5555/2627817.2627920",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Feldman",
                        "given": "Dan"
                    },
                    {
                        "family": "Schmidt",
                        "given": "Melanie"
                    },
                    {
                        "family": "Sohler",
                        "given": "Christian"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2013,
                            1,
                            6
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2013,
                            1,
                            6
                        ]
                    ]
                },
                "abstract": "We prove that the sum of the squared Euclidean distances from the n rows of an n x d matrix A to any compact set that is spanned by k vectors in @@@@d can be approximated up to (1 + ε)-factor, for an arbitrary small ε > 0, using the O(k/ε2)-rank approximation of A and a constant. This implies, for example, that the optimal k-means clustering of the rows of A is (1 + ε)-approximated by an optimal k-means clustering of their projection on the O(k/ε2) first right singular vectors (principle components) of A.A (j, k)-coreset for projective clustering is a small set of points that yields a (1 + ε)-approximation to the sum of squared distances from the n rows of A to any set of k affine subspaces, each of dimension at most j. Our embedding yields (0, k)-coresets of size O(k) for handling k-means queries, (j, 1)-coresets of size O(j) for PCA queries, and (j, k)-coresets of size (log n)O(jk) for any j, k ≥ 1 and constant ε ε (0, 1/2). Previous coresets usually have a size which is linearly or even exponentially dependent of d, which makes them useless when d ~ n.Using our coresets with the merge-and-reduce approach, we obtain embarrassingly parallel streaming algorithms for problems such as k-means, PCA and projective clustering. These algorithms use update time per point and memory that is polynomial in log n and only linear in d.For cost functions other than squared Euclidean distances we suggest a simple recursive coreset construction that produces coresets of size @@@@ for k-means and a special class of bregman divergences that is less dependent on the properties of the squared Euclidean distance.",
                "call-number": "10.5555/2627817.2627920",
                "collection-title": "SODA '13",
                "container-title": "Proceedings of the twenty-fourth annual ACM-SIAM symposium on Discrete algorithms",
                "event-place": "New Orleans, Louisiana",
                "ISBN": "9781611972511",
                "number-of-pages": "20",
                "page": "1434–1453",
                "publisher": "Society for Industrial and Applied Mathematics",
                "publisher-place": "USA",
                "title": "Turning big data into tiny data: constant-size coresets for k-means, PCA and projective clustering"
            }
        },
        {
            "10.14778/3368289.3368292": {
                "id": "10.14778/3368289.3368292",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Kandula",
                        "given": "Srikanth"
                    },
                    {
                        "family": "Orr",
                        "given": "Laurel"
                    },
                    {
                        "family": "Chaudhuri",
                        "given": "Surajit"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            11,
                            1
                        ]
                    ]
                },
                "abstract": "Using data statistics, we convert predicates on a table into data induced predicates (diPs) that apply on the joining tables. Doing so substantially speeds up multi-relation queries because the benefits of predicate pushdown can now apply beyond just the tables that have predicates. We use diPs to skip data exclusively during query optimization; i.e., diPs lead to better plans and have no overhead during query execution. We study how to apply diPs for complex query expressions and how the usefulness of diPs varies with the data statistics used to construct diPs and the data distributions. Our results show that building diPs using zone-maps which are already maintained in today's clusters leads to sizable data skipping gains. Using a new (slightly larger) statistic, 50% of the queries in the TPC-H, TPC-DS and JoinOrder benchmarks can skip at least 33% of the query input. Consequently, the median query in a production big-data cluster finishes roughly 2x faster.",
                "call-number": "10.14778/3368289.3368292",
                "container-title": "Proc. VLDB Endow.",
                "DOI": "10.14778/3368289.3368292",
                "ISSN": "2150-8097",
                "issue": "3",
                "number-of-pages": "14",
                "page": "252–265",
                "publisher": "VLDB Endowment",
                "source": "November 2019",
                "title": "Pushing data-induced predicates through joins in big-data clusters",
                "URL": "https://doi.org/10.14778/3368289.3368292",
                "volume": "13"
            }
        },
        {
            "10.1145/3510858.3510932": {
                "id": "10.1145/3510858.3510932",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Guo",
                        "given": "Jiang"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            12,
                            18
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2021,
                            12,
                            18
                        ]
                    ]
                },
                "abstract": "In today's era of big data, the application of big data has quietly changed the strategic layout of traditional business, and gradually integrated business intelligence and management ideas, improving the management level for all sides of the society. This paper mainly studies the construction of financial information management system based on big data analysis technology. This paper first analyzes the combined application of big data technology and financial information management, and uses big data technology and information technology to build a financial budget management system. Through the construction and use of the financial budget management system, it analyzes the role of big data and information technology in financial information management. Finally, the conclusion is drawn that the financial information system based on big data management is the future direction of the reform to adapt to the financial management mode of enterprises and governments. It is an important way to improve the quality and efficiency of financial information delivery, and it is also a means to improve the comprehensive security management level of enterprises.",
                "call-number": "10.1145/3510858.3510932",
                "collection-title": "ICASIT 2021",
                "container-title": "2021 International Conference on Aviation Safety and Information Technology",
                "DOI": "10.1145/3510858.3510932",
                "event-place": "Changsha, China",
                "ISBN": "9781450390422",
                "number-of-pages": "5",
                "page": "213–217",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Construction of Financial Information Management System Based on Big Data Analysis Technology",
                "URL": "https://doi.org/10.1145/3510858.3510932"
            }
        },
        {
            "10.1145/3361242.3361260": {
                "id": "10.1145/3361242.3361260",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "He",
                        "given": "Tianxing"
                    },
                    {
                        "family": "Yu",
                        "given": "Shengcheng"
                    },
                    {
                        "family": "Wang",
                        "given": "Ziyuan"
                    },
                    {
                        "family": "Li",
                        "given": "Jieqiong"
                    },
                    {
                        "family": "Chen",
                        "given": "Zhenyu"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            10,
                            28
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            10,
                            28
                        ]
                    ]
                },
                "abstract": "In the field of deep learning, people strive to construct high-quality deep neural networks (DNNs) to improve the accuracy of predicting. As well known, the quality of training data have great impacts on the quality of DNN models, since all the DNN models are obtained by training using these training data. However, there is not any reported systematic study on how the quality of training data affects the quality of DNN model. To study the relationships between data quality and model quality, we mainly consider four aspects of data quality including Skewed Classes, Sample Complexity, Label Quality, and Noisy Data in this paper. We design experiments on MNIST and Cifar-10, and attempt to find out the influences of four aspects on the quality of DNN models. Pearson correlation coefficient and Spearman correlation coefficient are utilized to evaluate such influences. Experimental results show that all the four aspects of data quality have significant impacts on the quality of DNN models. It means that the decrease of data quality in these four aspects will reduce the accuracy of the DNN models.",
                "call-number": "10.1145/3361242.3361260",
                "collection-number": "18",
                "collection-title": "Internetware '19",
                "container-title": "Proceedings of the 11th Asia-Pacific Symposium on Internetware",
                "DOI": "10.1145/3361242.3361260",
                "event-place": "Fukuoka, Japan",
                "ISBN": "9781450377010",
                "number": "Article 18",
                "number-of-pages": "6",
                "page": "1–6",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "From Data Quality to Model Quality: An Exploratory Study on Deep Learning",
                "URL": "https://doi.org/10.1145/3361242.3361260"
            }
        },
        {
            "10.1145/3102254.3102259": {
                "id": "10.1145/3102254.3102259",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Caruccio",
                        "given": "Loredana"
                    },
                    {
                        "family": "Deufemia",
                        "given": "Vincenzo"
                    },
                    {
                        "family": "Polese",
                        "given": "Giuseppe"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            6,
                            19
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            6,
                            19
                        ]
                    ]
                },
                "abstract": "Many modern application contexts, especially those related to the semantic Web, advocate for automatic techniques capable of extracting relationships between semi-structured data, for several purposes, such as the identification of inconsistencies or patterns of semantically related data, query rewriting, and so forth. One way to represent such relationships is to use relaxed functional dependencies (rfds), since they can embed approximate matching paradigms to compare unstructured data, and admit the possibility of exceptions for them. To this end, thresholds might need to be specified in order to limit the similarity degree in approximate comparisons or the occurrence of exceptions. Thanks to the availability of huge amount of data, including unstructured data available on the Web, nowadays it is possible to automatically discover rfds from data. However, due to the many different combinations of similarity and exception thresholds, the discovery process has an exponential complexity. Thus, it is vital devising proper optimization strategies, in order to make the discovery process feasible. To this end, in this paper, we propose a genetic algorithm to discover rfds from data, also providing an empirical evaluation demonstrating its effectiveness.",
                "call-number": "10.1145/3102254.3102259",
                "collection-number": "5",
                "collection-title": "WIMS '17",
                "container-title": "Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics",
                "DOI": "10.1145/3102254.3102259",
                "event-place": "Amantea, Italy",
                "ISBN": "9781450352253",
                "keyword": "functional dependency, discovery from data, genetic algorithm",
                "number": "Article 5",
                "number-of-pages": "10",
                "page": "1–10",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Evolutionary mining of relaxed dependencies from big data collections",
                "URL": "https://doi.org/10.1145/3102254.3102259"
            }
        }
    ]
}