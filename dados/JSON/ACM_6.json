{
    "exportedDoiLength": 100,
    "fileName": "acm",
    "style": "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<style xmlns=\"http://purl.org/net/xbiblio/csl\" class=\"in-text\" version=\"1.0\" demote-non-dropping-particle=\"sort-only\" default-locale=\"en-US\">\n    <!-- This style was edited with the Visual CSL Editor (http://editor.citationstyles.org/visualEditor/) -->\n    <info>\n        <title>BibTeX ACM citation style</title>\n        <id>http://www.zotero.org/styles/bibtex-acm-citation-style</id>\n        <link href=\"http://www.zotero.org/styles/bibtex-acm-citation-style\" rel=\"self\"/>\n        <link href=\"http://www.bibtex.org/\" rel=\"documentation\"/>\n        <author>\n            <name>Markus Schaffner</name>\n        </author>\n        <contributor>\n            <name>Richard Karnesky</name>\n            <email>karnesky+zotero@gmail.com</email>\n            <uri>http://arc.nucapt.northwestern.edu/Richard_Karnesky</uri>\n        </contributor>\n        <category citation-format=\"author-date\"/>\n        <category field=\"generic-base\"/>\n        <updated>2018-06-11T10:52:49+00:00</updated>\n        <rights license=\"http://creativecommons.org/licenses/by-sa/3.0/\">This work is licensed under a Creative Commons Attribution-ShareAlike 3.0 License</rights>\n    </info>\n    <macro name=\"zotero2bibtexType\">\n        <choose>\n            <if type=\"BILL BOOK GRAPHIC LEGAL_CASE LEGISLATION MOTION_PICTURE SONG\" match=\"any\">\n                <choose>\n                    <if genre=\"rfc\"  match=\"any\">\n                       <text value=\"rfc\"/>\n                    </if>\n                     <else-if  genre=\"bibliography\" match=\"any\">\n                        <text value=\"bibliography\"/>\n                    </else-if>\n                    <else-if  genre=\"play_drama\" match=\"any\">\n                        <text value=\"playdrama\"/>\n                    </else-if>\n                    <else-if  genre=\"proceeding\" match=\"any\">\n                        <text value=\"proceedings\"/>\n                    </else-if>\n                    <else-if  genre=\"tech_brief\" match=\"any\">\n                        <text value=\"tech-brief\"/>\n                    </else-if>\n                    <else>\n                       <text value=\"book\"/>\n                    </else>\n                </choose>\n            </if>\n            <else-if type=\"CHAPTER\" match=\"any\">\n                <text value=\"inbook\"/>\n            </else-if>\n            <else-if type=\"ARTICLE ARTICLE_JOURNAL ARTICLE_MAGAZINE ARTICLE_NEWSPAPER\" match=\"any\">\n                <text value=\"article\"/>\n            </else-if>\n            <else-if type=\"THESIS\" match=\"any\">\n                <choose>\n                    <if variable=\"genre\">\n                        <text variable=\"genre\" text-case=\"lowercase\" strip-periods=\"true\"/>\n                    </if>\n                </choose>\n                <text value=\"thesis\"/>\n            </else-if>\n            <else-if type=\"PAPER_CONFERENCE\" match=\"any\">\n                <text value=\"inproceedings\"/>\n            </else-if>\n            <else-if type=\"REPORT\" match=\"any\">\n                <text value=\"techreport\"/>\n            </else-if>\n            <else-if type=\"DATASET\" match=\"any\">\n                <choose>\n                    <if genre=\"software\"  match=\"any\">\n                       <text value=\"software\"/>\n                    </if>\n                    <else>\n                       <text value=\"dataset\"/>\n                    </else>\n                </choose>\n            </else-if>\n            <else>\n                <text value=\"misc\"/>\n            </else>\n        </choose>\n    </macro>\n    <macro name=\"citeKey\">\n           <text variable=\"call-number\"/>\n    </macro>\n    <macro name=\"editor-short\">\n        <names variable=\"editor\">\n            <name form=\"short\" delimiter=\":\" delimiter-precedes-last=\"always\"/>\n        </names>\n    </macro>\n    <macro name=\"author-short\">\n        <names variable=\"author\">\n            <name form=\"short\" delimiter=\":\" delimiter-precedes-last=\"always\"/>\n        </names>\n    </macro>\n    <macro name=\"issued-year\">\n        <date variable=\"issued\">\n            <date-part name=\"year\"/>\n        </date>\n    </macro>\n    <macro name=\"issued-month\">\n        <choose>\n            <if type=\"ARTICLE\" match=\"any\">\n                <date variable=\"issued\">\n                    <date-part name=\"month\" form=\"short\" strip-periods=\"true\" text-case=\"lowercase\"/>\n                </date>\n            </if>\n        </choose>\n    </macro>\n    <macro name=\"issue-date\">\n        <choose>\n            <if type=\"ARTICLE ARTICLE_JOURNAL\" match=\"any\">\n                <choose>\n                    <if variable=\"note\" match=\"none\">\n                        <choose>\n                            <if variable=\"source\">\n                                 <text variable=\"source\"/>\n                            </if>\n                               <else>\n                                 <date date-parts=\"year-month\" form=\"text\" variable=\"issued\"/>\n                               </else>\n                        </choose>\n                    </if>\n                </choose>\n            </if>\n        </choose>\n    </macro>\n    <macro name=\"author\">\n        <names variable=\"author\">\n            <name sort-separator=\", \" delimiter=\" and \" delimiter-precedes-last=\"always\" name-as-sort-order=\"all\"/>\n            <label form=\"long\" text-case=\"capitalize-first\"/>\n        </names>\n    </macro>\n    <macro name=\"editor-translator\">\n        <names variable=\"editor translator\" delimiter=\", \">\n            <name sort-separator=\", \" delimiter=\" and \" delimiter-precedes-last=\"always\" name-as-sort-order=\"all\"/>\n            <label form=\"long\" text-case=\"capitalize-first\"/>\n        </names>\n    </macro>\n    <macro name=\"title\">\n        <choose>\n            <if genre=\"proceeding\" match=\"any\">\n                <text variable=\"container-title-short\" suffix=\": \"/>\n                <text variable=\"title\" text-case=\"title\"/>\n            </if>\n            <else-if type=\"ARTICLE_JOURNAL\" match=\"none\">\n                <text variable=\"title\" text-case=\"title\"/>\n            </else-if>\n        </choose>\n    </macro>\n    <macro name=\"volume\">\n        <choose>\n            <if type=\"ARTICLE BOOK ARTICLE_JOURNAL\" match=\"any\">\n                <text variable=\"volume\" prefix=\"volume = {\" suffix=\"}\"/>\n            </if>\n        </choose>\n    </macro>\n    <macro name=\"DOI\">\n        <choose>\n            <if type=\"ARTICLE PAPER_CONFERENCE\" match=\"any\">\n                <text variable=\"DOI\" prefix=\"doi = {\" suffix=\"}\"/>\n            </if>\n        </choose>\n    </macro>\n    <macro name=\"abstract\">\n        <if match=\"any\" variable=\"abstract\">\n            <text variable=\"abstract\"/>\n        </if>\n    </macro>\n    <macro name=\"URL\">\n        <text variable=\"URL\" prefix=\"url = {\" suffix=\"}\"/>\n    </macro>\n    <macro name=\"container-title\">\n        <choose>\n            <if type=\"CHAPTER PAPER_CONFERENCE\" match=\"any\">\n                <text variable=\"container-title\" prefix=\"booktitle = {\" suffix=\"}\" text-case=\"title\"/>\n            </if>\n            <else-if type=\"ARTICLE ARTICLE_JOURNAL\" match=\"any\">\n                <text variable=\"container-title\" prefix=\"journal = {\" suffix=\"}\" text-case=\"title\"/>\n            </else-if>\n        </choose>\n    </macro>\n    <macro name=\"pages\">\n        <group delimiter=\",&#10;\">\n            <choose>\n                <if match=\"any\" variable=\"collection-number\">\n                    <text variable=\"collection-number\" prefix=\"articleno = {\" suffix=\"}\"/>\n                </if>\n                <else>\n                    <text variable=\"page\" prefix=\"pages = {\" suffix=\"}\"/>\n                </else>\n            </choose>\n            <text variable=\"number-of-pages\" prefix=\"numpages = {\" suffix=\"}\"/>\n        </group>\n    </macro>\n    <macro name=\"edition\">\n        <text variable=\"edition\"/>\n    </macro>\n    <macro name=\"editor\">\n        <choose>\n            <if match=\"any\" type=\"THESIS\">\n                <names variable=\"editor\" delimiter=\", \" prefix=\"advisor = {\" suffix=\"}\">\n                    <name delimiter=\" and \" delimiter-precedes-last=\"always\" name-as-sort-order=\"all\"/>\n                </names>\n            </if>\n            <else>\n               <names variable=\"editor\" delimiter=\", \" prefix=\"editor = {\" suffix=\"}\">\n                   <name delimiter=\" and \" delimiter-precedes-last=\"always\" name-as-sort-order=\"all\"/>\n               </names>\n            </else>\n        </choose>\n    </macro>\n    <macro name=\"keyword\">\n        <choose>\n            <if type=\"ARTICLE PAPER_CONFERENCE DATASET\" match=\"any\">\n                <text variable=\"keyword\" prefix=\"keywords = {\" suffix=\"}\"/>\n            </if>\n        </choose>\n    </macro>\n    <citation et-al-min=\"11\" et-al-use-first=\"10\" disambiguate-add-year-suffix=\"true\" disambiguate-add-names=\"false\" disambiguate-add-givenname=\"false\" collapse=\"year\">\n        <layout delimiter=\"_\">\n            <text macro=\"citeKey\"/>\n        </layout>\n    </citation>\n    <bibliography hanging-indent=\"false\">\n        <layout>\n            <group display=\"right-inline\">\n                <text macro=\"zotero2bibtexType\" prefix=\"@\"/>\n                <group prefix=\"{\" suffix=\"&#10;}\" delimiter=\",&#10;\">\n                    <text macro=\"citeKey\"/>\n                    <text macro=\"author\" prefix=\"author = {\" suffix=\"}\"/>\n                    <text macro=\"editor\"/>\n                    <text macro=\"title\" prefix=\"title = {\" suffix=\"}\"/>\n                    <text macro=\"issued-year\" prefix=\"year = {\" suffix=\"}\"/>\n                    <text macro=\"issue-date\" prefix=\"issue_date = {\" suffix=\"}\"/>\n                    <text variable=\"ISBN\" prefix=\"isbn = {\" suffix=\"}\"/>\n                    <text variable=\"publisher\" prefix=\"publisher = {\" suffix=\"}\"/>\n                    <text variable=\"publisher-place\" prefix=\"address = {\" suffix=\"}\"/>\n                    <text variable=\"chapter-number\" prefix=\"chapter = {\" suffix=\"}\"/>\n                    <text macro=\"edition\" prefix=\"edition = {\" suffix=\"}\"/>\n                    <text macro=\"volume\"/>\n                    <text variable=\"issue\" prefix=\"number = {\" suffix=\"}\"/>\n                    <text variable=\"ISSN\" prefix=\"issn = {\" suffix=\"}\"/>\n                    <text variable=\"archive_location\" prefix=\"archiveLocation = {\" suffix=\"}\"/>\n                    <text macro=\"URL\"/>\n                    <text macro=\"DOI\"/>\n                    <text macro=\"abstract\" prefix=\"abstract = {\" suffix=\"}\"/>\n                    <text variable=\"note\" prefix=\"note = {\" suffix=\"}\"/>\n                    <text macro=\"container-title\"/>\n                    <text macro=\"issued-month\" prefix=\"month = {\" suffix=\"}\"/>\n                    <text macro=\"pages\"/>\n                    <text macro=\"keyword\"/>\n                    <text variable=\"event-place\" prefix=\"location = {\" suffix=\"}\"/>\n                    <choose>\n                        <if type=\"PAPER_CONFERENCE\" match=\"any\">\n                            <text variable=\"collection-title\" prefix=\"series = {\" suffix=\"}\"/>\n                        </if>\n                        <else>\n                            <text variable=\"collection-title\" prefix=\"collection = {\" suffix=\"}\"/>\n                        </else>\n                    </choose>\n                </group>\n            </group>\n        </layout>\n    </bibliography>\n</style>\n",
    "suffix": "bib",
    "locale": "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<locale xmlns=\"http://purl.org/net/xbiblio/csl\" version=\"1.0\" xml:lang=\"en-US\">\n  <info>\n    <rights license=\"http://creativecommons.org/licenses/by-sa/3.0/\">This work is licensed under a Creative Commons Attribution-ShareAlike 3.0 License</rights>\n    <updated>2012-07-04T23:31:02+00:00</updated>\n  </info>\n  <style-options punctuation-in-quote=\"true\"\n                 leading-noise-words=\"a,an,the\"\n                 name-as-sort-order=\"ja zh kr my hu vi\"\n                 name-never-short=\"ja zh kr my hu vi\"/>\n  <date form=\"text\">\n    <date-part name=\"month\" suffix=\" \"/>\n    <date-part name=\"day\" suffix=\", \"/>\n    <date-part name=\"year\"/>\n  </date>\n  <date form=\"numeric\">\n    <date-part name=\"month\" form=\"numeric-leading-zeros\" suffix=\"/\"/>\n    <date-part name=\"day\" form=\"numeric-leading-zeros\" suffix=\"/\"/>\n    <date-part name=\"year\"/>\n  </date>\n  <terms>\n    <term name=\"radio-broadcast\">radio broadcast</term>\n    <term name=\"television-broadcast\">television broadcast</term>\n    <term name=\"podcast\">podcast</term>\n    <term name=\"instant-message\">instant message</term>\n    <term name=\"email\">email</term>\n    <term name=\"number-of-volumes\">\n      <single>volume</single>\n      <multiple>volumes</multiple>\n    </term>\n    <term name=\"accessed\">accessed</term>\n    <term name=\"and\">and</term>\n    <term name=\"and\" form=\"symbol\">&amp;</term>\n    <term name=\"and others\">and others</term>\n    <term name=\"anonymous\">anonymous</term>\n    <term name=\"anonymous\" form=\"short\">anon.</term>\n    <term name=\"at\">at</term>\n    <term name=\"available at\">available at</term>\n    <term name=\"by\">by</term>\n    <term name=\"circa\">circa</term>\n    <term name=\"circa\" form=\"short\">c.</term>\n    <term name=\"cited\">cited</term>\n    <term name=\"edition\">\n      <single>edition</single>\n      <multiple>editions</multiple>\n    </term>\n    <term name=\"edition\" form=\"short\">ed.</term>\n    <term name=\"et-al\">et al.</term>\n    <term name=\"forthcoming\">forthcoming</term>\n    <term name=\"from\">from</term>\n    <term name=\"ibid\">ibid.</term>\n    <term name=\"in\">in</term>\n    <term name=\"in press\">in press</term>\n    <term name=\"internet\">internet</term>\n    <term name=\"interview\">interview</term>\n    <term name=\"letter\">letter</term>\n    <term name=\"no date\">no date</term>\n    <term name=\"no date\" form=\"short\">n.d.</term>\n    <term name=\"online\">online</term>\n    <term name=\"presented at\">presented at the</term>\n    <term name=\"reference\">\n      <single>reference</single>\n      <multiple>references</multiple>\n    </term>\n    <term name=\"reference\" form=\"short\">\n      <single>ref.</single>\n      <multiple>refs.</multiple>\n    </term>\n    <term name=\"retrieved\">retrieved</term>\n    <term name=\"scale\">scale</term>\n    <term name=\"version\">version</term>\n\n    <!-- ANNO DOMINI; BEFORE CHRIST -->\n    <term name=\"ad\">AD</term>\n    <term name=\"bc\">BC</term>\n\n    <!-- PUNCTUATION -->\n    <term name=\"open-quote\">“</term>\n    <term name=\"close-quote\">”</term>\n    <term name=\"open-inner-quote\">‘</term>\n    <term name=\"close-inner-quote\">’</term>\n    <term name=\"page-range-delimiter\">–</term>\n\n    <!-- ORDINALS -->\n    <term name=\"ordinal\">th</term>\n    <term name=\"ordinal-01\">st</term>\n    <term name=\"ordinal-02\">nd</term>\n    <term name=\"ordinal-03\">rd</term>\n    <term name=\"ordinal-11\">th</term>\n    <term name=\"ordinal-12\">th</term>\n    <term name=\"ordinal-13\">th</term>\n\n    <!-- LONG ORDINALS -->\n    <term name=\"long-ordinal-01\">first</term>\n    <term name=\"long-ordinal-02\">second</term>\n    <term name=\"long-ordinal-03\">third</term>\n    <term name=\"long-ordinal-04\">fourth</term>\n    <term name=\"long-ordinal-05\">fifth</term>\n    <term name=\"long-ordinal-06\">sixth</term>\n    <term name=\"long-ordinal-07\">seventh</term>\n    <term name=\"long-ordinal-08\">eighth</term>\n    <term name=\"long-ordinal-09\">ninth</term>\n    <term name=\"long-ordinal-10\">tenth</term>\n\n    <!-- LONG LOCATOR FORMS -->\n    <term name=\"book\">\n      <single>book</single>\n      <multiple>books</multiple>\n    </term>\n    <term name=\"chapter\">\n      <single>chapter</single>\n      <multiple>chapters</multiple>\n    </term>\n    <term name=\"column\">\n      <single>column</single>\n      <multiple>columns</multiple>\n    </term>\n    <term name=\"figure\">\n      <single>figure</single>\n      <multiple>figures</multiple>\n    </term>\n    <term name=\"folio\">\n      <single>folio</single>\n      <multiple>folios</multiple>\n    </term>\n    <term name=\"issue\">\n      <single>number</single>\n      <multiple>numbers</multiple>\n    </term>\n    <term name=\"line\">\n      <single>line</single>\n      <multiple>lines</multiple>\n    </term>\n    <term name=\"note\">\n      <single>note</single>\n      <multiple>notes</multiple>\n    </term>\n    <term name=\"opus\">\n      <single>opus</single>\n      <multiple>opera</multiple>\n    </term>\n    <term name=\"page\">\n      <single>page</single>\n      <multiple>pages</multiple>\n    </term>\n    <term name=\"paragraph\">\n      <single>paragraph</single>\n      <multiple>paragraph</multiple>\n    </term>\n    <term name=\"part\">\n      <single>part</single>\n      <multiple>parts</multiple>\n    </term>\n    <term name=\"section\">\n      <single>section</single>\n      <multiple>sections</multiple>\n    </term>\n    <term name=\"sub verbo\">\n      <single>sub verbo</single>\n      <multiple>sub verbis</multiple>\n    </term>\n    <term name=\"verse\">\n      <single>verse</single>\n      <multiple>verses</multiple>\n    </term>\n    <term name=\"volume\">\n      <single>volume</single>\n      <multiple>volumes</multiple>\n    </term>\n\n    <!-- SHORT LOCATOR FORMS -->\n    <term name=\"book\" form=\"short\">bk.</term>\n    <term name=\"chapter\" form=\"short\">chap.</term>\n    <term name=\"column\" form=\"short\">col.</term>\n    <term name=\"figure\" form=\"short\">fig.</term>\n    <term name=\"folio\" form=\"short\">f.</term>\n    <term name=\"issue\" form=\"short\">no.</term>\n    <term name=\"line\" form=\"short\">l.</term>\n    <term name=\"note\" form=\"short\">n.</term>\n    <term name=\"opus\" form=\"short\">op.</term>\n    <term name=\"page\" form=\"short\">\n      <single>p.</single>\n      <multiple>pp.</multiple>\n    </term>\n    <term name=\"paragraph\" form=\"short\">para.</term>\n    <term name=\"part\" form=\"short\">pt.</term>\n    <term name=\"section\" form=\"short\">sec.</term>\n    <term name=\"sub verbo\" form=\"short\">\n      <single>s.v.</single>\n      <multiple>s.vv.</multiple>\n    </term>\n    <term name=\"verse\" form=\"short\">\n      <single>v.</single>\n      <multiple>vv.</multiple>\n    </term>\n    <term name=\"volume\" form=\"short\">\n      <single>vol.</single>\n      <multiple>vols.</multiple>\n    </term>\n\n    <!-- SYMBOL LOCATOR FORMS -->\n    <term name=\"paragraph\" form=\"symbol\">\n      <single>¶</single>\n      <multiple>¶¶</multiple>\n    </term>\n    <term name=\"section\" form=\"symbol\">\n      <single>§</single>\n      <multiple>§§</multiple>\n    </term>\n\n    <!-- LONG ROLE FORMS -->\n    <term name=\"director\">\n      <single>director</single>\n      <multiple>directors</multiple>\n    </term>\n    <term name=\"editor\">\n      <single>editor</single>\n      <multiple>editors</multiple>\n    </term>\n    <term name=\"editorial-director\">\n      <single>editor</single>\n      <multiple>editors</multiple>\n    </term>\n    <term name=\"illustrator\">\n      <single>illustrator</single>\n      <multiple>illustrators</multiple>\n    </term>\n    <term name=\"translator\">\n      <single>translator</single>\n      <multiple>translators</multiple>\n    </term>\n    <term name=\"editortranslator\">\n      <single>editor &amp; translator</single>\n      <multiple>editors &amp; translators</multiple>\n    </term>\n\n    <!-- SHORT ROLE FORMS -->\n    <term name=\"director\" form=\"short\">\n      <single>dir.</single>\n      <multiple>dirs.</multiple>\n    </term>\n    <term name=\"editor\" form=\"short\">\n      <single>ed.</single>\n      <multiple>eds.</multiple>\n    </term>\n    <term name=\"editorial-director\" form=\"short\">\n      <single>ed.</single>\n      <multiple>eds.</multiple>\n    </term>\n    <term name=\"illustrator\" form=\"short\">\n      <single>ill.</single>\n      <multiple>ills.</multiple>\n    </term>\n    <term name=\"translator\" form=\"short\">\n      <single>tran.</single>\n      <multiple>trans.</multiple>\n    </term>\n    <term name=\"editortranslator\" form=\"short\">\n      <single>ed. &amp; tran.</single>\n      <multiple>eds. &amp; trans.</multiple>\n    </term>\n\n    <!-- VERB ROLE FORMS -->\n    <term name=\"director\" form=\"verb\">directed by</term>\n    <term name=\"editor\" form=\"verb\">edited by</term>\n    <term name=\"editorial-director\" form=\"verb\">edited by</term>\n    <term name=\"illustrator\" form=\"verb\">illustrated by</term>\n    <term name=\"interviewer\" form=\"verb\">interview by</term>\n    <term name=\"recipient\" form=\"verb\">to</term>\n    <term name=\"reviewed-author\" form=\"verb\">by</term>\n    <term name=\"translator\" form=\"verb\">translated by</term>\n    <term name=\"editortranslator\" form=\"verb\">edited &amp; translated by</term>\n\n    <!-- SHORT VERB ROLE FORMS -->\n    <term name=\"container-author\" form=\"verb-short\">by</term>\n    <term name=\"director\" form=\"verb-short\">dir.</term>\n    <term name=\"editor\" form=\"verb-short\">ed.</term>\n    <term name=\"editorial-director\" form=\"verb-short\">ed.</term>\n    <term name=\"illustrator\" form=\"verb-short\">illus.</term>\n    <term name=\"translator\" form=\"verb-short\">trans.</term>\n    <term name=\"editortranslator\" form=\"verb-short\">ed. &amp; trans.</term>\n\n    <!-- LONG MONTH FORMS -->\n    <term name=\"month-01\">January</term>\n    <term name=\"month-02\">February</term>\n    <term name=\"month-03\">March</term>\n    <term name=\"month-04\">April</term>\n    <term name=\"month-05\">May</term>\n    <term name=\"month-06\">June</term>\n    <term name=\"month-07\">July</term>\n    <term name=\"month-08\">August</term>\n    <term name=\"month-09\">September</term>\n    <term name=\"month-10\">October</term>\n    <term name=\"month-11\">November</term>\n    <term name=\"month-12\">December</term>\n\n    <!-- SHORT MONTH FORMS -->\n    <term name=\"month-01\" form=\"short\">Jan.</term>\n    <term name=\"month-02\" form=\"short\">Feb.</term>\n    <term name=\"month-03\" form=\"short\">Mar.</term>\n    <term name=\"month-04\" form=\"short\">Apr.</term>\n    <term name=\"month-05\" form=\"short\">May</term>\n    <term name=\"month-06\" form=\"short\">Jun.</term>\n    <term name=\"month-07\" form=\"short\">Jul.</term>\n    <term name=\"month-08\" form=\"short\">Aug.</term>\n    <term name=\"month-09\" form=\"short\">Sep.</term>\n    <term name=\"month-10\" form=\"short\">Oct.</term>\n    <term name=\"month-11\" form=\"short\">Nov.</term>\n    <term name=\"month-12\" form=\"short\">Dec.</term>\n\n    <!-- SEASONS -->\n    <term name=\"season-01\">Spring</term>\n    <term name=\"season-02\">Summer</term>\n    <term name=\"season-03\">Autumn</term>\n    <term name=\"season-04\">Winter</term>\n  </terms>\n</locale>\n",
    "contentType": "Application/x-bibtex",
    "items": [
        {
            "10.1145/3278607": {
                "id": "10.1145/3278607",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Song",
                        "given": "Qingquan"
                    },
                    {
                        "family": "Ge",
                        "given": "Hancheng"
                    },
                    {
                        "family": "Caverlee",
                        "given": "James"
                    },
                    {
                        "family": "Hu",
                        "given": "Xia"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            1,
                            9
                        ]
                    ]
                },
                "abstract": "Tensor completion is a problem of filling the missing or unobserved entries of partially observed tensors. Due to the multidimensional character of tensors in describing complex datasets, tensor completion algorithms and their applications have received wide attention and achievement in areas like data mining, computer vision, signal processing, and neuroscience. In this survey, we provide a modern overview of recent advances in tensor completion algorithms from the perspective of big data analytics characterized by diverse variety, large volume, and high velocity. We characterize these advances from the following four perspectives: general tensor completion algorithms, tensor completion with auxiliary information (variety), scalable tensor completion algorithms (volume), and dynamic tensor completion algorithms (velocity). Further, we identify several tensor completion applications on real-world data-driven problems and present some common experimental frameworks popularized in the literature along with several available software repositories. Our goal is to summarize these popular methods and introduce them to researchers and practitioners for promoting future research and applications. We conclude with a discussion of key challenges and promising research directions in this community for future exploration.",
                "call-number": "10.1145/3278607",
                "collection-number": "6",
                "container-title": "ACM Trans. Knowl. Discov. Data",
                "DOI": "10.1145/3278607",
                "ISSN": "1556-4681",
                "issue": "1",
                "keyword": "dynamic data analysis, tensor factorization, tensor completion, big data analytics, tensor decomposition, multilinear data analysis, Tensor",
                "number": "Article 6",
                "number-of-pages": "48",
                "page": "1–48",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "February 2019",
                "title": "Tensor Completion Algorithms in Big Data Analytics",
                "URL": "https://doi.org/10.1145/3278607",
                "volume": "13"
            }
        },
        {
            "10.1145/3369555.3369573": {
                "id": "10.1145/3369555.3369573",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Ntehelang",
                        "given": "Gomotsegang"
                    },
                    {
                        "family": "Isong",
                        "given": "Bassey"
                    },
                    {
                        "family": "Lugayizi",
                        "given": "Francis"
                    },
                    {
                        "family": "Dladlu",
                        "given": "Nosipho"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            11,
                            9
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            11,
                            9
                        ]
                    ]
                },
                "abstract": "Internet of Things devices constantly generate big data which when analyzed reveals hidden patterns, information and trends, thus, enabling decision making. Several organization today utilized data analytics to improve organizational performance and deliver high quality of service and experience. Healthcare is not an exception and uses data analytics for some of its capabilities such as detecting diseases and diagnose patients at early stages, identifying high-risk patients and providing them treatment to reduce unnecessary hospitalization or readmission. However, the existing and expected increase of connected devices poses several challenges for data analysis, especially with little work being done to address them. Therefore, this paper brings together some of these challenges that needs to be addressed and some of the proposed solutions. We performed a review of some of the studies in the literature with a view of providing research directions for researchers.",
                "call-number": "10.1145/3369555.3369573",
                "collection-title": "ICTCE '19",
                "container-title": "Proceedings of the 3rd International Conference on Telecommunications and Communication Engineering",
                "DOI": "10.1145/3369555.3369573",
                "event-place": "Tokyo, Japan",
                "ISBN": "9781450371803",
                "keyword": "internet of things, big data analytics, healthcare",
                "number-of-pages": "6",
                "page": "16–21",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "IoT-based big data analytics issues in healthcare",
                "URL": "https://doi.org/10.1145/3369555.3369573"
            }
        },
        {
            "10.1145/2938503.2938539": {
                "id": "10.1145/2938503.2938539",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Cassavia",
                        "given": "Nunziato"
                    },
                    {
                        "family": "Ciampi",
                        "given": "Mario"
                    },
                    {
                        "family": "De Pietro",
                        "given": "Giuseppe"
                    },
                    {
                        "family": "Masciari",
                        "given": "Elio"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            7,
                            11
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2016,
                            7,
                            11
                        ]
                    ]
                },
                "abstract": "Information management in healthcare is nowadays experiencing a great revolution. After the impressive progress in digitizing medical data by private organizations, also the federal government and other public stakeholders have also started to make use of healthcare data for data analysis purposes in order to extract actionable knowledge. In this paper, we propose an architecture for supporting interoperability in healthcare systems by exploiting Big Data techniques. In particular, we describe a proposal based on big data techniques to implement a nationwide system able to improve EHR data access efficiency and reduce costs.",
                "call-number": "10.1145/2938503.2938539",
                "collection-title": "IDEAS '16",
                "container-title": "Proceedings of the 20th International Database Engineering & Applications Symposium",
                "DOI": "10.1145/2938503.2938539",
                "event-place": "Montreal, QC, Canada",
                "ISBN": "9781450341189",
                "keyword": "Healthcare, Interoperability, Big data",
                "number-of-pages": "6",
                "page": "212–217",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "A Big Data Approach For Querying Data in EHR Systems",
                "URL": "https://doi.org/10.1145/2938503.2938539"
            }
        },
        {
            "10.1145/3156818": {
                "id": "10.1145/3156818",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Bruno",
                        "given": "Rodrigo"
                    },
                    {
                        "family": "Ferreira",
                        "given": "Paulo"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            1,
                            10
                        ]
                    ]
                },
                "abstract": "The need to process and store massive amounts of data—Big Data—is a reality. In areas such as scientific experiments, social networks management, credit card fraud detection, targeted advertisement, and financial analysis, massive amounts of information are generated and processed daily to extract valuable, summarized information. Due to its fast development cycle (i.e., less expensive to develop), mainly because of automatic memory management, and rich community resources, managed object-oriented programming languages (e.g., Java) are the first choice to develop Big Data platforms (e.g., Cassandra, Spark) on which such Big Data applications are executed.However, automatic memory management comes at a cost. This cost is introduced by the garbage collector, which is responsible for collecting objects that are no longer being used. Although current (classic) garbage collection algorithms may be applicable to small-scale applications, these algorithms are not appropriate for large-scale Big Data environments, as they do not scale in terms of throughput and pause times.In this work, current Big Data platforms and their memory profiles are studied to understand why classic algorithms (which are still the most commonly used) are not appropriate, and also to analyze recently proposed and relevant memory management algorithms, targeted to Big Data environments. The scalability of recent memory management algorithms is characterized in terms of throughput (improves the throughput of the application) and pause time (reduces the latency of the application) when compared to classic algorithms. The study is concluded by presenting a taxonomy of the described works and some open problems, with regard to Big Data memory management, that could be addressed in future works.",
                "call-number": "10.1145/3156818",
                "collection-number": "20",
                "container-title": "ACM Comput. Surv.",
                "DOI": "10.1145/3156818",
                "ISSN": "0360-0300",
                "issue": "1",
                "keyword": "Big Data, scalability, Big Data environment, Garbage collection, storage platform, Java, memory managed runtime, processing platforms",
                "number": "Article 20",
                "number-of-pages": "35",
                "page": "1–35",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "January 2019",
                "title": "A Study on Garbage Collection Algorithms for Big Data Environments",
                "URL": "https://doi.org/10.1145/3156818",
                "volume": "51"
            }
        },
        {
            "10.1145/2905055.2905197": {
                "id": "10.1145/2905055.2905197",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Nunes",
                        "given": "Rickey T. P."
                    },
                    {
                        "family": "Desphande",
                        "given": "Santosh L."
                    },
                    {
                        "family": "Subramanian",
                        "given": "Sattanathan"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            3,
                            4
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2016,
                            3,
                            4
                        ]
                    ]
                },
                "abstract": "Rapid growth of biological-data size brings the field of bioinformatics into the era of big-data and challenges the process of coordinating biological data sources and tool services. One of the challenges is how to transfer big-data among the components of a workflow in a faster and efficient manner. In this paper, we therefore (i) show the state-of-the-art approaches and analyze big-data handling in orchestrated bioinformatics workflows, (ii) provide a hypothesis based on the analysis and show that the hypothesis performs better than the state-of-the-art approaches. Our analysis shows that none of the existing approaches can retain the execution time of a workflow stable with respect to the size of the data involved. The main reason is that all the approaches attempt to move data which is varying in size, but not the tool (or the computation associated with it) which is fixed in size. Considering this as a hypothesis, we have moved computation instead of data in a workflow and shown that the workflow performs much better than other approaches in terms of execution time.",
                "call-number": "10.1145/2905055.2905197",
                "collection-number": "137",
                "collection-title": "ICTCS '16",
                "container-title": "Proceedings of the Second International Conference on Information and Communication Technology for Competitive Strategies",
                "DOI": "10.1145/2905055.2905197",
                "event-place": "Udaipur, India",
                "ISBN": "9781450339629",
                "keyword": "workflows, Big-data, bioinformatics, moving computation, moving data, orchestration",
                "number": "Article 137",
                "number-of-pages": "7",
                "page": "1–7",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Big-data Transportation in Orchestrated Bioinformatics Workflows: An Analysis and Hypothesis",
                "URL": "https://doi.org/10.1145/2905055.2905197"
            }
        },
        {
            "10.1145/3274005.3274015": {
                "id": "10.1145/3274005.3274015",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Petrova-Antonova",
                        "given": "Dessislava"
                    },
                    {
                        "family": "Ilieva",
                        "given": "Sylvia"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            9,
                            13
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            9,
                            13
                        ]
                    ]
                },
                "abstract": "The Big Data Value Chain aims to discover patterns, correlations, and pattern deviations hidden in a dataset. This paper investigates (1) the possible approaches to applying Big Data Value Chain to decision making in the Public sector, specifically in Education; and (2) the ways in which such activities can be automated. The models created can be customized depending on a school's dropout rate and number of students with learning difficulties. This research is part of a current project at the Ministry of Education and Science (MES) of Bulgaria, funded by the Operational Programme Science and Education for Smart Growth which aims to increase student engagement.",
                "call-number": "10.1145/3274005.3274015",
                "collection-title": "CompSysTech'18",
                "container-title": "Proceedings of the 19th International Conference on Computer Systems and Technologies",
                "DOI": "10.1145/3274005.3274015",
                "event-place": "Ruse, Bulgaria",
                "ISBN": "9781450364256",
                "keyword": "Big Data Value Chain, Data model, Decision support, Education, Big Data",
                "number-of-pages": "8",
                "page": "42–49",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Using Big Data Value Chain to Create Government Education Policies",
                "URL": "https://doi.org/10.1145/3274005.3274015"
            }
        },
        {
            "10.14778/2367502.2367563": {
                "id": "10.14778/2367502.2367563",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Shim",
                        "given": "Kyuseok"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2012,
                            8,
                            1
                        ]
                    ]
                },
                "abstract": "There is a growing trend of applications that should handle big data. However, analyzing big data is a very challenging problem today. For such applications, the MapReduce framework has recently attracted a lot of attention. Google's MapReduce or its open-source equivalent Hadoop is a powerful tool for building such applications. In this tutorial, we will introduce the MapReduce framework based on Hadoop, discuss how to design efficient MapReduce algorithms and present the state-of-the-art in MapReduce algorithms for data mining, machine learning and similarity joins. The intended audience of this tutorial is professionals who plan to design and develop MapReduce algorithms and researchers who should be aware of the state-of-the-art in MapReduce algorithms available today for big data analysis.",
                "call-number": "10.14778/2367502.2367563",
                "container-title": "Proc. VLDB Endow.",
                "DOI": "10.14778/2367502.2367563",
                "ISSN": "2150-8097",
                "issue": "12",
                "number-of-pages": "2",
                "page": "2016–2017",
                "publisher": "VLDB Endowment",
                "source": "August 2012",
                "title": "MapReduce algorithms for big data analysis",
                "URL": "https://doi.org/10.14778/2367502.2367563",
                "volume": "5"
            }
        },
        {
            "10.1145/2377978": {
                "id": "10.1145/2377978",
                "type": "BOOK",
                "issued": {
                    "date-parts": [
                        [
                            2011
                        ]
                    ]
                },
                "call-number": "10.1145/2377978",
                "container-title-short": "ASBD '11",
                "event-place": "Galveston Island, Texas, USA",
                "genre": "proceeding",
                "ISBN": "9781450314398",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Proceedings of the 1st Workshop on Architectures and Systems for Big Data"
            }
        },
        {
            "10.1145/2611567": {
                "id": "10.1145/2611567",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Jagadish",
                        "given": "H. V."
                    },
                    {
                        "family": "Gehrke",
                        "given": "Johannes"
                    },
                    {
                        "family": "Labrinidis",
                        "given": "Alexandros"
                    },
                    {
                        "family": "Papakonstantinou",
                        "given": "Yannis"
                    },
                    {
                        "family": "Patel",
                        "given": "Jignesh M."
                    },
                    {
                        "family": "Ramakrishnan",
                        "given": "Raghu"
                    },
                    {
                        "family": "Shahabi",
                        "given": "Cyrus"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2014,
                            7,
                            1
                        ]
                    ]
                },
                "abstract": "Exploring the inherent technical challenges in realizing the potential of Big Data.",
                "call-number": "10.1145/2611567",
                "container-title": "Commun. ACM",
                "DOI": "10.1145/2611567",
                "ISSN": "0001-0782",
                "issue": "7",
                "number-of-pages": "9",
                "page": "86–94",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "July 2014",
                "title": "Big data and its technical challenges",
                "URL": "https://doi.org/10.1145/2611567",
                "volume": "57"
            }
        },
        {
            "10.1145/3453187.3453340": {
                "id": "10.1145/3453187.3453340",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Hu",
                        "given": "Zhifeng"
                    },
                    {
                        "family": "Zhao",
                        "given": "Feng"
                    },
                    {
                        "family": "Zhao",
                        "given": "Xiaona"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2020,
                            12,
                            5
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2020,
                            12,
                            5
                        ]
                    ]
                },
                "abstract": "The big data technology can be applied to build the education service platforms and construct the big data analysis and application system as well as the multi-dimensional perception system. The big data analysis assists in the teaching process and breaks the temporal and spatial restrictions of educational resources, to realize the diversification of educational resources and improve the effectiveness of teaching feedback. This paper proposes a smart education service platform based on big data, which can promote the organic integration of educational communication, educational research, learning activities, teaching affairs administration, and information infrastructures. At the same time, the platform provides smarter, more efficient, and accurate services for teaching.",
                "call-number": "10.1145/3453187.3453340",
                "collection-title": "EBIMCS 2020",
                "container-title": "Proceedings of the 2020 3rd International Conference on E-Business, Information Management and Computer Science",
                "DOI": "10.1145/3453187.3453340",
                "event-place": "Wuhan, China",
                "ISBN": "9781450389099",
                "keyword": "Big data, Smart education, Information-oriented education",
                "number-of-pages": "6",
                "page": "228–233",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Research on Smart Education Service Platform Based on Big Data",
                "URL": "https://doi.org/10.1145/3453187.3453340"
            }
        },
        {
            "10.1145/3358528.3358566": {
                "id": "10.1145/3358528.3358566",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Lu",
                        "given": "Zeshan"
                    },
                    {
                        "family": "Liu",
                        "given": "Kun"
                    },
                    {
                        "family": "Liu",
                        "given": "Zhen"
                    },
                    {
                        "family": "Wang",
                        "given": "Cong"
                    },
                    {
                        "family": "Shen",
                        "given": "Maoxin"
                    },
                    {
                        "family": "Xu",
                        "given": "Tao"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            8,
                            28
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            8,
                            28
                        ]
                    ]
                },
                "abstract": "High-resolution earth observation images have increased dramatically because of the increasing of remote sensing satellites. Researchers must do large-scale target annotations to meet the training needs of deep neural network based model. However, most existing datasets contain an insufficient number of annotated samples, due to the inefficient manual annotation process which reason lies in the large number of remote sensing images, huge size, numerous targets, and high accuracy requirements. This paper proposed an efficient annotation method for big data sets of high-resolution earth observation images, in which the annotation process is divided into two parallel sub-processes, fast panchromatic image labeling and multi-spectral image fusion. Automatic scale transform is utilized for annotation of fused imagery. Experimental results show that the proposed method could improve the accuracy and efficiency of target labeling. Mask-RCNN and Faster-RCNN based target detection results demonstrate the validity of the big dataset annotated via our method.",
                "call-number": "10.1145/3358528.3358566",
                "collection-title": "ICBDT2019",
                "container-title": "Proceedings of the 2nd International Conference on Big Data Technologies",
                "DOI": "10.1145/3358528.3358566",
                "event-place": "Jinan, China",
                "ISBN": "9781450371926",
                "keyword": "Mask-RCNN, high-resolution images, Annotation method, target detection",
                "number-of-pages": "4",
                "page": "240–243",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "An Efficient Annotation Method for Big Data Sets of High-Resolution Earth Observation Images",
                "URL": "https://doi.org/10.1145/3358528.3358566"
            }
        },
        {
            "10.1145/3240117.3240139": {
                "id": "10.1145/3240117.3240139",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Ben Amor",
                        "given": "Fatma"
                    },
                    {
                        "family": "Mkadmi",
                        "given": "Abderrazak"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            10,
                            3
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            10,
                            3
                        ]
                    ]
                },
                "abstract": "Big Data is now a cross-cutting research topic in all disciplines related to digital as content and as technology too. They lie in the intersection between all the massive data captured, obtained, created by different means and of various origins. They represent an advanced step in the re-development of information, particularly concerning data management and data retention issues. This upheaval due to these massive data has touched all sectors, particularly the archives. Indeed, given their volume, their speed of creation and their importance to the social, economic, scientific and cultural actors, the sorting, the treatment and the conservation of these data Massive e orts require memory capacity, new methods and techniques for processing, analyzing and managing particular flows. We will try in this article to bring some elements of primary answers on the modalities of generative multiplication. exponential of numerical data and archive in a mass of numeric data, data that are in flux and that occur in a sup speed.",
                "call-number": "10.1145/3240117.3240139",
                "collection-number": "18",
                "collection-title": "DTUC '18",
                "container-title": "Proceedings of the 1st International Conference on Digital Tools & Uses Congress",
                "DOI": "10.1145/3240117.3240139",
                "event-place": "Paris, France",
                "ISBN": "9781450364515",
                "keyword": "long-term conservation, big data access, Big Data, Digital archiving",
                "number": "Article 18",
                "number-of-pages": "6",
                "page": "1–6",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Les Archives à l'Ère des Big Data: Les Enjeux de l'Archivage des Données Numériques Massives",
                "URL": "https://doi.org/10.1145/3240117.3240139"
            }
        },
        {
            "10.1145/3152723.3152741": {
                "id": "10.1145/3152723.3152741",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Barakbah",
                        "given": "Ali Ridho"
                    },
                    {
                        "family": "Harsono",
                        "given": "Tri"
                    },
                    {
                        "family": "Sudarsono",
                        "given": "Amang"
                    },
                    {
                        "family": "Aliefyan",
                        "given": "Roy Advandy"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            10,
                            22
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            10,
                            22
                        ]
                    ]
                },
                "abstract": "Earthquake is one of nature disaster types. Almost all regions in Indonesia are often earthquakes ranging from small magnitude to large. Anticipation and mitigation of earthquake victims is one of the important points in preventing the occurrence of earthquake victims in large numbers. One of the important information in anticipating and handling earthquake victims is to provide information about earthquake risk mapping in a region (province). This information is given by big data processing of the earthquake distribution and big data analytics of spatio-temporal earthquake data. This paperpresented a big data analysis for earthquake risk mapping system based on earthquake density projected to provinces in Indonesia. This system has 4 main features: (1) Data acquisation and preprocessing, (2) Automatic clustering using our Valley Tracing algorithm, (3) Density measurement of earthquake data distribution, and (4) Risk-mapping visualization projected to provinces. For experimental study, earthquake data is obtained from Advanced National Seismic System(ANSS) year 1963-2016 in location of Indonesia. We made a series of experiments in the places hit by big earthquake in Andaman (Banca Aceh), West Sumatra, and Papua. Based on the big data processing of the earthquake distribution and big data analytics of spatio-temporal earthquake data, it performed that the high seismic density value affected the risk of earthquake occurrence in the next year in the area concerned.",
                "call-number": "10.1145/3152723.3152741",
                "collection-title": "ICBDR 2017",
                "container-title": "Proceedings of the 2017 International Conference on Big Data Research",
                "DOI": "10.1145/3152723.3152741",
                "event-place": "Osaka, Japan",
                "ISBN": "9781450353564",
                "keyword": "Risk-Mapping Visualization, Earthquake Data Distribution, Spatio-Temporal Earthquake Data Distribution, Automatic Clustering",
                "number-of-pages": "5",
                "page": "33–37",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Big Data Analysis for Spatio-Temporal Earthquake Risk-Mapping System in Indonesia with Automatic Clustering",
                "URL": "https://doi.org/10.1145/3152723.3152741"
            }
        },
        {
            "10.1145/3259177": {
                "id": "10.1145/3259177",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Özsu",
                        "given": "M. Tamer"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2012,
                            5,
                            20
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2012,
                            5,
                            20
                        ]
                    ]
                },
                "call-number": "10.1145/3259177",
                "collection-title": "PhD '12",
                "container-title": "Proceedings of the on SIGMOD/PODS 2012 PhD Symposium",
                "DOI": "10.1145/3259177",
                "event-place": "Scottsdale, Arizona, USA",
                "ISBN": "9781450313261",
                "page": "",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Session details: Big data",
                "URL": "https://doi.org/10.1145/3259177"
            }
        },
        {
            "10.1145/2339530.2339534": {
                "id": "10.1145/2339530.2339534",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Jordan",
                        "given": "Michael I."
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2012,
                            8,
                            12
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2012,
                            8,
                            12
                        ]
                    ]
                },
                "abstract": "I present some recent work on statistical inference for Big Data. Divide-and-conquer is a natural computational paradigm for approaching Big Data problems, particularly given recent developments in distributed and parallel computing, but some interesting challenges arise when applying divide-and-conquer algorithms to statistical inference problems. One interesting issue is that of obtaining confidence intervals in massive datasets.The bootstrap principle suggests resampling data to obtain fluctuations in the values of estimators, and thereby confidence intervals, but this is infeasible with massive data. Subsampling the data yields fluctuations on the wrong scale, which have to be corrected to provide calibrated statistical inferences. I present a new procedure, the \"bag of little bootstraps,\" which circumvents this problem, inheriting the favorable theoretical properties of the bootstrap but also having a much more favorable computational profile. Another issue that I discuss is the problem of large-scale matrix completion. Here divide-and-conquer is a natural heuristic that works well in practice, but new theoretical problems arise when attempting to characterize the statistical performance of divide-and-conquer algorithms. Here the theoretical support is provided by concentration theorems for random matrices, and I present a new approach to this problem based on Stein's method1.",
                "call-number": "10.1145/2339530.2339534",
                "collection-title": "KDD '12",
                "container-title": "Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining",
                "DOI": "10.1145/2339530.2339534",
                "event-place": "Beijing, China",
                "ISBN": "9781450314626",
                "keyword": "divide-and-conquer, confidence intervals, big data, subsampling",
                "number-of-pages": "1",
                "page": "4",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Divide-and-conquer and statistical inference for big data",
                "URL": "https://doi.org/10.1145/2339530.2339534"
            }
        },
        {
            "10.1145/3265689.3265706": {
                "id": "10.1145/3265689.3265706",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Huang",
                        "given": "Yue"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            7,
                            28
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            7,
                            28
                        ]
                    ]
                },
                "abstract": "With the advent of big data, data mining theories and methods face new challenges. This paper tries to find the impacts of big data on data mining research through 23377 data mining-related papers published in Chinese academic journals during 1996--2016. By utilization of various methods of bibliometrics, this study conducts three different levels of analysis to gradually dig deeper into the contents of literature. For the macro-level, paper amount analysis results show that big data-related research began in 2012 and has brought new growth to data mining area. For the meso-level, journal distribution analysis results indicate that many other disciplines, such as arts and agriculture science, began to apply data mining techniques with the wide spread of big data. For the micro-level, co-word-based research topic clustering results imply that new topics emerged due to the easy access of big data, such as 'clouding computing' and 'teaching and learning analysis'.",
                "call-number": "10.1145/3265689.3265706",
                "collection-number": "17",
                "collection-title": "ICCSE'18",
                "container-title": "Proceedings of the 3rd International Conference on Crowd Science and Engineering",
                "DOI": "10.1145/3265689.3265706",
                "event-place": "Singapore, Singapore",
                "ISBN": "9781450365871",
                "keyword": "Impact, Data mining, Bibliometrics, Big data",
                "number": "Article 17",
                "number-of-pages": "5",
                "page": "1–5",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Impacts of Big Data on Data Mining Research: An Empirical Study of Chinese Journals",
                "URL": "https://doi.org/10.1145/3265689.3265706"
            }
        },
        {
            "10.1145/3017680.3017705": {
                "id": "10.1145/3017680.3017705",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Eickholt",
                        "given": "Jesse"
                    },
                    {
                        "family": "Shrestha",
                        "given": "Sharad"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            3,
                            8
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            3,
                            8
                        ]
                    ]
                },
                "abstract": "Cloud Computing and Big Data continue to be disruptive forces in computing and have made inroads in the Computer Science curriculum, with courses in Cloud Computing and Big Data being routinely offered at the graduate and undergraduate level. One major challenge in offering courses in Big Data and Cloud Computing is resources. The question is how to provide students with authentic experiences making use of current Cloud and Big Data resources and tools and do so in a cost effective manner. Historically, three options, namely physical clusters, virtual clusters and cloud-based clusters, have been used to support Big Data and Cloud Computing courses. Virtual clusters and cloud-based options are those that institutions have typically adopted and many arguments in favor of these options exist in the literature, citing cost and performance. Here we argue that teaching Big Data and Cloud Computing courses can be done making use of a physical cluster and that many of the existing arguments fail to take into account many important factors in their calculations. These factors include the flexibility and control of a physical cluster in responding to changes in industry, the ability to work with much larger datasets, and the synergy and broad applicability of an appropriately equipped physical cluster for courses such as Cloud Computing, Big Data and Data Mining. We present three possible configurations of a physical cluster which span the spectrum in terms of cost and provide cost comparisons of these configurations against virtual and cloud-based options, taking into account the unique requirements of an academic setting. While limitations do exist with a physical cluster and it is not an option for all situations, our analysis and experience indicates that there is great value in using a physical cluster to support teaching Cloud Computing and Big Data courses and it should not be dismissed.",
                "call-number": "10.1145/3017680.3017705",
                "collection-title": "SIGCSE '17",
                "container-title": "Proceedings of the 2017 ACM SIGCSE Technical Symposium on Computer Science Education",
                "DOI": "10.1145/3017680.3017705",
                "event-place": "Seattle, Washington, USA",
                "ISBN": "9781450346986",
                "keyword": "cloud computing, computing cluster, big data",
                "number-of-pages": "5",
                "page": "177–181",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Teaching Big Data and Cloud Computing with a Physical Cluster",
                "URL": "https://doi.org/10.1145/3017680.3017705"
            }
        },
        {
            "10.1145/2837060": {
                "id": "10.1145/2837060",
                "type": "BOOK",
                "issued": {
                    "date-parts": [
                        [
                            2015
                        ]
                    ]
                },
                "call-number": "10.1145/2837060",
                "container-title-short": "BigDAS '15",
                "event-place": "Jeju Island, Republic of Korea",
                "genre": "proceeding",
                "ISBN": "9781450338462",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Proceedings of the 2015 International Conference on Big Data Applications and Services"
            }
        },
        {
            "10.5555/2819009.2819232": {
                "id": "10.5555/2819009.2819232",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Baresi",
                        "given": "Luciano"
                    },
                    {
                        "family": "Menzies",
                        "given": "Tim"
                    },
                    {
                        "family": "Metzger",
                        "given": "Andreas"
                    },
                    {
                        "family": "Zimmermann",
                        "given": "Thomas"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2015,
                            5,
                            16
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2015,
                            5,
                            16
                        ]
                    ]
                },
                "abstract": "Big Data is about extracting valuable information from data in order to use it in intelligent ways such as to revolutionize decision-making in businesses, science and society. BIGDSE 2015 discusses the link between Big Data and software engineering and critically looks into issues such as cost-benefit of big data.",
                "call-number": "10.5555/2819009.2819232",
                "collection-title": "ICSE '15",
                "container-title": "Proceedings of the 37th International Conference on Software Engineering - Volume 2",
                "event-place": "Florence, Italy",
                "keyword": "software analytics, software engineering, big data",
                "number-of-pages": "2",
                "page": "965–966",
                "publisher": "IEEE Press",
                "title": "1st international workshop on big data software engineering (BIGDSE 2015)"
            }
        },
        {
            "10.1145/3230348.3230368": {
                "id": "10.1145/3230348.3230368",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Youness",
                        "given": "Madani"
                    },
                    {
                        "family": "Mohammed",
                        "given": "Erritali"
                    },
                    {
                        "family": "Jamaa",
                        "given": "Bengourram"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            4,
                            25
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            4,
                            25
                        ]
                    ]
                },
                "abstract": "Tweets classification or in general the classification of the social network's data is a recent field of scientific research, where researchers look for new methods to classify users data (tweets, Facebook's post...) into classes (positive, negative, neutral).This type of scientific research called sentiment analysis (SA) or opinion mining and it allows to extract the feelings, opinions or attitudes expressed in a tweet or a facebook post ...In this article, we describe how we can collect and store a large volume of data, which is in the form of tweets, in Hadoop Distributed File System (HDFS), and how we can classify these tweets using different classification methods, making a comparison between the well-known machine learning algorithms and a dictionary based-approach using the AFINN dictionary. The experimental results show that the AFINN dictionary outperforms the well-known machine learning algorithms.",
                "call-number": "10.1145/3230348.3230368",
                "collection-title": "ICIEB '18",
                "container-title": "Proceedings of the 2018 1st International Conference on Internet and e-Business",
                "DOI": "10.1145/3230348.3230368",
                "event-place": "Singapore, Singapore",
                "ISBN": "9781450363754",
                "keyword": "sentiment analysis, Twitter, opinion mining, Hadoop, big data, data mining",
                "number-of-pages": "6",
                "page": "124–129",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Twitter Data Classification Using Big Data Technologies",
                "URL": "https://doi.org/10.1145/3230348.3230368"
            }
        },
        {
            "10.1145/3230348.3230425": {
                "id": "10.1145/3230348.3230425",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Xia",
                        "given": "Huan"
                    },
                    {
                        "family": "Tang",
                        "given": "Shiqi"
                    },
                    {
                        "family": "Li",
                        "given": "Shuang"
                    },
                    {
                        "family": "Yu",
                        "given": "Xiaomin"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            4,
                            25
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            4,
                            25
                        ]
                    ]
                },
                "abstract": "Big data e-commerce is melting into our lives with an overwhelming trend, the establishment of big data e-commerce in closed communities is much easier to meet the consumer demand of community residents. This article is based on the analysis of the development of big data electronic commerce, presented establish big data electronic commerce in a closed community, to take Guizhou University of Finance and Economics \"Trust Me\" platform as an example. It introduces the functional requirements, platform structure, idea, operation mechanism and operation mode of the platform, and summarizes the advantages and improvement.",
                "call-number": "10.1145/3230348.3230425",
                "collection-title": "ICIEB '18",
                "container-title": "Proceedings of the 2018 1st International Conference on Internet and e-Business",
                "DOI": "10.1145/3230348.3230425",
                "event-place": "Singapore, Singapore",
                "ISBN": "9781450363754",
                "keyword": "closed community, e-commerce, big data",
                "number-of-pages": "4",
                "page": "43–46",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Application Research of Big Data E-commerce in Closed Community",
                "URL": "https://doi.org/10.1145/3230348.3230425"
            }
        },
        {
            "10.1145/3474944": {
                "id": "10.1145/3474944",
                "type": "BOOK",
                "issued": {
                    "date-parts": [
                        [
                            2021
                        ]
                    ]
                },
                "call-number": "10.1145/3474944",
                "container-title-short": "BDET 2021",
                "event-place": "Singapore, Singapore",
                "genre": "proceeding",
                "ISBN": "9781450389280",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "2021 the 3rd International Conference on Big Data Engineering and Technology (BDET)"
            }
        },
        {
            "10.1145/3255001": {
                "id": "10.1145/3255001",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Narayanan",
                        "given": "Krish"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2014,
                            3,
                            5
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2014,
                            3,
                            5
                        ]
                    ]
                },
                "call-number": "10.1145/3255001",
                "collection-title": "SIGCSE '14",
                "container-title": "Proceedings of the 45th ACM technical symposium on Computer science education",
                "DOI": "10.1145/3255001",
                "event-place": "Atlanta, Georgia, USA",
                "ISBN": "9781450326056",
                "page": "",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Session details: Big data",
                "URL": "https://doi.org/10.1145/3255001"
            }
        },
        {
            "10.1145/2812428.2812429": {
                "id": "10.1145/2812428.2812429",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Pokorný",
                        "given": "Jaroslav"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2015,
                            6,
                            25
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2015,
                            6,
                            25
                        ]
                    ]
                },
                "abstract": "Now we have a number of database technologies called usually NoSQL, like key-value, column-oriented, and document stores as well as search engines and graph databases. Whereas SQL software vendors offer advanced products with the capability to handle highly complex queries and transactions, NoSQL databases share rather characteristics concerning scaling and performance, as e.g. auto-sharding, distributed query support, and integrated caching. Their drawbacks can be a lack of schema or data consistency, difficulty in testing and maintaining, and absence of a higher query language. Complex data modelling and the SQL language as the only access tool to data are missing here. On the other hand, last studies show that both SQL and NoSQL databases have value for both for transactional and analytical Big Data. Top databases providers offer rearchitected database technologies combining row data stores with columnar in-memory compression enabling processing large data sets and analytical querying, often over massive, continuous data streams. The technological progress led to development of massively parallel processing analytic databases. The paper presents some details of current database technologies, their pros and cons in different application environments, and emerging trends in this area.",
                "call-number": "10.1145/2812428.2812429",
                "collection-title": "CompSysTech '15",
                "container-title": "Proceedings of the 16th International Conference on Computer Systems and Technologies",
                "DOI": "10.1145/2812428.2812429",
                "event-place": "Dublin, Ireland",
                "ISBN": "9781450333573",
                "keyword": "transaction processing, NewSQL databases, data distribution, NoSQL databases, big analytics, database technologies, big data",
                "number-of-pages": "12",
                "page": "1–12",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Database technologies in the world of big data",
                "URL": "https://doi.org/10.1145/2812428.2812429"
            }
        },
        {
            "10.1145/3424978.3425001": {
                "id": "10.1145/3424978.3425001",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Wang",
                        "given": "Guotian"
                    },
                    {
                        "family": "Liu",
                        "given": "Xu"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2020,
                            10,
                            20
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2020,
                            10,
                            20
                        ]
                    ]
                },
                "abstract": "With the frequent occurrence of university security accidents in recent years, university security issues have become a hot topic of concern to society and research. Currently for university security, the most serious problems are the inability to response to emergencies timely due to the short of real-time data acquisition and processing devices. For these existing issues on the aspects of university security mentioned above, this paper focuses research on the university security big data and emergency collaborative disposal, the university big data integration system is established firstly for university security big data real-time collection, processing, analysis and storage, then the UML based multi-department emergency collaborative disposal organization structure is proposed to realize the efficient and coordinated emergency response operations of multi-department when emergencies occur, meanwhile, the emergency disposal and supplies dispatch scheme is designed to optimize relief supplies distribution and emergency response results. Lastly, the university security big data and emergency coordination system is implemented, and accuracy experiment of the system is conducted, the results of system experiment and implementation are not only indicate the effectiveness and feasibility of the methods proposed by this paper, but also proved to be capable of general applicability to university security management.",
                "call-number": "10.1145/3424978.3425001",
                "collection-number": "23",
                "collection-title": "CSAE 2020",
                "container-title": "Proceedings of the 4th International Conference on Computer Science and Application Engineering",
                "DOI": "10.1145/3424978.3425001",
                "event-place": "Sanya, China",
                "ISBN": "9781450377720",
                "keyword": "Big Data, Collaborative Disposal, University Security",
                "number": "Article 23",
                "number-of-pages": "8",
                "page": "1–8",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Research on University Security Big Data and Emergency Collaborative Disposal",
                "URL": "https://doi.org/10.1145/3424978.3425001"
            }
        },
        {
            "10.1145/2379436": {
                "id": "10.1145/2379436",
                "type": "BOOK",
                "issued": {
                    "date-parts": [
                        [
                            2012
                        ]
                    ]
                },
                "call-number": "10.1145/2379436",
                "container-title-short": "ASBD '12",
                "event-place": "Portland, Oregon, USA",
                "genre": "proceeding",
                "ISBN": "9781450314442",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Proceedings of the 2nd Workshop on Architectures and Systems for Big Data"
            }
        },
        {
            "10.5555/2840819.2840927": {
                "id": "10.5555/2840819.2840927",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Zhu",
                        "given": "Yada"
                    },
                    {
                        "family": "Xiong",
                        "given": "Jinjun"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2015,
                            11,
                            2
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2015,
                            11,
                            2
                        ]
                    ]
                },
                "abstract": "Big data analytics is the latest spotlight with all the glare of fame ranging from media coverage to booming start-up companies to eye-catching merges and acquisitions. On the contrary, the $336 billion industry of semiconductor was seen as an \"old-fashioned\" business, with fading interests from the best and brightest among young graduates and engineers. How will modern big data analytics help the semiconductor industry walk through this transition? This paper answers this question via a number of practical but challenging problems arising from semiconductor manufacturing process. We show that many existing machine learning algorithms are not well positioned to solve these problems, and novel techniques involving temporal, structural and hierarchical properties need to be developed to solve these problems.",
                "call-number": "10.5555/2840819.2840927",
                "collection-title": "ICCAD '15",
                "container-title": "Proceedings of the IEEE/ACM International Conference on Computer-Aided Design",
                "event-place": "Austin, TX, USA",
                "ISBN": "9781467383899",
                "keyword": "semiconductor, analytics, manufacturing, Big data",
                "number-of-pages": "5",
                "page": "776–780",
                "publisher": "IEEE Press",
                "title": "Modern Big Data Analytics for \"Old-fashioned\" Semiconductor Industry Applications"
            }
        },
        {
            "10.1145/2676536.2676538": {
                "id": "10.1145/2676536.2676538",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Chen",
                        "given": "Xin"
                    },
                    {
                        "family": "Vo",
                        "given": "Hoang"
                    },
                    {
                        "family": "Aji",
                        "given": "Ablimit"
                    },
                    {
                        "family": "Wang",
                        "given": "Fusheng"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2014,
                            11,
                            4
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2014,
                            11,
                            4
                        ]
                    ]
                },
                "abstract": "The growth of spatial big data has been explosive thanks to cost-effective and ubiquitous positioning technologies, and the generation of data from multiple sources in multi-forms. Such emerging spatial data has high potential to create new insights and values for our life through spatial analytics. However, spatial data analytics faces two major challenges. First, spatial data is both data-and compute-intensive due to the massive amounts of data and the multi-dimensional nature, which requires high performance spatial computing infrastructure and methods. Second, spatial big data sources are often isolated, for example, OpenStreetMap, census data and Twitter tweets are independent data sources. This leads to incompleteness of information and sometimes limited data accuracy, thus limited values from the data. Integrating spatial big data analytics by consolidating multiple data sources provides significant potential for data quality improvement in terms of completeness and accuracy, and much increased values derived from the data. In this paper, we present our vision of a high performance integrated spatial big data analytics framework. We provide a scalable spatial query based data integration engine with MapReduce, and demonstrate integrated spatial data analytics through a few use cases in our preliminary work. We then present our future plan on integrated spatial big data analytics for improving public health research and applications.",
                "call-number": "10.1145/2676536.2676538",
                "collection-title": "BigSpatial '14",
                "container-title": "Proceedings of the 3rd ACM SIGSPATIAL International Workshop on Analytics for Big Geospatial Data",
                "DOI": "10.1145/2676536.2676538",
                "event-place": "Dallas, Texas",
                "ISBN": "9781450331326",
                "keyword": "MapReduce, spatial analytics, GIS, database, data warehouse",
                "number-of-pages": "4",
                "page": "11–14",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "High performance integrated spatial big data analytics",
                "URL": "https://doi.org/10.1145/2676536.2676538"
            }
        },
        {
            "10.1145/3433996.3434027": {
                "id": "10.1145/3433996.3434027",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Ding",
                        "given": "Shifu"
                    },
                    {
                        "family": "Liu",
                        "given": "Yan"
                    },
                    {
                        "family": "Zhang",
                        "given": "Jianjun"
                    },
                    {
                        "family": "Tan",
                        "given": "Yaqi"
                    },
                    {
                        "family": "Li",
                        "given": "Xiaoxia"
                    },
                    {
                        "family": "Tang",
                        "given": "RuiChun"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2020,
                            10,
                            23
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2020,
                            10,
                            23
                        ]
                    ]
                },
                "abstract": "Healthcare Big Data Platform is the important content in the process of medical information industry. To a certain extent, it represents the overall level of the regional informatization. It is also a data exchange and sharing platform connecting the basic systems of local various medical and health institutions, and it is also the base and carrier to integrate the regional information system. This paper introduces the local regional medical informatization construction, planning architecture, data center construction mode and technical realization methods. Through this project, the informatization level of basic health agencies and all hospitals will have been greatly improved. It can provide more convenient and high-quality medical service for patients, alleviates \"difficulty and expensive\" problem effectively.",
                "call-number": "10.1145/3433996.3434027",
                "collection-title": "CAIH2020",
                "container-title": "Proceedings of the 2020 Conference on Artificial Intelligence and Healthcare",
                "DOI": "10.1145/3433996.3434027",
                "event-place": "Taiyuan, China",
                "ISBN": "9781450388641",
                "keyword": "Healthcare, Electronic Health Record, EMR, Big Data, SOA",
                "number-of-pages": "7",
                "page": "170–176",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "The Planning and Construction of Healthcare Big Data Platform",
                "URL": "https://doi.org/10.1145/3433996.3434027"
            }
        },
        {
            "10.1145/2379436.2379437": {
                "id": "10.1145/2379436.2379437",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Regola",
                        "given": "Nathan"
                    },
                    {
                        "family": "Cieslak",
                        "given": "David A."
                    },
                    {
                        "family": "Chawla",
                        "given": "Nitesh V."
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2012,
                            6,
                            9
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2012,
                            6,
                            9
                        ]
                    ]
                },
                "abstract": "Solid state disks (or flash disks) are decreasing in cost per gigabyte and are being incorporated into many appliances, such as the Oracle Database Appliance [8]. Databases--and more specifically data warehouses--are often utilized to support large scale data analysis and decision support systems. Decision makers prefer information in real time. Traditional storage systems that are based on magnetic disks achieve high performance by utilizing many disks for parallel operations in RAID arrays. However, this performance is only possible if requests represent a reasonable fraction of the RAID stripe size, or I/O transactions will suffer from high overhead. Solid state disks have the potential to increase the speed of data retrieval for mission critical workloads that require real time applications, such as analytic dashboards. However, solid state disks behave differently than magnetic hard disks due to the limitations of rewriting NAND flash based blocks. Therefore, this work presents benchmark results for a modern relational database that stores data on solid state disks, and contrasts this performance to a ten disk RAID 10 array, a traditional storage design for high performance database data blocks. The preliminary results show that a single solid state disk is able to outperform the array for queries summarizing a data set for a variety of OLAP cube dimensions. Future work will explore the low level database performance in more detail.",
                "call-number": "10.1145/2379436.2379437",
                "collection-title": "ASBD '12",
                "container-title": "Proceedings of the 2nd Workshop on Architectures and Systems for Big Data",
                "DOI": "10.1145/2379436.2379437",
                "event-place": "Portland, Oregon, USA",
                "ISBN": "9781450314442",
                "keyword": "query performance, solid state disk",
                "number-of-pages": "6",
                "page": "4–9",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "The constraints of magnetic versus flash disk capabilities in big data analysis",
                "URL": "https://doi.org/10.1145/2379436.2379437"
            }
        },
        {
            "10.1145/3424978.3425010": {
                "id": "10.1145/3424978.3425010",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Man",
                        "given": "Rui"
                    },
                    {
                        "family": "Zhou",
                        "given": "Guomin"
                    },
                    {
                        "family": "Fan",
                        "given": "Jingchao"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2020,
                            10,
                            20
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2020,
                            10,
                            20
                        ]
                    ]
                },
                "abstract": "Scientific data is an important strategic resource in the era of big data. Efficient management and wide circulation are the key ways to enhance the value of scientific data resources. With the transformation of the industrial society into the information society, the importance of scientific data management is also increasing all over the world, which continuously promotes the maturity of scientific data management and sharing. In this article, through comprehensive research of scientific data management ideas, policies, practices and results, the analysis summarizes the advanced experience of international scientific data management, for the similar problems and challenges existing in the research in China, puts forward the future a period of time the direction and suggestions on the development of scientific data management: 1. the specification of various kinds of degree of the standardization of scientific data resources; 2. To strengthen data mining capacity; 3. To strengthen the cultivation of talents in data science; 4. To strengthen international cooperation and enhance core competitiveness in the big data era.",
                "call-number": "10.1145/3424978.3425010",
                "collection-number": "32",
                "collection-title": "CSAE 2020",
                "container-title": "Proceedings of the 4th International Conference on Computer Science and Application Engineering",
                "DOI": "10.1145/3424978.3425010",
                "event-place": "Sanya, China",
                "ISBN": "9781450377720",
                "keyword": "Scientific data, Scientific data management, Big data, Opening and sharing of data resource",
                "number": "Article 32",
                "number-of-pages": "6",
                "page": "1–6",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Research on Scientific Data Management in Big Data Era",
                "URL": "https://doi.org/10.1145/3424978.3425010"
            }
        },
        {
            "10.1145/3411681.3412951": {
                "id": "10.1145/3411681.3412951",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Liao",
                        "given": "TongXin"
                    },
                    {
                        "family": "Feng",
                        "given": "XinHui"
                    },
                    {
                        "family": "Sun",
                        "given": "YuanLi"
                    },
                    {
                        "family": "Wang",
                        "given": "HongTing"
                    },
                    {
                        "family": "Liao",
                        "given": "Cong"
                    },
                    {
                        "family": "Li",
                        "given": "YuanBing"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2020,
                            7,
                            26
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2020,
                            7,
                            26
                        ]
                    ]
                },
                "abstract": "The essence of big data education is the combination of network education and traditional education. The development of big data and Internet industry has brought new changes to the traditional learning model. The way to acquire knowledge is not limited to books and classrooms, and the Internet era is the era of information explosion, so the Internet has gradually become the most important channel for people to acquire knowledge. Therefore, the big data learning platform has gradually become a new mode of teaching adopted by schools. In the big data teaching mode, personalized recommendation system plays an indispensable role in online education.",
                "call-number": "10.1145/3411681.3412951",
                "collection-title": "ICIEI '20",
                "container-title": "Proceedings of the 5th International Conference on Information and Education Innovations",
                "DOI": "10.1145/3411681.3412951",
                "event-place": "London, United Kingdom",
                "ISBN": "9781450375757",
                "keyword": "Online teaching and cloud platform, Keywords Big data, recommended",
                "number-of-pages": "5",
                "page": "35–39",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Online Teaching Platform Based on Big Data Recommendation System",
                "URL": "https://doi.org/10.1145/3411681.3412951"
            }
        },
        {
            "10.1145/3544538.3544635": {
                "id": "10.1145/3544538.3544635",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Alves Neto",
                        "given": "Antonio Jose"
                    },
                    {
                        "family": "Carneiro Neto",
                        "given": "Jose Aprigio"
                    },
                    {
                        "family": "Moreno Ordonez",
                        "given": "Edward David"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2022,
                            6,
                            1
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2022,
                            6,
                            1
                        ]
                    ]
                },
                "abstract": "The growing gap between users and the Big Data analytics requires innovative tools that address the challenges faced by big data such as volume, variety, and velocity. Therefore, it becomes computationally inefficient to analyze this massive volume of data. Moreover, advancements in the field of Big Data applications and data science poses additional challenges, where High-Performance Computing (HPC) solution has become a key issue and has attracted attention in recent years. Because of the high costs to obtain a HPC, the researchers are looking for a solution that copes with the increasing demand on processing power due to the expanding amount of the data produced. Eventually, they have been trying to implement big data clusters based on low-cost and low-energy hardware. The goal of this paper is to identify how is possible to develop a big data cluster using hardware structures of low cost, exposing the studies found in literature. In order to fulfill this, a Systematic Literature Mapping (SLM) was realized, resulting in several relevant papers which are able to response three research questions. The SLM identified Single Board Computers (SBC) as hardware structure most used, being the Raspberry PI with major citations, and Apache Hadoop and Apache Spark as big data platforms used in these clusters. The validation of these clusters it was done with some popular algorithms, where the algorithms K-Means and Map-Reduce were the most quoted in the selected studies, this last based on its original approach, unlike the approach from Hadoop Map-Reduce.",
                "call-number": "10.1145/3544538.3544635",
                "collection-number": "2",
                "collection-title": "EATIS '22",
                "container-title": "Proceedings of the 11th Euro American Conference on Telematics and Information Systems",
                "DOI": "10.1145/3544538.3544635",
                "event-place": "Aveiro, Portugal",
                "ISBN": "9781450397384",
                "keyword": "Big Data, Cluster, Low Cost, Raspberry Pi, Beowulf, Systematic Literature Mapping, Single Board Card",
                "number": "Article 2",
                "number-of-pages": "7",
                "page": "1–7",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Low-cost clusters on big data - A systematic study",
                "URL": "https://doi.org/10.1145/3544538.3544635"
            }
        },
        {
            "10.1145/1541880.1541883": {
                "id": "10.1145/1541880.1541883",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Batini",
                        "given": "Carlo"
                    },
                    {
                        "family": "Cappiello",
                        "given": "Cinzia"
                    },
                    {
                        "family": "Francalanci",
                        "given": "Chiara"
                    },
                    {
                        "family": "Maurino",
                        "given": "Andrea"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2009,
                            7,
                            30
                        ]
                    ]
                },
                "abstract": "The literature provides a wide range of techniques to assess and improve the quality of data. Due to the diversity and complexity of these techniques, research has recently focused on defining methodologies that help the selection, customization, and application of data quality assessment and improvement techniques. The goal of this article is to provide a systematic and comparative description of such methodologies. Methodologies are compared along several dimensions, including the methodological phases and steps, the strategies and techniques, the data quality dimensions, the types of data, and, finally, the types of information systems addressed by each methodology. The article concludes with a summary description of each methodology.",
                "call-number": "10.1145/1541880.1541883",
                "collection-number": "16",
                "container-title": "ACM Comput. Surv.",
                "DOI": "10.1145/1541880.1541883",
                "ISSN": "0360-0300",
                "issue": "3",
                "keyword": "information system, quality dimension, data quality measurement, data quality improvement, methodology, Data quality, data quality assessment",
                "number": "Article 16",
                "number-of-pages": "52",
                "page": "1–52",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "July 2009",
                "title": "Methodologies for data quality assessment and improvement",
                "URL": "https://doi.org/10.1145/1541880.1541883",
                "volume": "41"
            }
        },
        {
            "10.1145/3289402.3289525": {
                "id": "10.1145/3289402.3289525",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Aziz",
                        "given": "Khadija"
                    },
                    {
                        "family": "Zaidouni",
                        "given": "Dounia"
                    },
                    {
                        "family": "Bellafkih",
                        "given": "Mostafa"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            10,
                            24
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            10,
                            24
                        ]
                    ]
                },
                "abstract": "Machine learning is a field within artificial intelligence that allows machines to learn on their own from existing information to make predictions or/and decisions. There are three main categories of machine learning techniques: Collaborative filtering (for making recommendations), Clustering (for discovering structure in collections of data) and Classification (form of supervised learning). Machine learning helps users to make better decisions, Machine learning algorithms create patterns based on previous information and use them to design predictive models, then, use this models to obtain predictions about future data. A huge amount of data from several sources need methods and techniques to be processed correctly, in order to exploit this data efficiently, machine learning is a great technology for exploiting the needs in big data analysis. This paper describes the implementation of Apache Spark MLlib and Apache Mahout in order to process Big Data using Machine Learning algorithms. Furthermore, we conduct experimental simulations to show the difference between this two Machine Learning frameworks. Subsequently, we discuss the most striking observations that emerge from the comparison of these technologies through several experimental studies.",
                "call-number": "10.1145/3289402.3289525",
                "collection-number": "25",
                "collection-title": "SITA'18",
                "container-title": "Proceedings of the 12th International Conference on Intelligent Systems: Theories and Applications",
                "DOI": "10.1145/3289402.3289525",
                "event-place": "Rabat, Morocco",
                "ISBN": "9781450364621",
                "keyword": "MLlib, Mahout, Machine Learning, Collaborative Filtering, Hadoop, Classification, Clustering, Big Data, Spark",
                "number": "Article 25",
                "number-of-pages": "6",
                "page": "1–6",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Big Data Processing using Machine Learning algorithms: MLlib and Mahout Use Case",
                "URL": "https://doi.org/10.1145/3289402.3289525"
            }
        },
        {
            "10.1145/3281375.3281382": {
                "id": "10.1145/3281375.3281382",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Bieh-Zimmert",
                        "given": "Oliver"
                    },
                    {
                        "family": "Felden",
                        "given": "Carsten"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            9,
                            25
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            9,
                            25
                        ]
                    ]
                },
                "abstract": "The increasing variety of big data technologies in open source communities is challenging organizations to generate value from those advancements. The technology landscape is missing an overall perspective that clarifies the fragmented understanding of technologies, unpredictable lifecycles, and the unknown adoption for organizations to enable their business with useful technologies. More than one million contributions of features, bugs, and changes were pushed on public available code repositories to develop big data technologies with hidden understanding of the underlying data basis. Using this source could help to identify insights about technological domains as well as their adoption process of contributors to new uprising big data technologies. A knowledge discovery process provided the potential to analyze 269 big data technologies regarding their contribution behavior of over 21,000 contributors. As a result, investigations show an ecosystem of structuring big data technologies based on dynamic contributor networks that have implications on organizations adoption.",
                "call-number": "10.1145/3281375.3281382",
                "collection-title": "MEDES '18",
                "container-title": "Proceedings of the 10th International Conference on Management of Digital EcoSystems",
                "DOI": "10.1145/3281375.3281382",
                "event-place": "Tokyo, Japan",
                "ISBN": "9781450356220",
                "keyword": "big data, knowledge discovery, open source, adoption",
                "number-of-pages": "4",
                "page": "55–58",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "A knowledge discovery in community contributions of big data technologies",
                "URL": "https://doi.org/10.1145/3281375.3281382"
            }
        },
        {
            "10.1145/3220228.3220238": {
                "id": "10.1145/3220228.3220238",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Galletta",
                        "given": "Antonino"
                    },
                    {
                        "family": "Allam",
                        "given": "Salma"
                    },
                    {
                        "family": "Carnevale",
                        "given": "Lorenzo"
                    },
                    {
                        "family": "Bekri",
                        "given": "Moulay Ali"
                    },
                    {
                        "family": "Ouahbi",
                        "given": "Rachid El"
                    },
                    {
                        "family": "Villari",
                        "given": "Massimo"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            4,
                            20
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            4,
                            20
                        ]
                    ]
                },
                "abstract": "Nowadays, thanks to new technologies, we are observing an explosion of data in different fields such as clinical, environmental and so on. In this context, a typical example of the well-known Big Data problem is represented by visualization. In this work, we propose an innovative platform for managing the oceanographic acquisitions. More specifically, we present two innovative visualization techniques: general overview and site specific observation. Experiments prove the goodness of the proposed system in terms both of performance and user experience.",
                "call-number": "10.1145/3220228.3220238",
                "collection-title": "ICGDA '18",
                "container-title": "Proceedings of the International Conference on Geoinformatics and Data Analysis",
                "DOI": "10.1145/3220228.3220238",
                "event-place": "Prague, Czech Republic",
                "ISBN": "9781450364454",
                "keyword": "geolocation, big data, oceanography, microservices, IoT, big data visualization, acidification",
                "number-of-pages": "5",
                "page": "103–107",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "An innovative methodology for big data visualization in oceanographic domain",
                "URL": "https://doi.org/10.1145/3220228.3220238"
            }
        },
        {
            "10.1145/3234664.3234677": {
                "id": "10.1145/3234664.3234677",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Fei",
                        "given": "Xian-hong"
                    },
                    {
                        "family": "Zhai",
                        "given": "Cheng-gong"
                    },
                    {
                        "family": "Qiao",
                        "given": "Han"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            6,
                            22
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            6,
                            22
                        ]
                    ]
                },
                "abstract": "The research on the Clothing and Accouterment security based on big data plays an important role in deepening the army's reform and strengthening the scientific management of the army. This paper introduces the main influence of big data on the installation security, analyzes the application status of data in the installation security, and puts forward the application assumption of the big data in the installation security.",
                "call-number": "10.1145/3234664.3234677",
                "collection-title": "HPCCT 2018",
                "container-title": "Proceedings of the 2018 2nd High Performance Computing and Cluster Technologies Conference",
                "DOI": "10.1145/3234664.3234677",
                "event-place": "Beijing, China",
                "ISBN": "9781450364850",
                "keyword": "Big data, data analysis, clothing and accouterment security",
                "number-of-pages": "5",
                "page": "19–23",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Research on Clothing and Accouterment Security Based on Big Data",
                "URL": "https://doi.org/10.1145/3234664.3234677"
            }
        },
        {
            "10.1145/2448917.2448925": {
                "id": "10.1145/2448917.2448925",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Salvo",
                        "given": "Michael J."
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2012,
                            9,
                            1
                        ]
                    ]
                },
                "abstract": "The hype machine---media, corporate communications, and futurist prognosticators---are hard at work promoting Big Data. There are computing and storage resources that, like the \"dark fiber\" installed at the turn of the millennium that now carries streaming video, are looking for huge data sets that require the powerful processing and tremendous storage capacity of the new infrastructure. And there is no better confluence than that provided by the impetus to rearticulate Communication Design Quarterly in an age of Big Data. The New York Times has been running articles about Big Data for some time:\"Big data is all about exploration without preconceived notions.\"",
                "call-number": "10.1145/2448917.2448925",
                "container-title": "Commun. Des. Q. Rev",
                "DOI": "10.1145/2448917.2448925",
                "issue": "1",
                "number-of-pages": "4",
                "page": "37–40",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "September 2012",
                "title": "Visual rhetoric and big data: design of future communication",
                "URL": "https://doi.org/10.1145/2448917.2448925",
                "volume": "1"
            }
        },
        {
            "10.1145/2743065.2743110": {
                "id": "10.1145/2743065.2743110",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Padmapriya",
                        "given": "V."
                    },
                    {
                        "family": "Amudhavel",
                        "given": "J."
                    },
                    {
                        "family": "Gowri",
                        "given": "V."
                    },
                    {
                        "family": "Lakshmipriya",
                        "given": "K."
                    },
                    {
                        "family": "Vinothini",
                        "given": "S."
                    },
                    {
                        "family": "Kumar",
                        "given": "K. Prem"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2015,
                            3,
                            6
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2015,
                            3,
                            6
                        ]
                    ]
                },
                "abstract": "The Big data has a huge volume of data sets, which is used to process the conventional data processing applications. In this paper, the Flex Analytics framework is used to enhance the scalability and flexibility of analyzing and processing the data and provide visualization of data. The MapReduce framework of the fuzzy logic model is used in big data for the degree of uncertainty and to reduce the imbalance of data. The MapReduce framework for large-scale extreme learning machine for massive and dispersed data and machine learning approach is used to find the bottleneck in the peer-peer network. The emerging distributed cloud data centers with the security framework using Hadoop technology is used to process larger datasets. The financial data standard is utilized to eliminate the data redundancy and noise.",
                "call-number": "10.1145/2743065.2743110",
                "collection-number": "45",
                "collection-title": "ICARCSET '15",
                "container-title": "Proceedings of the 2015 International Conference on Advanced Research in Computer Science Engineering & Technology (ICARCSET 2015)",
                "DOI": "10.1145/2743065.2743110",
                "event-place": "Unnao, India",
                "ISBN": "9781450334419",
                "keyword": "unstructured data, Big data, Data analytics, infringement, monitor, application, exploration",
                "number": "Article 45",
                "number-of-pages": "5",
                "page": "1–5",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Demystifying Challenges, Opportunities and Issues of Big Data Frameworks",
                "URL": "https://doi.org/10.1145/2743065.2743110"
            }
        },
        {
            "10.1145/2361999.2362038": {
                "id": "10.1145/2361999.2362038",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Miner",
                        "given": "Donald"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2012,
                            8,
                            20
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2012,
                            8,
                            20
                        ]
                    ]
                },
                "abstract": "Greenplum is using Hadoop and several other open source tools in interesting ways as part of a big data architecture with their Greenplum Database (a scale-out MPP SQL database).",
                "call-number": "10.1145/2361999.2362038",
                "collection-title": "WICSA/ECSA '12",
                "container-title": "Proceedings of the WICSA/ECSA 2012 Companion Volume",
                "DOI": "10.1145/2361999.2362038",
                "event-place": "Helsinki, Finland",
                "ISBN": "9781450315685",
                "keyword": "EMC, MADlib, R, Mahout, data science, Hadoop, database, PostGIS, geospatial, Greenplum, MPP, Solr",
                "number-of-pages": "1",
                "page": "176",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Unified analytics platform for big data",
                "URL": "https://doi.org/10.1145/2361999.2362038"
            }
        },
        {
            "10.1145/2538862.2544280": {
                "id": "10.1145/2538862.2544280",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Hamid",
                        "given": "Nadeem Abdul"
                    },
                    {
                        "family": "Benzel",
                        "given": "Steven"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2014,
                            3,
                            5
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2014,
                            3,
                            5
                        ]
                    ]
                },
                "abstract": "A number of contextualized approaches to teaching introductory Computer Science (CS) courses have been developed in the past few years, catering to students with different interests and learning styles. For instance, entire courses have been developed around media computation or robots (real and virtual). There is however one context which, to our knowledge, has not been exploited in a systematic fashion - that of \"big data,\" by which we mean massive, openly accessible online datasets from a wide variety of sources. We present progress on a code framework and methodology to facilitate the incorporation of large, online data sets into traditional CS1 and CS2 courses. The goal of our project is to develop a way to provide students a library that relieves them from low-level issues of reading and parsing raw data from web-based data sources and that interfaces with data structures and representations defined by students themselves. In addition, the library requires minimal syntactic overhead to use its functionality and allows students and instructors to focus on algorithmic exercises involving processing live and large data obtained from the Internet. At a minimum, the library should serve to create drop-in replacements for traditional programming exercises in introductory courses - raising the engagement level by having students deal with \"real\" data rather than artificial data provided through standard input.",
                "call-number": "10.1145/2538862.2544280",
                "collection-title": "SIGCSE '14",
                "container-title": "Proceedings of the 45th ACM technical symposium on Computer science education",
                "DOI": "10.1145/2538862.2544280",
                "event-place": "Atlanta, Georgia, USA",
                "ISBN": "9781450326056",
                "keyword": "big data, CS1",
                "number-of-pages": "1",
                "page": "710",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Towards engaging big data for CS1/2 (abstract only)",
                "URL": "https://doi.org/10.1145/2538862.2544280"
            }
        },
        {
            "10.1145/2998575": {
                "id": "10.1145/2998575",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Labouseur",
                        "given": "Alan G."
                    },
                    {
                        "family": "Matheus",
                        "given": "Carolyn C."
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            1,
                            4
                        ]
                    ]
                },
                "call-number": "10.1145/2998575",
                "collection-number": "6",
                "container-title": "J. Data and Information Quality",
                "DOI": "10.1145/2998575",
                "ISSN": "1936-1955",
                "issue": "2",
                "keyword": "big data, graph systems, internet of things, relational systems, Dynamic data quality",
                "number": "Article 6",
                "number-of-pages": "3",
                "page": "1–3",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "February 2017",
                "title": "An Introduction to Dynamic Data Quality Challenges",
                "URL": "https://doi.org/10.1145/2998575",
                "volume": "8"
            }
        },
        {
            "10.1145/2656434.2657486": {
                "id": "10.1145/2656434.2657486",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Villanustre",
                        "given": "Flavio G."
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2014,
                            10,
                            13
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2014,
                            10,
                            13
                        ]
                    ]
                },
                "abstract": "The Big Data revolution has already happened and, through it, organizations started realizing the potential of using data to take better informed decisions, mitigate risks and overall better control their destiny. With all the benefits that Big Data brings, it also creates new challenges; the growing talent gap possibly being the most representative of them all. In order to effectively leverage Big Data, a new profession is emerging: the data scientist. Tasked with understanding the methodologies to process and analyze vast and complex data, this professional must possess knowledge in a broad spectrum of domains, including mathematics (calculus, linear algebra, statistics, probabilities and even possibly category theory), programming languages (Python and R being frequently cited), data processing and analysis expertise (profiling, parsing, cleansing, linking), machine learning techniques (supervised and unsupervised learning, dimensionality reduction, feature selection, etc.) and business domain knowledge. While it is conceivable to identify individuals that can achieve this breadth of knowledge with significant depth, it is unreasonable to expect this to be the norm, so these individuals fall usually far into the upper tail of the population distribution. To make things worse, the current toolsets available to the data scientist tend to be very involved and require considerable amounts of time to develop applications, reducing the overall effectiveness of these experts. The solution to this talent gap is certainly not to try and breed a new step up the evolutionary ladder that can cope with this vast knowledge, but to create radically different abstractions as part of the toolsets that data scientists use, to increase efficiency and reduce the scope of the basic knowledge required to build Big Data applications. During this presentation we will explore this challenge and provide a new perspective on more efficient toolsets for Big Data applications.",
                "call-number": "10.1145/2656434.2657486",
                "collection-title": "RIIT '14",
                "container-title": "Proceedings of the 3rd annual conference on Research in information technology",
                "DOI": "10.1145/2656434.2657486",
                "event-place": "Atlanta, Georgia, USA",
                "ISBN": "9781450327114",
                "keyword": "data analysis, KEL, dataflow programming, data science, HPCC, ECL, declarative programming",
                "number-of-pages": "2",
                "page": "1–2",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Big data trends and evolution: a human perspective",
                "URL": "https://doi.org/10.1145/2656434.2657486"
            }
        },
        {
            "10.1145/3386723.3387826": {
                "id": "10.1145/3386723.3387826",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Es-Sabery",
                        "given": "Fatima"
                    },
                    {
                        "family": "Hair",
                        "given": "Abdellatif"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2020,
                            3,
                            31
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2020,
                            3,
                            31
                        ]
                    ]
                },
                "abstract": "CCS (Cluster Computing System) is coming to solve the problems of standard technology. Whose, objective is to improve the performance/power efficiency of a single processor for storing and mining the large data sets, using the parallel programming to read and process the massive data sets on multiple disks and CPUs. The thing which makes these systems somewhat performant than the standard technology is the physical organization of computing nodes in the cluster. Currently, this kind of cluster does not entirely solve the problem because it comes with its challenges, which are Node failures, Computations, Network Bottleneck, and Distributed programming. All these problems are coming when we are mining and storing the massive volume of data using cluster computing. To solve these challenges, Google invented a new Big Data framework of data processing called MapReduce, to manage large scale data processing across large clusters of commodity servers. The paper outlines the running of CCS and presents its challenges in this era of Big Data. Moreover, it introduces the most popular Big Data solutions proposed to overcome the CCS challenges. Also, it shows how Big Data technologies solve CCS issues. Generally, the main goal of this work is to provide a better understanding of the challenges of CCS and identify the essential big data solutions in this increasingly important area.",
                "call-number": "10.1145/3386723.3387826",
                "collection-number": "7",
                "collection-title": "NISS2020",
                "container-title": "Proceedings of the 3rd International Conference on Networking, Information Systems & Security",
                "DOI": "10.1145/3386723.3387826",
                "event-place": "Marrakech, Morocco",
                "ISBN": "9781450376341",
                "keyword": "CCS, MapReduce, Big Data, Distributed File System, Challenges",
                "number": "Article 7",
                "number-of-pages": "7",
                "page": "1–7",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Big Data Solutions Proposed for Cluster Computing Systems Challenges: A survey",
                "URL": "https://doi.org/10.1145/3386723.3387826"
            }
        },
        {
            "10.1145/3469968": {
                "id": "10.1145/3469968",
                "type": "BOOK",
                "issued": {
                    "date-parts": [
                        [
                            2021
                        ]
                    ]
                },
                "call-number": "10.1145/3469968",
                "container-title-short": "ICBDC '21",
                "event-place": "Shenzhen, China",
                "genre": "proceeding",
                "ISBN": "9781450389808",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Proceedings of the 6th International Conference on Big Data and Computing"
            }
        },
        {
            "10.1145/3408127.3408180": {
                "id": "10.1145/3408127.3408180",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Yang",
                        "given": "Pinglin"
                    },
                    {
                        "family": "Guo",
                        "given": "Gaizhi"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2020,
                            6,
                            19
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2020,
                            6,
                            19
                        ]
                    ]
                },
                "abstract": "With the development of cloud computing, big data, and the Internet of Things, for the data collection and daily drama of the postal express industry, in the face of such a large-scale data set, the traditional storage and calculation related theories and methods can no longer meet the massive and multi-source access and processing of heterogeneous data. The article analyzes the characteristics of postal data, focuses on the Hadoop and Spark platform architectures, and compares the performance differences between the two platforms through experiments. At the same time, according to the characteristics of Hadoop and Spark big data platforms, they learn from each other's strengths and apply them to different stages of the postal big data system.",
                "call-number": "10.1145/3408127.3408180",
                "collection-title": "ICDSP 2020",
                "container-title": "Proceedings of the 2020 4th International Conference on Digital Signal Processing",
                "DOI": "10.1145/3408127.3408180",
                "event-place": "Chengdu, China",
                "ISBN": "9781450376877",
                "keyword": "Hadoop, Big data, Postal, Big data platform, Spark",
                "number-of-pages": "5",
                "page": "305–309",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Research on Big Data Parallel Processing Platform Based on Postal Industry",
                "URL": "https://doi.org/10.1145/3408127.3408180"
            }
        },
        {
            "10.1145/3035918.3058737": {
                "id": "10.1145/3035918.3058737",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Gulzar",
                        "given": "Muhammad Ali"
                    },
                    {
                        "family": "Interlandi",
                        "given": "Matteo"
                    },
                    {
                        "family": "Condie",
                        "given": "Tyson"
                    },
                    {
                        "family": "Kim",
                        "given": "Miryung"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            5,
                            9
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            5,
                            9
                        ]
                    ]
                },
                "abstract": "To process massive quantities of data, developers leverage Data-Intensive Scalable Computing (DISC) systems such as Apache Spark. In terms of debugging, DISC systems support only post-mortem log analysis and do not provide any debugging functionality. This demonstration paper showcases BigDebug: a tool enhancing Apache Spark with a set of interactive debugging features that can help users in debug their Big Data Applications.",
                "call-number": "10.1145/3035918.3058737",
                "collection-title": "SIGMOD '17",
                "container-title": "Proceedings of the 2017 ACM International Conference on Management of Data",
                "DOI": "10.1145/3035918.3058737",
                "event-place": "Chicago, Illinois, USA",
                "ISBN": "9781450341974",
                "keyword": "big data analytics, interactive tools, debugging, automatic fault localization, disc, data-intensive scalable computing",
                "number-of-pages": "4",
                "page": "1627–1630",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Debugging Big Data Analytics in Spark with BigDebug",
                "URL": "https://doi.org/10.1145/3035918.3058737"
            }
        },
        {
            "10.5555/3213032.3213044": {
                "id": "10.5555/3213032.3213044",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Kavak",
                        "given": "Hamdi"
                    },
                    {
                        "family": "Padilla",
                        "given": "Jose J."
                    },
                    {
                        "family": "Lynch",
                        "given": "Christopher J."
                    },
                    {
                        "family": "Diallo",
                        "given": "Saikou Y."
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            4,
                            15
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            4,
                            15
                        ]
                    ]
                },
                "abstract": "We have recently witnessed the proliferation of large-scale behavioral data that can be used to empirically develop agent-based models (ABMs). Despite this opportunity, the literature has neglected to offer a structured agent-based modeling approach to produce agents or its parts directly from data. In this paper, we present initial steps towards an agent-based modeling approach that focuses on individual-level data to generate agent behavioral rules and initialize agent attribute values. We present a structured way to integrate Big Data and machine learning techniques at the individual agent-level. We also describe a conceptual use-case study of an urban mobility simulation driven by millions of geo-tagged Twitter social media messages. We believe our approach will advance the-state-of-the-art in developing empirical ABMs and conducting their validation. Further work is needed to assess data suitability, to compare with other approaches, to standardize data collection, and to serve all these features in near-real time.",
                "call-number": "10.5555/3213032.3213044",
                "collection-number": "12",
                "collection-title": "ANSS '18",
                "container-title": "Proceedings of the Annual Simulation Symposium",
                "event-place": "Baltimore, Maryland",
                "ISBN": "9781510860148",
                "keyword": "machine learning, data-driven modeling, big data, agent-based simulation",
                "number": "Article 12",
                "number-of-pages": "12",
                "page": "1–12",
                "publisher": "Society for Computer Simulation International",
                "publisher-place": "San Diego, CA, USA",
                "title": "Big data, agents, and machine learning: towards a data-driven agent-based modeling approach"
            }
        },
        {
            "10.1145/3197091.3205834": {
                "id": "10.1145/3197091.3205834",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Sooriamurthi",
                        "given": "Raja"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            7,
                            2
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            7,
                            2
                        ]
                    ]
                },
                "abstract": "In this teaching tip and courseware note we describe a series of hands on activities and exercises that we've used to introduce the notion of big data analytics to a wide range of audience. These exercises range in complexity from a paper and pencil thought exercise, to using Google Trends for simple explorations, to using a spread sheet to simulate the iterative nature of Google's PageRank algorithm, to programming with a Python based map-reduce framework. These exercises have been used in courses to train high school teachers in data science, full semester university courses (undergraduate and graduate), and CS education outreach efforts. Feedback has been positive as to their efficacy.",
                "call-number": "10.1145/3197091.3205834",
                "collection-title": "ITiCSE 2018",
                "container-title": "Proceedings of the 23rd Annual ACM Conference on Innovation and Technology in Computer Science Education",
                "DOI": "10.1145/3197091.3205834",
                "event-place": "Larnaca, Cyprus",
                "ISBN": "9781450357074",
                "keyword": "Map Reduce, Outreac, Google Trends, PageRank, Big data",
                "number-of-pages": "2",
                "page": "373–374",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Introducing big data analytics in high school and college",
                "URL": "https://doi.org/10.1145/3197091.3205834"
            }
        },
        {
            "10.1145/3445945.3445962": {
                "id": "10.1145/3445945.3445962",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Zhao",
                        "given": "Liangbin"
                    },
                    {
                        "family": "Fu",
                        "given": "Xiuju"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2020,
                            11,
                            27
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2020,
                            11,
                            27
                        ]
                    ]
                },
                "abstract": "As an important application of big data technology in the maritime field, big data driven visualization of ship encounter patterns helps to intuitively understand the risk situation in the water traffic. However traditional methods based on fixed thresholds do not consider the fuzziness of classification on ship encounter situations. We proposed a visual method to visualize the risk situations caused by the interaction between vessel traffic flows in more detail based on fuzzy theory and big data intelligence on large scale AIS data. A case study is conducted to verify the applicability based on the AIS data from Singapore Strait. Visualization results of density in grids show that the proposed method can effectively reflect the ship encounter patterns, which are consistent with the real situation and can show more valuable details for the safety assessment of water traffic.",
                "call-number": "10.1145/3445945.3445962",
                "collection-title": "ICBDR 2020",
                "container-title": "2020 the 4th International Conference on Big Data Research (ICBDR'20)",
                "DOI": "10.1145/3445945.3445962",
                "event-place": "Tokyo, Japan",
                "ISBN": "9781450387750",
                "keyword": "Ship encounter, AIS data, Visualization, Fuzzy theory",
                "number-of-pages": "7",
                "page": "94–100",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "A Visual Method for Ship Close Encounter Pattern Recognition based on Fuzzy theory and Big Data Intelligence",
                "URL": "https://doi.org/10.1145/3445945.3445962"
            }
        },
        {
            "10.1145/3399205.3399225": {
                "id": "10.1145/3399205.3399225",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Elhassan",
                        "given": "Jamal"
                    },
                    {
                        "family": "Aniss",
                        "given": "Moumen"
                    },
                    {
                        "family": "Jamal",
                        "given": "Chao"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2020,
                            3,
                            11
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2020,
                            3,
                            11
                        ]
                    ]
                },
                "abstract": "The management of hydraulic resources and especially water resources has become a major priority for decision-makers and planners at the international level. The advent of new technologies such as big data analytics and IoT has led to exponential changes in the volume of data generated daily in real-time, this monitoring information has potential significance if it is collected and aggregated effectively. The data collected are increasingly complex and represent a central government issue, developing water resource management strategies. The exploitation of this type of data requires new methods of analysis and knowledge discovery. This work presents a literature review of articles related to big data and water resources. Also, we present our proposition of a new architecture to conduct a big data analytic. In conclusion, we discuss the impact of using this technology in water resource management.",
                "call-number": "10.1145/3399205.3399225",
                "collection-number": "19",
                "collection-title": "GEOIT4W-2020",
                "container-title": "Proceedings of the 4th Edition of International Conference on Geo-IT and Water Resources 2020, Geo-IT and Water Resources 2020",
                "DOI": "10.1145/3399205.3399225",
                "event-place": "Al-Hoceima, Morocco",
                "ISBN": "9781450375788",
                "keyword": "big data analytics, real-time, water resources, IoT",
                "number": "Article 19",
                "number-of-pages": "5",
                "page": "1–5",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Big Data Analytic Architecture for Water Resources Management: A Systematic Review",
                "URL": "https://doi.org/10.1145/3399205.3399225"
            }
        },
        {
            "10.1145/3456887.3457522": {
                "id": "10.1145/3456887.3457522",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "He",
                        "given": "Xiaohua"
                    },
                    {
                        "family": "Zhang",
                        "given": "Wei"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            5,
                            25
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2021,
                            5,
                            25
                        ]
                    ]
                },
                "abstract": "Colleges and universities are the most important positions for personnel training in the new era. With the coverage of China's 5th-Generation network and the construction of an information and intelligent smart campus in internet plus, the application of big data in university sports is bound to become a trend. This paper investigates the application status and influencing factors of big data in college sports in China, aiming to provide countermeasures and suggestions for the construction of smart sports campus in Colleges and universities.",
                "call-number": "10.1145/3456887.3457522",
                "collection-title": "CIPAE 2021",
                "container-title": "2021 2nd International Conference on Computers, Information Processing and Advanced Education",
                "DOI": "10.1145/3456887.3457522",
                "event-place": "Ottawa, ON, Canada",
                "ISBN": "9781450389969",
                "keyword": "Status quo, Big data, College physical education",
                "number-of-pages": "5",
                "page": "1355–1359",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Research on the Application Status of University Sports Big Data",
                "URL": "https://doi.org/10.1145/3456887.3457522"
            }
        },
        {
            "10.5555/2627435.2638579": {
                "id": "10.5555/2627435.2638579",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Tan",
                        "given": "Mingkui"
                    },
                    {
                        "family": "Tsang",
                        "given": "Ivor W."
                    },
                    {
                        "family": "Wang",
                        "given": "Li"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2014,
                            1,
                            1
                        ]
                    ]
                },
                "abstract": "In this paper, we present a new adaptive feature scaling scheme for ultrahigh-dimensional feature selection on Big Data, and then reformulate it as a convex semi-infinite programming (SIP) problem. To address the SIP, we propose an efficient feature generating paradigm. Different from traditional gradient-based approaches that conduct optimization on all input features, the proposed paradigm iteratively activates a group of features, and solves a sequence of multiple kernel learning (MKL) subproblems. To further speed up the training, we propose to solve the MKL subproblems in their primal forms through a modified accelerated proximal gradient approach. Due to such optimization scheme, some efficient cache techniques are also developed. The feature generating paradigm is guaranteed to converge globally under mild conditions, and can achieve lower feature selection bias. Moreover, the proposed method can tackle two challenging tasks in feature selection: 1) group-based feature selection with complex structures, and 2) nonlinear feature selection with explicit feature mappings. Comprehensive experiments on a wide range of synthetic and real-world data sets of tens of million data points with O(1014) features demonstrate the competitive performance of the proposed method over state-of-the-art feature selection methods in terms of generalization performance and training effciency.",
                "call-number": "10.5555/2627435.2638579",
                "container-title": "J. Mach. Learn. Res.",
                "ISSN": "1532-4435",
                "issue": "1",
                "keyword": "feature selection, nonlinear feature selection, feature generation, big data, ultrahigh dimensionality, multiple kernel learning",
                "number-of-pages": "59",
                "page": "1371–1429",
                "publisher": "JMLR.org",
                "source": "January 2014",
                "title": "Towards ultrahigh dimensional feature selection for big data",
                "volume": "15"
            }
        },
        {
            "10.1145/2168556.2168563": {
                "id": "10.1145/2168556.2168563",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Holmqvist",
                        "given": "Kenneth"
                    },
                    {
                        "family": "Nyström",
                        "given": "Marcus"
                    },
                    {
                        "family": "Mulvey",
                        "given": "Fiona"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2012,
                            3,
                            28
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2012,
                            3,
                            28
                        ]
                    ]
                },
                "abstract": "Data quality is essential to the validity of research results and to the quality of gaze interaction. We argue that the lack of standard measures for eye data quality makes several aspects of manufacturing and using eye trackers, as well as researching eye movements and vision, more difficult than necessary. Uncertainty regarding the comparability of research results is a considerable impediment to progress in the field. In this paper, we illustrate why data quality matters and review previous work on how eye data quality has been measured and reported. The goal is to achieve a common understanding of what data quality is and how it can be defined, measured, evaluated, and reported.",
                "call-number": "10.1145/2168556.2168563",
                "collection-title": "ETRA '12",
                "container-title": "Proceedings of the Symposium on Eye Tracking Research and Applications",
                "DOI": "10.1145/2168556.2168563",
                "event-place": "Santa Barbara, California",
                "ISBN": "9781450312219",
                "keyword": "latency, precision, accuracy, eye tracker, eye movements, data quality",
                "number-of-pages": "8",
                "page": "45–52",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Eye tracker data quality: what it is and how to measure it",
                "URL": "https://doi.org/10.1145/2168556.2168563"
            }
        },
        {
            "10.1145/3441369.3441376": {
                "id": "10.1145/3441369.3441376",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Song",
                        "given": "Changxin"
                    },
                    {
                        "family": "Ma",
                        "given": "Ke"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2020,
                            11,
                            6
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2020,
                            11,
                            6
                        ]
                    ]
                },
                "abstract": "With the rapid development of network information technology and big data mining technology, data collection, processing and analysis in different industries have begun to use big data Association rule algorithm, decision tree algorithm, etc, perform processing and display of various data resources. Hypertension, as a common cardiovascular disease in society, should be diagnosed and predicted in time according to the height, weight, smoking, drinking, kidney disease, family history of different individuals, physical exercise and eating habits and other data information, expand the sick population systolic blood pressure, diastolic blood pressure testing, and to predict the future development of hypertension disease, but the traditional data collection scheme through the way of inquiry, obviously, the accuracy of hypertension prediction cannot be guaranteed. Based on this, this paper uses the improved Big Data algorithm to carry out the collection, cleaning, transformation and modeling analysis of various characteristic parameters of hypertension diseases, and obtains the grades of the influence of different data indexes on hypertension, and put forward the coping strategies of hypertension disease prediction and prevention.",
                "call-number": "10.1145/3441369.3441376",
                "collection-title": "DMIP '20",
                "container-title": "2020 3rd International Conference on Digital Medicine and Image Processing",
                "DOI": "10.1145/3441369.3441376",
                "event-place": "Kyoto, Japan",
                "ISBN": "9781450389044",
                "keyword": "Big data, Algorithm research, Diagnosis, Hypertension prediction",
                "number-of-pages": "5",
                "page": "40–44",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Research on Hypertension Prediction and Diagnosis Based on Big Data",
                "URL": "https://doi.org/10.1145/3441369.3441376"
            }
        },
        {
            "10.1109/JCDL.2019.00118": {
                "id": "10.1109/JCDL.2019.00118",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Chen",
                        "given": "Jiangping"
                    },
                    {
                        "family": "Lu",
                        "given": "Wei"
                    },
                    {
                        "family": "Zavalina",
                        "given": "Oksana"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            6,
                            2
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            6,
                            2
                        ]
                    ]
                },
                "abstract": "The explosion of information and the availability of big data provide new significant challenges for digital libraries. For example, libraries face the \"cyberinfrastructural challenge\" and the need to develop better understanding of research data to support curation, sharing and reuse of data generated by data-intensive science [9][6]. The synergy between information science and data science is emerging to address these Big Data environment challenges. The most fruitful collaboration areas for this synergy include those related to information organization: \"big metadata, smart metadata\" the ways to leverage the \"metadata capital\"[4]. The proposed one-day workshop will seek to support the interdisciplinary collaboration in this important area. It will focus on the challenges and opportunities provided by Big Data environment for information and computing professionals to explore innovative strategies and solutions for organizing data, information, and knowledge.",
                "call-number": "10.1109/JCDL.2019.00118",
                "collection-title": "JCDL '19",
                "container-title": "Proceedings of the 18th Joint Conference on Digital Libraries",
                "DOI": "10.1109/JCDL.2019.00118",
                "event-place": "Champaign, Illinois",
                "ISBN": "9781728115474",
                "keyword": "knowledge organization, information organization, big data analytics, data management",
                "number-of-pages": "2",
                "page": "459–460",
                "publisher": "IEEE Press",
                "title": "Organizing data, information, and knowledge in big data environments",
                "URL": "https://doi.org/10.1109/JCDL.2019.00118"
            }
        },
        {
            "10.1145/3079452.3079474": {
                "id": "10.1145/3079452.3079474",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Altena",
                        "given": "Allard Jan-Jaap van"
                    },
                    {
                        "family": "Delgado Olabarriaga",
                        "given": "Sílvia"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            7,
                            2
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            7,
                            2
                        ]
                    ]
                },
                "abstract": "In 2011 the term \"Big Data\" was introduced by Gartner [5], and since then its use in literature has ever increased, also in the (bio)medical research field [1]. Although the term Big Data is widely used, studies show that its meaning is much debated and many different definitions exist [10]. This variety of definitions may lead to different understandings and therefore difficulties in communication. For example, a researcher that is looking for \"Big Data\" solutions might miss an interesting method that is not tagged as such. In previous work we studied major topics that appear in Big Data literature using a Topic Modelling approach [8]. However, from that study it was not possible to know whether those topics are exclusive to publications self-identified as Big Data (BD), or not. Therefore, here we investigate the research question: What are the differences between topics in BD and non-Big Data (NBD) corpora?",
                "call-number": "10.1145/3079452.3079474",
                "collection-title": "DH '17",
                "container-title": "Proceedings of the 2017 International Conference on Digital Health",
                "DOI": "10.1145/3079452.3079474",
                "event-place": "London, United Kingdom",
                "ISBN": "9781450352499",
                "keyword": "biomedical literature, big data, topic modelling",
                "number-of-pages": "2",
                "page": "221–222",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "(Bio)medical Publications in the Age of Big Data: Yes, They Are Different",
                "URL": "https://doi.org/10.1145/3079452.3079474"
            }
        },
        {
            "10.1145/3460866": {
                "id": "10.1145/3460866",
                "type": "BOOK",
                "issued": {
                    "date-parts": [
                        [
                            2021
                        ]
                    ]
                },
                "abstract": "The goal of this workshop is to bring together academic researchers and industry practitioners to address the challenges and report and exchange the research findings in Big Data in emergent distributed environments, including new approaches, techniques and applications, make substantial theoretical and empirical contributions to, and significantly advance the state of the art of Big Data in emergent distributed environments.",
                "call-number": "10.1145/3460866",
                "container-title-short": "BiDEDE '21",
                "event-place": "Virtual Event, China",
                "genre": "proceeding",
                "ISBN": "9781450384650",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Proceedings of the International Workshop on Big Data in Emergent Distributed Environments"
            }
        },
        {
            "10.1145/3152723.3152743": {
                "id": "10.1145/3152723.3152743",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Phing",
                        "given": "Chen Chai"
                    },
                    {
                        "family": "Kiong",
                        "given": "Tiong Sieh"
                    },
                    {
                        "family": "Yapandi",
                        "given": "Md Fauzan K. Mohd"
                    },
                    {
                        "family": "Paw",
                        "given": "Johnny Koh Siaw"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            10,
                            22
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            10,
                            22
                        ]
                    ]
                },
                "abstract": "Increase of electricity demand and urbanization process has caused more power plants to be built to meet the demand of electricity. However, development of power plant will cause environmental issue for its surrounding. Necessary measures need to be taken to ensure social and environmental sustainability. Among the requirements in Malaysia, discharge of air pollution emission of a gas- or distillate-fired power plant has to comply with air pollution level as described in the Malaysian Ambient Air Quality Standards ((MAAQS) 2013 and the Environmental Quality (Clean Air) Regulations 2014. Pertaining to the environmental requirements, this paper is to investigate the ability of a regression based artificial intelligence tool, namely Extreme Learning Machine (ELM) in correlating multiple sources of big data sets and subsequently predicting the air pollution emission level from the chimney of a Combined Cycle Gas Turbine (CCGT) power plant. This emission data is later being used to ensure the clean air regulatory requirement is fulfilled. The big data sources that have been used in this work are meteorological data, terrain and land use data, historical emission data and power plant parameters particularly related to the point source emitter. With the correlation of multiple big data sources, Extreme Learning Machine (ELM) is then trained for the prediction of emission rate at certain targeted areas, which are classified as air sensitive receptors (ASR) surrounding the power plant. Nitrogen dioxide (NO2) is the key emission that has been studied in this paper due to its criticality towards environment. A standalone application program has been developed to employ ELM based big data analytics tool for the prediction of NO2 pollution emission. The output of ELM is analyzed to ensure the emission at ground level of ASR is maintained within allowable limit.",
                "call-number": "10.1145/3152723.3152743",
                "collection-title": "ICBDR 2017",
                "container-title": "Proceedings of the 2017 International Conference on Big Data Research",
                "DOI": "10.1145/3152723.3152743",
                "event-place": "Osaka, Japan",
                "ISBN": "9781450353564",
                "keyword": "Terrain Profile, Extreme Learning Machine, Emission Prediction, Meteorology",
                "number-of-pages": "5",
                "page": "23–27",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Prediction of NO2 Emission Concentration via Correlation of Multiple Big Data Sources Using Extreme Learning Machine",
                "URL": "https://doi.org/10.1145/3152723.3152743"
            }
        },
        {
            "10.1145/3368756.3369080": {
                "id": "10.1145/3368756.3369080",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Tantaoui",
                        "given": "Mouad"
                    },
                    {
                        "family": "Laanaoui",
                        "given": "My Driss"
                    },
                    {
                        "family": "Kabil",
                        "given": "Mustapha"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            10,
                            2
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            10,
                            2
                        ]
                    ]
                },
                "abstract": "Big data has become essential given the mass of data generated by different domains and which has become impossible to manage by traditional data management tools; vehicular ad-hoc network is one of those areas that use the tools and technologies of data management of big data. In the present paper, we propose a method that aims to detect anomalies in the road and calculate the time spent in each road section in real time, which will allow us to have a base containing the estimated time spent in all sections in real time, it will help us to send to the vehicles the exact estimated time of arrival all along their way. This base will also allow us to detect an accident or anomaly in a section in real time as well.",
                "call-number": "10.1145/3368756.3369080",
                "collection-number": "93",
                "collection-title": "SCA '19",
                "container-title": "Proceedings of the 4th International Conference on Smart City Applications",
                "DOI": "10.1145/3368756.3369080",
                "event-place": "Casablanca, Morocco",
                "ISBN": "9781450362894",
                "keyword": "traffic congestions prediction, VANET, intelligent transportation systems (ITS), big data, traffic management",
                "number": "Article 93",
                "number-of-pages": "6",
                "page": "1–6",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Towards an efficient vehicle traffic management using big data",
                "URL": "https://doi.org/10.1145/3368756.3369080"
            }
        },
        {
            "10.1145/2757384": {
                "id": "10.1145/2757384",
                "type": "BOOK",
                "issued": {
                    "date-parts": [
                        [
                            2015
                        ]
                    ]
                },
                "abstract": "It is our great pleasure to welcome you to the 2015 ACM Workshop on Mobile Big Data -- Mobidata'15. This workshop aims to foster the exchange of new ideas in the synergy of mobile computing and big data research. We have selected 13 papers to be presented at the workshop. We hope the workshop will facilitate the discussion of the future research directions in mobile big data.",
                "call-number": "10.1145/2757384",
                "container-title-short": "Mobidata '15",
                "event-place": "Hangzhou, China",
                "genre": "proceeding",
                "ISBN": "9781450335249",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Proceedings of the 2015 Workshop on Mobile Big Data"
            }
        },
        {
            "10.1145/2769458.2769485": {
                "id": "10.1145/2769458.2769485",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Kacsuk",
                        "given": "Peter"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2015,
                            6,
                            10
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2015,
                            6,
                            10
                        ]
                    ]
                },
                "abstract": "Large scientific simulation projects should enable the collaboration of large scientific consortia where members are located in different countries and even continents storing their usually very large data set in different kind of storages. Therefore state-of-the-art simulations should process very large set of data stored in a distributed way in different kind of storages located in all over the world. As the data is big its processing time can be intolerably long. To reduce processing time we have to use large infrastructure that enables the exploitation of parallel processing wherever it is possible in the simulation process. Clouds provide the required large set of computing resources and hence we need simulation environments that enable the easy exploitation of cloud resources. This keynote speech introduces a cloud-oriented simulation platform that enables the exploitation of large cloud resources as well as accessing all the major data storage types. This platform called as WS-PGRADE/gUSE is intensively used in many EU FP7 projects among them in CloudSME where the main target is to enable particularly small and medium-sized manufacturing and engineering companies (SMEs), to use state of the art simulation technology as a Service (SaaS, one-stop-shop, pay-per-use) in the cloud.In this talk we will show the main features of WS-PGRADE/gUSE that enable the use of cloud and large data resources to conduct distributed simulations. First, the workflow creation and execution mechanism will be explained. Then the DCI Bridge service will be shown that enables the exploitation of many independent cloud resources in parallel. Finally, the Data Avenue service that enables the access and transfer of large data among various types of data storages will be described. These services together enable the creation of simulation workflows that are easily portable among different distributed computing and data infrastructures including various types of clouds and cloud storages. At the end of the talk some concrete examples from the CloudSME project (www.cloudsme.eu) will highlight the main advantages of using the platform.",
                "call-number": "10.1145/2769458.2769485",
                "collection-title": "SIGSIM PADS '15",
                "container-title": "Proceedings of the 3rd ACM SIGSIM Conference on Principles of Advanced Discrete Simulation",
                "DOI": "10.1145/2769458.2769485",
                "event-place": "London, United Kingdom",
                "ISBN": "9781450335836",
                "keyword": "simulation, big data, distributed simulation",
                "number-of-pages": "2",
                "page": "125–126",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Enabling Distributed Simulations Using Big Data and Clouds",
                "URL": "https://doi.org/10.1145/2769458.2769485"
            }
        },
        {
            "10.1145/2967938.2970374": {
                "id": "10.1145/2967938.2970374",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Arvind",
                        "given": "Arvind"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            9,
                            11
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2016,
                            9,
                            11
                        ]
                    ]
                },
                "abstract": "Complex analytics of the vast amount of data collected via social media, cell phones, ubiquitous smart sensors, and satellites is likely to be the biggest economic driver for the IT industry over the next decade. For many \"Big Data\" applications, the limiting factor in performance is often the transportation of large amount of data from hard disks to where it can be processed, i.e. DRAM. We will present BlueDBM, an architecture for a scalable distributed flash store which overcomes this limitation by providing a high-performance, high-capacity, scalable random-access flash storage, and by allowing computation near the data via a FPGA-based programmable flash controller. We will present the preliminary results for two applications, (1) key-value store (KVS) and (2) sparse-matrix accelerator for graph processing, on BlueDBM consisting of 20 nodes and 20TB of flash.",
                "call-number": "10.1145/2967938.2970374",
                "collection-title": "PACT '16",
                "container-title": "Proceedings of the 2016 International Conference on Parallel Architectures and Compilation",
                "DOI": "10.1145/2967938.2970374",
                "event-place": "Haifa, Israel",
                "ISBN": "9781450341219",
                "keyword": "in-storage computing, big data analytics, nand flash storage, hardware accelerators",
                "number-of-pages": "1",
                "page": "1",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Big Data Analytics on Flash Storage with Accelerators",
                "URL": "https://doi.org/10.1145/2967938.2970374"
            }
        },
        {
            "10.1145/3443467.3443749": {
                "id": "10.1145/3443467.3443749",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Qiu",
                        "given": "Yubing"
                    },
                    {
                        "family": "Sun",
                        "given": "Lan"
                    },
                    {
                        "family": "Wu",
                        "given": "Yingjie"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2020,
                            11,
                            6
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2020,
                            11,
                            6
                        ]
                    ]
                },
                "abstract": "Data privacy leakages and accuracy declines are major problems in the field of big data. In this work, an adaptive space partition algorithm based on traffic history data under differential privacy is proposed to meet the privacy demands of big data traffic. This paper first uses the counting values of tree nodes in the quad-tree index structure at the first n moments by considering the spatial and temporal characteristics of traffic data and then applies the weighted moving average filter in exponential decay mode to predict the statistical values of the corresponding regions of the data to be released at the next moment. Then, an adaptive quadtree index structure based on the predicted values of each region and the heuristic judgment strategy is established by using the top-down approach and applied to the real data released at the next moment. Finally, we combine the differential privacy tree structure publishing technology and adjust the consistency constraint. Theoretical analyses and simulation experiments show that our algorithm protects data privacy and improves the accuracy of data query compared with existing algorithms.",
                "call-number": "10.1145/3443467.3443749",
                "collection-title": "EITCE 2020",
                "container-title": "Proceedings of the 2020 4th International Conference on Electronic Information Technology and Computer Engineering",
                "DOI": "10.1145/3443467.3443749",
                "event-place": "Xiamen, China",
                "ISBN": "9781450387811",
                "keyword": "differential privacy space division, Big data traffic, data availability, adaptive quad-tree, privacy protection",
                "number-of-pages": "7",
                "page": "173–179",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Optimized Privacy Protection Method for Big Data Traffic",
                "URL": "https://doi.org/10.1145/3443467.3443749"
            }
        },
        {
            "10.1145/2460625.2460642": {
                "id": "10.1145/2460625.2460642",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Shaer",
                        "given": "Orit"
                    },
                    {
                        "family": "Mazalek",
                        "given": "Ali"
                    },
                    {
                        "family": "Ullmer",
                        "given": "Brygg"
                    },
                    {
                        "family": "Konkel",
                        "given": "Miriam"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2013,
                            2,
                            10
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2013,
                            2,
                            10
                        ]
                    ]
                },
                "abstract": "The combination of advanced genomic technologies and computational tools enables researchers to conduct large-scale experiments that answer biological questions in unprecedented ways. However, interaction tools in this area currently remain immature. We propose that tangible, embedded, and embodied interaction (TEI) offers unique opportunities for enhancing discovery and learning in genomics. Also, designing for problems in genomics can help move forward the theory and practice of TEI. We present challenges and key questions for TEI research in genomics, lessons learned from three case studies, and potential areas of focus for TEI research and design.",
                "call-number": "10.1145/2460625.2460642",
                "collection-title": "TEI '13",
                "container-title": "Proceedings of the 7th International Conference on Tangible, Embedded and Embodied Interaction",
                "DOI": "10.1145/2460625.2460642",
                "event-place": "Barcelona, Spain",
                "ISBN": "9781450318983",
                "keyword": "big data, biology, learning, scientific discovery, computational genomics, interactive tabletops, tangible interaction",
                "number-of-pages": "8",
                "page": "109–116",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "From big data to insights: opportunities and challenges for TEI in genomics",
                "URL": "https://doi.org/10.1145/2460625.2460642"
            }
        },
        {
            "10.1145/3389250": {
                "id": "10.1145/3389250",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Lv",
                        "given": "Zhihan"
                    },
                    {
                        "family": "Singh",
                        "given": "Amit Kumar"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            3,
                            30
                        ]
                    ]
                },
                "abstract": "The study aims at exploring the Internet of things (IoT) system from the perspective of data and further improving the performance of the IoT system. The IoT data energy collection and information transmission system model is constructed by combining IoT and wireless relay cooperative transmission technology. Moreover, the energy efficiency, outage probability (OP), and accuracy of the model are evaluated by simulation experiments. The results show that, in the energy efficiency analysis, with the increase of power split factor ρ, the information transmission ability of the system increases. Whereas, the energy collection ability decreases, so the energy efficiency is reduced. Thus, choosing a more suitable power split factor for the energy efficiency of IoT is important. By analyzing OP and bit error rate (BER), as the values of m (Nakagami, the fading index of the fading distribution) and multi-hop paths increase, the OP and BER are reduced while the system performance is increased. Therefore, this article uses wireless relay cooperative transmission technology to integrate big data analysis into the IoT system. Finally, by adding multi-hop path and other methods to reduce the OP and BER of system, the system performance is improved. It provides experimental basis for the development of IoT systems.",
                "call-number": "10.1145/3389250",
                "collection-number": "28",
                "container-title": "ACM Trans. Internet Technol.",
                "DOI": "10.1145/3389250",
                "ISSN": "1533-5399",
                "issue": "2",
                "keyword": "Internet of things, relay cooperation, energy efficiency, big data analysis",
                "number": "Article 28",
                "number-of-pages": "15",
                "page": "1–15",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "June 2021",
                "title": "Big Data Analysis of Internet of Things System",
                "URL": "https://doi.org/10.1145/3389250",
                "volume": "21"
            }
        },
        {
            "10.1145/3236461.3241967": {
                "id": "10.1145/3236461.3241967",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Barham",
                        "given": "Husam"
                    },
                    {
                        "family": "Daim",
                        "given": "Tugrul"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            6,
                            20
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            6,
                            20
                        ]
                    ]
                },
                "abstract": "Many cities across the globe are adopting smart city initiatives, as smart city holds the promise of better quality of life and equity for city's residents, more efficient use of city's infrastructure, and more effective city planning. Big data analytics is the backbone of smart city and the drive engine to achieve smart city's promises. However, statistics indicate that more than 50% of big data projects fail; they either never finish or do not offer the expected value. Resulting in severe consequences as such projects tends to be expensive and require allocating the organization's best resources while doing the project. This is even more crucial in the case of smart city, as cities usually have limited budget and resources.This paper conducted literature review and perspectives analysis to identify challenges, which can cause big data projects to fail, with focus on smart city related big data projects. The goal is to offer a list of challenges, that a project manager can consider as an initial list of risks for the upcoming project, and evaluate the city's readiness against each of them.",
                "call-number": "10.1145/3236461.3241967",
                "collection-number": "1",
                "collection-title": "SCC '18",
                "container-title": "Proceedings of the 1st ACM/EIGSCC Symposium on Smart Cities and Communities",
                "DOI": "10.1145/3236461.3241967",
                "event-place": "Portland, OR, USA",
                "ISBN": "9781450357869",
                "keyword": "smart cities, risk and challenges, project management, Big data",
                "number": "Article 1",
                "number-of-pages": "9",
                "page": "1–9",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Identifying Critical Issues in Smart City Big Data Project Implementation",
                "URL": "https://doi.org/10.1145/3236461.3241967"
            }
        },
        {
            "10.1145/2623330.2630811": {
                "id": "10.1145/2623330.2630811",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Cormode",
                        "given": "Graham"
                    },
                    {
                        "family": "Duffield",
                        "given": "Nick"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2014,
                            8,
                            24
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2014,
                            8,
                            24
                        ]
                    ]
                },
                "abstract": "One response to the proliferation of large datasets has been to develop ingenious ways to throw resources at the problem, using massive fault tolerant storage architectures, parallel and graphical computation models such as MapReduce, Pregel and Giraph. However, not all environments can support this scale of resources, and not all queries need an exact response. This motivates the use of sampling to generate summary datasets that support rapid queries, and prolong the useful life of the data in storage. To be effective, sampling must mediate the tensions between resource constraints, data characteristics, and the required query accuracy. The state-of-the-art in sampling goes far beyond simple uniform selection of elements, to maximize the usefulness of the resulting sample. This tutorial reviews progress in sample design for large datasets, including streaming and graph-structured data. Applications are discussed to sampling network traffic and social networks.",
                "call-number": "10.1145/2623330.2630811",
                "collection-title": "KDD '14",
                "container-title": "Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining",
                "DOI": "10.1145/2623330.2630811",
                "event-place": "New York, New York, USA",
                "ISBN": "9781450329569",
                "keyword": "random sampling",
                "number-of-pages": "1",
                "page": "1975",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Sampling for big data: a tutorial",
                "URL": "https://doi.org/10.1145/2623330.2630811"
            }
        },
        {
            "10.1145/3508259.3508284": {
                "id": "10.1145/3508259.3508284",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Sun",
                        "given": "Yu"
                    },
                    {
                        "family": "Niu",
                        "given": "Yanfang"
                    },
                    {
                        "family": "Lu",
                        "given": "Le"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            12,
                            17
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2021,
                            12,
                            17
                        ]
                    ]
                },
                "abstract": "The widespread application of big data has had a profound impact on social and economic development. Government auditing is the guarantee for the modernization of national governance, and the development of big data audit capability has become the key to improving national governance capability. This paper summarizes the concept of government audit big data capability, and constructs the influencing factor model of government audit big data capability. This study finds that the construction degree of audit big data platform, big data management ability, big data audit technology and auditors' big data technology ability have a significant positive impact on the government audit big data ability, and the audit organization coordination ability plays a positive moderating effect in the whole impact process. This study provides guidance for the improvement and development of government audit big data capability.",
                "call-number": "10.1145/3508259.3508284",
                "collection-title": "AICCC '21",
                "container-title": "2021 4th Artificial Intelligence and Cloud Computing Conference",
                "DOI": "10.1145/3508259.3508284",
                "event-place": "Kyoto, Japan",
                "ISBN": "9781450384162",
                "keyword": "Influencing factors, Big data analysis capability, Government audit",
                "number-of-pages": "7",
                "page": "172–178",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Research on Influencing Factors of Government Audit Big Data Capability",
                "URL": "https://doi.org/10.1145/3508259.3508284"
            }
        },
        {
            "10.1145/2640087.2644170": {
                "id": "10.1145/2640087.2644170",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Jupin",
                        "given": "Joseph"
                    },
                    {
                        "family": "Shi",
                        "given": "Justin Y."
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2014,
                            8,
                            4
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2014,
                            8,
                            4
                        ]
                    ]
                },
                "abstract": "Our research explores the practice of Record Linkage (RL), also known as Entity Resolution, Record Matching and the Object Identity Problem, in Big health services databases as is commonly practiced within the domain, and some of the approximate string matching methods used for this purpose. We also propose potential improvements to RL and string matching that have been shown in experiments to increase the quality and efficiency for information systems tasked with this problem. We have developed an in-memory graph-based data model, Aggregate Link and Iterative Match (ALIM), which compresses data by eliminating redundancy and stores alias, approximate and phonetic match links between stored data. We have also developed an enhanced edit-distance optimization, the Probabilistic Signature Hash Filter (PSH), which can perform the Damerau-Levenshtein (DL) edit-distance comparison nearly 6000 times faster than DL alone and produce the same exact approximate match results. Our experiments show significant accuracy and performance gains over a system currently in use by a local health department.",
                "call-number": "10.1145/2640087.2644170",
                "collection-number": "20",
                "collection-title": "BigDataScience '14",
                "container-title": "Proceedings of the 2014 International Conference on Big Data Science and Computing",
                "DOI": "10.1145/2640087.2644170",
                "event-place": "Beijing, China",
                "ISBN": "9781450328913",
                "keyword": "Record Matching, Filtering, String Matching, Object Identity Problem, Signatures, Record Linkage, Graph Models, Entity Resolution, Hashing",
                "number": "Article 20",
                "number-of-pages": "8",
                "page": "1–8",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Identity Tracking in Big Data: Preliminary Research Using In-Memory Data Graph Models for Record Linkage and Probabilistic Signature Hashing for Approximate String Matching in Big Health and Human Services Databases",
                "URL": "https://doi.org/10.1145/2640087.2644170"
            }
        },
        {
            "10.1145/3528114.3528119": {
                "id": "10.1145/3528114.3528119",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Zhao",
                        "given": "Yan"
                    },
                    {
                        "family": "Zhang",
                        "given": "Weidong"
                    },
                    {
                        "family": "Huang",
                        "given": "Rong"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2022,
                            2,
                            25
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2022,
                            2,
                            25
                        ]
                    ]
                },
                "abstract": "As the application of big data technology continues to expand. The rational use of big data technology in management accounting. Big data technology makes accounting work efficiency significantly improved. Big data technology has promoted the reform and innovation of accounting work mode. Big data technology has become an important part of management accounting. How to use big data technology to dig out valuable information from massive data for enterprise management decisions is particularly important. On the basis of systematically sorting out the concepts related to big data technology and management accounting, this paper expounds the influence mechanism of big data technology on management accounting, constructs the management accounting system framework under the big data analysis technology, and establishes the management accounting application framework under the big data analysis technology. This paper puts forward the implementation ideas and implementation paths of the application of management accounting system under the big data analysis technology, analyzes the mechanism of the influence of big data mining technology on management accounting, explores the application mechanism of big data mining technology in management accounting, and puts forward the strategy of using big data technology to promote the development of management accounting.",
                "call-number": "10.1145/3528114.3528119",
                "collection-title": "DSDE '22",
                "container-title": "2022 the 5th International Conference on Data Storage and Data Engineering",
                "DOI": "10.1145/3528114.3528119",
                "event-place": "Sanya, China",
                "ISBN": "9781450395724",
                "keyword": "big data technology, big data mining technology, big data analysis technology, management accounting",
                "number-of-pages": "7",
                "page": "26–32",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Research on the Impact of Big Data Technology on Management Accounting",
                "URL": "https://doi.org/10.1145/3528114.3528119"
            }
        },
        {
            "10.5555/2757761": {
                "id": "10.5555/2757761",
                "type": "BOOK",
                "issued": {
                    "date-parts": [
                        [
                            2014
                        ]
                    ]
                },
                "call-number": "10.5555/2757761",
                "container-title-short": "BDC '14",
                "genre": "proceeding",
                "ISBN": "9781479918973",
                "publisher": "IEEE Computer Society",
                "publisher-place": "USA",
                "title": "Proceedings of the 2014 IEEE/ACM International Symposium on Big Data Computing"
            }
        },
        {
            "10.1145/3251403": {
                "id": "10.1145/3251403",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Serrano",
                        "given": "Manuel"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2009,
                            11,
                            6
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2009,
                            11,
                            6
                        ]
                    ]
                },
                "call-number": "10.1145/3251403",
                "collection-title": "MoSE+DQS '09",
                "container-title": "Proceedings of the first international workshop on Model driven service engineering and data quality and security",
                "DOI": "10.1145/3251403",
                "event-place": "Hong Kong, China",
                "ISBN": "9781605588162",
                "page": "",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Session details: Data quality and security",
                "URL": "https://doi.org/10.1145/3251403"
            }
        },
        {
            "10.1145/3147234.3151010": {
                "id": "10.1145/3147234.3151010",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Gong",
                        "given": "Yikai"
                    },
                    {
                        "family": "Rimba",
                        "given": "Paul"
                    },
                    {
                        "family": "Sinnott",
                        "given": "Richard"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            12,
                            5
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            12,
                            5
                        ]
                    ]
                },
                "abstract": "Big data is a popular research topic that has brought about a range of new IT challenges and opportunities. The transport domain is one area that has much to benefit from big data platforms. It requires capabilities for processing voluminous amounts of heterogeneous data that is often created in near real time and at high velocity from a multitude of distributed sensors. It can also require the application of performance-oriented spatial data processing of such data. In this paper, we present a platform (SMASH) that tackles many of the specific challenges raised by the transport domain. We present a range of case studies applying SMASH to transport and other data used to understand traffic phenomenon across the State of Victoria, Australia. The novelty of this work is that this Cloud-based platform is not designed for a specific type of data or for a specific form of data processing. Rather it supports a range of data flavours with a range of data processing possibilities. In particular we show how the platform can be used for analyzing social media data used for traffic jam identification through spatial and temporal clustering tweets on the road network and compare the results with official real-time traffic data based on the Sydney Coordinated Adaptive Traffic System (SCATS - www.scats.com.au) that has been rolled out across Victoria.",
                "call-number": "10.1145/3147234.3151010",
                "collection-title": "UCC '17 Companion",
                "container-title": "Companion Proceedings of the10th International Conference on Utility and Cloud Computing",
                "DOI": "10.1145/3147234.3151010",
                "event-place": "Austin, Texas, USA",
                "ISBN": "9781450351959",
                "keyword": "traffic analysis, cloud, big data",
                "number-of-pages": "6",
                "page": "157–162",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "A Big Data Architecture for Near Real-time Traffic Analytics",
                "URL": "https://doi.org/10.1145/3147234.3151010"
            }
        },
        {
            "10.1145/3481646.3481652": {
                "id": "10.1145/3481646.3481652",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Cuzzocrea",
                        "given": "Alfredo"
                    },
                    {
                        "family": "Fadda",
                        "given": "Edoardo"
                    },
                    {
                        "family": "Baldo",
                        "given": "Alessandro"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            8,
                            13
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2021,
                            8,
                            13
                        ]
                    ]
                },
                "abstract": "Central Limit Theorems have a fundamental role in statistics and in a wide range of practical applications. The most famous formulation was proposed by Lindeberg–Lévy and it requires the variables to be independent and identically distributed. In the real setting these conditions are rarely matched, though. The Lyapunov Central Limit Theorem overcomes this limitation, since it does not require the same distribution of the random variables. However, the cost of this generalization is an increased complexity, moderately limiting its effective applicability. In this paper, we resume the main results on the Lyapunov Central Limit Theorem, providing an easy-to-prove condition to put in practice, and demonstrating its uniform convergence. These theoretical results are supported by some relevant applications in the field of big data in smart city settings.",
                "call-number": "10.1145/3481646.3481652",
                "collection-title": "ICCBDC 2021",
                "container-title": "2021 5th International Conference on Cloud and Big Data Computing (ICCBDC)",
                "DOI": "10.1145/3481646.3481652",
                "event-place": "Liverpool, United Kingdom",
                "ISBN": "9781450390408",
                "keyword": "Lyapunov Central Limit Theorem, Smart City, Uniform Convergence Condition",
                "number-of-pages": "5",
                "page": "34–38",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Lyapunov Central Limit Theorem: Theoretical Properties and Applications in Big-Data-Populated Smart City Settings",
                "URL": "https://doi.org/10.1145/3481646.3481652"
            }
        },
        {
            "10.1145/2910896.2925466": {
                "id": "10.1145/2910896.2925466",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Farag",
                        "given": "Mohamed"
                    },
                    {
                        "family": "Nakate",
                        "given": "Pranav"
                    },
                    {
                        "family": "Fox",
                        "given": "Edward A."
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            6,
                            19
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2016,
                            6,
                            19
                        ]
                    ]
                },
                "abstract": "Web archives about school shootings consist of webpages that may or may not be relevant to the events of interest. There are 3 main goals of this work; first is to clean the webpages, which involves getting rid of the stop words and non-relevant parts of a webpage. The second goal is to select just webpages relevant to the events of interest. The third goal is to upload the cleaned and relevant webpages to Apache Solr so that they are easily accessible. We show the details of all the steps required to achieve these goals. The results show that representative Web archives are noisy, with 2% - 40% relevant content. By cleaning the archives, we aid researchers to focus on relevant content for their analysis.",
                "call-number": "10.1145/2910896.2925466",
                "collection-title": "JCDL '16",
                "container-title": "Proceedings of the 16th ACM/IEEE-CS on Joint Conference on Digital Libraries",
                "DOI": "10.1145/2910896.2925466",
                "event-place": "Newark, New Jersey, USA",
                "ISBN": "9781450342292",
                "keyword": "classification, digital libraries, web archives, big data processing",
                "number-of-pages": "2",
                "page": "271–272",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Big Data Processing of School Shooting Archives",
                "URL": "https://doi.org/10.1145/2910896.2925466"
            }
        },
        {
            "10.1145/2481244.2481247": {
                "id": "10.1145/2481244.2481247",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Lin",
                        "given": "Jimmy"
                    },
                    {
                        "family": "Ryaboy",
                        "given": "Dmitriy"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2013,
                            4,
                            30
                        ]
                    ]
                },
                "abstract": "The analytics platform at Twitter has experienced tremendous growth over the past few years in terms of size, complexity, number of users, and variety of use cases. In this paper, we discuss the evolution of our infrastructure and the development of capabilities for data mining on \"big data\". One important lesson is that successful big data mining in practice is about much more than what most academics would consider data mining: life \"in the trenches\" is occupied by much preparatory work that precedes the application of data mining algorithms and followed by substantial effort to turn preliminary models into robust solutions. In this context, we discuss two topics: First, schemas play an important role in helping data scientists understand petabyte-scale data stores, but they're insufficient to provide an overall \"big picture\" of the data available to generate insights. Second, we observe that a major challenge in building data analytics platforms stems from the heterogeneity of the various components that must be integrated together into production workflows---we refer to this as \"plumbing\". This paper has two goals: For practitioners, we hope to share our experiences to flatten bumps in the road for those who come after us. For academic researchers, we hope to provide a broader context for data mining in production environments, pointing out opportunities for future work.",
                "call-number": "10.1145/2481244.2481247",
                "container-title": "SIGKDD Explor. Newsl.",
                "DOI": "10.1145/2481244.2481247",
                "ISSN": "1931-0145",
                "issue": "2",
                "number-of-pages": "14",
                "page": "6–19",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "December 2012",
                "title": "Scaling big data mining infrastructure: the twitter experience",
                "URL": "https://doi.org/10.1145/2481244.2481247",
                "volume": "14"
            }
        },
        {
            "10.1145/3383464": {
                "id": "10.1145/3383464",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Zeng",
                        "given": "Xuezhi"
                    },
                    {
                        "family": "Garg",
                        "given": "Saurabh"
                    },
                    {
                        "family": "Barika",
                        "given": "Mutaz"
                    },
                    {
                        "family": "Zomaya",
                        "given": "Albert Y."
                    },
                    {
                        "family": "Wang",
                        "given": "Lizhe"
                    },
                    {
                        "family": "Villari",
                        "given": "Massimo"
                    },
                    {
                        "family": "Chen",
                        "given": "Dan"
                    },
                    {
                        "family": "Ranjan",
                        "given": "Rajiv"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2020,
                            6,
                            12
                        ]
                    ]
                },
                "abstract": "Recent years have witnessed the booming of big data analytical applications (BDAAs). This trend provides unrivaled opportunities to reveal the latent patterns and correlations embedded in the data, and thus productive decisions may be made. This was previously a grand challenge due to the notoriously high dimensionality and scale of big data, whereas the quality of service offered by providers is the first priority. As BDAAs are routinely deployed on Clouds with great complexities and uncertainties, it is a critical task to manage the service level agreements (SLAs) so that a high quality of service can then be guaranteed. This study performs a systematic literature review of the state of the art of SLA-specific management for Cloud-hosted BDAAs. The review surveys the challenges and contemporary approaches along this direction centering on SLA. A research taxonomy is proposed to formulate the results of the systematic literature review. A new conceptual SLA model is defined and a multi-dimensional categorization scheme is proposed on its basis to apply the SLA metrics for an in-depth understanding of managing SLAs and the motivation of trends for future research.",
                "call-number": "10.1145/3383464",
                "collection-number": "46",
                "container-title": "ACM Comput. Surv.",
                "DOI": "10.1145/3383464",
                "ISSN": "0360-0300",
                "issue": "3",
                "keyword": "service layer, big data analytics application, Big data, service level agreement, SLA metrics, SLA",
                "number": "Article 46",
                "number-of-pages": "40",
                "page": "1–40",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "May 2021",
                "title": "SLA Management for Big Data Analytical Applications in Clouds: A Taxonomy Study",
                "URL": "https://doi.org/10.1145/3383464",
                "volume": "53"
            }
        },
        {
            "10.1145/2345316.2345320": {
                "id": "10.1145/2345316.2345320",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Berkovich",
                        "given": "Simon"
                    },
                    {
                        "family": "Liao",
                        "given": "Duoduo"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2012,
                            7,
                            1
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2012,
                            7,
                            1
                        ]
                    ]
                },
                "abstract": "Big Data refers to the rising flood of digital data from many different sources, including the sensors, digitizers, scanners, mobile phones, cameras, software-based tools, internet, and so on. \"Big\" and \"diverse\" are two important characteristics of Big Data. The diversity of the Big Data, such as text, geometry, image, video, or sound, also increases difficulties of big data processing.Coping with the \"Big Data\" problems requires a radical change in the philosophy of the organization of information processing. Primarily, the Big Data approach has to modify the underlying computational model in order to manage the uncertainty in the access to information items in a huge nebulous environment. As a result, the produced outcomes are directly influenced only by some active part of all information items, while the rest of the available information items just indirectly affect the choice of the active part. An analogous functionality exhibits the organization of the brain featuring the unconsciousness, and a characteristic similarity shows the retrieval process in Google.In this talk, we introduce a novel method for on-the-fly clusterization of amorphous data from diverse sources. The devised construction is based on the previously developed FuzzyFind Dictionary reversing the error-correction scheme of Golay Code. This clusterization involves processing of intensive continuous data streams that can be effectively implemented using multi-core pipelining with forced interrupts. The suggested clusterization is especially suitable for the Big Data computational model as it materializes the requirement of purposeful selection of information items in unsteady framework of cloud computing and stream processing. Furthermore, the uncertainties in relation to the considered method of clusterization are moderated due to the idea of the bounded rationality, an approach that does not require a complete exact knowledge for sensible decision-making.",
                "call-number": "10.1145/2345316.2345320",
                "collection-number": "3",
                "collection-title": "COM.Geo '12",
                "container-title": "Proceedings of the 3rd International Conference on Computing for Geospatial Research and Applications",
                "DOI": "10.1145/2345316.2345320",
                "event-place": "Washington, D.C., USA",
                "ISBN": "9781450311137",
                "number": "Article 3",
                "number-of-pages": "1",
                "page": "1",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "On clusterization of \"big data\" streams",
                "URL": "https://doi.org/10.1145/2345316.2345320"
            }
        },
        {
            "10.1145/2536714": {
                "id": "10.1145/2536714",
                "type": "BOOK",
                "issued": {
                    "date-parts": [
                        [
                            2013
                        ]
                    ]
                },
                "abstract": "We are glad to welcome you to the First International Workshop on Sensing and Big Data Mining (SenseMine 2013). This workshop presents a new forum, established to bring together researchers, practitioners, and academics in the fields of sensor networks, distributed systems, and big data mining and machine learning, with the goal of driving cross-disciplinary research in support of novel emerging applications in our daily lives.",
                "call-number": "10.1145/2536714",
                "container-title-short": "SENSEMINE'13",
                "event-place": "Roma, Italy",
                "genre": "proceeding",
                "ISBN": "9781450324304",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Proceedings of First International Workshop on Sensing and Big Data Mining"
            }
        },
        {
            "10.1145/2507157.2508005": {
                "id": "10.1145/2507157.2508005",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "de Gemmis",
                        "given": "Marco"
                    },
                    {
                        "family": "Di Noia",
                        "given": "Tommaso"
                    },
                    {
                        "family": "Lassila",
                        "given": "Ora"
                    },
                    {
                        "family": "Lops",
                        "given": "Pasquale"
                    },
                    {
                        "family": "Lukasiewicz",
                        "given": "Thomas"
                    },
                    {
                        "family": "Semeraro",
                        "given": "Giovanni"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2013,
                            10,
                            12
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2013,
                            10,
                            12
                        ]
                    ]
                },
                "abstract": "The primary goal of the workshop is to showcase cutting edge research on the intersection of Recommender Systems and Semantic Technologies, by taking the best of the two worlds. This combination may provide the RecSys community with important scenarios where the potential of Semantic Technologies can be effectively exploited into systems performing complex tasks, such as recommendation engines processing Big Data.",
                "call-number": "10.1145/2507157.2508005",
                "collection-title": "RecSys '13",
                "container-title": "Proceedings of the 7th ACM conference on Recommender systems",
                "DOI": "10.1145/2507157.2508005",
                "event-place": "Hong Kong, China",
                "ISBN": "9781450324090",
                "keyword": "linked data, big data, semantic technologies, recommendation algorithms",
                "number-of-pages": "2",
                "page": "483–484",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Workshop on recommender systems meet big data & semantic technologies: SeRSy 2013",
                "URL": "https://doi.org/10.1145/2507157.2508005"
            }
        },
        {
            "10.1145/3434581.3434720": {
                "id": "10.1145/3434581.3434720",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Zhang",
                        "given": "Renjie"
                    },
                    {
                        "family": "Ren",
                        "given": "Chuanrong"
                    },
                    {
                        "family": "Yang",
                        "given": "Weishu"
                    },
                    {
                        "family": "Wang",
                        "given": "Yan"
                    },
                    {
                        "family": "Ding",
                        "given": "Qian"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2020,
                            10,
                            14
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2020,
                            10,
                            14
                        ]
                    ]
                },
                "abstract": "Based on the current digital industry background, the production processes become centralized and closely related to each other. If industrial production fails, it will bring great damage to it, which makes it more and more important to accurately identify the status of industrial equipment and repair it in real time by collecting time series data with industrial sensors. However, due to the large number, variety and high-frequency sampling of sensors, industrial big data has certain complexity, i.e. high spatial dimension, complex logical relationship, changeable rules, large amount of data and so on. At present, end-to-end algorithms of deep learning are widely used in many fields. From the perspective of application of deep learning, this paper studies its application in industrial big data time series classification, and analyzes the characteristics of industrial data and the challenges of industrial time series classification from three aspects: accurate classification, efficient classification and incremental learning.",
                "call-number": "10.1145/3434581.3434720",
                "collection-title": "ICASIT 2020",
                "container-title": "Proceedings of the 2020 International Conference on Aviation Safety and Information Technology",
                "DOI": "10.1145/3434581.3434720",
                "event-place": "Weihai City, China",
                "ISBN": "9781450375764",
                "keyword": "Big data, Data classification, Deep learning",
                "number-of-pages": "4",
                "page": "718–721",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Research on Big Data Classification Technology Based on Deep Learning",
                "URL": "https://doi.org/10.1145/3434581.3434720"
            }
        },
        {
            "10.1145/3323878.3325807": {
                "id": "10.1145/3323878.3325807",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Holubová",
                        "given": "Irena"
                    },
                    {
                        "family": "Scherzinger",
                        "given": "Stefanie"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            7,
                            5
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            7,
                            5
                        ]
                    ]
                },
                "abstract": "A new vision in semantic big data processing is to create enterprise data hubs, with a 360° view on all data that matters to a corporation. As we discuss in this paper, a new generation of multi-model database systems seems a promising architectural choice for building such scalable, non-native triple stores. In this paper, we first characterize this new generation of multi-model databases. Then, discussing an example scenario, we show how they allow for agile and flexible schema management, spanning a large design space for creative and incremental data modelling. We identify the challenge of generating sound triple-views from data stored in several, interlinked models, for SPARQL querying. We regard this as one of several appealing research challenges where the semantic big data and the database architecture community may join forces.",
                "call-number": "10.1145/3323878.3325807",
                "collection-number": "6",
                "collection-title": "SBD '19",
                "container-title": "Proceedings of the International Workshop on Semantic Big Data",
                "DOI": "10.1145/3323878.3325807",
                "event-place": "Amsterdam, Netherlands",
                "ISBN": "9781450367660",
                "keyword": "schema evolution, semantic data management, multi-model DBMS",
                "number": "Article 6",
                "number-of-pages": "6",
                "page": "1–6",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Unlocking the potential of nextGen multi-model databases for semantic big data projects",
                "URL": "https://doi.org/10.1145/3323878.3325807"
            }
        },
        {
            "10.1145/2640087": {
                "id": "10.1145/2640087",
                "type": "BOOK",
                "issued": {
                    "date-parts": [
                        [
                            2014
                        ]
                    ]
                },
                "call-number": "10.1145/2640087",
                "container-title-short": "BigDataScience '14",
                "event-place": "Beijing, China",
                "genre": "proceeding",
                "ISBN": "9781450328913",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Proceedings of the 2014 International Conference on Big Data Science and Computing"
            }
        },
        {
            "10.1145/1240616.1240623": {
                "id": "10.1145/1240616.1240623",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Even",
                        "given": "Adir"
                    },
                    {
                        "family": "Shankaranarayanan",
                        "given": "G."
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2007,
                            5,
                            28
                        ]
                    ]
                },
                "abstract": "Data consumers assess quality within specific business contexts or decision tasks. The same data resource may have an acceptable level of quality for some contexts but this quality may be unacceptable for other contexts. However, existing data quality metrics are mostly derived impartially, disconnected from the specific contextual characteristics. This study argues for the need to revise data quality metrics and measurement techniques to incorporate and better reflect contextual assessment. It contributes to that end by developing new metrics for assessing data quality along commonly used dimensions - completeness, validity, accuracy, and currency. The metrics are driven by data utility, a conceptual measure of the business value that is associated with the data within a specific usage context. The suggested data quality measurement framework uses utility as a scaling factor for calculating quality measurements at different levels of data hierarchy. Examples are used to demonstrate the use of utility-driven assessment in real-world data management scenarios and the broader implications for data management are discussed",
                "call-number": "10.1145/1240616.1240623",
                "container-title": "SIGMIS Database",
                "DOI": "10.1145/1240616.1240623",
                "ISSN": "0095-0033",
                "issue": "2",
                "keyword": "decision making, metadata, data management, utility, data quality, information products, information value, database",
                "number-of-pages": "19",
                "page": "75–93",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "May 2007",
                "title": "Utility-driven assessment of data quality",
                "URL": "https://doi.org/10.1145/1240616.1240623",
                "volume": "38"
            }
        },
        {
            "10.1145/3219104.3229288": {
                "id": "10.1145/3219104.3229288",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Cheng",
                        "given": "Yanzhe"
                    },
                    {
                        "family": "Liu",
                        "given": "Fang Cherry"
                    },
                    {
                        "family": "Jing",
                        "given": "Shan"
                    },
                    {
                        "family": "Xu",
                        "given": "Weijia"
                    },
                    {
                        "family": "Chau",
                        "given": "Duen Horng"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            7,
                            22
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            7,
                            22
                        ]
                    ]
                },
                "abstract": "Big data analytics pipeline becomes popular for large volume data processing, Apache Zeppelin provides an integrated environment for data ingestion, data discovery, data analytics and data visualization and collaboration with an extended framework which allows different programming languages and data processing back ends to be plugged in. The supported languages include Scala, Python, SQL, and Shell script as well as big data processing back ends including Hadoop, Spark and Hive. With the necessary tool sets, an interactive and dynamic data analysis can be done on the fly with heterogeneous programming interfaces. Although Zeppelin is great for code development and interactive analysis with small scale data set for proof-of-concept or use-case presentations, running the data processing pipeline in the batch mode is still needed for performance, robustness to fit in an automated workflow in some cases. We are developing a tool to convert Zeppelin notebook into a workflow with a set of codes that can run in a batch mode through command line interface without requiring running Zeppelin, so that the prototype code can be seamlessly deployed on the production cluster after demo stage. The entire workflow can be preserved, configured manually and run automatically. Zeppelin also provides a flexible way to integrate the visualization functionality, another contribution of this paper is to extend the Zeppelin's existing built-in visualization component for D3Network. With two added features described above, Zeppelin can help users to develop big data pipeline and visualizing graph data quickly and efficiently.",
                "call-number": "10.1145/3219104.3229288",
                "collection-number": "57",
                "collection-title": "PEARC '18",
                "container-title": "Proceedings of the Practice and Experience on Advanced Research Computing",
                "DOI": "10.1145/3219104.3229288",
                "event-place": "Pittsburgh, PA, USA",
                "ISBN": "9781450364461",
                "keyword": "Scala, Visualization, Big Data, Batch processing, Spark",
                "number": "Article 57",
                "number-of-pages": "7",
                "page": "1–7",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Building Big Data Processing and Visualization Pipeline through Apache Zeppelin",
                "URL": "https://doi.org/10.1145/3219104.3229288"
            }
        },
        {
            "10.1145/3286606.3286841": {
                "id": "10.1145/3286606.3286841",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Elyusufi",
                        "given": "Z."
                    },
                    {
                        "family": "Elyusufi",
                        "given": "Y."
                    },
                    {
                        "family": "Aitkbir",
                        "given": "M."
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            10,
                            10
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            10,
                            10
                        ]
                    ]
                },
                "abstract": "Today Big Data tools are not just a phenomenon of the massive information collection; they are also the best way to approach a customer target. These technologies allow the profiling of the customers of an organization thanks to the histories of purchases, the products that they consult; the data that they share through the social networks. They also make it possible to anticipate the purchase of actions via behavioral analysis. Therefore, the combination of the power of CRM and the performance of BIG DATA tools brings a great added value for customers profile analysis, especially if it is about events triggered in real time. It is in this context that the present work is positioned. Our goal is to intercept events (customer behaviors) and analyze them in real time. We will use the Complex Events Process (CEP) architecture that perfectly meets this need. In order to successfully implement our CEP architecture, we will use the ontology approach.",
                "call-number": "10.1145/3286606.3286841",
                "collection-number": "64",
                "collection-title": "SCA '18",
                "container-title": "Proceedings of the 3rd International Conference on Smart City Applications",
                "DOI": "10.1145/3286606.3286841",
                "event-place": "Tetouan, Morocco",
                "ISBN": "9781450365628",
                "keyword": "Profiling, CEP, CRM, Big Data, Ontology",
                "number": "Article 64",
                "number-of-pages": "6",
                "page": "1–6",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Customer profiling using CEP architecture in a Big Data context",
                "URL": "https://doi.org/10.1145/3286606.3286841"
            }
        },
        {
            "10.1145/3507473.3507479": {
                "id": "10.1145/3507473.3507479",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Hu",
                        "given": "Shuang"
                    },
                    {
                        "family": "Chen",
                        "given": "Zaihe"
                    },
                    {
                        "family": "Wei",
                        "given": "Jingbin"
                    },
                    {
                        "family": "Zhang",
                        "given": "Lianchao"
                    },
                    {
                        "family": "Li",
                        "given": "Haitao"
                    },
                    {
                        "family": "Wu",
                        "given": "Xiaoyu"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            11,
                            19
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2021,
                            11,
                            19
                        ]
                    ]
                },
                "abstract": "Cloud processing provides elastic and scalable infrastructure for big data. Shulu Sag is a typical big data processing system, the slope zone of Shulu sag has good oil and gas exploration prospects in recent years. The faults in this area are well developed, the structure is complex, and the structure interpretation scheme has multiple solutions. In order to solve this problem, this paper carried out 3D structural interpretation of Shulu slope area used big data processing, and studied the structural morphology and fault characteristics of the north, central and south members of the slope zone used Cloud processing, so as to provide a basic basis for the subsequent prediction and exploration and development of favorable oil-gas areas.",
                "call-number": "10.1145/3507473.3507479",
                "collection-title": "ICSED 2021",
                "container-title": "2021 3rd International Conference on Software Engineering and Development (ICSED)",
                "DOI": "10.1145/3507473.3507479",
                "event-place": "Xiamen, China",
                "ISBN": "9781450385213",
                "keyword": "Fault characteristics, 3D structural, Structure characteristics, Big data, Cloud processing",
                "number-of-pages": "4",
                "page": "36–39",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "3D Structural Characteristics Used Big Data of Shulu Slope Zone",
                "URL": "https://doi.org/10.1145/3507473.3507479"
            }
        },
        {
            "10.1145/2932707": {
                "id": "10.1145/2932707",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Fang",
                        "given": "Ruogu"
                    },
                    {
                        "family": "Pouyanfar",
                        "given": "Samira"
                    },
                    {
                        "family": "Yang",
                        "given": "Yimin"
                    },
                    {
                        "family": "Chen",
                        "given": "Shu-Ching"
                    },
                    {
                        "family": "Iyengar",
                        "given": "S. S."
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            6,
                            14
                        ]
                    ]
                },
                "abstract": "The explosive growth and widespread accessibility of digital health data have led to a surge of research activity in the healthcare and data sciences fields. The conventional approaches for health data management have achieved limited success as they are incapable of handling the huge amount of complex data with high volume, high velocity, and high variety. This article presents a comprehensive overview of the existing challenges, techniques, and future directions for computational health informatics in the big data age, with a structured analysis of the historical and state-of-the-art methods. We have summarized the challenges into four Vs (i.e., volume, velocity, variety, and veracity) and proposed a systematic data-processing pipeline for generic big data in health informatics, covering data capturing, storing, sharing, analyzing, searching, and decision support. Specifically, numerous techniques and algorithms in machine learning are categorized and compared. On the basis of this material, we identify and discuss the essential prospects lying ahead for computational health informatics in this big data age.",
                "call-number": "10.1145/2932707",
                "collection-number": "12",
                "container-title": "ACM Comput. Surv.",
                "DOI": "10.1145/2932707",
                "ISSN": "0360-0300",
                "issue": "1",
                "keyword": "survey, Big data analytics, clinical decision support, 4V challenges, computational health informatics, data mining, machine learning",
                "number": "Article 12",
                "number-of-pages": "36",
                "page": "1–36",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "March 2017",
                "title": "Computational Health Informatics in the Big Data Age: A Survey",
                "URL": "https://doi.org/10.1145/2932707",
                "volume": "49"
            }
        },
        {
            "10.1145/2554688.2554789": {
                "id": "10.1145/2554688.2554789",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Jun",
                        "given": "Sang-Woo"
                    },
                    {
                        "family": "Liu",
                        "given": "Ming"
                    },
                    {
                        "family": "Fleming",
                        "given": "Kermin Elliott"
                    },
                    {
                        "family": "Arvind"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2014,
                            2,
                            26
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2014,
                            2,
                            26
                        ]
                    ]
                },
                "abstract": "For many \"Big Data\" applications, the limiting factor in performance is often the transportation of large amount of data from hard disks to where it can be processed, i.e. DRAM. In this paper we examine an architecture for a scalable distributed flash store which aims to overcome this limitation in two ways. First, the architecture provides a high-performance, high-capacity, scalable random-access storage. It achieves high-throughput by sharing large numbers of flash chips across a low-latency, chip-to-chip backplane network managed by the flash controllers. The additional latency for remote data access via this network is negligible as compared to flash access time. Second, it permits some computation near the data via a FPGA-based programmable flash controller. The controller is located in the datapath between the storage and the host, and provides hardware acceleration for applications without any additional latency. We have constructed a small-scale prototype whose network bandwidth scales directly with the number of nodes, and where average latency for user software to access flash store is less than 70mus, including 3.5mus of network overhead.",
                "call-number": "10.1145/2554688.2554789",
                "collection-title": "FPGA '14",
                "container-title": "Proceedings of the 2014 ACM/SIGDA international symposium on Field-programmable gate arrays",
                "DOI": "10.1145/2554688.2554789",
                "event-place": "Monterey, California, USA",
                "ISBN": "9781450326711",
                "keyword": "ssd, big data, flash, storage system, fpga networks",
                "number-of-pages": "10",
                "page": "55–64",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Scalable multi-access flash store for big data analytics",
                "URL": "https://doi.org/10.1145/2554688.2554789"
            }
        },
        {
            "10.1145/2566486.2568002": {
                "id": "10.1145/2566486.2568002",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Kontokostas",
                        "given": "Dimitris"
                    },
                    {
                        "family": "Westphal",
                        "given": "Patrick"
                    },
                    {
                        "family": "Auer",
                        "given": "Sören"
                    },
                    {
                        "family": "Hellmann",
                        "given": "Sebastian"
                    },
                    {
                        "family": "Lehmann",
                        "given": "Jens"
                    },
                    {
                        "family": "Cornelissen",
                        "given": "Roland"
                    },
                    {
                        "family": "Zaveri",
                        "given": "Amrapali"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2014,
                            4,
                            7
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2014,
                            4,
                            7
                        ]
                    ]
                },
                "abstract": "Linked Open Data (LOD) comprises an unprecedented volume of structured data on the Web. However, these datasets are of varying quality ranging from extensively curated datasets to crowdsourced or extracted data of often relatively low quality. We present a methodology for test-driven quality assessment of Linked Data, which is inspired by test-driven software development. We argue that vocabularies, ontologies and knowledge bases should be accompanied by a number of test cases, which help to ensure a basic level of quality. We present a methodology for assessing the quality of linked data resources, based on a formalization of bad smells and data quality problems. Our formalization employs SPARQL query templates, which are instantiated into concrete quality test case queries. Based on an extensive survey, we compile a comprehensive library of data quality test case patterns. We perform automatic test case instantiation based on schema constraints or semi-automatically enriched schemata and allow the user to generate specific test case instantiations that are applicable to a schema or dataset. We provide an extensive evaluation of five LOD datasets, manual test case instantiation for five schemas and automatic test case instantiations for all available schemata registered with Linked Open Vocabularies (LOV). One of the main advantages of our approach is that domain specific semantics can be encoded in the data quality test cases, thus being able to discover data quality problems beyond conventional quality heuristics.",
                "call-number": "10.1145/2566486.2568002",
                "collection-title": "WWW '14",
                "container-title": "Proceedings of the 23rd international conference on World wide web",
                "DOI": "10.1145/2566486.2568002",
                "event-place": "Seoul, Korea",
                "ISBN": "9781450327442",
                "keyword": "data quality, dbpedia, linked data",
                "number-of-pages": "12",
                "page": "747–758",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Test-driven evaluation of linked data quality",
                "URL": "https://doi.org/10.1145/2566486.2568002"
            }
        },
        {
            "10.1145/3090354.3090373": {
                "id": "10.1145/3090354.3090373",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Mdarbi",
                        "given": "Fatima Ezzahra"
                    },
                    {
                        "family": "Afifi",
                        "given": "Nadia"
                    },
                    {
                        "family": "Hilal",
                        "given": "Imane"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            3,
                            29
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            3,
                            29
                        ]
                    ]
                },
                "abstract": "Cloud Computing provides computing resources remotely and on demand. These resources can be infrastructures, platforms, or application software. Whereas Big Data is well known for needing high capacity of both storage and computation's data streams. Thus, dependability must be studied in order to determine the attitude of Cloud systems to complete the features required by Big Data. The objective of this article is to deal with the general context of the Dependability and Cloud environment for Big Data. Thus, we carried out a statistic study of the research work that have been conducted in this context. The state of the art will allow us to approach the problematic of the dependability of Big Data in the Cloud. Our first objective is to know if there are many papers that deal at the same time a three thematics: the Cloud, the Big Data and the Dependability",
                "call-number": "10.1145/3090354.3090373",
                "collection-number": "19",
                "collection-title": "BDCA'17",
                "container-title": "Proceedings of the 2nd international Conference on Big Data, Cloud and Applications",
                "DOI": "10.1145/3090354.3090373",
                "event-place": "Tetouan, Morocco",
                "ISBN": "9781450348522",
                "keyword": "PaaS, Big Data, Value, Dependability, SaaS, Cloud Computing, Velocity, Variety, IaaS, Volume, Veracity",
                "number": "Article 19",
                "number-of-pages": "6",
                "page": "1–6",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Comparative Study: Dependability of Big Data in the Cloud",
                "URL": "https://doi.org/10.1145/3090354.3090373"
            }
        },
        {
            "10.1145/2539150.2539224": {
                "id": "10.1145/2539150.2539224",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Girtelschmid",
                        "given": "Sylva"
                    },
                    {
                        "family": "Steinbauer",
                        "given": "Matthias"
                    },
                    {
                        "family": "Kumar",
                        "given": "Vikash"
                    },
                    {
                        "family": "Fensel",
                        "given": "Anna"
                    },
                    {
                        "family": "Kotsis",
                        "given": "Gabriele"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2013,
                            12,
                            2
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2013,
                            12,
                            2
                        ]
                    ]
                },
                "abstract": "This paper highlights how the domain of Smart Cities is often modeled by ontologies to create applications and services that are highly flexible, (re)configurable, and inter-operable. However, ontology repositories and their accompanying reasoning and rule languages face the disadvantage of bad runtime behavior, especially if the models grow large in size. We propose an architecture that uses tools and methods from the domain of Big Data processing in conjunction with an ontology repository and a rule engine to overcome potential performance bottlenecks that will occur in this scenario.",
                "call-number": "10.1145/2539150.2539224",
                "collection-title": "IIWAS '13",
                "container-title": "Proceedings of International Conference on Information Integration and Web-based Applications & Services",
                "DOI": "10.1145/2539150.2539224",
                "event-place": "Vienna, Austria",
                "ISBN": "9781450321136",
                "keyword": "Semantic Modeling, Real-time Streaming, Energy Efficiency, Ontology, Big Data, Smart City",
                "number-of-pages": "5",
                "page": "428–432",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Big Data in Large Scale Intelligent Smart City Installations",
                "URL": "https://doi.org/10.1145/2539150.2539224"
            }
        },
        {
            "10.1145/3402569.3409041": {
                "id": "10.1145/3402569.3409041",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Han",
                        "given": "Ping"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2020,
                            5,
                            22
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2020,
                            5,
                            22
                        ]
                    ]
                },
                "abstract": "The rapid development of Internet technology has promoted the process of economic globalization, and the application of big data technology has injected new vitality into the innovation of foreign exchange management models. Businesses such as foreign exchange deposits and loans and foreign currency exchange have encountered new development opportunities. Under the effect of big data technology, it can quickly process massive amounts of information, respond to various exchange rate changes, and achieve the improvement of foreign exchange business management. Especially under the circumstances of the current RMB marketization, exchange rate reform and the diversification of the international situation, the difficulty of foreign exchange management is gradually increasing. How to better improve the efficiency of foreign exchange management has become a problem that must be solved at present. Therefore, it is of great significance to explore the foreign exchange management model based on the background of big data, build a big data computing mechanism, give full play to its advantages in foreign exchange management, and promote the improvement of foreign exchange management.",
                "call-number": "10.1145/3402569.3409041",
                "collection-title": "ICDEL 2020",
                "container-title": "Proceedings of the 5th International Conference on Distance Education and Learning",
                "DOI": "10.1145/3402569.3409041",
                "event-place": "Beijing, China",
                "ISBN": "9781450377546",
                "keyword": "mode, foreign exchange management, Big data background",
                "number-of-pages": "4",
                "page": "162–165",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Research on Foreign Exchange Management Model Based on Big Data",
                "URL": "https://doi.org/10.1145/3402569.3409041"
            }
        },
        {
            "10.5555/3213069.3213074": {
                "id": "10.5555/3213069.3213074",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Abidi",
                        "given": "Faiz"
                    },
                    {
                        "family": "Polys",
                        "given": "Nicholas"
                    },
                    {
                        "family": "Rajamohan",
                        "given": "Srijith"
                    },
                    {
                        "family": "Arsenault",
                        "given": "Lance"
                    },
                    {
                        "family": "Mohammed",
                        "given": "Ayat"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            4,
                            15
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            4,
                            15
                        ]
                    ]
                },
                "abstract": "Remote visualization has emerged as a necessary tool in the analysis of big data. High-performance computing clusters can provide several benefits in scaling to larger data sizes, from parallel file systems to larger RAM profiles to parallel computation among many CPUs and GPUs. For scalable data visualization, remote visualization tools and infrastructure is critical where only pixels and interaction events are sent over the network instead of the data. In this paper, we present our pipeline using VirtualGL, TurboVNC, and ParaView to render over 40 million points using remote HPC clusters and project over 26 million pixels in a CAVE-style system. We benchmark the system by varying the video stream compression parameters supported by TurboVNC and establish some best practices for typical usage scenarios. This work will help research scientists and academicians in scaling their big data visualizations for remote, real-time interaction.",
                "call-number": "10.5555/3213069.3213074",
                "collection-number": "5",
                "collection-title": "HPC '18",
                "container-title": "Proceedings of the High Performance Computing Symposium",
                "event-place": "Baltimore, Maryland",
                "ISBN": "9781510860162",
                "keyword": "big data, remote rendering, CAVE, HPC, paraview",
                "number": "Article 5",
                "number-of-pages": "12",
                "page": "1–12",
                "publisher": "Society for Computer Simulation International",
                "publisher-place": "San Diego, CA, USA",
                "title": "Remote high performance visualization of big data for immersive science"
            }
        },
        {
            "10.1145/3264560.3264562": {
                "id": "10.1145/3264560.3264562",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Ezzikouri",
                        "given": "Hanane"
                    },
                    {
                        "family": "Oukessou",
                        "given": "Mohamed"
                    },
                    {
                        "family": "Youness",
                        "given": "Madani"
                    },
                    {
                        "family": "Erritali",
                        "given": "Mohamed"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            8,
                            3
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            8,
                            3
                        ]
                    ]
                },
                "abstract": "Cross-Language Plagiarism refers to the unacknowledged reuse of a text involving its translation from one natural language to another without proper referencing to the original source. One of the common problems in data processing is efficient large-scale text comparison, especially semantic based similarity due to the increase in the number of publications and the rate of suspicious documents sources of plagiarism. CLPD nature could be more complicated than simple copy+translate and paste, thus the detecting process exposes the need for a vague concept and fuzzy sets techniques in a big data environment to reveal dishonest practices in Arabic documents. In this paper, we propose a new Cross-Language Plagiarism Detection based on fuzzy-semantic similarity using WordNet and two semantic approaches Wu&Palmer and Lin; the work is done in a parallel way using Apache Hadoop with its distributed file system HDFS and the MapReduce programming model. The experimental results show that the Fuzzy Wu & Palmer have high performance than Fuzzy Lin.",
                "call-number": "10.1145/3264560.3264562",
                "collection-title": "ICCBDC'18",
                "container-title": "Proceedings of the 2018 2nd International Conference on Cloud and Big Data Computing",
                "DOI": "10.1145/3264560.3264562",
                "event-place": "Barcelona, Spain",
                "ISBN": "9781450364744",
                "keyword": "HDFS, Semantic Similarity, CLPD, Hadoop, MapReduce, Fuzzy sets",
                "number-of-pages": "6",
                "page": "22–27",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Fuzzy Cross Language Plagiarism Detection (Arabic-English) using WordNet in a Big Data environment",
                "URL": "https://doi.org/10.1145/3264560.3264562"
            }
        },
        {
            "10.1145/3230744.3230751": {
                "id": "10.1145/3230744.3230751",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Kwon",
                        "given": "Byungjun"
                    },
                    {
                        "family": "Yu",
                        "given": "Moonwon"
                    },
                    {
                        "family": "Jang",
                        "given": "Hanyoung"
                    },
                    {
                        "family": "Cho",
                        "given": "KyuHyun"
                    },
                    {
                        "family": "Lee",
                        "given": "Hyundong"
                    },
                    {
                        "family": "Hahn",
                        "given": "Taesung"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            8,
                            12
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            8,
                            12
                        ]
                    ]
                },
                "abstract": "This paper presents a novel motion transfer algorithm that copies content motion into a specific style character. The input consists of two motions. One is a content motion such as walking or running, and the other is movement style such as zombie or Krall. The algorithm automatically generates the synthesized motion such as walking zombie, walking Krall, running zombie, or running Krall. In order to obtain natural results, the method adopts the generative power of deep neural networks. Compared to previous neural approaches, the proposed algorithm shows better quality, runs extremely fast, does not require big data, and supports user-controllable style weights.",
                "call-number": "10.1145/3230744.3230751",
                "collection-number": "58",
                "collection-title": "SIGGRAPH '18",
                "container-title": "ACM SIGGRAPH 2018 Posters",
                "DOI": "10.1145/3230744.3230751",
                "event-place": "Vancouver, British Columbia, Canada",
                "ISBN": "9781450358170",
                "keyword": "character animation synthesis, deep neural networks",
                "number": "Article 58",
                "number-of-pages": "2",
                "page": "1–2",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Deep motion transfer without big data",
                "URL": "https://doi.org/10.1145/3230744.3230751"
            }
        },
        {
            "10.1145/3056662.3056667": {
                "id": "10.1145/3056662.3056667",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Shi",
                        "given": "Yingjie"
                    },
                    {
                        "family": "Wang",
                        "given": "Lei"
                    },
                    {
                        "family": "Du",
                        "given": "Fang"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            2,
                            26
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            2,
                            26
                        ]
                    ]
                },
                "abstract": "Large volume of data is produced by various applications in the world, processing such scale of data has great challenges in not only performance but also energy efficiency. Researchers propose various techniques to either improve the performance or the energy efficiency. The techniques of these two trends, however, are significantly different. When both performance and energy efficiency are concerned in the big data systems, how to get balance has become an issuing and challenging problem for data center administrators and hardware designers. In this paper, we conduct comprehensive evaluations on two representative platforms with different types of processors. We quantify the performance and energy efficiency, relating the evaluation results to micro-architectural activities and application characteristics. Two interesting findings are made from our evaluations: (1) the performance and energy efficiency are not only determined by the hardware technology, but also associated with the application characteristics; (2) there is no ever-victorious microprocessor in terms of both performance and energy efficiency in all the big data workloads. Based on the findings and quantified evaluation results, we provide great guidance and implications for both data center administrators and big data system designers, and we argue that a hybrid-core is an efficient way to improve the energy efficiency of big data systems with minimum performance degradation.",
                "call-number": "10.1145/3056662.3056667",
                "collection-title": "ICSCA '17",
                "container-title": "Proceedings of the 6th International Conference on Software and Computer Applications",
                "DOI": "10.1145/3056662.3056667",
                "event-place": "Bangkok, Thailand",
                "ISBN": "9781450348577",
                "keyword": "performance, big data systems, hybrid core, energy efficiency",
                "number-of-pages": "7",
                "page": "55–61",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Performance and energy efficiency of big data systems: characterization, implication and improvement",
                "URL": "https://doi.org/10.1145/3056662.3056667"
            }
        },
        {
            "10.1145/2744680.2744687": {
                "id": "10.1145/2744680.2744687",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Fang",
                        "given": "Xiu Susie"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2015,
                            5,
                            31
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2015,
                            5,
                            31
                        ]
                    ]
                },
                "abstract": "The last few years have seen a rapid increase of sheer amount of data produced and communicated over the Internet and the Web. While it is widely believed that the availability of such ``Big Data'' holds the potential to revolutionize many aspects of our modern society (e.g., intelligent transportation, environmental monitoring, and energy saving), many challenges need to be addressed before this potential can be realized. This PhD project focuses on one critical challenge, namely extracting actionable knowledge from Big Data. Tremendous efforts have been contributed on mining large-scale data on the Web and constructing comprehensive knowledge bases (KBs). However, existing knowledge extraction systems retrieve data from limited types of Web sources. In addition, data fusion approaches consider very little of the noises produced by those knowledge extraction systems. Consequently, the constructed KBs are far from being comprehensive and accurate. In this paper, we present our initial design of a framework for extracting machine-readable data with high precision and recall from four types of data sources, namely Web texts, Document Object Model (DOM) trees, existing KBs, and query stream. Confidence scores are attached to the resulting knowledge, which can be used to further improve the knowledge fusion results.",
                "call-number": "10.1145/2744680.2744687",
                "collection-title": "SIGMOD '15 PhD Symposium",
                "container-title": "Proceedings of the 2015 ACM SIGMOD on PhD Symposium",
                "DOI": "10.1145/2744680.2744687",
                "event-place": "Melbourne, Victoria, Australia",
                "ISBN": "9781450335294",
                "keyword": "knowledge base, dom tree, knowledge fusion",
                "number-of-pages": "6",
                "page": "3–8",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Generating Actionable Knowledge from Big Data",
                "URL": "https://doi.org/10.1145/2744680.2744687"
            }
        }
    ]
}