{
    "exportedDoiLength": 101,
    "fileName": "acm",
    "style": "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<style xmlns=\"http://purl.org/net/xbiblio/csl\" class=\"in-text\" version=\"1.0\" demote-non-dropping-particle=\"sort-only\" default-locale=\"en-US\">\n    <!-- This style was edited with the Visual CSL Editor (http://editor.citationstyles.org/visualEditor/) -->\n    <info>\n        <title>BibTeX ACM citation style</title>\n        <id>http://www.zotero.org/styles/bibtex-acm-citation-style</id>\n        <link href=\"http://www.zotero.org/styles/bibtex-acm-citation-style\" rel=\"self\"/>\n        <link href=\"http://www.bibtex.org/\" rel=\"documentation\"/>\n        <author>\n            <name>Markus Schaffner</name>\n        </author>\n        <contributor>\n            <name>Richard Karnesky</name>\n            <email>karnesky+zotero@gmail.com</email>\n            <uri>http://arc.nucapt.northwestern.edu/Richard_Karnesky</uri>\n        </contributor>\n        <category citation-format=\"author-date\"/>\n        <category field=\"generic-base\"/>\n        <updated>2018-06-11T10:52:49+00:00</updated>\n        <rights license=\"http://creativecommons.org/licenses/by-sa/3.0/\">This work is licensed under a Creative Commons Attribution-ShareAlike 3.0 License</rights>\n    </info>\n    <macro name=\"zotero2bibtexType\">\n        <choose>\n            <if type=\"BILL BOOK GRAPHIC LEGAL_CASE LEGISLATION MOTION_PICTURE SONG\" match=\"any\">\n                <choose>\n                    <if genre=\"rfc\"  match=\"any\">\n                       <text value=\"rfc\"/>\n                    </if>\n                     <else-if  genre=\"bibliography\" match=\"any\">\n                        <text value=\"bibliography\"/>\n                    </else-if>\n                    <else-if  genre=\"play_drama\" match=\"any\">\n                        <text value=\"playdrama\"/>\n                    </else-if>\n                    <else-if  genre=\"proceeding\" match=\"any\">\n                        <text value=\"proceedings\"/>\n                    </else-if>\n                    <else-if  genre=\"tech_brief\" match=\"any\">\n                        <text value=\"tech-brief\"/>\n                    </else-if>\n                    <else>\n                       <text value=\"book\"/>\n                    </else>\n                </choose>\n            </if>\n            <else-if type=\"CHAPTER\" match=\"any\">\n                <text value=\"inbook\"/>\n            </else-if>\n            <else-if type=\"ARTICLE ARTICLE_JOURNAL ARTICLE_MAGAZINE ARTICLE_NEWSPAPER\" match=\"any\">\n                <text value=\"article\"/>\n            </else-if>\n            <else-if type=\"THESIS\" match=\"any\">\n                <choose>\n                    <if variable=\"genre\">\n                        <text variable=\"genre\" text-case=\"lowercase\" strip-periods=\"true\"/>\n                    </if>\n                </choose>\n                <text value=\"thesis\"/>\n            </else-if>\n            <else-if type=\"PAPER_CONFERENCE\" match=\"any\">\n                <text value=\"inproceedings\"/>\n            </else-if>\n            <else-if type=\"REPORT\" match=\"any\">\n                <text value=\"techreport\"/>\n            </else-if>\n            <else-if type=\"DATASET\" match=\"any\">\n                <choose>\n                    <if genre=\"software\"  match=\"any\">\n                       <text value=\"software\"/>\n                    </if>\n                    <else>\n                       <text value=\"dataset\"/>\n                    </else>\n                </choose>\n            </else-if>\n            <else>\n                <text value=\"misc\"/>\n            </else>\n        </choose>\n    </macro>\n    <macro name=\"citeKey\">\n           <text variable=\"call-number\"/>\n    </macro>\n    <macro name=\"editor-short\">\n        <names variable=\"editor\">\n            <name form=\"short\" delimiter=\":\" delimiter-precedes-last=\"always\"/>\n        </names>\n    </macro>\n    <macro name=\"author-short\">\n        <names variable=\"author\">\n            <name form=\"short\" delimiter=\":\" delimiter-precedes-last=\"always\"/>\n        </names>\n    </macro>\n    <macro name=\"issued-year\">\n        <date variable=\"issued\">\n            <date-part name=\"year\"/>\n        </date>\n    </macro>\n    <macro name=\"issued-month\">\n        <choose>\n            <if type=\"ARTICLE\" match=\"any\">\n                <date variable=\"issued\">\n                    <date-part name=\"month\" form=\"short\" strip-periods=\"true\" text-case=\"lowercase\"/>\n                </date>\n            </if>\n        </choose>\n    </macro>\n    <macro name=\"issue-date\">\n        <choose>\n            <if type=\"ARTICLE ARTICLE_JOURNAL\" match=\"any\">\n                <choose>\n                    <if variable=\"note\" match=\"none\">\n                        <choose>\n                            <if variable=\"source\">\n                                 <text variable=\"source\"/>\n                            </if>\n                               <else>\n                                 <date date-parts=\"year-month\" form=\"text\" variable=\"issued\"/>\n                               </else>\n                        </choose>\n                    </if>\n                </choose>\n            </if>\n        </choose>\n    </macro>\n    <macro name=\"author\">\n        <names variable=\"author\">\n            <name sort-separator=\", \" delimiter=\" and \" delimiter-precedes-last=\"always\" name-as-sort-order=\"all\"/>\n            <label form=\"long\" text-case=\"capitalize-first\"/>\n        </names>\n    </macro>\n    <macro name=\"editor-translator\">\n        <names variable=\"editor translator\" delimiter=\", \">\n            <name sort-separator=\", \" delimiter=\" and \" delimiter-precedes-last=\"always\" name-as-sort-order=\"all\"/>\n            <label form=\"long\" text-case=\"capitalize-first\"/>\n        </names>\n    </macro>\n    <macro name=\"title\">\n        <choose>\n            <if genre=\"proceeding\" match=\"any\">\n                <text variable=\"container-title-short\" suffix=\": \"/>\n                <text variable=\"title\" text-case=\"title\"/>\n            </if>\n            <else-if type=\"ARTICLE_JOURNAL\" match=\"none\">\n                <text variable=\"title\" text-case=\"title\"/>\n            </else-if>\n        </choose>\n    </macro>\n    <macro name=\"volume\">\n        <choose>\n            <if type=\"ARTICLE BOOK ARTICLE_JOURNAL\" match=\"any\">\n                <text variable=\"volume\" prefix=\"volume = {\" suffix=\"}\"/>\n            </if>\n        </choose>\n    </macro>\n    <macro name=\"DOI\">\n        <choose>\n            <if type=\"ARTICLE PAPER_CONFERENCE\" match=\"any\">\n                <text variable=\"DOI\" prefix=\"doi = {\" suffix=\"}\"/>\n            </if>\n        </choose>\n    </macro>\n    <macro name=\"abstract\">\n        <if match=\"any\" variable=\"abstract\">\n            <text variable=\"abstract\"/>\n        </if>\n    </macro>\n    <macro name=\"URL\">\n        <text variable=\"URL\" prefix=\"url = {\" suffix=\"}\"/>\n    </macro>\n    <macro name=\"container-title\">\n        <choose>\n            <if type=\"CHAPTER PAPER_CONFERENCE\" match=\"any\">\n                <text variable=\"container-title\" prefix=\"booktitle = {\" suffix=\"}\" text-case=\"title\"/>\n            </if>\n            <else-if type=\"ARTICLE ARTICLE_JOURNAL\" match=\"any\">\n                <text variable=\"container-title\" prefix=\"journal = {\" suffix=\"}\" text-case=\"title\"/>\n            </else-if>\n        </choose>\n    </macro>\n    <macro name=\"pages\">\n        <group delimiter=\",&#10;\">\n            <choose>\n                <if match=\"any\" variable=\"collection-number\">\n                    <text variable=\"collection-number\" prefix=\"articleno = {\" suffix=\"}\"/>\n                </if>\n                <else>\n                    <text variable=\"page\" prefix=\"pages = {\" suffix=\"}\"/>\n                </else>\n            </choose>\n            <text variable=\"number-of-pages\" prefix=\"numpages = {\" suffix=\"}\"/>\n        </group>\n    </macro>\n    <macro name=\"edition\">\n        <text variable=\"edition\"/>\n    </macro>\n    <macro name=\"editor\">\n        <choose>\n            <if match=\"any\" type=\"THESIS\">\n                <names variable=\"editor\" delimiter=\", \" prefix=\"advisor = {\" suffix=\"}\">\n                    <name delimiter=\" and \" delimiter-precedes-last=\"always\" name-as-sort-order=\"all\"/>\n                </names>\n            </if>\n            <else>\n               <names variable=\"editor\" delimiter=\", \" prefix=\"editor = {\" suffix=\"}\">\n                   <name delimiter=\" and \" delimiter-precedes-last=\"always\" name-as-sort-order=\"all\"/>\n               </names>\n            </else>\n        </choose>\n    </macro>\n    <macro name=\"keyword\">\n        <choose>\n            <if type=\"ARTICLE PAPER_CONFERENCE DATASET\" match=\"any\">\n                <text variable=\"keyword\" prefix=\"keywords = {\" suffix=\"}\"/>\n            </if>\n        </choose>\n    </macro>\n    <citation et-al-min=\"11\" et-al-use-first=\"10\" disambiguate-add-year-suffix=\"true\" disambiguate-add-names=\"false\" disambiguate-add-givenname=\"false\" collapse=\"year\">\n        <layout delimiter=\"_\">\n            <text macro=\"citeKey\"/>\n        </layout>\n    </citation>\n    <bibliography hanging-indent=\"false\">\n        <layout>\n            <group display=\"right-inline\">\n                <text macro=\"zotero2bibtexType\" prefix=\"@\"/>\n                <group prefix=\"{\" suffix=\"&#10;}\" delimiter=\",&#10;\">\n                    <text macro=\"citeKey\"/>\n                    <text macro=\"author\" prefix=\"author = {\" suffix=\"}\"/>\n                    <text macro=\"editor\"/>\n                    <text macro=\"title\" prefix=\"title = {\" suffix=\"}\"/>\n                    <text macro=\"issued-year\" prefix=\"year = {\" suffix=\"}\"/>\n                    <text macro=\"issue-date\" prefix=\"issue_date = {\" suffix=\"}\"/>\n                    <text variable=\"ISBN\" prefix=\"isbn = {\" suffix=\"}\"/>\n                    <text variable=\"publisher\" prefix=\"publisher = {\" suffix=\"}\"/>\n                    <text variable=\"publisher-place\" prefix=\"address = {\" suffix=\"}\"/>\n                    <text variable=\"chapter-number\" prefix=\"chapter = {\" suffix=\"}\"/>\n                    <text macro=\"edition\" prefix=\"edition = {\" suffix=\"}\"/>\n                    <text macro=\"volume\"/>\n                    <text variable=\"issue\" prefix=\"number = {\" suffix=\"}\"/>\n                    <text variable=\"ISSN\" prefix=\"issn = {\" suffix=\"}\"/>\n                    <text variable=\"archive_location\" prefix=\"archiveLocation = {\" suffix=\"}\"/>\n                    <text macro=\"URL\"/>\n                    <text macro=\"DOI\"/>\n                    <text macro=\"abstract\" prefix=\"abstract = {\" suffix=\"}\"/>\n                    <text variable=\"note\" prefix=\"note = {\" suffix=\"}\"/>\n                    <text macro=\"container-title\"/>\n                    <text macro=\"issued-month\" prefix=\"month = {\" suffix=\"}\"/>\n                    <text macro=\"pages\"/>\n                    <text macro=\"keyword\"/>\n                    <text variable=\"event-place\" prefix=\"location = {\" suffix=\"}\"/>\n                    <choose>\n                        <if type=\"PAPER_CONFERENCE\" match=\"any\">\n                            <text variable=\"collection-title\" prefix=\"series = {\" suffix=\"}\"/>\n                        </if>\n                        <else>\n                            <text variable=\"collection-title\" prefix=\"collection = {\" suffix=\"}\"/>\n                        </else>\n                    </choose>\n                </group>\n            </group>\n        </layout>\n    </bibliography>\n</style>\n",
    "suffix": "bib",
    "locale": "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<locale xmlns=\"http://purl.org/net/xbiblio/csl\" version=\"1.0\" xml:lang=\"en-US\">\n  <info>\n    <rights license=\"http://creativecommons.org/licenses/by-sa/3.0/\">This work is licensed under a Creative Commons Attribution-ShareAlike 3.0 License</rights>\n    <updated>2012-07-04T23:31:02+00:00</updated>\n  </info>\n  <style-options punctuation-in-quote=\"true\"\n                 leading-noise-words=\"a,an,the\"\n                 name-as-sort-order=\"ja zh kr my hu vi\"\n                 name-never-short=\"ja zh kr my hu vi\"/>\n  <date form=\"text\">\n    <date-part name=\"month\" suffix=\" \"/>\n    <date-part name=\"day\" suffix=\", \"/>\n    <date-part name=\"year\"/>\n  </date>\n  <date form=\"numeric\">\n    <date-part name=\"month\" form=\"numeric-leading-zeros\" suffix=\"/\"/>\n    <date-part name=\"day\" form=\"numeric-leading-zeros\" suffix=\"/\"/>\n    <date-part name=\"year\"/>\n  </date>\n  <terms>\n    <term name=\"radio-broadcast\">radio broadcast</term>\n    <term name=\"television-broadcast\">television broadcast</term>\n    <term name=\"podcast\">podcast</term>\n    <term name=\"instant-message\">instant message</term>\n    <term name=\"email\">email</term>\n    <term name=\"number-of-volumes\">\n      <single>volume</single>\n      <multiple>volumes</multiple>\n    </term>\n    <term name=\"accessed\">accessed</term>\n    <term name=\"and\">and</term>\n    <term name=\"and\" form=\"symbol\">&amp;</term>\n    <term name=\"and others\">and others</term>\n    <term name=\"anonymous\">anonymous</term>\n    <term name=\"anonymous\" form=\"short\">anon.</term>\n    <term name=\"at\">at</term>\n    <term name=\"available at\">available at</term>\n    <term name=\"by\">by</term>\n    <term name=\"circa\">circa</term>\n    <term name=\"circa\" form=\"short\">c.</term>\n    <term name=\"cited\">cited</term>\n    <term name=\"edition\">\n      <single>edition</single>\n      <multiple>editions</multiple>\n    </term>\n    <term name=\"edition\" form=\"short\">ed.</term>\n    <term name=\"et-al\">et al.</term>\n    <term name=\"forthcoming\">forthcoming</term>\n    <term name=\"from\">from</term>\n    <term name=\"ibid\">ibid.</term>\n    <term name=\"in\">in</term>\n    <term name=\"in press\">in press</term>\n    <term name=\"internet\">internet</term>\n    <term name=\"interview\">interview</term>\n    <term name=\"letter\">letter</term>\n    <term name=\"no date\">no date</term>\n    <term name=\"no date\" form=\"short\">n.d.</term>\n    <term name=\"online\">online</term>\n    <term name=\"presented at\">presented at the</term>\n    <term name=\"reference\">\n      <single>reference</single>\n      <multiple>references</multiple>\n    </term>\n    <term name=\"reference\" form=\"short\">\n      <single>ref.</single>\n      <multiple>refs.</multiple>\n    </term>\n    <term name=\"retrieved\">retrieved</term>\n    <term name=\"scale\">scale</term>\n    <term name=\"version\">version</term>\n\n    <!-- ANNO DOMINI; BEFORE CHRIST -->\n    <term name=\"ad\">AD</term>\n    <term name=\"bc\">BC</term>\n\n    <!-- PUNCTUATION -->\n    <term name=\"open-quote\">“</term>\n    <term name=\"close-quote\">”</term>\n    <term name=\"open-inner-quote\">‘</term>\n    <term name=\"close-inner-quote\">’</term>\n    <term name=\"page-range-delimiter\">–</term>\n\n    <!-- ORDINALS -->\n    <term name=\"ordinal\">th</term>\n    <term name=\"ordinal-01\">st</term>\n    <term name=\"ordinal-02\">nd</term>\n    <term name=\"ordinal-03\">rd</term>\n    <term name=\"ordinal-11\">th</term>\n    <term name=\"ordinal-12\">th</term>\n    <term name=\"ordinal-13\">th</term>\n\n    <!-- LONG ORDINALS -->\n    <term name=\"long-ordinal-01\">first</term>\n    <term name=\"long-ordinal-02\">second</term>\n    <term name=\"long-ordinal-03\">third</term>\n    <term name=\"long-ordinal-04\">fourth</term>\n    <term name=\"long-ordinal-05\">fifth</term>\n    <term name=\"long-ordinal-06\">sixth</term>\n    <term name=\"long-ordinal-07\">seventh</term>\n    <term name=\"long-ordinal-08\">eighth</term>\n    <term name=\"long-ordinal-09\">ninth</term>\n    <term name=\"long-ordinal-10\">tenth</term>\n\n    <!-- LONG LOCATOR FORMS -->\n    <term name=\"book\">\n      <single>book</single>\n      <multiple>books</multiple>\n    </term>\n    <term name=\"chapter\">\n      <single>chapter</single>\n      <multiple>chapters</multiple>\n    </term>\n    <term name=\"column\">\n      <single>column</single>\n      <multiple>columns</multiple>\n    </term>\n    <term name=\"figure\">\n      <single>figure</single>\n      <multiple>figures</multiple>\n    </term>\n    <term name=\"folio\">\n      <single>folio</single>\n      <multiple>folios</multiple>\n    </term>\n    <term name=\"issue\">\n      <single>number</single>\n      <multiple>numbers</multiple>\n    </term>\n    <term name=\"line\">\n      <single>line</single>\n      <multiple>lines</multiple>\n    </term>\n    <term name=\"note\">\n      <single>note</single>\n      <multiple>notes</multiple>\n    </term>\n    <term name=\"opus\">\n      <single>opus</single>\n      <multiple>opera</multiple>\n    </term>\n    <term name=\"page\">\n      <single>page</single>\n      <multiple>pages</multiple>\n    </term>\n    <term name=\"paragraph\">\n      <single>paragraph</single>\n      <multiple>paragraph</multiple>\n    </term>\n    <term name=\"part\">\n      <single>part</single>\n      <multiple>parts</multiple>\n    </term>\n    <term name=\"section\">\n      <single>section</single>\n      <multiple>sections</multiple>\n    </term>\n    <term name=\"sub verbo\">\n      <single>sub verbo</single>\n      <multiple>sub verbis</multiple>\n    </term>\n    <term name=\"verse\">\n      <single>verse</single>\n      <multiple>verses</multiple>\n    </term>\n    <term name=\"volume\">\n      <single>volume</single>\n      <multiple>volumes</multiple>\n    </term>\n\n    <!-- SHORT LOCATOR FORMS -->\n    <term name=\"book\" form=\"short\">bk.</term>\n    <term name=\"chapter\" form=\"short\">chap.</term>\n    <term name=\"column\" form=\"short\">col.</term>\n    <term name=\"figure\" form=\"short\">fig.</term>\n    <term name=\"folio\" form=\"short\">f.</term>\n    <term name=\"issue\" form=\"short\">no.</term>\n    <term name=\"line\" form=\"short\">l.</term>\n    <term name=\"note\" form=\"short\">n.</term>\n    <term name=\"opus\" form=\"short\">op.</term>\n    <term name=\"page\" form=\"short\">\n      <single>p.</single>\n      <multiple>pp.</multiple>\n    </term>\n    <term name=\"paragraph\" form=\"short\">para.</term>\n    <term name=\"part\" form=\"short\">pt.</term>\n    <term name=\"section\" form=\"short\">sec.</term>\n    <term name=\"sub verbo\" form=\"short\">\n      <single>s.v.</single>\n      <multiple>s.vv.</multiple>\n    </term>\n    <term name=\"verse\" form=\"short\">\n      <single>v.</single>\n      <multiple>vv.</multiple>\n    </term>\n    <term name=\"volume\" form=\"short\">\n      <single>vol.</single>\n      <multiple>vols.</multiple>\n    </term>\n\n    <!-- SYMBOL LOCATOR FORMS -->\n    <term name=\"paragraph\" form=\"symbol\">\n      <single>¶</single>\n      <multiple>¶¶</multiple>\n    </term>\n    <term name=\"section\" form=\"symbol\">\n      <single>§</single>\n      <multiple>§§</multiple>\n    </term>\n\n    <!-- LONG ROLE FORMS -->\n    <term name=\"director\">\n      <single>director</single>\n      <multiple>directors</multiple>\n    </term>\n    <term name=\"editor\">\n      <single>editor</single>\n      <multiple>editors</multiple>\n    </term>\n    <term name=\"editorial-director\">\n      <single>editor</single>\n      <multiple>editors</multiple>\n    </term>\n    <term name=\"illustrator\">\n      <single>illustrator</single>\n      <multiple>illustrators</multiple>\n    </term>\n    <term name=\"translator\">\n      <single>translator</single>\n      <multiple>translators</multiple>\n    </term>\n    <term name=\"editortranslator\">\n      <single>editor &amp; translator</single>\n      <multiple>editors &amp; translators</multiple>\n    </term>\n\n    <!-- SHORT ROLE FORMS -->\n    <term name=\"director\" form=\"short\">\n      <single>dir.</single>\n      <multiple>dirs.</multiple>\n    </term>\n    <term name=\"editor\" form=\"short\">\n      <single>ed.</single>\n      <multiple>eds.</multiple>\n    </term>\n    <term name=\"editorial-director\" form=\"short\">\n      <single>ed.</single>\n      <multiple>eds.</multiple>\n    </term>\n    <term name=\"illustrator\" form=\"short\">\n      <single>ill.</single>\n      <multiple>ills.</multiple>\n    </term>\n    <term name=\"translator\" form=\"short\">\n      <single>tran.</single>\n      <multiple>trans.</multiple>\n    </term>\n    <term name=\"editortranslator\" form=\"short\">\n      <single>ed. &amp; tran.</single>\n      <multiple>eds. &amp; trans.</multiple>\n    </term>\n\n    <!-- VERB ROLE FORMS -->\n    <term name=\"director\" form=\"verb\">directed by</term>\n    <term name=\"editor\" form=\"verb\">edited by</term>\n    <term name=\"editorial-director\" form=\"verb\">edited by</term>\n    <term name=\"illustrator\" form=\"verb\">illustrated by</term>\n    <term name=\"interviewer\" form=\"verb\">interview by</term>\n    <term name=\"recipient\" form=\"verb\">to</term>\n    <term name=\"reviewed-author\" form=\"verb\">by</term>\n    <term name=\"translator\" form=\"verb\">translated by</term>\n    <term name=\"editortranslator\" form=\"verb\">edited &amp; translated by</term>\n\n    <!-- SHORT VERB ROLE FORMS -->\n    <term name=\"container-author\" form=\"verb-short\">by</term>\n    <term name=\"director\" form=\"verb-short\">dir.</term>\n    <term name=\"editor\" form=\"verb-short\">ed.</term>\n    <term name=\"editorial-director\" form=\"verb-short\">ed.</term>\n    <term name=\"illustrator\" form=\"verb-short\">illus.</term>\n    <term name=\"translator\" form=\"verb-short\">trans.</term>\n    <term name=\"editortranslator\" form=\"verb-short\">ed. &amp; trans.</term>\n\n    <!-- LONG MONTH FORMS -->\n    <term name=\"month-01\">January</term>\n    <term name=\"month-02\">February</term>\n    <term name=\"month-03\">March</term>\n    <term name=\"month-04\">April</term>\n    <term name=\"month-05\">May</term>\n    <term name=\"month-06\">June</term>\n    <term name=\"month-07\">July</term>\n    <term name=\"month-08\">August</term>\n    <term name=\"month-09\">September</term>\n    <term name=\"month-10\">October</term>\n    <term name=\"month-11\">November</term>\n    <term name=\"month-12\">December</term>\n\n    <!-- SHORT MONTH FORMS -->\n    <term name=\"month-01\" form=\"short\">Jan.</term>\n    <term name=\"month-02\" form=\"short\">Feb.</term>\n    <term name=\"month-03\" form=\"short\">Mar.</term>\n    <term name=\"month-04\" form=\"short\">Apr.</term>\n    <term name=\"month-05\" form=\"short\">May</term>\n    <term name=\"month-06\" form=\"short\">Jun.</term>\n    <term name=\"month-07\" form=\"short\">Jul.</term>\n    <term name=\"month-08\" form=\"short\">Aug.</term>\n    <term name=\"month-09\" form=\"short\">Sep.</term>\n    <term name=\"month-10\" form=\"short\">Oct.</term>\n    <term name=\"month-11\" form=\"short\">Nov.</term>\n    <term name=\"month-12\" form=\"short\">Dec.</term>\n\n    <!-- SEASONS -->\n    <term name=\"season-01\">Spring</term>\n    <term name=\"season-02\">Summer</term>\n    <term name=\"season-03\">Autumn</term>\n    <term name=\"season-04\">Winter</term>\n  </terms>\n</locale>\n",
    "contentType": "Application/x-bibtex",
    "items": [
        {
            "10.1109/CCGrid.2015.175": {
                "id": "10.1109/CCGrid.2015.175",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Zeng",
                        "given": "Xuezhi"
                    },
                    {
                        "family": "Ranjan",
                        "given": "Rajiv"
                    },
                    {
                        "family": "Strazdins",
                        "given": "Peter"
                    },
                    {
                        "family": "Garg",
                        "given": "Saurabh Kumar"
                    },
                    {
                        "family": "Wang",
                        "given": "Lizhe"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2015,
                            5,
                            4
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2015,
                            5,
                            4
                        ]
                    ]
                },
                "abstract": "As we come to terms with various big data challenges, one vital issue remains largely untouched. That is service level agreement (SLA) management to deliver strong Quality of Service (QoS) guarantees for big data analytics applications (BDAA) sharing the same underlying infrastructure, for example, a public cloud platform. Although SLA and QoS are not new concepts as they originated much before the cloud computing and big data era, its importance is amplified and complexity is aggravated by the emergence of time-sensitive BDAAs such as social network-based stock recommendation and environmental monitoring. These applications require strong QoS guarantees and dependability from the underlying cloud computing platform to accommodate real-time responses while handling ever-increasing complexities and uncertainties. Hence, the over-reaching goal of this PhD research is to develop novel simulation, modeling and benchmarking tools and techniques that can aid researchers and practitioners in studying the impact of uncertainties (contention, failures, anomalies, etc.) on the final SLA and QoS of a cloud-hosted BDAA.",
                "call-number": "10.1109/CCGrid.2015.175",
                "collection-title": "CCGRID '15",
                "container-title": "Proceedings of the 15th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing",
                "DOI": "10.1109/CCGrid.2015.175",
                "event-place": "Shenzhen, China",
                "ISBN": "9781479980062",
                "keyword": "big data, cloud computing, service level agreement",
                "number-of-pages": "4",
                "page": "765–768",
                "publisher": "IEEE Press",
                "title": "Cross-layer SLA management for cloud-hosted big data analytics applications",
                "URL": "https://doi.org/10.1109/CCGrid.2015.175"
            }
        },
        {
            "10.1145/3175603.3175613": {
                "id": "10.1145/3175603.3175613",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Cao",
                        "given": "Mengmeng"
                    },
                    {
                        "family": "Guo",
                        "given": "Chaoyou"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            12,
                            29
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            12,
                            29
                        ]
                    ]
                },
                "abstract": "The application of big data techniques will contribute to the transforming and upgrading in shipbuilding industry and produce a profound influence in the development of intelligent ship. An overall framework of shipbuilding big data platform is established in this paper, which is based on the relationships among the intelligent ship, cloud computing and big data. Then key techniques of big data are discussed in four aspects, including data generation, data acquisition, data storage, and data analysis. Finally, an overview of the architecture of big data and relevant techniques are demonstrated.",
                "call-number": "10.1145/3175603.3175613",
                "collection-title": "ICRAI 2017",
                "container-title": "Proceedings of the 2017 International Conference on Robotics and Artificial Intelligence",
                "DOI": "10.1145/3175603.3175613",
                "event-place": "Shanghai, China",
                "ISBN": "9781450353588",
                "keyword": "data mining, data storage, data visualization, intelligent ship, big data, data acquisition",
                "number-of-pages": "5",
                "page": "61–65",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Key Technologies of Big Data and Its Development in Intelligent Ship",
                "URL": "https://doi.org/10.1145/3175603.3175613"
            }
        },
        {
            "10.1145/2609876.2609883": {
                "id": "10.1145/2609876.2609883",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Thomson",
                        "given": "Robert"
                    },
                    {
                        "family": "Lebiere",
                        "given": "Christian"
                    },
                    {
                        "family": "Bennati",
                        "given": "Stefano"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2014,
                            4,
                            1
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2014,
                            4,
                            1
                        ]
                    ]
                },
                "abstract": "In this paper, we describe a framework for processing big data that maximizing the efficiency of human data scientists by having them primarily operate over information that is best structured to human processing demands. We accomplish this through the use of cognitive models as an intermediary between machine learning algorithms and human data scientists. The ACT-R cognitive architecture is a computational implementation of a unified theory of cognition. ACT-R cognitive models can take weakly structured data and learn to filter information and make accurate inferences orders of magnitude faster than machine learning, and then present these well-structured inferences to human data scientists. The role for human data scientists is both oversight and feedback; one complementary piece of a hierarchy of cognitive and machine learning techniques that are computationally appropriate for their level of information complexity.",
                "call-number": "10.1145/2609876.2609883",
                "collection-title": "HCBDR '14",
                "container-title": "Proceedings of the 2014 Workshop on Human Centered Big Data Research",
                "DOI": "10.1145/2609876.2609883",
                "event-place": "Raleigh, NC, USA",
                "ISBN": "9781450329385",
                "keyword": "ACT-R, Big Data, Cognitive architectures, deep learning",
                "number-of-pages": "5",
                "page": "27–31",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Human, Model and Machine: A Complementary Approach to Big Data",
                "URL": "https://doi.org/10.1145/2609876.2609883"
            }
        },
        {
            "10.1145/3297663.3310302": {
                "id": "10.1145/3297663.3310302",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Talluri",
                        "given": "Sacheendra"
                    },
                    {
                        "family": "Łuszczak",
                        "given": "Alicja"
                    },
                    {
                        "family": "Abad",
                        "given": "Cristina L."
                    },
                    {
                        "family": "Iosup",
                        "given": "Alexandru"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            4,
                            4
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            4,
                            4
                        ]
                    ]
                },
                "abstract": "The proliferation of big data processing platforms has led to radically different system designs, such as MapReduce and the newer Spark. Understanding the workloads of such systems facilitates tuning and could foster new designs. However, whereas MapReduce workloads have been characterized extensively, relatively little public knowledge exists about the characteristics of Spark workloads in representative environments. To address this problem, in this work we collect and analyze a 6-month Spark workload from a major provider of big data processing services, Databricks. Our analysis focuses on a number of key features, such as the long-term trends of reads and modifications, the statistical properties of reads, and the popularity of clusters and of file formats. Overall, we present numerous findings that could form the basis of new systems studies and designs. Our quantitative evidence and its analysis suggest the existence of daily and weekly load imbalances, of heavy-tailed and bursty behaviour, of the relative rarity of modifications, and of proliferation of big data specific formats.",
                "call-number": "10.1145/3297663.3310302",
                "collection-title": "ICPE '19",
                "container-title": "Proceedings of the 2019 ACM/SPEC International Conference on Performance Engineering",
                "DOI": "10.1145/3297663.3310302",
                "event-place": "Mumbai, India",
                "ISBN": "9781450362399",
                "keyword": "popularity, interarrival time, apache spark, characterization, long-term trend, file formats, big data, cloud storage",
                "number-of-pages": "12",
                "page": "33–44",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Characterization of a Big Data Storage Workload in the Cloud",
                "URL": "https://doi.org/10.1145/3297663.3310302"
            }
        },
        {
            "10.1145/3530050": {
                "id": "10.1145/3530050",
                "type": "BOOK",
                "issued": {
                    "date-parts": [
                        [
                            2022
                        ]
                    ]
                },
                "abstract": "Today, new forms of distributed environments beyond Cloud Computing occur that offer new kinds of applications, but pose new challenges for data management. The recent efforts for serverless computing aim at simplifying the process of deploying code in the Cloud into production by hiding scaling, capacity planning and maintenance operations from the developer or operator. Other initiatives work on avoiding the communication to the Cloud by deploying and running environments for data processing near data sources in Internet-of-Things scenarios (e.g., fog and edge computing) for large-scale smart homes, companies and cities, and near the applications (e.g., Cloudlets for mobile applications and Offline First technologies for web applications).Research on distributed data management evolves addressing new challenges specific to these new environments. Properties of emergent distributed environments regarding capabilities of nodes, bandwidth for communication, battery lifetime of nodes, reliability of nodes and communication, and heterogeneity of configurations impact data management mechanisms and approaches, such as those for fault tolerance, replication, resource provisioning, buffer management, query processing and optimization, and transaction management. In addition, federated approaches and polystores spanning over several emergent distributed environments are also remaining research challenges based on the need for combining these different distributed environments into one distributed runtime environment for easy handling of Big Data in different models and globally optimizing data management tasks across these different environments.The goal of this workshop is to bring together academic researchers and industry practitioners to discuss the challenges and solutions, including new approaches, techniques and applications, that significantly would advance the state of the art of Big Data in emergent distributed environments.",
                "call-number": "10.1145/3530050",
                "container-title-short": "BiDEDE '22",
                "event-place": "Philadelphia, Pennsylvania",
                "genre": "proceeding",
                "ISBN": "9781450393461",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Proceedings of The International Workshop on Big Data in Emergent Distributed Environments"
            }
        },
        {
            "10.5555/2849516": {
                "id": "10.5555/2849516",
                "type": "REPORT",
                "author": [
                    {
                        "family": "Markus",
                        "given": "M. Lynne"
                    },
                    {
                        "family": "Topi",
                        "given": "Heikki"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2015
                        ]
                    ]
                },
                "abstract": "The report from the workshop, \"Big Data, Big Decisions for Government, Business and Society,\" makes a number of astute contributions. There is no need to replicate them in this foreword - they are in the report. What might be missed comes between the lines, where provocative points are made. Big Data means big opinions and big stakes. Those who think Big Data important want to be proven right, those who think Big Data a passing fad want Big Data to fade, and those who think Big Data will bring profound change hope for change. Big Data, like everything important, is political.",
                "call-number": "10.5555/2849516",
                "genre": "Technical Report",
                "publisher": "National Science Foundation",
                "publisher-place": "USA",
                "title": "Big Data, Big Decisions for Science, Society, and Business: Report on a Research Agenda Setting Workshop"
            }
        },
        {
            "10.1145/2094114.2094129": {
                "id": "10.1145/2094114.2094129",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Bizer",
                        "given": "Christian"
                    },
                    {
                        "family": "Boncz",
                        "given": "Peter"
                    },
                    {
                        "family": "Brodie",
                        "given": "Michael L."
                    },
                    {
                        "family": "Erling",
                        "given": "Orri"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2012,
                            1,
                            11
                        ]
                    ]
                },
                "abstract": "Twenty-five Semantic Web and Database researchers met at the 2011 STI Semantic Summit in Riga, Latvia July 6-8, 2011[1] to discuss the opportunities and challenges posed by Big Data for the Semantic Web, Semantic Technologies, and Database communities. The unanimous conclusion was that the greatest shared challenge was not only engineering Big Data, but also doing so meaningfully. The following are four expressions of that challenge from different perspectives.",
                "call-number": "10.1145/2094114.2094129",
                "container-title": "SIGMOD Rec.",
                "DOI": "10.1145/2094114.2094129",
                "ISSN": "0163-5808",
                "issue": "4",
                "number-of-pages": "5",
                "page": "56–60",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "December 2011",
                "title": "The meaningful use of big data: four perspectives -- four challenges",
                "URL": "https://doi.org/10.1145/2094114.2094129",
                "volume": "40"
            }
        },
        {
            "10.1109/CCGRID.2017.73": {
                "id": "10.1109/CCGRID.2017.73",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Wu",
                        "given": "Dongyao"
                    },
                    {
                        "family": "Sakr",
                        "given": "Sherif"
                    },
                    {
                        "family": "Zhu",
                        "given": "Liming"
                    },
                    {
                        "family": "Wu",
                        "given": "Huijun"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            5,
                            14
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            5,
                            14
                        ]
                    ]
                },
                "abstract": "Big data are increasingly collected and stored in a highly distributed infrastructures due to the development of sensor network, cloud computing, IoT and mobile computing among many other emerging technologies. In practice, the majority of existing big-data-processing frameworks (e.g., Hadoop and Spark) are designed based on the single-cluster setup with the assumptions of centralized management and homogeneous connectivity which makes them sub-optimal and sometimes infeasible to apply for scenarios that require implementing data analytics jobs on highly distributed data sets (across racks, data centers or multi-organizations). In order to tackle this challenge, we present HDM-MC, a multi-cluster big data processing framework which is designed to enable the capability of performing large scale data analytics across multi-clusters with minimum extra overhead due to additional scheduling requirements. In this paper, we present the architecture and realization of the system. In addition, we evaluate the performance of our framework in comparison to other state-of-art single cluster big data processing frameworks.",
                "call-number": "10.1109/CCGRID.2017.73",
                "collection-title": "CCGrid '17",
                "container-title": "Proceedings of the 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing",
                "DOI": "10.1109/CCGRID.2017.73",
                "event-place": "Madrid, Spain",
                "ISBN": "9781509066100",
                "number-of-pages": "10",
                "page": "218–227",
                "publisher": "IEEE Press",
                "title": "Towards Big Data Analytics across Multiple Clusters",
                "URL": "https://doi.org/10.1109/CCGRID.2017.73"
            }
        },
        {
            "10.14778/1453856.1453980": {
                "id": "10.14778/1453856.1453980",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Chiang",
                        "given": "Fei"
                    },
                    {
                        "family": "Miller",
                        "given": "Renée J."
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2008,
                            8,
                            1
                        ]
                    ]
                },
                "abstract": "Dirty data is a serious problem for businesses leading to incorrect decision making, inefficient daily operations, and ultimately wasting both time and money. Dirty data often arises when domain constraints and business rules, meant to preserve data consistency and accuracy, are enforced incompletely or not at all in application code.In this work, we propose a new data-driven tool that can be used within an organization's data quality management process to suggest possible rules, and to identify conformant and non-conformant records. Data quality rules are known to be contextual, so we focus on the discovery of context-dependent rules. Specifically, we search for conditional functional dependencies (CFDs), that is, functional dependencies that hold only over a portion of the data. The output of our tool is a set of functional dependencies together with the context in which they hold (for example, a rule that states for CS graduate courses, the course number and term functionally determines the room and instructor). Since the input to our tool will likely be a dirty database, we also search for CFDs that almost hold. We return these rules together with the non-conformant records (as these are potentially dirty records).We present effective algorithms for discovering CFDs and dirty values in a data instance. Our discovery algorithm searches for minimal CFDs among the data values and prunes redundant candidates. No universal objective measures of data quality or data quality rules are known. Hence, to avoid returning an unnecessarily large number of CFDs and only those that are most interesting, we evaluate a set of interest metrics and present comparative results using real datasets. We also present an experimental study showing the scalability of our techniques.",
                "call-number": "10.14778/1453856.1453980",
                "container-title": "Proc. VLDB Endow.",
                "DOI": "10.14778/1453856.1453980",
                "ISSN": "2150-8097",
                "issue": "1",
                "number-of-pages": "12",
                "page": "1166–1177",
                "publisher": "VLDB Endowment",
                "source": "August 2008",
                "title": "Discovering data quality rules",
                "URL": "https://doi.org/10.14778/1453856.1453980",
                "volume": "1"
            }
        },
        {
            "10.1145/2396636.2396678": {
                "id": "10.1145/2396636.2396678",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Leftheriotis",
                        "given": "Ioannis"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2012,
                            11,
                            11
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2012,
                            11,
                            11
                        ]
                    ]
                },
                "abstract": "Novel input devices such as tangibles, smartphones, multi-touch surfaces etc. have given impetus to new interaction techniques. In this PhD research, the main motivation is to study novel interaction techniques and designs that augment collaboration in a collocated environment. Furthermore, the main research aim is to take advantage of scalable interaction design techniques and tools that can be applied in a variety of devices so as to help users to work together on a problem with an abstract big data set, using visualizations on a collocated context.",
                "call-number": "10.1145/2396636.2396678",
                "collection-title": "ITS '12",
                "container-title": "Proceedings of the 2012 ACM international conference on Interactive tabletops and surfaces",
                "DOI": "10.1145/2396636.2396678",
                "event-place": "Cambridge, Massachusetts, USA",
                "ISBN": "9781450312097",
                "keyword": "big data, collaboration, user interfaces, interaction design, scalability, visual exploration",
                "number-of-pages": "6",
                "page": "271–276",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Scalable interaction design for collaborative visual exploration of big data",
                "URL": "https://doi.org/10.1145/2396636.2396678"
            }
        },
        {
            "10.1145/3299815.3314439": {
                "id": "10.1145/3299815.3314439",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Faker",
                        "given": "Osama"
                    },
                    {
                        "family": "Dogdu",
                        "given": "Erdogan"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            4,
                            18
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            4,
                            18
                        ]
                    ]
                },
                "abstract": "In this paper, Big Data and Deep Learning Techniques are integrated to improve the performance of intrusion detection systems. Three classifiers are used to classify network traffic datasets, and these are Deep Feed-Forward Neural Network (DNN) and two ensemble techniques, Random Forest and Gradient Boosting Tree (GBT). To select the most relevant attributes from the datasets, we use a homogeneity metric to evaluate features. Two recently published datasets UNSW NB15 and CICIDS2017 are used to evaluate the proposed method. 5-fold cross validation is used in this work to evaluate the machine learning models. We implemented the method using the distributed computing environment Apache Spark, integrated with Keras Deep Learning Library to implement the deep learning technique while the ensemble techniques are implemented using Apache Spark Machine Learning Library. The results show a high accuracy with DNN for binary and multiclass classification on UNSW NB15 dataset with accuracies at 99.16% for binary classification and 97.01% for multiclass classification. While GBT classifier achieved the best accuracy for binary classification with the CICIDS2017 dataset at 99.99%, for multiclass classification DNN has the highest accuracy with 99.56%.",
                "call-number": "10.1145/3299815.3314439",
                "collection-title": "ACM SE '19",
                "container-title": "Proceedings of the 2019 ACM Southeast Conference",
                "DOI": "10.1145/3299815.3314439",
                "event-place": "Kennesaw, GA, USA",
                "ISBN": "9781450362511",
                "keyword": "Intrusion detection system, machine learning, feature selection, artificial neural networks, deep learning, big data, ensemble techniques",
                "number-of-pages": "8",
                "page": "86–93",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Intrusion Detection Using Big Data and Deep Learning Techniques",
                "URL": "https://doi.org/10.1145/3299815.3314439"
            }
        },
        {
            "10.1109/CCGrid.2015.138": {
                "id": "10.1109/CCGrid.2015.138",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Rosà",
                        "given": "Andrea"
                    },
                    {
                        "family": "Chen",
                        "given": "Lydia Y."
                    },
                    {
                        "family": "Binder",
                        "given": "Walter"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2015,
                            5,
                            4
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2015,
                            5,
                            4
                        ]
                    ]
                },
                "abstract": "Big-data applications are being increasingly used in today's large-scale datacenters for a large variety of purposes, such as solving scientific problems, running enterprise services, and computing data-intensive tasks. Due to the growing scale of these systems and the complexity of running applications, jobs running in big-data systems experience unsuccessful terminations of different nature. While a large body of existing studies sheds light on failures occurred in large-scale datacenters, the current literature overlooks the characteristics and the performance impairment of a broader class of unsuccessful executions which can arise due to application failures, dependency violations, machine constraints, job kills, and task preemption. Nonetheless, deepening our understanding in this field is of paramount importance, as unsuccessful executions can lower user satisfaction, impair reliability, and lead to a high resource waste. In this paper, we describe the problem of unsuccessful executions in big-data systems, and highlight the critical importance of improving our knowledge on this subject. We review the existing literature on this field, discuss its limitations, and present our own contributions to the problem, along with our research plan for the future.",
                "call-number": "10.1109/CCGrid.2015.138",
                "collection-title": "CCGRID '15",
                "container-title": "Proceedings of the 15th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing",
                "DOI": "10.1109/CCGrid.2015.138",
                "event-place": "Shenzhen, China",
                "ISBN": "9781479980062",
                "number-of-pages": "4",
                "page": "741–744",
                "publisher": "IEEE Press",
                "title": "Understanding unsuccessful executions in big-data systems",
                "URL": "https://doi.org/10.1109/CCGrid.2015.138"
            }
        },
        {
            "10.1145/1651415.1651421": {
                "id": "10.1145/1651415.1651421",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Mehmood",
                        "given": "Kashif"
                    },
                    {
                        "family": "Si-Said Cherfi",
                        "given": "Samira"
                    },
                    {
                        "family": "Comyn-Wattiau",
                        "given": "Isabelle"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2009,
                            11,
                            6
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2009,
                            11,
                            6
                        ]
                    ]
                },
                "abstract": "Data quality has emerged as an important and challenging topic in recent years. This article addresses the conceptual model quality as it has been widely accepted that better conceptual models produce better information systems and thus implicitly improve the data quality. Conceptual Models are designed as part of the analysis phase and serve as a communicating mediator between the users and the development team. Consequently, their understandability is a real challenge to avoid the propagation of inaccurate interpretation of the user requirements to the underlying system design and implementation. In this paper, we propose an adaptive quality model. We illustrate its usefulness by describing how it can be used to model and evaluate the understandability of conceptual models. Our quality evaluation is enriched with corrective actions provided to the designer, leading to a guidance modeling process. A first validation based on a survey is proposed.",
                "call-number": "10.1145/1651415.1651421",
                "collection-title": "MoSE+DQS '09",
                "container-title": "Proceedings of the first international workshop on Model driven service engineering and data quality and security",
                "DOI": "10.1145/1651415.1651421",
                "event-place": "Hong Kong, China",
                "ISBN": "9781605588162",
                "keyword": "conceptual model, conceptual modeling quality, quality factor, conceptual model understandability, quality metrics",
                "number-of-pages": "4",
                "page": "29–32",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Data quality through model quality: a quality model for measuring and improving the understandability of conceptual models",
                "URL": "https://doi.org/10.1145/1651415.1651421"
            }
        },
        {
            "10.1145/3428363.3428369": {
                "id": "10.1145/3428363.3428369",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Meem",
                        "given": "Jannat Ara"
                    },
                    {
                        "family": "Ahmad",
                        "given": "Farzana Yasmin"
                    },
                    {
                        "family": "Adnan",
                        "given": "Muhammad Abdullah"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2020,
                            12,
                            22
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2020,
                            12,
                            22
                        ]
                    ]
                },
                "abstract": "Real-time big data analytics, which is the combination of real-time analytics and big data, works on processing large scale data as it arrives and strives to obtain insights from it without exceeding a limited time period. Massive amount of data is being generated every moment through web sites, social networks, scientific experiments, etc. which is stored in the cloud. When decision making requires an insight of the raw data in real-time (often by applying machine learning algorithms such as dimensionality reduction), these algorithms fail to analyze these incessantly flowing high volume data i.e dynamic big data. Our work proposes a variant of scalable principal component analysis (PCA) which is suited for real-time big data applications. We maintain a sliding window (representing incoming data in real-time) over the most recent data and project every incoming data into lower dimensional subspace. Our goal is to minimize the reconstruction error of the output from the input and keep updating the principal components depending on it. We have implemented our scalable algorithm on popular Spark framework for distributed platform and performed extensive experiments on datasets from a variety of real-time applications e.g. activity recognition, customer expenditure, etc. Furthermore, we have demonstrated that our algorithm can capture the changing distributions of real-life datasets, thus enabling real-time PCA. We have also compared the performances of distributed and non-distributed versions of our algorithm over a variety of window sizes and showed that our distributed algorithm is scalable and performs better when window size and target dimension increase.",
                "call-number": "10.1145/3428363.3428369",
                "collection-title": "7th NSysS 2020",
                "container-title": "7th International Conference on Networking, Systems and Security",
                "DOI": "10.1145/3428363.3428369",
                "event-place": "Dhaka, Bangladesh",
                "ISBN": "9781450389051",
                "keyword": "Real-time Processing, Distributed Algorithm., Big Data Analytics",
                "number-of-pages": "11",
                "page": "89–99",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Distributed Principal Component Analysis for Real-time Big Data Processing",
                "URL": "https://doi.org/10.1145/3428363.3428369"
            }
        },
        {
            "10.5555/2483628.2483630": {
                "id": "10.5555/2483628.2483630",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Abbass",
                        "given": "Hussein"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2011,
                            12,
                            1
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2011,
                            12,
                            1
                        ]
                    ]
                },
                "abstract": "Big data streams mark a new era in artificial intelligence and the data mining literature. Video and voice streams have grown rapidly in recent years. A single lab--based human--computer interaction experiment with one human subject collecting Cognitive, Physiological, and other data can easily generate a few terabytes of data in a single hour; growing rapidly to a Petabyte within a timeframe less than a month. In an article in the Wired Magazine, 2008, by Chris Anderson, he wrote \"the data deluge makes the scientific method obsolete\" He predicted that in the age of Petabyte and beyond, a meaningful correlation analysis is enough! Chris comment was provocative; but some started believing it. So was Chris right or wrong? Why? What can we do to face the outburst of big data? Do we have the data mining tools to manage these data? Where is the future of data mining heading? In this talk, I will discuss the above questions and demonstrate some answers using examples of my work and analysis.",
                "call-number": "10.5555/2483628.2483630",
                "collection-title": "AusDM '11",
                "container-title": "Proceedings of the Ninth Australasian Data Mining Conference - Volume 121",
                "event-place": "Ballarat, Australia",
                "ISBN": "9781921770029",
                "number-of-pages": "2",
                "page": "5–6",
                "publisher": "Australian Computer Society, Inc.",
                "publisher-place": "AUS",
                "title": "Mining big data streams: the fallacy of blind correlation and the importance of models"
            }
        },
        {
            "10.1145/3262388": {
                "id": "10.1145/3262388",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Fan",
                        "given": "Wei"
                    },
                    {
                        "family": "Bifet",
                        "given": "Albert"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2013,
                            4,
                            30
                        ]
                    ]
                },
                "call-number": "10.1145/3262388",
                "container-title": "SIGKDD Explor. Newsl.",
                "DOI": "10.1145/3262388",
                "ISSN": "1931-0145",
                "issue": "2",
                "page": "",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "December 2012",
                "title": "Session details: Mining big data",
                "URL": "https://doi.org/10.1145/3262388",
                "volume": "14"
            }
        },
        {
            "10.5555/3204979.3204985": {
                "id": "10.5555/3204979.3204985",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Mackey",
                        "given": "Andrew"
                    },
                    {
                        "family": "Cuevas",
                        "given": "Israel"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            5,
                            1
                        ]
                    ]
                },
                "abstract": "Data increasing at unabated rates will prove to be challenging for individuals trying to effectively leverage it. This paper describes an approach to automatically summarize text documents by utilizing statistical and information retrieval methodologies within big data frameworks that support the distributed parallel processing of data. A pedagogical approach to the incorporation of automatic summarization and big data topics within data mining, machine learning, or natural language processing courses is detailed.",
                "call-number": "10.5555/3204979.3204985",
                "container-title": "J. Comput. Sci. Coll.",
                "ISSN": "1937-4771",
                "issue": "5",
                "number-of-pages": "7",
                "page": "26–32",
                "publisher": "Consortium for Computing Sciences in Colleges",
                "publisher-place": "Evansville, IN, USA",
                "source": "May 2018",
                "title": "Automatic text summarization within big data frameworks",
                "volume": "33"
            }
        },
        {
            "10.1145/2839509.2850516": {
                "id": "10.1145/2839509.2850516",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Nagar",
                        "given": "Anurag"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            2,
                            17
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2016,
                            2,
                            17
                        ]
                    ]
                },
                "abstract": "This lightning talk will focus on our experience of teaching a graduate level Big Data course. Traditionally, such courses have relied on \"WordCount\" style problems, which involve computing the simple count of words in a corpus of documents using the distributed MapReduce framework. While this is certainly a good way of introducing the students to the BigData framework, more real world examples are needed to motivate students. Further, since a majority of courses require students to work on a large project as part of this course, it is essential that they have access to a diverse and interesting set of data. In our course, we experimented with various data sources, such as text from real-time, streaming news articles, twitter feeds, and property price data from various zip codes in a county. The students were involved in gathering the data, designing and implementing MapReduce style algorithms for distributed processing, and presenting their findings. The feedback was extremely positive and we would like to develop this approach further. In this talk, we will present some ideas on how to collect and analyze real world datasets that are suitable for Big Data analysis. We would also encourage further inputs from the audience about this topic.",
                "call-number": "10.1145/2839509.2850516",
                "collection-title": "SIGCSE '16",
                "container-title": "Proceedings of the 47th ACM Technical Symposium on Computing Science Education",
                "DOI": "10.1145/2839509.2850516",
                "event-place": "Memphis, Tennessee, USA",
                "ISBN": "9781450336857",
                "keyword": "distributed computing teaching, big data teaching, datasets for teaching big data",
                "number-of-pages": "1",
                "page": "496",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Enhancing Teaching of Big Data by Using Real World Datasets",
                "URL": "https://doi.org/10.1145/2839509.2850516"
            }
        },
        {
            "10.1145/3321454.3321474": {
                "id": "10.1145/3321454.3321474",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Yu",
                        "given": "Bangbo"
                    },
                    {
                        "family": "Zhao",
                        "given": "Haijun"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            2,
                            20
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            2,
                            20
                        ]
                    ]
                },
                "abstract": "As a new type of asset, the value of big data resources can only be realized in the transaction circulation. Establishing and improving the big data trading platform market system is a systematic project to transform data from resources into assets. Through the comparative analysis of several typical big data trading platform construction practices in China, this research finds some problems, such as unclear positioning of some platforms leading to overlapping functions, extensive data transaction, lack of unified data pricing methods, unclear data ownership. In addition, there is no difference between the data escrow transaction mode and the aggregate transaction mode, and the rights of the data supply parties cannot be guaranteed. And it also discusses how to promote the construction and improvement of China's big data trading market more systematically, normally and institutionally. Finally, it proposes to build local big data trading platforms according to local conditions, establish a data transaction system based on blockchain, establish a big data transaction pricing index system, and establish a big data standard system.",
                "call-number": "10.1145/3321454.3321474",
                "collection-title": "ICIIT '19",
                "container-title": "Proceedings of the 2019 4th International Conference on Intelligent Information Technology",
                "DOI": "10.1145/3321454.3321474",
                "event-place": "Da, Nang, Viet Nam",
                "ISBN": "9781450366335",
                "keyword": "regulatory construction, Data assets, big data trading platform",
                "number-of-pages": "6",
                "page": "107–112",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Research on the Construction of Big Data Trading Platform in China",
                "URL": "https://doi.org/10.1145/3321454.3321474"
            }
        },
        {
            "10.1145/2903150.2917755": {
                "id": "10.1145/2903150.2917755",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Girone",
                        "given": "Maria"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            5,
                            16
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2016,
                            5,
                            16
                        ]
                    ]
                },
                "abstract": "The Large Hadron Collider is one of the largest and most complicated pieces of scientific apparatus ever constructed. The detectors along the LHC ring see as many as 800 million proton-proton collisions per second. An event in 10 to the 11th power is new physics and there is a hierarchical series of steps to extract a tiny signal from an enormous background. High energy physics (HEP) has long been a driver in managing and processing enormous scientific datasets and the largest scale high throughput computing centers. HEP developed one of the first scientific computing grids that now regularly operates 500k processor cores and half of an exabyte of disk storage located on 5 continents including hundred of connected facilities. In this presentation I will discuss the techniques used to extract scientific discovery from a large and complicated dataset. While HEP has developed many tools and techniques for handling big datasets, there is an increasing desire within the field to make more effective use of additional industry developments. I will discuss some of the ongoing work to adopt industry techniques in big data analytics to improve the discovery potential of the LHC and the effectiveness of the scientists who work on it.",
                "call-number": "10.1145/2903150.2917755",
                "collection-title": "CF '16",
                "container-title": "Proceedings of the ACM International Conference on Computing Frontiers",
                "DOI": "10.1145/2903150.2917755",
                "event-place": "Como, Italy",
                "ISBN": "9781450341288",
                "page": "ii",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Big data analytics and the LHC",
                "URL": "https://doi.org/10.1145/2903150.2917755"
            }
        },
        {
            "10.1145/2896387.2900334": {
                "id": "10.1145/2896387.2900334",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Boubiche",
                        "given": "Djallel Eddine"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            3,
                            22
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2016,
                            3,
                            22
                        ]
                    ]
                },
                "abstract": "The emergence of heterogeneous wireless sensor networks in recent years has helped to address the limitations of conventional WSNs resources and has opened new research areas. When more than one type of node is integrated in a WSN, it is called heterogeneous. Placing heterogeneous nodes in a WSN is an effective way to increase the life of the network. While most of the existing civil and military applications of the heterogeneous WSNs are not materially different from their homogeneous counterparts, there are compelling reasons of incorporating the heterogeneity in the network. These reasons include: improving scalability of wireless sensor networks, reducing energy requirements without sacrificing performance, the equilibration of the cost and the network functionality, improving the security mechanisms using more complex protocols and supporting new broadband applications and big data.Recently, and because of the growth of the amount of data transmitted in the heterogeneous sensor networks, the term big data has emerged as a widely recognized trend. The term Big Data does not only concern the volume of data but also the high-speed transmission and the variety of information which are difficult to collect, store, and process using available technologies. Although the data generated by the individual sensors may not appear to be significant, all the data generated through the many sensors are capable of producing large volumes of data. The management of big data imposes additional constraints on WSNs. Effectively manage and secure big data gathering is a challenge in heterogeneous WSNs. Therefore, it represents an interesting research area. In this talk, I will address the emerging big data concept in heterogeneous wireless sensor networks and point out its main research issues.",
                "call-number": "10.1145/2896387.2900334",
                "collection-number": "3",
                "collection-title": "ICC '16",
                "container-title": "Proceedings of the International Conference on Internet of things and Cloud Computing",
                "DOI": "10.1145/2896387.2900334",
                "event-place": "Cambridge, United Kingdom",
                "ISBN": "9781450340632",
                "keyword": "Heterogeneous Sensor Networks, Big data, Secure and efficient data gathering",
                "number": "Article 3",
                "number-of-pages": "1",
                "page": "1",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Secure and Efficient Big Data Gathering in Heterogeneous Wireless Sensor Networks",
                "URL": "https://doi.org/10.1145/2896387.2900334"
            }
        },
        {
            "10.1109/CCGrid.2015.122": {
                "id": "10.1109/CCGrid.2015.122",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Fox",
                        "given": "Geoffrey C."
                    },
                    {
                        "family": "Qiu",
                        "given": "Judy"
                    },
                    {
                        "family": "Kamburugamuve",
                        "given": "Supun"
                    },
                    {
                        "family": "Jha",
                        "given": "Shantenu"
                    },
                    {
                        "family": "Luckow",
                        "given": "Andre"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2015,
                            5,
                            4
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2015,
                            5,
                            4
                        ]
                    ]
                },
                "abstract": "We review the High Performance Computing Enhanced Apache Big Data Stack HPC-ABDS and summarize the capabilities in 21 identified architecture layers. These cover Message and Data Protocols, Distributed Coordination, Security & Privacy, Monitoring, Infrastructure Management, DevOps, Interoperability, File Systems, Cluster & Resource management, Data Transport, File management, NoSQL, SQL (NewSQL), Extraction Tools, Object-relational mapping, In-memory caching and databases, Inter-process Communication, Batch Programming model and Runtime, Stream Processing, High-level Programming, Application Hosting and PaaS, Libraries and Applications, Workflow and Orchestration. We summarize status of these layers focusing on issues of importance for data analytics. We highlight areas where HPC and ABDS have good opportunities for integration.",
                "call-number": "10.1109/CCGrid.2015.122",
                "collection-title": "CCGRID '15",
                "container-title": "Proceedings of the 15th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing",
                "DOI": "10.1109/CCGrid.2015.122",
                "event-place": "Shenzhen, China",
                "ISBN": "9781479980062",
                "keyword": "HPC, apache big data stack",
                "number-of-pages": "10",
                "page": "1057–1066",
                "publisher": "IEEE Press",
                "title": "HPC-ABDS high performance computing enhanced apache big data stack",
                "URL": "https://doi.org/10.1109/CCGrid.2015.122"
            }
        },
        {
            "10.1145/3148453.3306250": {
                "id": "10.1145/3148453.3306250",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Wang",
                        "given": "Pengpeng"
                    },
                    {
                        "family": "Liu",
                        "given": "Hongri"
                    },
                    {
                        "family": "Wang",
                        "given": "Bailing"
                    },
                    {
                        "family": "Dong",
                        "given": "Kaikun"
                    },
                    {
                        "family": "Wang",
                        "given": "Lianhai"
                    },
                    {
                        "family": "Xu",
                        "given": "Shujiang"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            12,
                            7
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            12,
                            7
                        ]
                    ]
                },
                "abstract": "In the era of big data, the amount of information on dark network resources has exploded. Massive dark network data contain abundant information. To detect dark network resources and obtain dark network information, in-depth understanding of the dark network is a prerequisite. However, due to the high anonymity of dark network, it is usually difficult to be found by traditional search engines. Users need to register strictly and use specific tools to log in dynamically. In this paper, we explore the simulation of dark network scene in the big data environment. The Tor network is built on the openstack platform, which simulate the dark network scene. By using wireshark software to analyze network traffic, and using nmon tool to analyze network performance, the results show that the dark network scene can be simulated realistically.",
                "call-number": "10.1145/3148453.3306250",
                "collection-number": "11",
                "collection-title": "ICITEE '18",
                "container-title": "Proceedings of the International Conference on Information Technology and Electrical Engineering 2018",
                "DOI": "10.1145/3148453.3306250",
                "event-place": "Xiamen, Fujian, China",
                "ISBN": "9781450363525",
                "keyword": "Dark network, Tor, Big data, Scene simulation",
                "number": "Article 11",
                "number-of-pages": "6",
                "page": "1–6",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Simulation of Dark Network Scene Based on the Big Data Environment",
                "URL": "https://doi.org/10.1145/3148453.3306250"
            }
        },
        {
            "10.14778/2367502.2367562": {
                "id": "10.14778/2367502.2367562",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Dittrich",
                        "given": "Jens"
                    },
                    {
                        "family": "Quiané-Ruiz",
                        "given": "Jorge-Arnulfo"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2012,
                            8,
                            1
                        ]
                    ]
                },
                "abstract": "This tutorial is motivated by the clear need of many organizations, companies, and researchers to deal with big data volumes efficiently. Examples include web analytics applications, scientific applications, and social networks. A popular data processing engine for big data is Hadoop MapReduce. Early versions of Hadoop MapReduce suffered from severe performance problems. Today, this is becoming history. There are many techniques that can be used with Hadoop MapReduce jobs to boost performance by orders of magnitude. In this tutorial we teach such techniques. First, we will briefly familiarize the audience with Hadoop MapReduce and motivate its use for big data processing. Then, we will focus on different data management techniques, going from job optimization to physical data organization like data layouts and indexes. Throughout this tutorial, we will highlight the similarities and differences between Hadoop MapReduce and Parallel DBMS. Furthermore, we will point out unresolved research problems and open issues.",
                "call-number": "10.14778/2367502.2367562",
                "container-title": "Proc. VLDB Endow.",
                "DOI": "10.14778/2367502.2367562",
                "ISSN": "2150-8097",
                "issue": "12",
                "number-of-pages": "2",
                "page": "2014–2015",
                "publisher": "VLDB Endowment",
                "source": "August 2012",
                "title": "Efficient big data processing in Hadoop MapReduce",
                "URL": "https://doi.org/10.14778/2367502.2367562",
                "volume": "5"
            }
        },
        {
            "10.1145/2287076.2287078": {
                "id": "10.1145/2287076.2287078",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Budiu",
                        "given": "Mihai"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2012,
                            6,
                            18
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2012,
                            6,
                            18
                        ]
                    ]
                },
                "call-number": "10.1145/2287076.2287078",
                "collection-title": "HPDC '12",
                "container-title": "Proceedings of the 21st international symposium on High-Performance Parallel and Distributed Computing",
                "DOI": "10.1145/2287076.2287078",
                "event-place": "Delft, The Netherlands",
                "ISBN": "9781450308052",
                "keyword": "dryadlinq, linq, big data, kinect, parallel computing",
                "number-of-pages": "2",
                "page": "1–2",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Putting a \"big-data\" platform to good use: training kinect",
                "URL": "https://doi.org/10.1145/2287076.2287078"
            }
        },
        {
            "10.1145/3134472.3134516": {
                "id": "10.1145/3134472.3134516",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Nguyen",
                        "given": "Quang Vinh"
                    },
                    {
                        "family": "Engelke",
                        "given": "Ulrich"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            11,
                            27
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            11,
                            27
                        ]
                    ]
                },
                "call-number": "10.1145/3134472.3134516",
                "collection-number": "2",
                "collection-title": "SA '17",
                "container-title": "SIGGRAPH Asia 2017 Courses",
                "DOI": "10.1145/3134472.3134516",
                "event-place": "Bangkok, Thailand",
                "ISBN": "9781450354035",
                "number": "Article 2",
                "number-of-pages": "203",
                "page": "1–203",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Big data visual analytics: fundamentals, techniques, and tools",
                "URL": "https://doi.org/10.1145/3134472.3134516"
            }
        },
        {
            "10.1109/CHASE.2017.81": {
                "id": "10.1109/CHASE.2017.81",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Katsis",
                        "given": "Yannis"
                    },
                    {
                        "family": "Balac",
                        "given": "Natasha"
                    },
                    {
                        "family": "Chapman",
                        "given": "Derek"
                    },
                    {
                        "family": "Kapoor",
                        "given": "Madhur"
                    },
                    {
                        "family": "Block",
                        "given": "Jessica"
                    },
                    {
                        "family": "Griswold",
                        "given": "William G."
                    },
                    {
                        "family": "Huang",
                        "given": "Jeannie"
                    },
                    {
                        "family": "Koulouris",
                        "given": "Nikos"
                    },
                    {
                        "family": "Menarini",
                        "given": "Massimiliano"
                    },
                    {
                        "family": "Nandigam",
                        "given": "Viswanath"
                    },
                    {
                        "family": "Ngo",
                        "given": "Mandy"
                    },
                    {
                        "family": "Ong",
                        "given": "Kian Win"
                    },
                    {
                        "family": "Papakonstantinou",
                        "given": "Yannis"
                    },
                    {
                        "family": "Smith",
                        "given": "Besa"
                    },
                    {
                        "family": "Zarifis",
                        "given": "Konstantinos"
                    },
                    {
                        "family": "Woolf",
                        "given": "Steven"
                    },
                    {
                        "family": "Patrick",
                        "given": "Kevin"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            7,
                            17
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            7,
                            17
                        ]
                    ]
                },
                "abstract": "Public health researchers increasingly recognize that to advance their field they must grapple with the availability of increasingly large (i.e., thousands of variables) traditional population-level datasets (e.g., electronic medical records), while at the same time integrating additional large datasets (e.g., data on genomics, the microbiome, environmental exposures, socioeconomic factors, and health behaviors). Leveraging these multiple forms of data might well provide unique and unexpected discoveries about the determinants of health and wellbeing. However, we are in the very early stages of advancing the techniques required to understand and analyze big population-level data for public health research.To address this problem, this paper describes how we propose that big data can be efficiently used for public health discoveries. We show that data analytics techniques traditionally employed in public health studies are not up to the task of the data we now have in hand. Instead we present techniques adapted from big data visualization and analytics approaches used in other domains that can be used to answer important public health questions utilizing these existing and new datasets. Our findings are based on an exploratory big data case study carried out in San Diego County, California where we analyzed thousands of variables related to health to gain interesting insights on the determinants of several health outcomes, including life expectancy and anxiety disorders. These findings provide a promising early indication that public health research will benefit from the larger set of activities in contemporary big data research.",
                "call-number": "10.1109/CHASE.2017.81",
                "collection-title": "CHASE '17",
                "container-title": "Proceedings of the Second IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies",
                "DOI": "10.1109/CHASE.2017.81",
                "event-place": "Philadelphia, Pennsylvania",
                "ISBN": "9781509047215",
                "number-of-pages": "10",
                "page": "222–231",
                "publisher": "IEEE Press",
                "title": "Big data techniques for public health: a case study",
                "URL": "https://doi.org/10.1109/CHASE.2017.81"
            }
        },
        {
            "10.1145/2872518.2890583": {
                "id": "10.1145/2872518.2890583",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Maret",
                        "given": "Pierre"
                    },
                    {
                        "family": "Akerkar",
                        "given": "Rajendra"
                    },
                    {
                        "family": "Vercouter",
                        "given": "Laurent"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            4,
                            11
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2016,
                            4,
                            11
                        ]
                    ]
                },
                "abstract": "Web-based community is a self-defined web-based network of interactive communication organized around a shared interest or purpose. It provides the means of interactions among people in which they create, share, and exchange information and ideas in virtual space and networks. Working with big data often requires querying and reasoning that data to isolate information of interest and manipulate it in various ways. This editorial paper explores recent big data research topics -- stream querying and reasoning -- over data from web based communities. It combines aspects from some well-studied research domains, such as, social network analysis, graph databases, and data streams. We provide a brief synopsis of some research issues in supporting reasoning and querying tasks. This editorial also presents the WI&C-16 workshop's goal and programme.",
                "call-number": "10.1145/2872518.2890583",
                "collection-title": "WWW '16 Companion",
                "container-title": "Proceedings of the 25th International Conference Companion on World Wide Web",
                "DOI": "10.1145/2872518.2890583",
                "event-place": "Montréal, Québec, Canada",
                "ISBN": "9781450341448",
                "keyword": "web communities, datastream, querying, and reasoning",
                "number-of-pages": "3",
                "page": "945–947",
                "publisher": "International World Wide Web Conferences Steering Committee",
                "publisher-place": "Republic and Canton of Geneva, CHE",
                "title": "Web Communities in Big Data Era. Editorial",
                "URL": "https://doi.org/10.1145/2872518.2890583"
            }
        },
        {
            "10.1145/3252644": {
                "id": "10.1145/3252644",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Graus",
                        "given": "Mark"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            3,
                            13
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            3,
                            13
                        ]
                    ]
                },
                "call-number": "10.1145/3252644",
                "collection-title": "HUMANIZE '17",
                "container-title": "Proceedings of the 2017 ACM Workshop on Theory-Informed User Modeling for Tailoring and Personalizing Interfaces",
                "DOI": "10.1145/3252644",
                "event-place": "Limassol, Cyprus",
                "ISBN": "9781450349055",
                "page": "",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Session details: Exploiting Big Data",
                "URL": "https://doi.org/10.1145/3252644"
            }
        },
        {
            "10.1145/3289430.3289431": {
                "id": "10.1145/3289430.3289431",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Yang",
                        "given": "Xin"
                    },
                    {
                        "family": "Lu",
                        "given": "Xinsheng"
                    },
                    {
                        "family": "Geng",
                        "given": "Chao"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            10,
                            24
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            10,
                            24
                        ]
                    ]
                },
                "abstract": "With the rapid development of data and information, API system provided massive data downloading platform for various industries, which provided opportunities and challenges for urban microclimate research. This paper briefly introduced the research and application status of related data of API system and utilization of various data. Taking Beijing as an example, the paper extracted and analyzed meteorological station data in the city area, including temperature, relative humidity, solar radiation, wind speed and pressure. Interpolation analysis was carried out on the ArcGIS platform to get the climate distribution characteristics in a certain day in summer. Then the paper analyzed influence of transportation infrastructure distribution density on temperature, relative humidity, wind speed and pressure, and proposed the correlation between transportation infrastructure and temperature and relative humidity and no correlation with pressure. The climate environment in the city were complex and the big data platform provided new ways and new ideas for the related research.",
                "call-number": "10.1145/3289430.3289431",
                "collection-title": "BDIOT 2018",
                "container-title": "Proceedings of the 2018 2nd International Conference on Big Data and Internet of Things",
                "DOI": "10.1145/3289430.3289431",
                "event-place": "Beijing, China",
                "ISBN": "9781450365192",
                "keyword": "city microclimate environment, Beijing, API data system",
                "number-of-pages": "4",
                "page": "3–6",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Study on Urban Microclimate Based on API System on the Background of Big Data: A Case Study of Beijing",
                "URL": "https://doi.org/10.1145/3289430.3289431"
            }
        },
        {
            "10.1145/3127479.3129253": {
                "id": "10.1145/3127479.3129253",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Hwang",
                        "given": "Eunji"
                    },
                    {
                        "family": "Kim",
                        "given": "Hyungoo"
                    },
                    {
                        "family": "Nam",
                        "given": "Beomseok"
                    },
                    {
                        "family": "Choi",
                        "given": "Young-ri"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            9,
                            24
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            9,
                            24
                        ]
                    ]
                },
                "abstract": "In this work, we investigate techniques to improve the performance of big data analytics in virtualized clusters by effectively increasing the utilization of cached data and efficiently using scarce memory resources.",
                "call-number": "10.1145/3127479.3129253",
                "collection-title": "SoCC '17",
                "container-title": "Proceedings of the 2017 Symposium on Cloud Computing",
                "DOI": "10.1145/3127479.3129253",
                "event-place": "Santa Clara, California",
                "ISBN": "9781450350280",
                "keyword": "big data analytics frameworks, hadoop, cloud computing, memory locality",
                "number-of-pages": "1",
                "page": "657",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Exploring memory locality for big data analytics in virtualized clusters",
                "URL": "https://doi.org/10.1145/3127479.3129253"
            }
        },
        {
            "10.1145/3234698.3234723": {
                "id": "10.1145/3234698.3234723",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "El Bousty",
                        "given": "Hicham"
                    },
                    {
                        "family": "krit",
                        "given": "Salah-ddine"
                    },
                    {
                        "family": "Elasikri",
                        "given": "Mohamed"
                    },
                    {
                        "family": "Dani",
                        "given": "Hassan"
                    },
                    {
                        "family": "Karimi",
                        "given": "Khaoula"
                    },
                    {
                        "family": "Bendaoud",
                        "given": "Kaoutar"
                    },
                    {
                        "family": "Kabrane",
                        "given": "Mustapha"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            6,
                            19
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            6,
                            19
                        ]
                    ]
                },
                "abstract": "Business intelligence suppose retrieving value from data floating in the organization environment. It provides methods and tools for collecting, storing, formatting and analyzing data for the purpose of helping managers in decision-making. At the start, only data from enterprise internal activities were examined. Now and in this turbulent business environment, organizations should incorporate analysis of the huge amount of external data gathered from multifarious sources. It is argued that BI systems accuracy depends on quantity of data at their disposal, yet some storage and analysis methods are phased out and should be reviewed by academics and practitioners.This paper presents an overview of BI challenges in the context of Big Data (BD) and some available solutions provided, either by using Cloud Computing (CC) or improving Data Warehouse (DW) efficiency.",
                "call-number": "10.1145/3234698.3234723",
                "collection-number": "25",
                "collection-title": "ICEMIS '18",
                "container-title": "Proceedings of the Fourth International Conference on Engineering & MIS 2018",
                "DOI": "10.1145/3234698.3234723",
                "event-place": "Istanbul, Turkey",
                "ISBN": "9781450363921",
                "keyword": "Big Data, Business Intelligence, Data Warehouse, Cloud Computing",
                "number": "Article 25",
                "number-of-pages": "9",
                "page": "1–9",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Investigating Business Intelligence in the era of Big Data: concepts, benefits and challenges",
                "URL": "https://doi.org/10.1145/3234698.3234723"
            }
        },
        {
            "10.1145/3407666": {
                "id": "10.1145/3407666",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Lofstead",
                        "given": "Jay"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2020,
                            6,
                            23
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2020,
                            6,
                            23
                        ]
                    ]
                },
                "call-number": "10.1145/3407666",
                "collection-title": "HPDC '20",
                "container-title": "Proceedings of the 29th International Symposium on High-Performance Parallel and Distributed Computing",
                "DOI": "10.1145/3407666",
                "event-place": "Stockholm, Sweden",
                "ISBN": "9781450370523",
                "page": "",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Session details: Big Data Management",
                "URL": "https://doi.org/10.1145/3407666"
            }
        },
        {
            "10.1145/3253877": {
                "id": "10.1145/3253877",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Lempel",
                        "given": "Ronny"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            2,
                            8
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2016,
                            2,
                            8
                        ]
                    ]
                },
                "call-number": "10.1145/3253877",
                "collection-title": "WSDM '16",
                "container-title": "Proceedings of the Ninth ACM International Conference on Web Search and Data Mining",
                "DOI": "10.1145/3253877",
                "event-place": "San Francisco, California, USA",
                "ISBN": "9781450337168",
                "page": "",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Session details: Big Data Algorithms",
                "URL": "https://doi.org/10.1145/3253877"
            }
        },
        {
            "10.1145/2664591.2664619": {
                "id": "10.1145/2664591.2664619",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Ayankoya",
                        "given": "Kayode"
                    },
                    {
                        "family": "Calitz",
                        "given": "Andre"
                    },
                    {
                        "family": "Greyling",
                        "given": "Jean"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2014,
                            9,
                            29
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2014,
                            9,
                            29
                        ]
                    ]
                },
                "abstract": "Data recording and storage have evolved over the past decades from manual gathering of data by using simple writing materials to the automation of data collection. Data storage has evolved significantly in the past decades and today databases no longer suffice as the only medium for the storage and management of data. This is due to the emergence of the Big Data and Data Science concepts. Previous studies have indicated that the multiplication of processing power of computers and the availability of larger data storage at reduced cost are part of the catalysts for the volume and rate at which data is now made available and captured.In this paper, the concepts of Big Data, Data Science and Business Analytics are reviewed. This paper discusses datafication of different aspects of life as the fundamental concept behind the growth of Big Data and Data Science. A review of the characteristics and value of Big Data and Data Science suggests that these emerging concepts will bring a paradigm change to a number of areas. Big Data was described as the basis for Data Science and Business Analytics which are tools employed in Data Science. Because these fields are still developing, there are diverse opinions, especially on the definition of Data Science. This paper provides a revised definition of Data Science, based on the review of available literature and proposes a schematic representation of the concepts.",
                "call-number": "10.1145/2664591.2664619",
                "collection-title": "SAICSIT '14",
                "container-title": "Proceedings of the Southern African Institute for Computer Scientist and Information Technologists Annual Conference 2014 on SAICSIT 2014 Empowered by Technology",
                "DOI": "10.1145/2664591.2664619",
                "event-place": "Centurion, South Africa",
                "ISBN": "9781450332460",
                "keyword": "Business Analytics, Business Intelligence, Data Science, Datafication, Big Data",
                "number-of-pages": "7",
                "page": "192–198",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Intrinsic Relations between Data Science, Big Data, Business Analytics and Datafication",
                "URL": "https://doi.org/10.1145/2664591.2664619"
            }
        },
        {
            "10.1145/3440084.3441210": {
                "id": "10.1145/3440084.3441210",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Xiao",
                        "given": "Wei"
                    },
                    {
                        "family": "Tu",
                        "given": "Yaqing"
                    },
                    {
                        "family": "Wan",
                        "given": "Ping"
                    },
                    {
                        "family": "Li",
                        "given": "Ming"
                    },
                    {
                        "family": "Ma",
                        "given": "Jingheng"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2020,
                            11,
                            17
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2020,
                            11,
                            17
                        ]
                    ]
                },
                "abstract": "For working out the problem of ownership rights of data in the Big Data era, this paper proposes an establishment method of data ownership rights based on data classification. By summarizing characteristics of big data and analyzing current main views of the data ownership rights, this proposed method is following the principles of protecting data confidentiality and acknowledging the greatest contributors. First, according to the different involvement degree of participants in data generation processes, the data is divided into two categories: participatory data and non-participatory data. The participatory data is subdivided into equal participatory data and non-equal participatory data based on different contributions of participants. Since the non-participatory data generally involves the private and confidential information of the recorded parties, it is proposed that the ownership rights of this kind of data should belong to the recorded parties. Following the principle of acknowledging the greatest contributors, this paper proposes that the ownership rights of the equal participatory data belongs to all participants and that of non-equal participatory data belongs to the active participants.",
                "call-number": "10.1145/3440084.3441210",
                "collection-number": "34",
                "collection-title": "ISCSIC 2020",
                "container-title": "Proceedings of the 2020 4th International Symposium on Computer Science and Intelligent Control",
                "DOI": "10.1145/3440084.3441210",
                "event-place": "Newcastle upon Tyne, United Kingdom",
                "ISBN": "9781450388894",
                "keyword": "The Big Data Era, Non-Equal Participatory Data, Non-Participatory Data, Big Data Ownership Rights, Participatory Data, Equal Participatory Data",
                "number": "Article 34",
                "number-of-pages": "5",
                "page": "1–5",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Analysis of Data Ownership Rights in the Big Data Era",
                "URL": "https://doi.org/10.1145/3440084.3441210"
            }
        },
        {
            "10.1145/2737817.2737829": {
                "id": "10.1145/2737817.2737829",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Ellis",
                        "given": "Jason"
                    },
                    {
                        "family": "Fokoue",
                        "given": "Achille"
                    },
                    {
                        "family": "Hassanzadeh",
                        "given": "Oktie"
                    },
                    {
                        "family": "Kementsietsidis",
                        "given": "Anastasios"
                    },
                    {
                        "family": "Srinivas",
                        "given": "Kavitha"
                    },
                    {
                        "family": "Ward",
                        "given": "Michael J."
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2015,
                            2,
                            18
                        ]
                    ]
                },
                "abstract": "While much work has focused on efficient processing of Big Data, little work considers how to understand them. In this paper, we describe Helix, a system for guided exploration of Big Data. Helix provides a unified view of sources, ranging from spreadsheets and XML files with no schema, all the way to RDF graphs and relational data with well-defined schemas. Helix users explore these heterogeneous data sources through a combination of keyword searches and navigation of linked web pages that include information about the schemas, as well as data and semantic links within and across sources. At a technical level, the paper describes the research challenges involved in developing Helix, along with a set of real-world usage scenarios and the lessons learned.",
                "call-number": "10.1145/2737817.2737829",
                "container-title": "SIGMOD Rec.",
                "DOI": "10.1145/2737817.2737829",
                "ISSN": "0163-5808",
                "issue": "4",
                "number-of-pages": "12",
                "page": "43–54",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "December 2014",
                "title": "Exploring Big Data with Helix: Finding Needles in a Big Haystack",
                "URL": "https://doi.org/10.1145/2737817.2737829",
                "volume": "43"
            }
        },
        {
            "10.1145/2789168.2802150": {
                "id": "10.1145/2789168.2802150",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Aguiar",
                        "given": "Rui Luis"
                    },
                    {
                        "family": "Benhabiles",
                        "given": "Nora"
                    },
                    {
                        "family": "Pfeiffer",
                        "given": "Tobias"
                    },
                    {
                        "family": "Rodriguez",
                        "given": "Pablo"
                    },
                    {
                        "family": "Viswanathan",
                        "given": "Harish"
                    },
                    {
                        "family": "Wang",
                        "given": "Jia"
                    },
                    {
                        "family": "Zang",
                        "given": "Hui"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2015,
                            9,
                            7
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2015,
                            9,
                            7
                        ]
                    ]
                },
                "abstract": "The concepts of Big Data have became intertwined with those of the Internet of Things, creating mental pictures of a fully connected, all-encompassing, cyber-physical world, where each and every object will contribute with information to a \"fully aware\" society. Academic works are presenting this as the natural evolution for our current technologies. The panel looks at these promises from the hard perspective of reality: what is being done, how much it cost, what needs to be developed, and what can be expected in the near and mid-term.",
                "call-number": "10.1145/2789168.2802150",
                "collection-title": "MobiCom '15",
                "container-title": "Proceedings of the 21st Annual International Conference on Mobile Computing and Networking",
                "DOI": "10.1145/2789168.2802150",
                "event-place": "Paris, France",
                "ISBN": "9781450336192",
                "keyword": "IoT, industry-applications, big data",
                "number-of-pages": "2",
                "page": "550–551",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Big Data, IoT, .... Buzz Words for Academia or Reality for Industry?",
                "URL": "https://doi.org/10.1145/2789168.2802150"
            }
        },
        {
            "10.1145/3448748.3448787": {
                "id": "10.1145/3448748.3448787",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Guo",
                        "given": "Xuebing"
                    },
                    {
                        "family": "Yuan",
                        "given": "Kexin"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            1,
                            22
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2021,
                            1,
                            22
                        ]
                    ]
                },
                "abstract": "Small and medium-sized enterprises (SMEs) are an important part of the national economic system, which can promote employment, improve people's living standards, and promote economic development and technological innovation. In the new business environment based on big data and artificial intelligence, it is a key issue for SMEs to flexibly use big data services, design competitive marketing strategies, and achieve efficient market management. This paper will explore the impact of marketing environment changes on SMEs in the era of big data, and improve the marketing efficiency of SMEs by establishing relevant models based on big data analysis.",
                "call-number": "10.1145/3448748.3448787",
                "collection-title": "BIC 2021",
                "container-title": "Proceedings of the 2021 International Conference on Bioinformatics and Intelligent Computing",
                "DOI": "10.1145/3448748.3448787",
                "event-place": "Harbin, China",
                "ISBN": "9781450390002",
                "keyword": "Precision marketing, Marketing effectiveness, Big data analysis, Small and medium-sized enterprises (SMEs)",
                "number-of-pages": "6",
                "page": "244–249",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Promotion of Marketing Efficiency of SMEs Based on Big Data",
                "URL": "https://doi.org/10.1145/3448748.3448787"
            }
        },
        {
            "10.1145/3231830.3231840": {
                "id": "10.1145/3231830.3231840",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Kaloyanova",
                        "given": "Kalinka"
                    },
                    {
                        "family": "Hristov",
                        "given": "Tsvetomir"
                    },
                    {
                        "family": "Naydenova",
                        "given": "Ina"
                    },
                    {
                        "family": "Kovacheva",
                        "given": "Zlatinka"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            11,
                            13
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            11,
                            13
                        ]
                    ]
                },
                "abstract": "During the last decade the volume, the rate of accumulation and the diversity of data in general have been steadily increasing, which leads to the rapid development of Big data and technological enhancements associated with it. Many leading companies in the area took the challenge and provided different solutions to manage Big data. New hardware and software technologies are introduced for information management. The paper analyzes the main Big data technologies provided by Oracle as well their implementation in several specific cases.",
                "call-number": "10.1145/3231830.3231840",
                "collection-number": "10",
                "collection-title": "AWICT 2017",
                "container-title": "Proceedings of the Second International Conference on Advanced Wireless Information, Data, and Communication Technologies",
                "DOI": "10.1145/3231830.3231840",
                "event-place": "Paris, France",
                "ISBN": "9781450353106",
                "keyword": "Database, RDBMS, MapReduce, NoSQL",
                "number": "Article 10",
                "number-of-pages": "4",
                "page": "1–4",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Information Management Technologies for Big Data: A Case of Oracle",
                "URL": "https://doi.org/10.1145/3231830.3231840"
            }
        },
        {
            "10.1145/2567948.2577274": {
                "id": "10.1145/2567948.2577274",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Wang",
                        "given": "Lidan"
                    },
                    {
                        "family": "Lin",
                        "given": "Jimmy"
                    },
                    {
                        "family": "Metzler",
                        "given": "Donald"
                    },
                    {
                        "family": "Han",
                        "given": "Jiawei"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2014,
                            4,
                            7
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2014,
                            4,
                            7
                        ]
                    ]
                },
                "abstract": "Ranking in response to user queries is a central problem in information retrieval, data mining, and machine learning. In the era of \"Big data\", traditional effectiveness-centric ranking techniques tend to get more and more costly (requiring additional hardware and energy costs) to sustain reasonable ranking speed on large data. The mentality of combating big data by throwing in more hardware/machines will quickly become highly expensive since data is growing at an extremely fast rate oblivious to any cost concerns from us. \"Learning to efficiently rank\" offers a cost-effective solution to ranking on large data (e.g., billions of documents). That is, it addresses a critically important question -- whether it is possible to improve ranking effectiveness on large data without incurring (too much) additional cost?",
                "call-number": "10.1145/2567948.2577274",
                "collection-title": "WWW '14 Companion",
                "container-title": "Proceedings of the 23rd International Conference on World Wide Web",
                "DOI": "10.1145/2567948.2577274",
                "event-place": "Seoul, Korea",
                "ISBN": "9781450327459",
                "keyword": "effectiveness, efficiency",
                "number-of-pages": "2",
                "page": "209–210",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Learning to efficiently rank on big data",
                "URL": "https://doi.org/10.1145/2567948.2577274"
            }
        },
        {
            "10.1145/3331453.3361308": {
                "id": "10.1145/3331453.3361308",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Jing",
                        "given": "Furong"
                    },
                    {
                        "family": "Cao",
                        "given": "Yongsheng"
                    },
                    {
                        "family": "Fang",
                        "given": "Wei"
                    },
                    {
                        "family": "Chen",
                        "given": "Yanqing"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            10,
                            22
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            10,
                            22
                        ]
                    ]
                },
                "abstract": "Based on understanding the application of big data and the research status of crop germplasm resources, this paper proposes a system architecture that is suitable for crop germplasm resources big data. Among them, the overall architecture of germplasm resources is elaborated through six functional modules, including data source, data integration, data processing, data application, big data operation and maintenance platform, and data management and security. The logical functional architecture specification was formulated and the technical implementation and selection are defined. The technical implementation framework describes the technical implementation of germplasm resources big data, and jointly supports the construction and operation of germplasm resources big data. Finally, a verification system is established to verify the feasibility of the big data system framework for germplasm resources.",
                "call-number": "10.1145/3331453.3361308",
                "collection-number": "27",
                "collection-title": "CSAE 2019",
                "container-title": "Proceedings of the 3rd International Conference on Computer Science and Application Engineering",
                "DOI": "10.1145/3331453.3361308",
                "event-place": "Sanya, China",
                "ISBN": "9781450362948",
                "keyword": "Crop germplasm resources, Data analysis, Data management, Big data architecture",
                "number": "Article 27",
                "number-of-pages": "7",
                "page": "1–7",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Construction and Implementation of Big Data Framework for Crop Germplasm Resources",
                "URL": "https://doi.org/10.1145/3331453.3361308"
            }
        },
        {
            "10.1145/3178212.3178229": {
                "id": "10.1145/3178212.3178229",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Li",
                        "given": "Jiaxue"
                    },
                    {
                        "family": "Song",
                        "given": "Wei"
                    },
                    {
                        "family": "Fong",
                        "given": "Simon"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            12,
                            28
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            12,
                            28
                        ]
                    ]
                },
                "abstract": "This paper proposes a research on real-time analysis and visualization for big data of energy consumption. In this research, we access real-time energy consumption data from cloud storage by a Transmission Control Protocol/Internet Protocol (TCP/IP). In order to optimize K-Means clustering algorithm, we implement CUDA C programming to finish data-intensive calculation in the Graphic Processing Unit (GPU), which enhances the efficiency of analysis for big data of energy consumption. Meanwhile, to realize data visualization, we draw the data mining results in a multidimensional plane utilizing DirectX, which is a standard graphics API. We also render the original energy consumption data directly in the form of four-dimensional geometry with the plane together, so as to obtain more useful information intuitively.",
                "call-number": "10.1145/3178212.3178229",
                "collection-title": "ICSEB 2017",
                "container-title": "Proceedings of the 2017 International Conference on Software and e-Business",
                "DOI": "10.1145/3178212.3178229",
                "event-place": "Hong Kong, Hong Kong",
                "ISBN": "9781450354882",
                "keyword": "CUDA, DirectX, big data, K-Means, energy consumption",
                "number-of-pages": "4",
                "page": "13–16",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Real-time Analysis and Visualization for Big Data of Energy Consumption",
                "URL": "https://doi.org/10.1145/3178212.3178229"
            }
        },
        {
            "10.1145/3403951": {
                "id": "10.1145/3403951",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Khader",
                        "given": "Mariam"
                    },
                    {
                        "family": "Al-Naymat",
                        "given": "Ghazi"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2020,
                            9,
                            28
                        ]
                    ]
                },
                "abstract": "Clustering is used to extract hidden patterns and similar groups from data. Therefore, clustering as a method of unsupervised learning is a crucial technique for big data analysis owing to the massive number of unlabeled objects involved. Density-based algorithms have attracted research interest, because they help to better understand complex patterns in spatial datasets that contain information about data related to co-located objects. Big data clustering is a challenging task, because the volume of data increases exponentially. However, clustering using MapReduce can help answer this challenge. In this context, density-based algorithms in MapReduce have been largely investigated in the past decade to eliminate the problem of big data clustering. Despite the diversity of the algorithms proposed, the field lacks a structured review of the available algorithms and techniques for desirable partitioning, local clustering, and merging. This study formalizes the problem of density-based clustering using MapReduce, proposes a taxonomy to categorize the proposed algorithms, and provides a systematic and comprehensive comparison of these algorithms according to the partitioning technique, type of local clustering, merging technique, and exactness of their implementations. Finally, the study highlights outstanding challenges and opportunities to contribute to the field of density-based clustering using MapReduce.",
                "call-number": "10.1145/3403951",
                "collection-number": "93",
                "container-title": "ACM Comput. Surv.",
                "DOI": "10.1145/3403951",
                "ISSN": "0360-0300",
                "issue": "5",
                "keyword": "clustering, mapreduce framework, Big data, density clustering",
                "number": "Article 93",
                "number-of-pages": "38",
                "page": "1–38",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "September 2021",
                "title": "Density-based Algorithms for Big Data Clustering Using MapReduce Framework: A Comprehensive Study",
                "URL": "https://doi.org/10.1145/3403951",
                "volume": "53"
            }
        },
        {
            "10.1145/3299902.3311063": {
                "id": "10.1145/3299902.3311063",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Nurvitadhi",
                        "given": "Eriko"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            4,
                            4
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            4,
                            4
                        ]
                    ]
                },
                "abstract": "The continued rapid growth of data, along with advances in Artificial Intelligence (AI) to extract knowledge from such data, is reshaping the computing ecosystem landscape. With AI becoming an essential part of almost every end-user application, our current computing platforms are facing several challenges. The data-intensive nature of current AI models requires minimizing data movement. Furthermore, interactive intelligent datacenter-scale services require scalable and real-time solutions to provide a compelling user experience. Finally, algorithmic innovations in AI demand a flexible and programmable computing platform that can keep up with this rapidly changing field. We believe that these trends and their accompanying challenges present tremendous opportunities for FPGAs. FPGAs are a natural substrate to provide a programmable, near-data, real-time, and scalable platform for AI analytics. FPGAs are already embedded in several places where data flows throughout the computing ecosystem (e.g., \"smart\" network/storage, near image/audio sensors). Intel FPGAs are System-in-Package (SiP), scalable with 2.5D chiplets. They are also scalable at datacenter-scale as reconfigurable cloud, enabling real-time AI services. Using overlays, FPGAs can be programmed through software without needing long-running RTL synthesis. With further innovations, and leveraging their existing strengths, FPGAs can leap forward to realize their true potentials in AI analytics. In this talk, we first discuss the current trends in AI and big data. We then present trends in FPGA and opportunities for FPGAs in the era of AI and big data. Finally, we highlight selected research efforts to seize some of these opportunities: (1) 2.5D SiP integration of FPGA and AI chiplets to improve the performance and efficiency of AI workloads, and (2) AI overlay for FPGA to facilitate software-level programmability and compilation-speed.",
                "call-number": "10.1145/3299902.3311063",
                "collection-title": "ISPD '19",
                "container-title": "Proceedings of the 2019 International Symposium on Physical Design",
                "DOI": "10.1145/3299902.3311063",
                "event-place": "San Francisco, CA, USA",
                "ISBN": "9781450362535",
                "keyword": "big data, artificial intelligence, fpga",
                "number-of-pages": "1",
                "page": "35",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "FPGA-based Computing in the Era of AI and Big Data",
                "URL": "https://doi.org/10.1145/3299902.3311063"
            }
        },
        {
            "10.1145/3510003.3510619": {
                "id": "10.1145/3510003.3510619",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Gote",
                        "given": "Christoph"
                    },
                    {
                        "family": "Mavrodiev",
                        "given": "Pavlin"
                    },
                    {
                        "family": "Schweitzer",
                        "given": "Frank"
                    },
                    {
                        "family": "Scholtes",
                        "given": "Ingo"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2022,
                            5,
                            21
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2022,
                            5,
                            21
                        ]
                    ]
                },
                "abstract": "Massive data from software repositories and collaboration tools are widely used to study social aspects in software development. One question that several recent works have addressed is how a software project's size and structure influence team productivity, a question famously considered in Brooks' law. Recent studies using massive repository data suggest that developers in larger teams tend to be less productive than smaller teams. Despite using similar methods and data, other studies argue for a positive linear or even super-linear relationship between team size and productivity, thus contesting the view of software economics that software projects are diseconomies of scale.In our work, we study challenges that can explain the disagreement between recent studies of developer productivity in massive repository data. We further provide, to the best of our knowledge, the largest, curated corpus of GitHub projects tailored to investigate the influence of team size and collaboration patterns on individual and collective productivity. Our work contributes to the ongoing discussion on the choice of productivity metrics in the operationalisation of hypotheses about determinants of successful software projects. It further highlights general pitfalls in big data analysis and shows that the use of bigger data sets does not automatically lead to more reliable insights.",
                "call-number": "10.1145/3510003.3510619",
                "collection-title": "ICSE '22",
                "container-title": "Proceedings of the 44th International Conference on Software Engineering",
                "DOI": "10.1145/3510003.3510619",
                "event-place": "Pittsburgh, Pennsylvania",
                "ISBN": "9781450392211",
                "number-of-pages": "12",
                "page": "262–273",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Big data = big insights? operationalising brooks' law in a massive GitHub data set",
                "URL": "https://doi.org/10.1145/3510003.3510619"
            }
        },
        {
            "10.1145/2783258.2788563": {
                "id": "10.1145/2783258.2788563",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Dhurandhar",
                        "given": "Amit"
                    },
                    {
                        "family": "Graves",
                        "given": "Bruce"
                    },
                    {
                        "family": "Ravi",
                        "given": "Rajesh"
                    },
                    {
                        "family": "Maniachari",
                        "given": "Gopikrishanan"
                    },
                    {
                        "family": "Ettl",
                        "given": "Markus"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2015,
                            8,
                            10
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2015,
                            8,
                            10
                        ]
                    ]
                },
                "abstract": "An accredited biennial 2014 study by the Association of Certified Fraud Examiners claims that on average 5% of a company's revenue is lost because of unchecked fraud every year. The reason for such heavy losses are that it takes around 18 months for a fraud to be caught and audits catch only 3% of the actual fraud. This begs the need for better tools and processes to be able to quickly and cheaply identify potential malefactors. In this paper, we describe a robust tool to identify procurement related fraud/risk, though the general design and the analytical components could be adapted to detecting fraud in other domains. Besides analyzing standard transactional data, our solution analyzes multiple public and private data sources leading to wider coverage of fraud types than what generally exists in the marketplace. Moreover, our approach is more principled in the sense that the learning component, which is based on investigation feedback has formal guarantees. Though such a tool is ever evolving, a deployment of this tool over the past 12 months has found many interesting cases from compliance risk and fraud point of view across more than 150 countries and 65000+ vendors, increasing the number of true positives found by over 80\\% compared with other state-of-the-art tools that the domain experts were previously using.",
                "call-number": "10.1145/2783258.2788563",
                "collection-title": "KDD '15",
                "container-title": "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
                "DOI": "10.1145/2783258.2788563",
                "event-place": "Sydney, NSW, Australia",
                "ISBN": "9781450336642",
                "keyword": "online learning, fraud, social network, collusion, big data, risk, procurement",
                "number-of-pages": "10",
                "page": "1741–1750",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Big Data System for Analyzing Risky Procurement Entities",
                "URL": "https://doi.org/10.1145/2783258.2788563"
            }
        },
        {
            "10.1145/3132747.3132773": {
                "id": "10.1145/3132747.3132773",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Schlaipfer",
                        "given": "Matthias"
                    },
                    {
                        "family": "Rajan",
                        "given": "Kaushik"
                    },
                    {
                        "family": "Lal",
                        "given": "Akash"
                    },
                    {
                        "family": "Samak",
                        "given": "Malavika"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            10,
                            14
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            10,
                            14
                        ]
                    ]
                },
                "abstract": "Classical query optimization relies on a predefined set of rewrite rules to re-order and substitute SQL operators at a logical level. This paper proposes Blitz, a system that can synthesize efficient query-specific operators using automated program reasoning. Blitz uses static analysis to identify sub-queries as potential targets for optimization. For each sub-query, it constructs a template that defines a large space of possible operator implementations, all restricted to have linear time and space complexity. Blitz then employs program synthesis to instantiate the template and obtain a data-parallel operator implementation that is functionally equivalent to the original sub-query up to a bound on the input size.Program synthesis is an undecidable problem in general and often difficult to scale, even for bounded inputs. Blitz therefore uses a series of analyses to judiciously use program synthesis and incrementally construct complex operators.We integrated Blitz with existing big-data query languages by embedding the synthesized operators back into the query as User Defined Operators. We evaluated Blitz on several production queries from Microsoft running on two state-of-the-art query engines: SparkSQL as well as Scope, the big-data engine of Microsoft. Blitz produces correct optimizations despite the synthesis being bounded. The resulting queries have much more succinct query plans and demonstrate significant performance improvements on both big-data systems (1.3x --- 4.7x).",
                "call-number": "10.1145/3132747.3132773",
                "collection-title": "SOSP '17",
                "container-title": "Proceedings of the 26th Symposium on Operating Systems Principles",
                "DOI": "10.1145/3132747.3132773",
                "event-place": "Shanghai, China",
                "ISBN": "9781450350853",
                "keyword": "Program Synthesis, Query Optimization, User-Defined Operators",
                "number-of-pages": "16",
                "page": "631–646",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Optimizing Big-Data Queries Using Program Synthesis",
                "URL": "https://doi.org/10.1145/3132747.3132773"
            }
        },
        {
            "10.1145/3134271.3134296": {
                "id": "10.1145/3134271.3134296",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Peng",
                        "given": "Michael Yao-Ping"
                    },
                    {
                        "family": "Tuan",
                        "given": "Sheng-Hwa"
                    },
                    {
                        "family": "Liu",
                        "given": "Feng-Chi"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            7,
                            23
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            7,
                            23
                        ]
                    ]
                },
                "abstract": "The applications on business intelligence and big data analysis to extract useful information are getting more mature, but the development and operations in higher education institutions are still be lack. This study aims to explore how HEIs employ business intelligence to analysis and mining student learning and HEIs' operation data from database. The outcomes are benefit for universities to support the management of decision-making.",
                "call-number": "10.1145/3134271.3134296",
                "collection-title": "ICBIM 2017",
                "container-title": "Proceedings of the International Conference on Business and Information Management",
                "DOI": "10.1145/3134271.3134296",
                "event-place": "Bei Jing, China",
                "ISBN": "9781450352765",
                "keyword": "Business Intelligence, Institutional Research, Big data, Database",
                "number-of-pages": "5",
                "page": "121–125",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Establishment of Business Intelligence and Big Data Analysis for Higher Education",
                "URL": "https://doi.org/10.1145/3134271.3134296"
            }
        },
        {
            "10.1145/2448917.2448923": {
                "id": "10.1145/2448917.2448923",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "McNely",
                        "given": "Brian"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2012,
                            9,
                            1
                        ]
                    ]
                },
                "abstract": "In his 2005 book Ambient Findability, Peter Morville argued that what we find changes who we become. In 2012 and beyond---in an information environment of filter bubbles, contextual advertising, and friend-of-friend chains that push ordinary folks well beyond the Dunbar number---perhaps Morville is in need of some updating: what finds us changes who we become.",
                "call-number": "10.1145/2448917.2448923",
                "container-title": "Commun. Des. Q. Rev",
                "DOI": "10.1145/2448917.2448923",
                "issue": "1",
                "number-of-pages": "4",
                "page": "27–30",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "September 2012",
                "title": "Big data, situated people: humane approaches to communication design",
                "URL": "https://doi.org/10.1145/2448917.2448923",
                "volume": "1"
            }
        },
        {
            "10.1145/2024587.2024599": {
                "id": "10.1145/2024587.2024599",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Sneed",
                        "given": "Harry M."
                    },
                    {
                        "family": "Majnar",
                        "given": "Rudolf"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2011,
                            9,
                            4
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2011,
                            9,
                            4
                        ]
                    ]
                },
                "abstract": "Abstract: This industrial report stems from practical experience in assessing the quality of customer databases. The process it describes unites three automated audits, - an audit of the database schema, an audit of the database structure and an audit of the database content. The audit of the database schema checks for design smells and rule violations. The audit of the database structure measures the size, complexity and quality of the database model. The audit of the database content processes the data itself to uncover invalid data values, missing records and redundant records. The purpose of these audits is to assess the quality of the database and to determine whether a data reengineering or data clean-up project is required.",
                "call-number": "10.1145/2024587.2024599",
                "collection-title": "WoSQ '11",
                "container-title": "Proceedings of the 8th international workshop on Software quality",
                "DOI": "10.1145/2024587.2024599",
                "event-place": "Szeged, Hungary",
                "ISBN": "9781450308519",
                "keyword": "data auditing, data metrics, data content validation, data quality",
                "number-of-pages": "8",
                "page": "50–57",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "A process for assessing data quality",
                "URL": "https://doi.org/10.1145/2024587.2024599"
            }
        },
        {
            "10.1145/2949550.2949651": {
                "id": "10.1145/2949550.2949651",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Schmidt",
                        "given": "Drew"
                    },
                    {
                        "family": "Chen",
                        "given": "Wei-Chen"
                    },
                    {
                        "family": "Ostrouchov",
                        "given": "George"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            7,
                            17
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2016,
                            7,
                            17
                        ]
                    ]
                },
                "abstract": "Historically, large scale computing and interactivity have been at odds. This is a particularly sore spot for data analytics applications, which are typically interactive in nature. To help address this problem, we introduce a new client/server framework for the R language. This framework allows the R programmer to remotely control anywhere from one to thousands of batch servers running as cooperating instances of R. And all of this is done from the user's local R session. Additionally, no specialized software environment is needed; the framework is a series of R packages, available from CRAN. The communication between client and server(s) is handled by the well-known ZeroMQ library. To handle server side computations, we use our established pbdR packages for large scale distributed computing. These packages utilize HPC standards like MPI and ScaLAPACK to handle complex, tightly-coupled computations on large datasets. In this paper, we outline the new client/server architecture components, discuss the pros and cons to this approach, and provide several example workflows that bring interactivity to potentially terabyte size computations.",
                "call-number": "10.1145/2949550.2949651",
                "collection-number": "38",
                "collection-title": "XSEDE16",
                "container-title": "Proceedings of the XSEDE16 Conference on Diversity, Big Data, and Science at Scale",
                "DOI": "10.1145/2949550.2949651",
                "event-place": "Miami, USA",
                "ISBN": "9781450347556",
                "keyword": "Remote Computing, Analytics, Big Data, R",
                "number": "Article 38",
                "number-of-pages": "9",
                "page": "1–9",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Introducing a New Client/Server Framework for Big Data Analytics with the R Language",
                "URL": "https://doi.org/10.1145/2949550.2949651"
            }
        },
        {
            "10.1145/3011141.3011185": {
                "id": "10.1145/3011141.3011185",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Mountasser",
                        "given": "Imadeddine"
                    },
                    {
                        "family": "Ouhbi",
                        "given": "Brahim"
                    },
                    {
                        "family": "Frikh",
                        "given": "Bouchra"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            11,
                            28
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2016,
                            11,
                            28
                        ]
                    ]
                },
                "abstract": "Ontology matching is one of the essential methodologies to overcome heterogeneity issues. Multiple knowledge-based and information systems perform ontology matching strategies to find correspondences between several ontologies for the purpose of discovering valuable information across various domains. The design and implementation of matching systems raises several challenges, especially, the matching accuracy and the performance issues. Accordingly, adapting the system to the requirements of Big Data era brings additional perspectives and challenges. Furthermore, to provide on-the-fly matching and in-time processing, the system must handle matching accuracy, runtime complexity and performance issues as an entire matching strategy. To this end, this paper presents a new hybrid ontology matching approach that benefit on one hand from the opportunities offered by parallel platforms, and on the other hand from ontology matching techniques, while applying a resource-based decomposition to improve the performance of the system.",
                "call-number": "10.1145/3011141.3011185",
                "collection-title": "iiWAS '16",
                "container-title": "Proceedings of the 18th International Conference on Information Integration and Web-based Applications and Services",
                "DOI": "10.1145/3011141.3011185",
                "event-place": "Singapore, Singapore",
                "ISBN": "9781450348072",
                "keyword": "large-scale ontology matching, big data, parallel platforms, parallel matching, heterogeneity resolution",
                "number-of-pages": "6",
                "page": "282–287",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Hybrid large-scale ontology matching strategy on big data environment",
                "URL": "https://doi.org/10.1145/3011141.3011185"
            }
        },
        {
            "10.1145/3484377.3484388": {
                "id": "10.1145/3484377.3484388",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Kuo",
                        "given": "Jen-hwa"
                    },
                    {
                        "family": "B. J. Kuo",
                        "given": "Terry"
                    },
                    {
                        "family": "C. H. Yang",
                        "given": "Cheryl"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            8,
                            13
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2021,
                            8,
                            13
                        ]
                    ]
                },
                "abstract": "In order to explore the knowledge structure relevance of cloud computing, big data and healthcare research in the past ten years, this study identified the most important publications and the most influential papers, countries, research institutions, and the relationship between these scholars’ publications. While analyzing the development and influence of the main prominent keywords. In this study, bibliometrics and social network analysis techniques were used to investigate the knowledge pillars of cloud computing, big data, and healthcare literature. This research draws a knowledge network of research by analyzing the citations of 2,358 articles in the field of cloud computing, big data, and healthcare published in SCI and SSCI journals from July 2011 to June 2021. The mapping results help determine the research direction of cloud computing, big data and healthcare research, and provide valuable knowledge and information for researchers to obtain literature in this field.",
                "call-number": "10.1145/3484377.3484388",
                "collection-title": "ICIMH 2021",
                "container-title": "2021 the 3rd International Conference on Intelligent Medicine and Health",
                "DOI": "10.1145/3484377.3484388",
                "event-place": "Macau, China",
                "ISBN": "9781450385909",
                "keyword": "bibliometrics, cloud computing, healthcare, big data",
                "number-of-pages": "7",
                "page": "64–70",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Mapping the Intellectual Structure of Cloud Computing, Big Data and Healthcare Research",
                "URL": "https://doi.org/10.1145/3484377.3484388"
            }
        },
        {
            "10.1145/3230833.3232816": {
                "id": "10.1145/3230833.3232816",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Velthuis",
                        "given": "Paul J. E."
                    },
                    {
                        "family": "Schäfer",
                        "given": "Marcel"
                    },
                    {
                        "family": "Steinebach",
                        "given": "Martin"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            8,
                            27
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            8,
                            27
                        ]
                    ]
                },
                "abstract": "Companies analyse large amounts of data on clusters of machines, using big data analytic tools such as Apache Spark and Apache Flink to analyse the data. Big data analytic tools are mainly tested regarding speed and reliability. Efforts about Security and thus authentication are spent only at second glance. In such big data analytic tools, authentication is achieved with the help of the Kerberos protocol that is basically built as authentication on top of big data analytic tools. However, Kerberos is vulnerable to attacks, and it lacks providing high availability when users are all over the world. To improve the authentication, this work presents first an analysis of the authentication in Hadoop and the data analytic tools. Second, we propose a concept to deploy Transport Layer Security (TLS) not only for the security of data transportation but as well for authentication within the big data tools. This is done by establishing the connections using certificates with a short lifetime.The proof of concept is realized in Apache Spark, where Kerberos is replaced by the method proposed. We deploy new short living certificates for authentication that are less vulnerable to abuse. With our approach the requirements of the industry regarding multi-factor authentication and scalability are met.",
                "call-number": "10.1145/3230833.3232816",
                "collection-number": "40",
                "collection-title": "ARES 2018",
                "container-title": "Proceedings of the 13th International Conference on Availability, Reliability and Security",
                "DOI": "10.1145/3230833.3232816",
                "event-place": "Hamburg, Germany",
                "ISBN": "9781450364485",
                "keyword": "Kerberos, Multi Factor Authentication, Transport Layer Security (TLS), Apache Spark, Big Data Analytic Tools",
                "number": "Article 40",
                "number-of-pages": "7",
                "page": "1–7",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "New authentication concept using certificates for big data analytic tools",
                "URL": "https://doi.org/10.1145/3230833.3232816"
            }
        },
        {
            "10.1145/3127404.3130250": {
                "id": "10.1145/3127404.3130250",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Zhong",
                        "given": "Ning"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            9,
                            22
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            9,
                            22
                        ]
                    ]
                },
                "abstract": "In this talk, I demonstrate a Brain Informatics based systematic approach to an integrated understanding of macroscopic and microscopic level working principles of the brain by means of experimental, computational, and cognitive neuroscience studies, as well as utilizing advanced Web intelligence centric information technologies. I discuss research issues and challenges with respect to brain computing from three aspects of Brain Informatics studies that deserve closer attention: systematic investigations for complex brain science problems, new information technologies for supporting systematic brain science studies, and Brain Informatics studies based on Web intelligence (AI on the Internet) research needs. These three aspects offer different ways to study traditional cognitive science, neuroscience, brain and mental health, and artificial intelligence.",
                "call-number": "10.1145/3127404.3130250",
                "collection-title": "ChineseCSCW '17",
                "container-title": "Proceedings of the 12th Chinese Conference on Computer Supported Cooperative Work and Social Computing",
                "DOI": "10.1145/3127404.3130250",
                "event-place": "Chongqing, China",
                "ISBN": "9781450353526",
                "keyword": "brain big data, Brain informatics, wisdom service, cognitive science",
                "number-of-pages": "1",
                "page": "1",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Brain Big Data Based Wisdom Service: A Brain Informatics Based Systematic Approach",
                "URL": "https://doi.org/10.1145/3127404.3130250"
            }
        },
        {
            "10.14778/3007263.3007324": {
                "id": "10.14778/3007263.3007324",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Stoica",
                        "given": "Ion"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            9,
                            1
                        ]
                    ]
                },
                "abstract": "Almost six years ago we started the Spark project at UC Berkeley. Spark is a cluster computing engine that is optimized for in-memory processing, and unifies support for a variety of workloads, including batch, interactive querying, streaming, and iterative computations. Spark is now the most active big data project in the open source community, and is already being used by over one thousand organizations.One of the reasons behind Spark's success has been our early bet on the continuous increase in the memory capacity and the feasibility to fit many realistic workloads in the aggregate memory of typical production clusters. Today, we are witnessing new trends, such as Moore's law slowing down, and the emergence of a variety of computation and storage technologies, such as GPUs, FPGAs, and 3D Xpoint. In this talk, I'll discuss some of the lessons we learned in developing Spark as a unified computation platform, and the implications of today's hardware and software trends on the development of the next generation of big data processing systems.",
                "call-number": "10.14778/3007263.3007324",
                "container-title": "Proc. VLDB Endow.",
                "DOI": "10.14778/3007263.3007324",
                "ISSN": "2150-8097",
                "issue": "13",
                "number-of-pages": "1",
                "page": "1619",
                "publisher": "VLDB Endowment",
                "source": "September 2016",
                "title": "Trends and challenges in big data processing",
                "URL": "https://doi.org/10.14778/3007263.3007324",
                "volume": "9"
            }
        },
        {
            "10.1145/2611286.2611311": {
                "id": "10.1145/2611286.2611311",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Stojanovic",
                        "given": "Nenad"
                    },
                    {
                        "family": "Stojanovic",
                        "given": "Ljiljana"
                    },
                    {
                        "family": "Xu",
                        "given": "Yongchun"
                    },
                    {
                        "family": "Stajic",
                        "given": "Boban"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2014,
                            5,
                            26
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2014,
                            5,
                            26
                        ]
                    ]
                },
                "abstract": "The expansion of the mobile applications in various domains has opened the issue of the optimal usage of the limited resources (e.g. data storage capacity and processing power). This is especially important for the so-called data-intensive applications, which deal with huge amount of mobile data, like in the case of m-Health (i.e. wearable sensing). Many of these applications are oriented towards detecting of particular real-time situations, which brings them to the domain of Event Processing. However, we argue that real-time, big data driven applications require a novel infrastructure for distributed complex event processing that is only partially executed on the mobile devices. This tutorial paper presents a foundation for an efficient development of such mobile applications, by introducing a mobile-driven distributed CEP infrastructure. We present the technical details of the infrastructure and its initial implementation.",
                "call-number": "10.1145/2611286.2611311",
                "collection-title": "DEBS '14",
                "container-title": "Proceedings of the 8th ACM International Conference on Distributed Event-Based Systems",
                "DOI": "10.1145/2611286.2611311",
                "event-place": "Mumbai, India",
                "ISBN": "9781450327374",
                "keyword": "remote personal monitoring, mobile complex event processing, fast and big data, semantic technologies",
                "number-of-pages": "10",
                "page": "256–265",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Mobile CEP in real-time big data processing: challenges and opportunities",
                "URL": "https://doi.org/10.1145/2611286.2611311"
            }
        },
        {
            "10.1145/2647908.2655958": {
                "id": "10.1145/2647908.2655958",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Eichelberger",
                        "given": "Holger"
                    },
                    {
                        "family": "Schmid",
                        "given": "Klaus"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2014,
                            9,
                            15
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2014,
                            9,
                            15
                        ]
                    ]
                },
                "abstract": "The resource requirements of Big Data applications may vary dramatically over time, depending on changes in the context. If resources should not be defined for the maximum case, but available resources are mostly static, there is a need to adapt resource usage by modifying the processing behavior. The QualiMaster project researches such an approach for the analysis of systemic risks in the financial markets.",
                "call-number": "10.1145/2647908.2655958",
                "collection-title": "SPLC '14",
                "container-title": "Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2",
                "DOI": "10.1145/2647908.2655958",
                "event-place": "Florence, Italy",
                "ISBN": "9781450327398",
                "keyword": "financial markets, stream-processing, adaptive systems, resource adaptation, systematic-risks, QualiMaster",
                "number-of-pages": "2",
                "page": "10–11",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Resource-optimizing adaptation for big data applications",
                "URL": "https://doi.org/10.1145/2647908.2655958"
            }
        },
        {
            "10.1145/3443467.3443723": {
                "id": "10.1145/3443467.3443723",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Yuan",
                        "given": "Ming"
                    },
                    {
                        "family": "Yang",
                        "given": "Shulin"
                    },
                    {
                        "family": "Gu",
                        "given": "Mengdie"
                    },
                    {
                        "family": "Gu",
                        "given": "Huijie"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2020,
                            11,
                            6
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2020,
                            11,
                            6
                        ]
                    ]
                },
                "abstract": "In the context of big data, how to define user behavior models and provide personalized reading services for readers by mining large amounts of user data is a problem that the current reading platform needs to optimize urgently. First, we need to analyze the user behavior model to construct the research status and existing problems, in order to provide large data personalized services, targeted user behavior model based on reading platform construction strategies and construction methods, and designs a user logging library is utilized to extract the user interest in dominant and recessive demand ontology of personalized service plan. The user behavior model based on ontology can be technically seamlessly connected with the big data analysis platform, so as to provide real-time and accurate services, which can effectively deal with the challenges of \"knowledge trek\", \"information overload\" and \"emotional loss\" faced by the personalized service of the reading platform in the current big data environment.",
                "call-number": "10.1145/3443467.3443723",
                "collection-title": "EITCE 2020",
                "container-title": "Proceedings of the 2020 4th International Conference on Electronic Information Technology and Computer Engineering",
                "DOI": "10.1145/3443467.3443723",
                "event-place": "Xiamen, China",
                "ISBN": "9781450387811",
                "keyword": "Big Data, User Behavior Model, Linked Data, Personalized Service, Ontology",
                "number-of-pages": "5",
                "page": "21–25",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Analysis and Research on Book Recommendation Model Based on Big Data",
                "URL": "https://doi.org/10.1145/3443467.3443723"
            }
        },
        {
            "10.1145/3544109.3544166": {
                "id": "10.1145/3544109.3544166",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Yu",
                        "given": "Lejie"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2022,
                            4,
                            14
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2022,
                            4,
                            14
                        ]
                    ]
                },
                "abstract": "With the development of IT technology and the popularization of Internet applications, the scale and value of information carried and disseminated on the Internet are increasing. It has become one of the most important sources of information for all walks of life, institutions and individuals. The core of the network public opinion system is to use intelligent mining, machine learning and other computer technologies to identify, mine and analyze the public opinion information existing on the network, and solve the problem of the inability to realize the timely supervision of massive and dynamic Internet content by manual means. This paper takes network public opinion management as the research object, and constructs a public opinion management system model under the big data network environment. In the context of big data, this paper focuses on the work flow of the public opinion management system, and designs the system components and implementation methods in detail. The public opinion management system based on big data designed in this paper can meet the needs of users from many aspects. The system can efficiently mine and identify public opinion information from a large amount of data, thus providing a solution for government and enterprise public opinion monitoring.",
                "call-number": "10.1145/3544109.3544166",
                "collection-title": "IPEC '22",
                "container-title": "Proceedings of the 3rd Asia-Pacific Conference on Image Processing, Electronics and Computers",
                "DOI": "10.1145/3544109.3544166",
                "event-place": "Dalian, China",
                "ISBN": "9781450395786",
                "keyword": "Information resource management, Big data, Hadoop, Network public opinion",
                "number-of-pages": "5",
                "page": "309–313",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Design of Network Public Opinion Management System Based on Big Data",
                "URL": "https://doi.org/10.1145/3544109.3544166"
            }
        },
        {
            "10.1145/3319535.3363267": {
                "id": "10.1145/3319535.3363267",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Verma",
                        "given": "Rakesh M."
                    },
                    {
                        "family": "Zeng",
                        "given": "Victor"
                    },
                    {
                        "family": "Faridi",
                        "given": "Houtan"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            11,
                            6
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            11,
                            6
                        ]
                    ]
                },
                "abstract": "Techniques from data science are increasingly being applied by researchers to security challenges. However, challenges unique to the security domain necessitate painstaking care for the models to be valid and robust. In this paper, we explain key dimensions of data quality relevant for security, illustrate them with several popular datasets for phishing, intrusion detection and malware, indicate operational methods for assuring data quality and seek to inspire the audience to generate high quality datasets for security challenges.",
                "call-number": "10.1145/3319535.3363267",
                "collection-title": "CCS '19",
                "container-title": "Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security",
                "DOI": "10.1145/3319535.3363267",
                "event-place": "London, United Kingdom",
                "ISBN": "9781450367479",
                "keyword": "data quality, semiotics, data difficulty, data poisoning",
                "number-of-pages": "3",
                "page": "2605–2607",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Data Quality for Security Challenges: Case Studies of Phishing, Malware and Intrusion Detection Datasets",
                "URL": "https://doi.org/10.1145/3319535.3363267"
            }
        },
        {
            "10.1145/2693182.2693185": {
                "id": "10.1145/2693182.2693185",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Yesudas",
                        "given": "Michael"
                    },
                    {
                        "family": "S",
                        "given": "Girish Menon"
                    },
                    {
                        "family": "Nair",
                        "given": "Satheesh K"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2015,
                            2,
                            1
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2015,
                            2,
                            1
                        ]
                    ]
                },
                "abstract": "The inherent issues with handling large files and complex scenarios cause the data-driven approach [1] to be rarely used for performance tests. Volume and scalability testing of enterprise solutions typically requires custom-made test frameworks because of the complexity and uniqueness of data flow. The generation, transformation and transmission of large sets of data pose a unique challenge for testing a highly transactional back-end system like the IBM Sterling Order Management (OMS). This paper describes a test framework built on document-oriented NoSQL database, a design that helps validate the functionality and scalability of the solution simultaneously. This paper also describes various phases of planning, development, and testing of the OMS solution that was executed for a large retailer in Europe to test an extremely high online sales scenario. An out-of-the-box configuration of the OMS with the feature support for database sharding was used to drive scalability. The exercise was a success, and it is the world's largest IBM Sterling Order Management benchmark in terms of sales order volume, to date.",
                "call-number": "10.1145/2693182.2693185",
                "collection-title": "LT '15",
                "container-title": "Proceedings of the 4th International Workshop on Large-Scale Testing",
                "DOI": "10.1145/2693182.2693185",
                "event-place": "Austin, Texas, USA",
                "ISBN": "9781450333375",
                "keyword": "order management, big data, test automation tool, load testing, rapid prototyping, document oriented storage, test harness",
                "number-of-pages": "4",
                "page": "13–16",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "High-Volume Performance Test Framework using Big Data",
                "URL": "https://doi.org/10.1145/2693182.2693185"
            }
        },
        {
            "10.1145/3352740": {
                "id": "10.1145/3352740",
                "type": "BOOK",
                "issued": {
                    "date-parts": [
                        [
                            2019
                        ]
                    ]
                },
                "call-number": "10.1145/3352740",
                "container-title-short": "EBDIT 2019",
                "event-place": "Guilin, China",
                "genre": "proceeding",
                "ISBN": "9781450372053",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Proceedings of the 2019 3rd International Workshop on Education, Big Data and Information Technology"
            }
        },
        {
            "10.1145/3219788.3219809": {
                "id": "10.1145/3219788.3219809",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Ren",
                        "given": "Da Qi"
                    },
                    {
                        "family": "Xia",
                        "given": "Bing"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            5,
                            4
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            5,
                            4
                        ]
                    ]
                },
                "abstract": "Modern file system manages super large data sets to perform data intensive and cost-effective analytical processing. Performance of a file system relies on storages, software, workload characteristic and configurations. Complex techniques have to be used in analysis because the data are often hybrid mix of different formats and different structured datasets. Performance study helps to optimize these factors and improve the design of a file system to meet the requirements of a specific application. A promising approach is to allocate the diverse data of various applications on different file systems according to their individual properties, in order to support the best possible performance to every particular application. Some basis that simulate the characters and scenarios of each step of data execution procedures are addressed in this paper. Based on workload characteristic analysis, administrator can implement tuning methods in the large and high-dimensional configuration parameter settings provided by the platform accordingly. Preliminary results are provided by running standard benchmark TPCx-HS, TPCx-BB, TPC-H and HiBench K-means on Ext4 and Btrfs file systems, and the impactions of workload characteristics to the benchmark performance have been analysed.",
                "call-number": "10.1145/3219788.3219809",
                "collection-title": "ICCDE 2018",
                "container-title": "Proceedings of the 2018 International Conference on Computing and Data Engineering",
                "DOI": "10.1145/3219788.3219809",
                "event-place": "Shanghai, China",
                "ISBN": "9781450363938",
                "keyword": "Performance model, Benchmarks, Big data, File system",
                "number-of-pages": "5",
                "page": "22–26",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "File System Performance Tuning for Standard Big Data Benchmarks",
                "URL": "https://doi.org/10.1145/3219788.3219809"
            }
        },
        {
            "10.1109/CCGrid.2013.53": {
                "id": "10.1109/CCGrid.2013.53",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Ghiţ",
                        "given": "Bogdan"
                    },
                    {
                        "family": "Iosup",
                        "given": "Alexandru"
                    },
                    {
                        "family": "Epema",
                        "given": "Dick"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2013,
                            5,
                            13
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2013,
                            5,
                            13
                        ]
                    ]
                },
                "abstract": "Scalable by design to very large computing systems such as grids and clouds, MapReduce is currently a major big data processing paradigm. Nevertheless, existing performance models for MapReduce only comply with specific workloads that process a small fraction of the entire data set, thus failing to assess the capabilities of the MapReduce paradigm under heavy workloads that process exponentially increasing data volumes. The goal of my PhD is to build and analyze a scalable and dynamic big data processing system, including storage (distributed file system), execution engine (MapReduce), and query language (Pig). My contributions for the first two years of PhD research are the following: 1) the design and implementation of a resource management system part of a MapReduce-based processing system for deploying and resizing MapReduce clusters over multicluster systems, 2) the design and implementation of a benchmarking tool for the MapReduce processing system, and 3) the evaluation and modeling of MapReduce using workloads with very large data sets. Furthermore, based on the first two years research, we will optimize the MapReduce system to efficiently process terabytes of data.",
                "call-number": "10.1109/CCGrid.2013.53",
                "collection-title": "CCGRID '13",
                "container-title": "Proceedings of the 13th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing",
                "DOI": "10.1109/CCGrid.2013.53",
                "event-place": "Delft, Netherlands",
                "ISBN": "9780768549965",
                "number-of-pages": "4",
                "page": "83–86",
                "publisher": "IEEE Press",
                "title": "Towards an optimized big data processing system",
                "URL": "https://doi.org/10.1109/CCGrid.2013.53"
            }
        },
        {
            "10.1145/3319647.3325854": {
                "id": "10.1145/3319647.3325854",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Feder",
                        "given": "Oshrit"
                    },
                    {
                        "family": "Khazma",
                        "given": "Guy"
                    },
                    {
                        "family": "Lushi",
                        "given": "Gal"
                    },
                    {
                        "family": "Moatti",
                        "given": "Yosef"
                    },
                    {
                        "family": "Ta-Shma",
                        "given": "Paula"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            5,
                            22
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            5,
                            22
                        ]
                    ]
                },
                "abstract": "According to today's best practices, cloud compute and storage services should be deployed and managed independently. However, this generates a problem for big data analytics in the cloud: potentially huge datasets need to be shipped from the storage service to the compute service to analyse the data. To address this, minimizing the amount of data sent across the network is critical to achieve good performance and low cost. Data skipping is a technique which achieves this for SQL style analytics on structured data.Data skipping stores summary metadata for each object (or file) in a dataset. For each column in the object, the summary might include minimum and maximum values, a list or bloom filter of the appearing values, or other metadata which succinctly represents the data in that column. This metadata can then be indexed to support efficient retrieval, although since it can be orders of magnitude smaller than the data itself, this step may not be essential. This metadata can be used during query evaluation to skip over objects which have no relevant data. False positives for object relevance are acceptable since the query execution engine will ultimately filter the data at the row level. However false negatives must be avoided to ensure correctness of query results.Unlike fully inverted database indexes, data skipping indexes are much smaller than the data itself. This property is critical in the cloud, since otherwise a full index scan could increase the amount of data sent across the network instead of reducing it. In the context of database systems, data skipping is used as an additional technique which complements classical indexes. It is referred to as synopsis in DB2 [6] and zone maps in Oracle [9], where in both cases it is limited to min/max metadata. Data skipping and the associated topic of data layout, has been addressed in recent research papers [7, 8] and is also used in cloud analytics platforms [3,4]. Data skipping can also be built into specific data formats [1].We implemented data skipping support for Apache Spark SQL [2] without changing core Spark, in the form of an addon Scala library which can be added to the classpath and used in Spark applications. Our work applies to storage systems which implement the Hadoop FileSystem API, which includes various object storage systems as well as HDFS. Metadata is stored in Elasticsearch (ES) [5], and additional metadata stores can be supported in future using a pluggable API. Our approach prunes the list of candidate objects for any given Spark SQL query according to the associated data skipping metadata, stored and indexed in ES. Our technique applies to all Spark supported native formats e.g. JSON, CSV, Avro, Parquet, ORC, and can benefit from the latest optimizations built in to those formats in Spark. Unlike approaches which embed data skipping metadata inside the data format itself [1], which require reading at least part of the object, our approach avoids touching irrelevant objects altogether.",
                "call-number": "10.1145/3319647.3325854",
                "collection-title": "SYSTOR '19",
                "container-title": "Proceedings of the 12th ACM International Conference on Systems and Storage",
                "DOI": "10.1145/3319647.3325854",
                "event-place": "Haifa, Israel",
                "ISBN": "9781450367493",
                "number-of-pages": "1",
                "page": "193",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Big data skipping in the cloud",
                "URL": "https://doi.org/10.1145/3319647.3325854"
            }
        },
        {
            "10.1145/3365109.3368775": {
                "id": "10.1145/3365109.3368775",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Yan",
                        "given": "Tao"
                    },
                    {
                        "family": "Han",
                        "given": "Chongzhao"
                    },
                    {
                        "family": "Jia",
                        "given": "Yong"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            12,
                            2
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            12,
                            2
                        ]
                    ]
                },
                "abstract": "This research makes an attempt of using historical stock big data to predict near future trend of the stock. To fulfill this task, a novel approach based on fused intelligent computing is introduced and investigated. It is composited of four main parts, including data discretization, attribute reduction, classification and decision fusion. Further, one or two algorithms are adopted to realize specific function in each part, respectively. The given stock indexes are selected by the reduction algorithm of discernibility matrix, and the outputs of multiple classifiers are fused by the decision fusion algorithm. These processes and other ones are dedicated to enhancing the accuracy of stock trend prediction. To demonstrate the effectiveness of our approach, a variety of experimental simulations utilizing historical data of three stocks in NASDAQ are carried out, and the prediction accuracy of the proposed approach are compared as well. The experimental results prove that our approach could accomplish the prediction task with high accuracy.",
                "call-number": "10.1145/3365109.3368775",
                "collection-title": "BDCAT '19",
                "container-title": "Proceedings of the 6th IEEE/ACM International Conference on Big Data Computing, Applications and Technologies",
                "DOI": "10.1145/3365109.3368775",
                "event-place": "Auckland, New Zealand",
                "ISBN": "9781450370165",
                "keyword": "attribute reduction, decision fusion, stock trend prediction, fused intelligent computing, data discretization",
                "number-of-pages": "4",
                "page": "113–116",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "A Fused Intelligent Computing Approach Using Stock Big Data for Near Future Trend Prediction",
                "URL": "https://doi.org/10.1145/3365109.3368775"
            }
        },
        {
            "10.1145/1577840.1577845": {
                "id": "10.1145/1577840.1577845",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Klein",
                        "given": "A."
                    },
                    {
                        "family": "Lehner",
                        "given": "W."
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2009,
                            9,
                            1
                        ]
                    ]
                },
                "abstract": "Sensors in smart-item environments capture data about product conditions and usage to support business decisions as well as production automation processes. A challenging issue in this application area is the restricted quality of sensor data due to limited sensor precision and sensor failures. Moreover, data stream processing to meet resource constraints in streaming environments introduces additional noise and decreases the data quality. In order to avoid wrong business decisions due to dirty data, quality characteristics have to be captured, processed, and provided to the respective business task. However, the issue of how to efficiently provide applications with information about data quality is still an open research problem.In this article, we address this problem by presenting a flexible model for the propagation and processing of data quality. The comprehensive analysis of common data stream processing operators and their impact on data quality allows a fruitful data evaluation and diminishes incorrect business decisions. Further, we propose the data quality model control to adapt the data quality granularity to the data stream interestingness.",
                "call-number": "10.1145/1577840.1577845",
                "collection-number": "10",
                "container-title": "J. Data and Information Quality",
                "DOI": "10.1145/1577840.1577845",
                "ISSN": "1936-1955",
                "issue": "2",
                "keyword": "Data stream processing, smart items, data quality",
                "number": "Article 10",
                "number-of-pages": "28",
                "page": "1–28",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "September 2009",
                "title": "Representing Data Quality in Sensor Data Streaming Environments",
                "URL": "https://doi.org/10.1145/1577840.1577845",
                "volume": "1"
            }
        },
        {
            "10.1145/3486189.3490018": {
                "id": "10.1145/3486189.3490018",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Sevim",
                        "given": "Akil"
                    },
                    {
                        "family": "Mahin",
                        "given": "Mehnaz Tabassum"
                    },
                    {
                        "family": "Vu",
                        "given": "Tin"
                    },
                    {
                        "family": "Maxon",
                        "given": "Ian"
                    },
                    {
                        "family": "Eldawy",
                        "given": "Ahmed"
                    },
                    {
                        "family": "Carey",
                        "given": "Michael"
                    },
                    {
                        "family": "Tsotras",
                        "given": "Vassilis"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            11,
                            2
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2021,
                            11,
                            2
                        ]
                    ]
                },
                "abstract": "There is immense potential with spatial data, which is even more significant when combined with temporal or textual features, or both. However, it is expensive to store and analyze spatial data, and it is even more challenging with the combined features due to the additional optimization requirements. There are numerous successful solutions for big spatial data management, but they do not well support non-spatial operations. The options for the systems are even smaller for the open sources systems, and there are not a handful of options that provide good coverage of care about the spatial and non-spatial operations. This tutorial introduces Apache AsterixDB, a scalable open-source Big Data Management System, which supports standard vector spatial data types as well as non-spatial attributes, e.g., numerical, temporal, and textual. The participants will get hands-on experience on how Apache AsterixDB can efficiently process complex SQL++ queries that require multiple special handling by a team from its kitchen.",
                "call-number": "10.1145/3486189.3490018",
                "collection-number": "4",
                "collection-title": "SpatialAPI '21",
                "container-title": "Proceedings of the 3rd ACM SIGSPATIAL International Workshop on APIs and Libraries for Geospatial Data Science",
                "DOI": "10.1145/3486189.3490018",
                "event-place": "Beijing, China",
                "ISBN": "9781450391030",
                "keyword": "big data analytics, spatial analysis, geospatial big data",
                "number": "Article 4",
                "number-of-pages": "2",
                "page": "1–2",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "A brief introduction to geospatial big data analytics with apache AsterixDB",
                "URL": "https://doi.org/10.1145/3486189.3490018"
            }
        },
        {
            "10.1145/3053600.3053624": {
                "id": "10.1145/3053600.3053624",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Zibitsker",
                        "given": "Boris"
                    },
                    {
                        "family": "Lupersolsky",
                        "given": "Alex"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            4,
                            18
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            4,
                            18
                        ]
                    ]
                },
                "abstract": "Performance testing of Big Data applications is performed typically on small test environment with limited volume of data. The results of these types of tests do not take into consideration differences between test and production hardware and software environment and contention for resources with many applications in production environments. In this paper we will review application of the modeling for extending the results of performance testing, predicting how new application will perform in production environment. We will review how modeling results can be used to evaluate different options and justify decisions during design, development, implementation and performance management of the production environment.",
                "call-number": "10.1145/3053600.3053624",
                "collection-title": "ICPE '17 Companion",
                "container-title": "Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering Companion",
                "DOI": "10.1145/3053600.3053624",
                "event-place": "L&apos;Aquila, Italy",
                "ISBN": "9781450348997",
                "keyword": "benchmark, performance models, performance engineering, big data infrastructure, performance assurance, performance prediction., performance testing, big data applications",
                "number-of-pages": "5",
                "page": "119–123",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Modeling Expands Value of Performance Testing for Big Data Applications",
                "URL": "https://doi.org/10.1145/3053600.3053624"
            }
        },
        {
            "10.1145/2660517.2660529": {
                "id": "10.1145/2660517.2660529",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Bissiriou",
                        "given": "Cherif A. A."
                    },
                    {
                        "family": "Chaoui",
                        "given": "Habiba"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2014,
                            9,
                            4
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2014,
                            9,
                            4
                        ]
                    ]
                },
                "abstract": "High performance and scalability are two essentials requirements for data analytics systems as the amount of data being collected, stored and processed continue to grow rapidly. In this paper, we propose a new approach based on HadoopDB. Our main goal is to improve HadoopDB performance by adding some components. To achieve this, we incorporate a fast and space-efficient data placement structure in MapReduce-based Warehouse systems and another SQL-to-MapReduce translator. We also replace the initial Database implemented in HadoopDB with other column oriented Database. In addition we add security mechanism to protect MapReduce processing integrity.",
                "call-number": "10.1145/2660517.2660529",
                "collection-title": "SEM '14",
                "container-title": "Proceedings of the 10th International Conference on Semantic Systems",
                "DOI": "10.1145/2660517.2660529",
                "event-place": "Leipzig, Germany",
                "ISBN": "9781450329279",
                "keyword": "query execution, big data, Hadoop, MapReduce, optimization",
                "number-of-pages": "4",
                "page": "1–4",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Big data analysis and query optimization improve HadoopDB performance",
                "URL": "https://doi.org/10.1145/2660517.2660529"
            }
        },
        {
            "10.1007/s00778-018-0514-9": {
                "id": "10.1007/s00778-018-0514-9",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "To",
                        "given": "Quoc-Cuong"
                    },
                    {
                        "family": "Soto",
                        "given": "Juan"
                    },
                    {
                        "family": "Markl",
                        "given": "Volker"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            12,
                            1
                        ]
                    ]
                },
                "abstract": "The concept of state and its applications vary widely across big data processing systems. This is evident in both the research literature and existing systems, such as Apache Flink, Apache Heron, Apache Samza, Apache Spark, and Apache Storm. Given the pivotal role that state management plays, particularly, for iterative batch and stream processing, in this survey, we present examples of state as an enabler, discuss the alternative approaches used to handle and implement state, capture the many facets of state management, and highlight new research directions. Our aim is to provide insight into disparate state management techniques, motivate others to pursue research in this area, and draw attention to open problems.",
                "call-number": "10.1007/s00778-018-0514-9",
                "container-title": "The VLDB Journal",
                "DOI": "10.1007/s00778-018-0514-9",
                "ISSN": "1066-8888",
                "issue": "6",
                "keyword": "State management, Big data processing systems, Survey",
                "number-of-pages": "26",
                "page": "847–872",
                "publisher": "Springer-Verlag",
                "publisher-place": "Berlin, Heidelberg",
                "source": "December  2018",
                "title": "A survey of state management in big data processing systems",
                "URL": "https://doi.org/10.1007/s00778-018-0514-9",
                "volume": "27"
            }
        },
        {
            "10.1145/3349341.3349371": {
                "id": "10.1145/3349341.3349371",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Li",
                        "given": "Jiale"
                    },
                    {
                        "family": "Liao",
                        "given": "Shunbao"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            7,
                            12
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            7,
                            12
                        ]
                    ]
                },
                "abstract": "Agricultural meteorological disasters, including floods, droughts, dry hot winds, low temperature chills, typhoons, hail and continuous rain, can lead to significant reduction in agricultural output. Big data platform for early warning of agricultural meteorological disaster is the basis of business operation system for early warning of agricultural meteorological disasters, and the data quality is an important guarantee for success of the early warning. Quality control of big data for early warning of agricultural meteorological disaster involves names of data sets, metadata, data documents and content of data sets. The quality control for contents of data sets is divided into quality control of attribute data and that of spatial data, and quality control of spatial data is divided into quality control of vector data and that of raster data. Methods for data quality control are divided into fully automatic, semi-automatic and full manual control methods.",
                "call-number": "10.1145/3349341.3349371",
                "collection-title": "AICS 2019",
                "container-title": "Proceedings of the 2019 International Conference on Artificial Intelligence and Computer Science",
                "DOI": "10.1145/3349341.3349371",
                "event-place": "Wuhan, Hubei, China",
                "ISBN": "9781450371506",
                "keyword": "agro-meteorological disasters, big data, framework, early warning, quality control",
                "number-of-pages": "5",
                "page": "74–78",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Quality Control Framework of Big Data for Early Warning of Agricultural Meteorological Disasters",
                "URL": "https://doi.org/10.1145/3349341.3349371"
            }
        },
        {
            "10.1145/2851141.2851187": {
                "id": "10.1145/2851141.2851187",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Rabozzi",
                        "given": "Marco"
                    },
                    {
                        "family": "Mazzucchelli",
                        "given": "Matteo"
                    },
                    {
                        "family": "Cordone",
                        "given": "Roberto"
                    },
                    {
                        "family": "Fumarola",
                        "given": "Giovanni Matteo"
                    },
                    {
                        "family": "Santambrogio",
                        "given": "Marco D."
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            2,
                            27
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2016,
                            2,
                            27
                        ]
                    ]
                },
                "abstract": "Recent developments in Big Data frameworks are moving towards reservation based approaches as a mean to manage the increasingly complex mix of computations, whereas preemption techniques are employed to meet strict jobs deadlines. Within this work we propose and evaluate a new planning algorithm in the context of reservation based scheduling. Our approach is able to achieve high cluster utilization while minimizing the need for preemption that causes system overheads and planning mispredictions.",
                "call-number": "10.1145/2851141.2851187",
                "collection-number": "48",
                "collection-title": "PPoPP '16",
                "container-title": "Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming",
                "DOI": "10.1145/2851141.2851187",
                "event-place": "Barcelona, Spain",
                "ISBN": "9781450340922",
                "number": "Article 48",
                "number-of-pages": "2",
                "page": "1–2",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Preemption-aware planning on big-data systems",
                "URL": "https://doi.org/10.1145/2851141.2851187"
            }
        },
        {
            "10.1145/3016078.2851187": {
                "id": "10.1145/3016078.2851187",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Rabozzi",
                        "given": "Marco"
                    },
                    {
                        "family": "Mazzucchelli",
                        "given": "Matteo"
                    },
                    {
                        "family": "Cordone",
                        "given": "Roberto"
                    },
                    {
                        "family": "Fumarola",
                        "given": "Giovanni Matteo"
                    },
                    {
                        "family": "Santambrogio",
                        "given": "Marco D."
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            2,
                            27
                        ]
                    ]
                },
                "abstract": "Recent developments in Big Data frameworks are moving towards reservation based approaches as a mean to manage the increasingly complex mix of computations, whereas preemption techniques are employed to meet strict jobs deadlines. Within this work we propose and evaluate a new planning algorithm in the context of reservation based scheduling. Our approach is able to achieve high cluster utilization while minimizing the need for preemption that causes system overheads and planning mispredictions.",
                "call-number": "10.1145/3016078.2851187",
                "collection-number": "48",
                "container-title": "SIGPLAN Not.",
                "DOI": "10.1145/3016078.2851187",
                "ISSN": "0362-1340",
                "issue": "8",
                "number": "Article 48",
                "number-of-pages": "2",
                "page": "1–2",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "August 2016",
                "title": "Preemption-aware planning on big-data systems",
                "URL": "https://doi.org/10.1145/3016078.2851187",
                "volume": "51"
            }
        },
        {
            "10.1145/3148055": {
                "id": "10.1145/3148055",
                "type": "BOOK",
                "issued": {
                    "date-parts": [
                        [
                            2017
                        ]
                    ]
                },
                "abstract": "It is with great pleasure, on behalf of the program committee, that we welcome you to the fourth IEEE/ACM International Conference on Big Data Computing, Applications and Technologies (BDCAT2017), to be held in Austin, Texas, USA.BDCAT, as an international conference series, has established itself as the forum for researchers and practitioners in the varied spectrum of human endeavors where data is produced and consumed; from health and personalized medicine, to social services, to industrial processes, to security, to retail business and to high energy physics to identify elementary particle to unlock the secrets of the universe, among many other fields. Big data is an all-encompassing term combining the various characteristics of data that includes their volume, the velocity of data generation and consumption, the variety of data sources and formats, and the variability in their characteristics. The Big Data ecosystem encompasses theoretical and computational frameworks, the applications that deal with such data, and the emerging technologies that ultimately benefit the masses.Since its birth in 2014 in London, UK, BDCAT has become one of the premier forums for sharing of new advances in the methodology, the applications and technologies for big data. Today, BDCAT continues its success. This year we have received 93 submissions from 22 countries. Of these submissions, 27 were accepted for publication, leading to an acceptance rate of 29%.A monumental effort such as BDCAT2017 would not come to fruition without the vision and cooperative and dedicated work of many individuals across the globe. In particular we would like to thank the experts comprising the BDCAT Technical Program Committee for preserving the tradition of rigorous, high-quality peer reviews through their dedication, hard work, and discussions leading up to the selection of the papers. We acknowledge the relentless support that we received from our honorary leadership, Professors Rajkumar Buyya at the University of Melbourne, Australia, Geoffrey Fox at Indian University, USA and Beng Chin OOI of the National University of Singapore, Singapore. We also kindly acknowledge the dedicated support of the local organizing committee chairs: Professors Tim Cockerill of Texas Advanced Computing Center, Jerry Perez, Texas Tech University, Ravi Vadapalli, Texas Tech University and Zhangxi Lin, Texas Tech University, all of the USA. With efforts that spanned almost a year, we also acknowledge the efforts of the publicity chairs, professors David Chiu, University of Puget Sound, USA, Ningfang Mi, Northeastern University, USA, Gleb Radchenko, South Ural State University, Russia, Andrei Tchernykh, CICESE Research Center, Mexico, Yan Tang, Hohai University, China and Iman Elghandour, Alexandria University, Egypt.",
                "call-number": "10.1145/3148055",
                "container-title-short": "BDCAT '17",
                "event-place": "Austin, Texas, USA",
                "genre": "proceeding",
                "ISBN": "9781450355490",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Proceedings of the Fourth IEEE/ACM International Conference on Big Data Computing, Applications and Technologies"
            }
        },
        {
            "10.1145/3293614.3293626": {
                "id": "10.1145/3293614.3293626",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "García-Ojeda",
                        "given": "J. C."
                    },
                    {
                        "family": "Ortíz",
                        "given": "Meleny Luna"
                    },
                    {
                        "family": "García",
                        "given": "Rodolfo Sánchez"
                    },
                    {
                        "family": "Cáceres",
                        "given": "Javier Hernández"
                    },
                    {
                        "family": "Argoti",
                        "given": "Andrés"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            11,
                            12
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            11,
                            12
                        ]
                    ]
                },
                "abstract": "This paper deals with the implementation of a computer cluster based on Apache Hadoop technology to provide Big Data and Data Analytics services. The cluster is an innovative proposal of the TIC consulting office (CTIC) of the Universidad Santo Tomás - Bucaramanga, which aims at satisfying the needs regarding Big Data and Data Analytics of the Bucaramanga Metropolitan Area. The cluster implementation includes the description of the architecture and configuration of the equipment involved as well as the series of experiments carried out to assess the cluster's performance in terms of computing time, energy consumption, cost, and carbon footprint.",
                "call-number": "10.1145/3293614.3293626",
                "collection-number": "45",
                "collection-title": "EATIS '18",
                "container-title": "Proceedings of the Euro American Conference on Telematics and Information Systems",
                "DOI": "10.1145/3293614.3293626",
                "event-place": "Fortaleza, Brazil",
                "ISBN": "9781450365727",
                "keyword": "Big Data, Apache Hadoop, IT services, Data Analytics",
                "number": "Article 45",
                "number-of-pages": "8",
                "page": "1–8",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "A Computer Cluster for Big Data and Data Analytics Management: Design, Implementation, and Assessment",
                "URL": "https://doi.org/10.1145/3293614.3293626"
            }
        },
        {
            "10.1145/3006386.3006391": {
                "id": "10.1145/3006386.3006391",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Sorokine",
                        "given": "Alexandre"
                    },
                    {
                        "family": "Karthik",
                        "given": "Rajasekar"
                    },
                    {
                        "family": "King",
                        "given": "Anthony"
                    },
                    {
                        "family": "Budhendra",
                        "given": "Bhaduri"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            10,
                            31
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2016,
                            10,
                            31
                        ]
                    ]
                },
                "abstract": "Big Data has already proven itself as a valuable tool that lets geographers and urban researchers utilize large data resources to generate new insights. However, wider adoption of Big Data techniques in these areas is impeded by a number of difficulties in both knowledge discovery and data and science production. Typically users face such problems as disparate and scattered data, data management, spatial searching, insufficient computational capacity for data-driven analysis and modelling, and the lack of tools to quickly visualize and summarize large data and analysis results. Here we propose an architecture for an Urban Information System (UrbIS) that mitigates these problems by utilizing the Big Data as a Service (BDaaS) concept. With technological roots in High-performance Computing (HPC), BDaaS is based on the idea of outsourcing computations to different computing paradigms, scalable to super-computers. UrbIS aims to incorporate federated metadata search, integrated modeling and analysis, and geovisualization into a single seamless workflow. The system is under active development and is built around various emerging technologies that include hybrid and NoSQL databases, massively parallel systems, GPU computing, and WebGL-based geographic visualization. UrbIS is designed to facilitate the use of Big Data across multiple cities to better understand how urban areas impact the environment and how climate change and other environmental change impact urban areas.",
                "call-number": "10.1145/3006386.3006391",
                "collection-title": "BigSpatial '16",
                "container-title": "Proceedings of the 5th ACM SIGSPATIAL International Workshop on Analytics for Big Geospatial Data",
                "DOI": "10.1145/3006386.3006391",
                "event-place": "Burlingame, California",
                "ISBN": "9781450345811",
                "keyword": "environmental change impact, big data as a service, urban informatics, high-performance geocomputing",
                "number-of-pages": "8",
                "page": "34–41",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Big data as a service from an urban information system",
                "URL": "https://doi.org/10.1145/3006386.3006391"
            }
        },
        {
            "10.1145/2744700.2744705": {
                "id": "10.1145/2744700.2744705",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Ravada",
                        "given": "Siva"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2015,
                            3,
                            10
                        ]
                    ]
                },
                "abstract": "Big data is present everywhere and it can help organization in any industry in many different ways. One of the main sources of this data is the increased digitization in every aspect of life. In general terms, digitization is the process of making something digital. That is, use computer technology in the middle of an activity that used to be done without computers. For example, people used to shoot pictures on film, but now most of the pictures are digital. People used to pay tolls with cash, and now it is digital. People used to drive cars with rack-and-pinion steering, now they are all drive-by-wire, fully digital rolling computers.",
                "call-number": "10.1145/2744700.2744705",
                "container-title": "SIGSPATIAL Special",
                "DOI": "10.1145/2744700.2744705",
                "issue": "2",
                "number-of-pages": "8",
                "page": "34–41",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "July 2014",
                "title": "Big data spatial analytics for enterprise applications",
                "URL": "https://doi.org/10.1145/2744700.2744705",
                "volume": "6"
            }
        },
        {
            "10.1145/3366030.3366044": {
                "id": "10.1145/3366030.3366044",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Cuzzocrea",
                        "given": "Alfredo"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            12,
                            2
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            12,
                            2
                        ]
                    ]
                },
                "abstract": "This paper focuses on big data management and analytics in intelligent smart environments, with particular regards to intelligent transportation and logistics systems, and provides relevant research directions that may represent a milestone for future years.",
                "call-number": "10.1145/3366030.3366044",
                "collection-title": "iiWAS2019",
                "container-title": "Proceedings of the 21st International Conference on Information Integration and Web-based Applications & Services",
                "DOI": "10.1145/3366030.3366044",
                "event-place": "Munich, Germany",
                "ISBN": "9781450371797",
                "keyword": "Big data analytics, Big data management, Intelligent smart environments",
                "number-of-pages": "3",
                "page": "5–7",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Big Data Management and Analytics in Intelligent Smart Environments: State-of-the-Art Analysis and Future Research Directions",
                "URL": "https://doi.org/10.1145/3366030.3366044"
            }
        },
        {
            "10.1145/3219819.3219941": {
                "id": "10.1145/3219819.3219941",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Teh",
                        "given": "Yee Whye"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            7,
                            19
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            7,
                            19
                        ]
                    ]
                },
                "abstract": "Much recent progress in machine learning have been fueled by the explosive growth in the amount and diversity of data available, and the computational resources needed to crunch through the data. This begs the question of whether machine learning systems necessarily need large amounts of data to solve a task well. An exciting recent development, under the banners of meta-learning, lifelong learning, learning to learn, multitask learning etc., has been the observation that often there is heterogeneity within the data sets at hand, and in fact a large data set can be viewed more productively as many smaller data sets, each pertaining to a different task. For example, in recommender systems each user can be said to be a different task with a small associated data set, and in AI one holy grail is how to develop systems that can learn to solve new tasks quickly from small amounts of data. In such settings, the problem is then how to \"learn to learn quickly\", by making use of similarities among tasks. One perspective for how this is achievable is that exposure to lots of previous tasks allows the system to learn a rich prior knowledge about the world in which tasks are sampled from, and it is with rich world knowledge that the system is able to solve new tasks quickly. This is a very active, vibrant and diverse area of research, with many different approaches proposed recently. In this talk I will describe a view of this problem from probabilistic and deep learning perspectives, and describe a number of efforts in this direction that I have recently been involved in.",
                "call-number": "10.1145/3219819.3219941",
                "collection-title": "KDD '18",
                "container-title": "Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining",
                "DOI": "10.1145/3219819.3219941",
                "event-place": "London, United Kingdom",
                "ISBN": "9781450355520",
                "keyword": "multitask learning, learning to learn, learn to learn, big data, machine learning, meta-learning, small data, lifelong learning",
                "number-of-pages": "1",
                "page": "3",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "On Big Data Learning for Small Data Problems",
                "URL": "https://doi.org/10.1145/3219819.3219941"
            }
        },
        {
            "10.1145/2429376.2429382": {
                "id": "10.1145/2429376.2429382",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Acar",
                        "given": "Umut A."
                    },
                    {
                        "family": "Chen",
                        "given": "Yan"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2013,
                            1,
                            22
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2013,
                            1,
                            22
                        ]
                    ]
                },
                "abstract": "Many big data computations involve processing data that changes incrementally or dynamically over time. Using existing techniques, such computations quickly become impractical. For example, computing the frequency of words in the first ten thousand paragraphs of a publicly available Wikipedia data set in a streaming fashion using MapReduce can take as much as a full day. In this paper, we propose an approach based on self-adjusting computation that can dramatically improve the efficiency of such computations. As an example, we can perform the aforementioned streaming computation in just a couple of minutes.",
                "call-number": "10.1145/2429376.2429382",
                "collection-title": "DDFP '13",
                "container-title": "Proceedings of the 2013 workshop on Data driven functional programming",
                "DOI": "10.1145/2429376.2429382",
                "event-place": "Rome, Italy",
                "ISBN": "9781450318716",
                "keyword": "self-adjusting computation, incremental mapreduce",
                "number-of-pages": "4",
                "page": "15–18",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Streaming big data with self-adjusting computation",
                "URL": "https://doi.org/10.1145/2429376.2429382"
            }
        },
        {
            "10.5555/1325851.1325890": {
                "id": "10.5555/1325851.1325890",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Cong",
                        "given": "Gao"
                    },
                    {
                        "family": "Fan",
                        "given": "Wenfei"
                    },
                    {
                        "family": "Geerts",
                        "given": "Floris"
                    },
                    {
                        "family": "Jia",
                        "given": "Xibei"
                    },
                    {
                        "family": "Ma",
                        "given": "Shuai"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2007,
                            9,
                            23
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2007,
                            9,
                            23
                        ]
                    ]
                },
                "abstract": "Two central criteria for data quality are consistency and accuracy. Inconsistencies and errors in a database often emerge as violations of integrity constraints. Given a dirty database D, one needs automated methods to make it consistent, i.e., find a repair D' that satisfies the constraints and \"minimally\" differs from D. Equally important is to ensure that the automatically-generated repair D' is accurate, or makes sense, i.e., D' differs from the \"correct\" data within a predefined bound. This paper studies effective methods for improving both data consistency and accuracy. We employ a class of conditional functional dependencies (CFDs) proposed in [6] to specify the consistency of the data, which are able to capture inconsistencies and errors beyond what their traditional counterparts can catch. To improve the consistency of the data, we propose two algorithms: one for automatically computing a repair D' that satisfies a given set of CFDs, and the other for incrementally finding a repair in response to updates to a clean database. We show that both problems are intractable. Although our algorithms are necessarily heuristic, we experimentally verify that the methods are effective and efficient. Moreover, we develop a statistical method that guarantees that the repairs found by the algorithms are accurate above a predefined rate without incurring excessive user interaction.",
                "call-number": "10.5555/1325851.1325890",
                "collection-title": "VLDB '07",
                "container-title": "Proceedings of the 33rd international conference on Very large data bases",
                "event-place": "Vienna, Austria",
                "ISBN": "9781595936493",
                "number-of-pages": "12",
                "page": "315–326",
                "publisher": "VLDB Endowment",
                "title": "Improving data quality: consistency and accuracy"
            }
        },
        {
            "10.1145/3170521.3170535": {
                "id": "10.1145/3170521.3170535",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Rastogi",
                        "given": "Avnish Kumar"
                    },
                    {
                        "family": "Narang",
                        "given": "Nitin"
                    },
                    {
                        "family": "Siddiqui",
                        "given": "Zamir Ahmad"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            1,
                            4
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            1,
                            4
                        ]
                    ]
                },
                "abstract": "In the domain of machine learning, quality of data is most critical component for building good models. Predictive analytics is an AI stream used to predict future events based on historical learnings and is used in diverse fields like predicting online frauds, oil slicks, intrusion attacks, credit defaults, prognosis of disease cells etc. Unfortunately, in most of these cases, traditional learning models fail to generate required results due to imbalanced nature of data. Here imbalance denotes small number of instances belonging to the class under prediction like fraud instances in the total online transactions. The prediction in imbalanced classification gets further limited due to factors like small disjuncts which get accentuated during the partitioning of data when learning at scale. Synthetic generation of minority class data (SMOTE [<u>1</u>]) is one pioneering approach by Chawla [<u>1</u>] to offset said limitations and generate more balanced datasets. Although there exists a standard implementation of SMOTE in python, it is unavailable for distributed computing environments for large datasets. Bringing SMOTE to distributed environment under spark is the key motivation for our research. In this paper we present our algorithm, observations and results for synthetic generation of minority class data under spark using Locality Sensitivity Hashing [LSH]. We were able to successfully demonstrate a distributed version of Spark SMOTE which generated quality artificial samples preserving spatial distribution1.",
                "call-number": "10.1145/3170521.3170535",
                "collection-number": "14",
                "collection-title": "Workshops ICDCN '18",
                "container-title": "Proceedings of the Workshop Program of the 19th International Conference on Distributed Computing and Networking",
                "DOI": "10.1145/3170521.3170535",
                "event-place": "Varanasi, India",
                "ISBN": "9781450363976",
                "keyword": "imbalanced classification, SMOTE, spark, nearest neighbors, map reduce, locality sensitivity hashing",
                "number": "Article 14",
                "number-of-pages": "6",
                "page": "1–6",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Imbalanced big data classification: a distributed implementation of SMOTE",
                "URL": "https://doi.org/10.1145/3170521.3170535"
            }
        },
        {
            "10.1145/3158421.3158427": {
                "id": "10.1145/3158421.3158427",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Jennex",
                        "given": "Murray E."
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            11,
                            7
                        ]
                    ]
                },
                "abstract": "The knowledge pyramid has been used for several years to illustrate the hierarchical relationships between data, information, knowledge, and wisdom. An earlier version of this paper presented a revised knowledge-KM pyramid that included processes such as filtering and sense making, reversed the pyramid by positing there was more knowledge than data, and showed knowledge management as an extraction of the pyramid. This paper expands the revised knowledge pyramid to include the Internet of Things and Big Data. The result is a revision of the data aspect of the knowledge pyramid. Previous thought was of data as reflections of reality as recorded by sensors. Big Data and the Internet of Things expand sensors and readings to create two layers of data. The top layer of data is the traditional transaction / operational data and the bottom layer of data is an expanded set of data reflecting massive data sets and sensors that are near mirrors of reality. The result is a knowledge pyramid that appears as an hourglass.",
                "call-number": "10.1145/3158421.3158427",
                "container-title": "SIGMIS Database",
                "DOI": "10.1145/3158421.3158427",
                "ISSN": "0095-0033",
                "issue": "4",
                "keyword": "knowledge pyramid, big data, internet of things, analytics, knowledge management",
                "number-of-pages": "11",
                "page": "69–79",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "November 2017",
                "title": "Big Data, the Internet of Things, and the Revised Knowledge Pyramid",
                "URL": "https://doi.org/10.1145/3158421.3158427",
                "volume": "48"
            }
        },
        {
            "10.1145/2912845.2912869": {
                "id": "10.1145/2912845.2912869",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Luyen",
                        "given": "LE Ngoc"
                    },
                    {
                        "family": "Tireau",
                        "given": "Anne"
                    },
                    {
                        "family": "Venkatesan",
                        "given": "Aravind"
                    },
                    {
                        "family": "Neveu",
                        "given": "Pascal"
                    },
                    {
                        "family": "Larmande",
                        "given": "Pierre"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            6,
                            13
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2016,
                            6,
                            13
                        ]
                    ]
                },
                "abstract": "In the recent years, the data deluge in many areas of scientific research brings challenges in the treatment and improvement of agricultural data. Research in bioinformatics field does not outside this trend. This paper presents some approaches aiming to solve the Big Data problem by combining the increase in semantic search capacity on existing data in the plant research laboratories. This helps us to strengthen user experiments on the data obtained in this research by infering new knowledge. To achieve this, there exist several approaches having different characteristics and using different platforms. Nevertheless, we can summarize it in two main directions: the query re-writing and data transformation to RDF graphs. In reality, we can solve the problem from origin of increasing capacity on semantic data with triplets. Thus, data transformation to RDF graphs direction was chosen to work on the practical part. However, the synchronization data in the same format is required before processing the triplets because our current data are heterogeneous. The data obtained for triplets are larger that regular triplestores could manage. So we evaluate some of them thus we can compare the benefits and drawbacks of each and choose the best system for our problem.",
                "call-number": "10.1145/2912845.2912869",
                "collection-number": "27",
                "collection-title": "WIMS '16",
                "container-title": "Proceedings of the 6th International Conference on Web Intelligence, Mining and Semantics",
                "DOI": "10.1145/2912845.2912869",
                "event-place": "Nîmes, France",
                "ISBN": "9781450340564",
                "keyword": "Ontology, xR2RML, NoSQL, Knowledge base, Inference, Benchmark, Big Data, Triplestore, SPARQL, Reasoning",
                "number": "Article 27",
                "number-of-pages": "9",
                "page": "1–9",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Development of a knowledge system for Big Data: Case study to plant phenotyping data",
                "URL": "https://doi.org/10.1145/2912845.2912869"
            }
        },
        {
            "10.1145/3456565.3460026": {
                "id": "10.1145/3456565.3460026",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Graux",
                        "given": "Damien"
                    },
                    {
                        "family": "Janev",
                        "given": "Valentina"
                    },
                    {
                        "family": "Jabeen",
                        "given": "Hajira"
                    },
                    {
                        "family": "Sallinger",
                        "given": "Emanuel"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            6,
                            26
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2021,
                            6,
                            26
                        ]
                    ]
                },
                "abstract": "As the number of Big Data related methods, tools, frameworks, and solutions is growing, there is a need to classify and make available the knowledge related to this domain. This is especially useful for countries which, so far, have suffered from some lack of infra-structure in Big Data. In this article, we describe the deployment of an open online platform gathering lectures and resources, from multiple partnered European institutions, tailored for West Balkan students with a particular focus on local Big Data challenges.",
                "call-number": "10.1145/3456565.3460026",
                "collection-title": "ITiCSE '21",
                "container-title": "Proceedings of the 26th ACM Conference on Innovation and Technology in Computer Science Education V. 2",
                "DOI": "10.1145/3456565.3460026",
                "event-place": "Virtual Event, Germany",
                "ISBN": "9781450383974",
                "keyword": "west Balkans, European collaboration, teaching big data analytics",
                "number-of-pages": "2",
                "page": "617–618",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "A Big Data Learning Platform for the West Balkans and Beyond",
                "URL": "https://doi.org/10.1145/3456565.3460026"
            }
        },
        {
            "10.1145/2743065.2743121": {
                "id": "10.1145/2743065.2743121",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Amudhavel",
                        "given": "J."
                    },
                    {
                        "family": "Sathian",
                        "given": "D."
                    },
                    {
                        "family": "Raghav",
                        "given": "R. S."
                    },
                    {
                        "family": "Rao",
                        "given": "Dhanawada Nirmala"
                    },
                    {
                        "family": "Dhavachelvan",
                        "given": "P."
                    },
                    {
                        "family": "Kumar",
                        "given": "K. Prem"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2015,
                            3,
                            6
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2015,
                            3,
                            6
                        ]
                    ]
                },
                "abstract": "In the recent years; with the rise in usage of the devices that could connect itself to the network and could share data, there is a steady increase in the number of applications that are being introduced for providing various services to the users who rely on the devices that are being connected on the network to use the application. The biggest issue that these applications will face is how these applications will have to handle the data that is being generated by its users and also how these applications will provide the security to the data. For any application it is important to provide the security to the data of its users. Some of the major applications will involve high privacy data of the users which providing security will play a vital role and any compromise in the security [7] aspects of the applications will lead to enormous loss. The second issue that the application must focus upon is the scalability. There are two important key points why the scalability [8] is important. One, when the applications is being created it is the services that is being more focused upon rather than the count of the users that could use so providing a scalable system that could incorporate as many users as the users rise [9] is important for the application. Second, the hardware and the software configuration for the system will not be more focused upon during the development of the system, even though the hardware and the software configuration would be focused upon it is to be seen than they are satisfied for the services [10] the application provide. So providing a scalable system that can adapt the change of the hardware and of the software as they are being upgraded is an important element [11] in any part of the applications.",
                "call-number": "10.1145/2743065.2743121",
                "collection-number": "56",
                "collection-title": "ICARCSET '15",
                "container-title": "Proceedings of the 2015 International Conference on Advanced Research in Computer Science Engineering & Technology (ICARCSET 2015)",
                "DOI": "10.1145/2743065.2743121",
                "event-place": "Unnao, India",
                "ISBN": "9781450334419",
                "keyword": "big-data, Scalability, privacy driven data",
                "number": "Article 56",
                "number-of-pages": "5",
                "page": "1–5",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Big Data Scalability, Methods and its Implications: A Survey of Current Practice",
                "URL": "https://doi.org/10.1145/2743065.2743121"
            }
        },
        {
            "10.1145/3453187.3453361": {
                "id": "10.1145/3453187.3453361",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Quanli",
                        "given": "Wang"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2020,
                            12,
                            5
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2020,
                            12,
                            5
                        ]
                    ]
                },
                "abstract": "Big data is of direct significance for analyzing the career planning of college students. The current career planning of college students has problems such as unclear goals and inaccurate identity recognition. Based on this consideration, this article uses big data analysis to select five representative colleges and universities for analysis, and proposes several recommended measures to optimize the career planning of college students.",
                "call-number": "10.1145/3453187.3453361",
                "collection-title": "EBIMCS 2020",
                "container-title": "Proceedings of the 2020 3rd International Conference on E-Business, Information Management and Computer Science",
                "DOI": "10.1145/3453187.3453361",
                "event-place": "Wuhan, China",
                "ISBN": "9781450389099",
                "keyword": "Survey, Career planning, College students, Big data",
                "number-of-pages": "7",
                "page": "363–369",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Survey of College Students' Career Planning Based on Big Data Statistical Analysis",
                "URL": "https://doi.org/10.1145/3453187.3453361"
            }
        },
        {
            "10.1145/2968219.2968282": {
                "id": "10.1145/2968219.2968282",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Matekenya",
                        "given": "Dunstan"
                    },
                    {
                        "family": "Ito",
                        "given": "Masaki"
                    },
                    {
                        "family": "Shibasaki",
                        "given": "Ryosuke"
                    },
                    {
                        "family": "Sezaki",
                        "given": "Kaoru"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            9,
                            12
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2016,
                            9,
                            12
                        ]
                    ]
                },
                "abstract": "In recent years, the study of location prediction has received heightened attention due to its applications in LBS and other areas. However, most of the techniques and subsequent conclusions drawn from previous research works are specific to the data used in the study. For instance, resolution of location data and inclusion of external data (e.g., from social networks) may limit application of previous techniques to new situations. Therefore, we explore ways of enhancing location prediction techniques which leverage big data without the need for external data sources. To this end, we study a large CDR dataset with more than 3.5 billion calls from a leading cellular network provider in Dhaka, Bangladesh. The research question we tackle is how we can leverage big data to enhance performance of location predictors? Based on spatio-temporal analysis of call activity, we devise a scheme to compute prior probabilities from cell call activity. With this reasoning, we develop an enhanced Bayes predictor which uses a distance threshold and the users' regular location to improve generation of prior probabilities. Experimental results show that overall the enhanced Bayes predictor improves accuracy by 17 percentage points.",
                "call-number": "10.1145/2968219.2968282",
                "collection-title": "UbiComp '16",
                "container-title": "Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct",
                "DOI": "10.1145/2968219.2968282",
                "event-place": "Heidelberg, Germany",
                "ISBN": "9781450344623",
                "keyword": "data mining, location prediction. supervised classification, human mobility",
                "number-of-pages": "10",
                "page": "753–762",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Enhancing location prediction with big data: evidence from dhaka",
                "URL": "https://doi.org/10.1145/2968219.2968282"
            }
        },
        {
            "10.1145/3503928.3503933": {
                "id": "10.1145/3503928.3503933",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Faccia",
                        "given": "Alessio"
                    },
                    {
                        "family": "Pandey",
                        "given": "Vishal"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            11,
                            12
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2021,
                            11,
                            12
                        ]
                    ]
                },
                "abstract": "The business planning process can be considered a strategic phase of any business. Since the business plan is a management accounting tool, countless approaches can be adopted to prepare it since there is no legal requirement other than financial accounting obligations. A general structure usually consists of numerical statements and descriptive notes. This research is based on the authors’ experiences and commonly used theories, and it highlights a standard process that can be adaptable to the business plan of any activity. The use of big data is an essential part of feeding the data of almost all Budget steps. The authors then determine a generally applicable standard process, indicating all the data necessary to prepare an accurate and reliable business plan. A case study will provide adequate support to the demonstration of the immediate applicability of the proposed model.",
                "call-number": "10.1145/3503928.3503933",
                "collection-title": "ICISE 2021",
                "container-title": "2021 the 6th International Conference on Information Systems Engineering",
                "DOI": "10.1145/3503928.3503933",
                "event-place": "Shanghai, China",
                "ISBN": "9781450385220",
                "keyword": "Business strategy, Budgeting, Business plan, Big Data",
                "number-of-pages": "5",
                "page": "21–25",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Business Planning and Big Data, Budget Modelling Upgrade Through Data Science",
                "URL": "https://doi.org/10.1145/3503928.3503933"
            }
        },
        {
            "10.1145/3407947.3407977": {
                "id": "10.1145/3407947.3407977",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Wang",
                        "given": "Bo"
                    },
                    {
                        "family": "Chen",
                        "given": "Zhiguang"
                    },
                    {
                        "family": "Xiao",
                        "given": "Nong"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2020,
                            6,
                            27
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2020,
                            6,
                            27
                        ]
                    ]
                },
                "abstract": "In the rapidly expanding field of parallel processing, job schedulers act as the \"operating systems\" of the clusters, including modern big data architectures and supercomputing systems. Job schedulers manage and allocate system resources, dispatch the queued jobs, and control the execution of processes on the allocated resources. In this paper, we firstly make an introduction to the cluster schedulers. Then according to the scenarios, we make a comprehensive survey of schedulers for HPC and Big Data. We can conclude that most of these current schedulers are centralized, which means master assigns jobs to the slaves. We call this mode Push, which is different from our new idea that introduces Pull to the schedulers. We proposed a novel scheduling model that allow slaves to actively pull jobs from master to execute. By analyzing the execution time and resource requests of jobs in \"Tianhe-II\", we will clarify that scheduling based on Push & Pull is a direction worthy of in-depth study in the future.",
                "call-number": "10.1145/3407947.3407977",
                "collection-title": "HP3C 2020",
                "container-title": "Proceedings of the 2020 4th International Conference on High Performance Compilation, Computing and Communications",
                "DOI": "10.1145/3407947.3407977",
                "event-place": "Guangzhou, China",
                "ISBN": "9781450376914",
                "keyword": "Job scheduler, high performance computing, decentralized scheduling, big data cluster",
                "number-of-pages": "6",
                "page": "178–183",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "A Survey of System Scheduling for HPC and Big Data",
                "URL": "https://doi.org/10.1145/3407947.3407977"
            }
        },
        {
            "10.1145/3365109": {
                "id": "10.1145/3365109",
                "type": "BOOK",
                "issued": {
                    "date-parts": [
                        [
                            2019
                        ]
                    ]
                },
                "abstract": "On behalf of the program committee, it is our pleasure to welcome you to the 6th IEEE/ACM International Conference on Big Data Computing, Applications and Technologies, being held in Auckland, New Zealand.Rapid advances in digital sensors, networks, storage, and computation along with their availability at low cost is leading to the creation of huge collections of data - dubbed as Big Data. As a result, Big Data Computing paradigm has emerged, enabling new insights that can change the way business, science, and governments deliver services to their consumers, and can impact society as a whole. The IEEE/ACM International Conference on Big Data Computing, Applications and Technologies (BDCAT) is an annual conference series aiming to provide a platform for researchers from both academia and industry to present new discoveries in the broad area of big data computing and applications.BDCAT 2019 received 47 submissions from 17 countries. The conference accepted 13 papers as regular papers, leading to acceptance rate of 27.7%. The conference also accepted an additional 10 papers as short papers. For this we would like to acknowledge the dedication and tremendous efforts of the program committee and reviewers, who provided nearly 200 reviews in a 3-week turnaround time.",
                "call-number": "10.1145/3365109",
                "container-title-short": "BDCAT '19",
                "event-place": "Auckland, New Zealand",
                "genre": "proceeding",
                "ISBN": "9781450370165",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Proceedings of the 6th IEEE/ACM International Conference on Big Data Computing, Applications and Technologies"
            }
        },
        {
            "10.1145/1659225.1659228": {
                "id": "10.1145/1659225.1659228",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Even",
                        "given": "Adir"
                    },
                    {
                        "family": "Shankaranarayanan",
                        "given": "G."
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2009,
                            12,
                            1
                        ]
                    ]
                },
                "abstract": "Quantitative assessment of data quality is critical for identifying the presence of data defects and the extent of the damage due to these defects. Quantitative assessment can help define realistic quality improvement targets, track progress, evaluate the impacts of different solutions, and prioritize improvement efforts accordingly. This study describes a methodology for quantitatively assessing both impartial and contextual data quality in large datasets. Impartial assessment measures the extent to which a dataset is defective, independent of the context in which that dataset is used. Contextual assessment, as defined in this study, measures the extent to which the presence of defects reduces a dataset’s utility, the benefits gained by using that dataset in a specific context. The dual assessment methodology is demonstrated in the context of Customer Relationship Management (CRM), using large data samples from real-world datasets. The results from comparing the two assessments offer important insights for directing quality maintenance efforts and prioritizing quality improvement solutions for this dataset. The study describes the steps and the computation involved in the dual-assessment methodology and discusses the implications for applying the methodology in other business contexts and data environments.",
                "call-number": "10.1145/1659225.1659228",
                "collection-number": "15",
                "container-title": "J. Data and Information Quality",
                "DOI": "10.1145/1659225.1659228",
                "ISSN": "1936-1955",
                "issue": "3",
                "keyword": "information value, Data quality, total data quality management, CRM, databases, customer relationship management",
                "number": "Article 15",
                "number-of-pages": "29",
                "page": "1–29",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "December 2009",
                "title": "Dual Assessment of Data Quality in Customer Databases",
                "URL": "https://doi.org/10.1145/1659225.1659228",
                "volume": "1"
            }
        },
        {
            "10.1145/3318464.3384677": {
                "id": "10.1145/3318464.3384677",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Wang",
                        "given": "Jin"
                    },
                    {
                        "family": "Xiao",
                        "given": "Guorui"
                    },
                    {
                        "family": "Gu",
                        "given": "Jiaqi"
                    },
                    {
                        "family": "Wu",
                        "given": "Jiacheng"
                    },
                    {
                        "family": "Zaniolo",
                        "given": "Carlo"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2020,
                            6,
                            11
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2020,
                            6,
                            11
                        ]
                    ]
                },
                "abstract": "There is a growing interest in supporting advanced Big Data applications on distributed data processing platforms. Most of these systems support SQL or its dialect as the query interface due to its portability and declarative nature. However, current SQL standard cannot effectively express advanced analytical queries due to its limitation in supporting recursive queries. In this demonstration, we show that this problem can be resolved via a simple SQL extension that delivers greater expressive power by allowing aggregates in recursion. To this end, we propose the Recursive-aggregate-SQL (RASQL) language and its system on top of Apache Spark to express and execute complex queries and declarative algorithms in many applications, such as graph search and machine learning. With a variety of examples, we will (i) show how complicated analytic queries can be expressed with RASQL; (ii) illustrate formal semantics of the powerful new constructs; and (iii) present a user-friendly interface to interact with the RASQL system and monitor the query results.",
                "call-number": "10.1145/3318464.3384677",
                "collection-title": "SIGMOD '20",
                "container-title": "Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data",
                "DOI": "10.1145/3318464.3384677",
                "event-place": "Portland, OR, USA",
                "ISBN": "9781450367356",
                "keyword": "big data, recursive query, query language",
                "number-of-pages": "4",
                "page": "2673–2676",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "RASQL: A Powerful Language and its System for Big Data Applications",
                "URL": "https://doi.org/10.1145/3318464.3384677"
            }
        },
        {
            "10.1145/3330482.3330510": {
                "id": "10.1145/3330482.3330510",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Gu",
                        "given": "Yuanhu"
                    },
                    {
                        "family": "Malicdem",
                        "given": "Alvin R."
                    },
                    {
                        "family": "Cruz",
                        "given": "Josephine S. Dela"
                    },
                    {
                        "family": "Palaoag",
                        "given": "Thelma Domingo"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            4,
                            19
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            4,
                            19
                        ]
                    ]
                },
                "abstract": "Nowadays, telecommunication markets are becoming more and more competitive, and customer churn is becoming more and more serious. In the tough competitive mobile market, Customer Churn Management is becoming more and more critical. In developing countries, most customers switch service providers because of good promotional incentives and lower monthly costs offered by competitive service providers. How to predict customer churn quickly and accurately becomes very important. In this paper, the researchers successfully analyzed the customer churn using big data feature analysis and multi-feature analysis. User data were modeled by XGBoost algorithm. The model is optimized repeatedly with GridSearchCV as a parameter tool. The accuracy of the model on the test set is 85.1%. The researchers predicted about 11000 customer lists per month that may be about to churn. Using K-means clustering method, 11000 churn target customers per month were classified into three categories and telecom companies are suggested to take some solutions which are found by feature analysis to retain customers. This big data analysis can be used to retain customers for the telecom industry.",
                "call-number": "10.1145/3330482.3330510",
                "collection-title": "ICCAI '19",
                "container-title": "Proceedings of the 2019 5th International Conference on Computing and Artificial Intelligence",
                "DOI": "10.1145/3330482.3330510",
                "event-place": "Bali, Indonesia",
                "ISBN": "9781450361064",
                "keyword": "retain customers, telecom industry, customer churn, feature analysis, Big data analysis",
                "number-of-pages": "6",
                "page": "38–43",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Using Big Data Analysis to Retain Customers for Telecom Industry",
                "URL": "https://doi.org/10.1145/3330482.3330510"
            }
        },
        {
            "10.1145/2968456.2976765": {
                "id": "10.1145/2968456.2976765",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Neshatpour",
                        "given": "Katayoun"
                    },
                    {
                        "family": "Sasan",
                        "given": "Avesta"
                    },
                    {
                        "family": "Homayoun",
                        "given": "Houman"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            10,
                            1
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2016,
                            10,
                            1
                        ]
                    ]
                },
                "abstract": "In this paper, we present the implementation of big data analytics applications in a heterogeneous CPU+FPGA accelerator architecture. We develop the MapReduce implementation of K-means, K nearest neighbor, support vector machine and Naive Bayes in a Hadoop Streaming environment that allows developing mapper/reducer functions in a non-Java based language suited for interfacing with FPGA-based hardware accelerating environment. We present a full implementation of the HW+SW mappers on the Zynq FPGA platform. A promising speedup as well as energy-efficiency gains of upto 4.5X and 22X is achieved, respectively, in an end-to-end Hadoop implementation.",
                "call-number": "10.1145/2968456.2976765",
                "collection-number": "16",
                "collection-title": "CODES '16",
                "container-title": "Proceedings of the Eleventh IEEE/ACM/IFIP International Conference on Hardware/Software Codesign and System Synthesis",
                "DOI": "10.1145/2968456.2976765",
                "event-place": "Pittsburgh, Pennsylvania",
                "ISBN": "9781450344838",
                "number": "Article 16",
                "number-of-pages": "3",
                "page": "1–3",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Big data analytics on heterogeneous accelerator architectures",
                "URL": "https://doi.org/10.1145/2968456.2976765"
            }
        },
        {
            "10.1145/2659532.2659593": {
                "id": "10.1145/2659532.2659593",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Radenski",
                        "given": "Atanas"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2014,
                            6,
                            27
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2014,
                            6,
                            27
                        ]
                    ]
                },
                "abstract": "We discuss the emergence of data-intensive computing and then explore the applicability of business-oriented big-data platforms, such as Hadoop MapReduce, to traditional scientific computing processes. In particular, we investigate the suitability of MapReduce parallelism for simulation of grid-based models by developing message-passing MapReduce algorithms and empirically evaluating their performance on the Amazon's Elastic MapReduce cloud. We outline MapReduce challenges (such as insufficient speed) and opportunities (such as fault-tolerance and ease of use) in scientific computing.",
                "call-number": "10.1145/2659532.2659593",
                "collection-title": "CompSysTech '14",
                "container-title": "Proceedings of the 15th International Conference on Computer Systems and Technologies",
                "DOI": "10.1145/2659532.2659593",
                "event-place": "Ruse, Bulgaria",
                "ISBN": "9781450327534",
                "keyword": "life simulation, partitioning, Hadoop, local aggregation, MapReduce, relaxation",
                "number-of-pages": "12",
                "page": "13–24",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Big data, high-performance computing, and MapReduce",
                "URL": "https://doi.org/10.1145/2659532.2659593"
            }
        },
        {
            "10.1145/3379247.3379282": {
                "id": "10.1145/3379247.3379282",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Liyao",
                        "given": "Zhou"
                    },
                    {
                        "family": "Xiaofang",
                        "given": "Liu"
                    },
                    {
                        "family": "Chunyu",
                        "given": "Hu"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2020,
                            1,
                            4
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2020,
                            1,
                            4
                        ]
                    ]
                },
                "abstract": "In the evaluation of equipment combat effectiveness, it is necessary to comprehensively analyze the data of outfield test and infield test, including a variety of audio-visual, image and other combat test data. These data can be classified, extracted, stored and managed by building data model through big data mining technology. The evaluation method of equipment combat effectiveness based on big data mining is based on massive data, through machine learning, statistical analysis, neural network, database and other methods to analyze and process the data, mining the correlation between test data, evaluation index and evaluation conclusion, and extracting useful information and finding new knowledge from it to realize the evaluation of the combat effectiveness of the tested system.",
                "call-number": "10.1145/3379247.3379282",
                "collection-title": "ICCDE 2020",
                "container-title": "Proceedings of 2020 the 6th International Conference on Computing and Data Engineering",
                "DOI": "10.1145/3379247.3379282",
                "event-place": "Sanya, China",
                "ISBN": "9781450376730",
                "keyword": "Big data mining, combat test, combat effectiveness evaluation",
                "number-of-pages": "5",
                "page": "131–135",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Evaluation Method of Equipment Combat Effectiveness Based On Big Data Mining",
                "URL": "https://doi.org/10.1145/3379247.3379282"
            }
        },
        {
            "10.14778/2824032.2824067": {
                "id": "10.14778/2824032.2824067",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Hu",
                        "given": "Xueyang"
                    },
                    {
                        "family": "Yuan",
                        "given": "Mingxuan"
                    },
                    {
                        "family": "Yao",
                        "given": "Jianguo"
                    },
                    {
                        "family": "Deng",
                        "given": "Yu"
                    },
                    {
                        "family": "Chen",
                        "given": "Lei"
                    },
                    {
                        "family": "Yang",
                        "given": "Qiang"
                    },
                    {
                        "family": "Guan",
                        "given": "Haibing"
                    },
                    {
                        "family": "Zeng",
                        "given": "Jia"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2015,
                            8,
                            1
                        ]
                    ]
                },
                "abstract": "Differential privacy (DP) has been widely explored in academia recently but less so in industry possibly due to its strong privacy guarantee. This paper makes the first attempt to implement three basic DP architectures in the deployed telecommunication (telco) big data platform for data mining applications. We find that all DP architectures have less than 5% loss of prediction accuracy when the weak privacy guarantee is adopted (e.g., privacy budget parameter ε ≥ 3). However, when the strong privacy guarantee is assumed (e.g., privacy budget parameter ε ≤ 0:1), all DP architectures lead to 15% ~ 30% accuracy loss, which implies that real-word industrial data mining systems cannot work well under such a strong privacy guarantee recommended by previous research works. Among the three basic DP architectures, the Hybridized DM (Data Mining) and DB (Database) architecture performs the best because of its complicated privacy protection design for the specific data mining algorithm. Through extensive experiments on big data, we also observe that the accuracy loss increases by increasing the variety of features, but decreases by increasing the volume of training data. Therefore, to make DP practically usable in large-scale industrial systems, our observations suggest that we may explore three possible research directions in future: (1) Relaxing the privacy guarantee (e.g., increasing privacy budget ε) and studying its effectiveness on specific industrial applications; (2) Designing specific privacy scheme for specific data mining algorithms; and (3) Using large volume of data but with low variety for training the classification models.",
                "call-number": "10.14778/2824032.2824067",
                "container-title": "Proc. VLDB Endow.",
                "DOI": "10.14778/2824032.2824067",
                "ISSN": "2150-8097",
                "issue": "12",
                "number-of-pages": "12",
                "page": "1692–1703",
                "publisher": "VLDB Endowment",
                "source": "August 2015",
                "title": "Differential privacy in telco big data platform",
                "URL": "https://doi.org/10.14778/2824032.2824067",
                "volume": "8"
            }
        }
    ]
}