{
    "exportedDoiLength": 101,
    "fileName": "acm",
    "style": "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<style xmlns=\"http://purl.org/net/xbiblio/csl\" class=\"in-text\" version=\"1.0\" demote-non-dropping-particle=\"sort-only\" default-locale=\"en-US\">\n    <!-- This style was edited with the Visual CSL Editor (http://editor.citationstyles.org/visualEditor/) -->\n    <info>\n        <title>BibTeX ACM citation style</title>\n        <id>http://www.zotero.org/styles/bibtex-acm-citation-style</id>\n        <link href=\"http://www.zotero.org/styles/bibtex-acm-citation-style\" rel=\"self\"/>\n        <link href=\"http://www.bibtex.org/\" rel=\"documentation\"/>\n        <author>\n            <name>Markus Schaffner</name>\n        </author>\n        <contributor>\n            <name>Richard Karnesky</name>\n            <email>karnesky+zotero@gmail.com</email>\n            <uri>http://arc.nucapt.northwestern.edu/Richard_Karnesky</uri>\n        </contributor>\n        <category citation-format=\"author-date\"/>\n        <category field=\"generic-base\"/>\n        <updated>2018-06-11T10:52:49+00:00</updated>\n        <rights license=\"http://creativecommons.org/licenses/by-sa/3.0/\">This work is licensed under a Creative Commons Attribution-ShareAlike 3.0 License</rights>\n    </info>\n    <macro name=\"zotero2bibtexType\">\n        <choose>\n            <if type=\"BILL BOOK GRAPHIC LEGAL_CASE LEGISLATION MOTION_PICTURE SONG\" match=\"any\">\n                <choose>\n                    <if genre=\"rfc\"  match=\"any\">\n                       <text value=\"rfc\"/>\n                    </if>\n                     <else-if  genre=\"bibliography\" match=\"any\">\n                        <text value=\"bibliography\"/>\n                    </else-if>\n                    <else-if  genre=\"play_drama\" match=\"any\">\n                        <text value=\"playdrama\"/>\n                    </else-if>\n                    <else-if  genre=\"proceeding\" match=\"any\">\n                        <text value=\"proceedings\"/>\n                    </else-if>\n                    <else-if  genre=\"tech_brief\" match=\"any\">\n                        <text value=\"tech-brief\"/>\n                    </else-if>\n                    <else>\n                       <text value=\"book\"/>\n                    </else>\n                </choose>\n            </if>\n            <else-if type=\"CHAPTER\" match=\"any\">\n                <text value=\"inbook\"/>\n            </else-if>\n            <else-if type=\"ARTICLE ARTICLE_JOURNAL ARTICLE_MAGAZINE ARTICLE_NEWSPAPER\" match=\"any\">\n                <text value=\"article\"/>\n            </else-if>\n            <else-if type=\"THESIS\" match=\"any\">\n                <choose>\n                    <if variable=\"genre\">\n                        <text variable=\"genre\" text-case=\"lowercase\" strip-periods=\"true\"/>\n                    </if>\n                </choose>\n                <text value=\"thesis\"/>\n            </else-if>\n            <else-if type=\"PAPER_CONFERENCE\" match=\"any\">\n                <text value=\"inproceedings\"/>\n            </else-if>\n            <else-if type=\"REPORT\" match=\"any\">\n                <text value=\"techreport\"/>\n            </else-if>\n            <else-if type=\"DATASET\" match=\"any\">\n                <choose>\n                    <if genre=\"software\"  match=\"any\">\n                       <text value=\"software\"/>\n                    </if>\n                    <else>\n                       <text value=\"dataset\"/>\n                    </else>\n                </choose>\n            </else-if>\n            <else>\n                <text value=\"misc\"/>\n            </else>\n        </choose>\n    </macro>\n    <macro name=\"citeKey\">\n           <text variable=\"call-number\"/>\n    </macro>\n    <macro name=\"editor-short\">\n        <names variable=\"editor\">\n            <name form=\"short\" delimiter=\":\" delimiter-precedes-last=\"always\"/>\n        </names>\n    </macro>\n    <macro name=\"author-short\">\n        <names variable=\"author\">\n            <name form=\"short\" delimiter=\":\" delimiter-precedes-last=\"always\"/>\n        </names>\n    </macro>\n    <macro name=\"issued-year\">\n        <date variable=\"issued\">\n            <date-part name=\"year\"/>\n        </date>\n    </macro>\n    <macro name=\"issued-month\">\n        <choose>\n            <if type=\"ARTICLE\" match=\"any\">\n                <date variable=\"issued\">\n                    <date-part name=\"month\" form=\"short\" strip-periods=\"true\" text-case=\"lowercase\"/>\n                </date>\n            </if>\n        </choose>\n    </macro>\n    <macro name=\"issue-date\">\n        <choose>\n            <if type=\"ARTICLE ARTICLE_JOURNAL\" match=\"any\">\n                <choose>\n                    <if variable=\"note\" match=\"none\">\n                        <choose>\n                            <if variable=\"source\">\n                                 <text variable=\"source\"/>\n                            </if>\n                               <else>\n                                 <date date-parts=\"year-month\" form=\"text\" variable=\"issued\"/>\n                               </else>\n                        </choose>\n                    </if>\n                </choose>\n            </if>\n        </choose>\n    </macro>\n    <macro name=\"author\">\n        <names variable=\"author\">\n            <name sort-separator=\", \" delimiter=\" and \" delimiter-precedes-last=\"always\" name-as-sort-order=\"all\"/>\n            <label form=\"long\" text-case=\"capitalize-first\"/>\n        </names>\n    </macro>\n    <macro name=\"editor-translator\">\n        <names variable=\"editor translator\" delimiter=\", \">\n            <name sort-separator=\", \" delimiter=\" and \" delimiter-precedes-last=\"always\" name-as-sort-order=\"all\"/>\n            <label form=\"long\" text-case=\"capitalize-first\"/>\n        </names>\n    </macro>\n    <macro name=\"title\">\n        <choose>\n            <if genre=\"proceeding\" match=\"any\">\n                <text variable=\"container-title-short\" suffix=\": \"/>\n                <text variable=\"title\" text-case=\"title\"/>\n            </if>\n            <else-if type=\"ARTICLE_JOURNAL\" match=\"none\">\n                <text variable=\"title\" text-case=\"title\"/>\n            </else-if>\n        </choose>\n    </macro>\n    <macro name=\"volume\">\n        <choose>\n            <if type=\"ARTICLE BOOK ARTICLE_JOURNAL\" match=\"any\">\n                <text variable=\"volume\" prefix=\"volume = {\" suffix=\"}\"/>\n            </if>\n        </choose>\n    </macro>\n    <macro name=\"DOI\">\n        <choose>\n            <if type=\"ARTICLE PAPER_CONFERENCE\" match=\"any\">\n                <text variable=\"DOI\" prefix=\"doi = {\" suffix=\"}\"/>\n            </if>\n        </choose>\n    </macro>\n    <macro name=\"abstract\">\n        <if match=\"any\" variable=\"abstract\">\n            <text variable=\"abstract\"/>\n        </if>\n    </macro>\n    <macro name=\"URL\">\n        <text variable=\"URL\" prefix=\"url = {\" suffix=\"}\"/>\n    </macro>\n    <macro name=\"container-title\">\n        <choose>\n            <if type=\"CHAPTER PAPER_CONFERENCE\" match=\"any\">\n                <text variable=\"container-title\" prefix=\"booktitle = {\" suffix=\"}\" text-case=\"title\"/>\n            </if>\n            <else-if type=\"ARTICLE ARTICLE_JOURNAL\" match=\"any\">\n                <text variable=\"container-title\" prefix=\"journal = {\" suffix=\"}\" text-case=\"title\"/>\n            </else-if>\n        </choose>\n    </macro>\n    <macro name=\"pages\">\n        <group delimiter=\",&#10;\">\n            <choose>\n                <if match=\"any\" variable=\"collection-number\">\n                    <text variable=\"collection-number\" prefix=\"articleno = {\" suffix=\"}\"/>\n                </if>\n                <else>\n                    <text variable=\"page\" prefix=\"pages = {\" suffix=\"}\"/>\n                </else>\n            </choose>\n            <text variable=\"number-of-pages\" prefix=\"numpages = {\" suffix=\"}\"/>\n        </group>\n    </macro>\n    <macro name=\"edition\">\n        <text variable=\"edition\"/>\n    </macro>\n    <macro name=\"editor\">\n        <choose>\n            <if match=\"any\" type=\"THESIS\">\n                <names variable=\"editor\" delimiter=\", \" prefix=\"advisor = {\" suffix=\"}\">\n                    <name delimiter=\" and \" delimiter-precedes-last=\"always\" name-as-sort-order=\"all\"/>\n                </names>\n            </if>\n            <else>\n               <names variable=\"editor\" delimiter=\", \" prefix=\"editor = {\" suffix=\"}\">\n                   <name delimiter=\" and \" delimiter-precedes-last=\"always\" name-as-sort-order=\"all\"/>\n               </names>\n            </else>\n        </choose>\n    </macro>\n    <macro name=\"keyword\">\n        <choose>\n            <if type=\"ARTICLE PAPER_CONFERENCE DATASET\" match=\"any\">\n                <text variable=\"keyword\" prefix=\"keywords = {\" suffix=\"}\"/>\n            </if>\n        </choose>\n    </macro>\n    <citation et-al-min=\"11\" et-al-use-first=\"10\" disambiguate-add-year-suffix=\"true\" disambiguate-add-names=\"false\" disambiguate-add-givenname=\"false\" collapse=\"year\">\n        <layout delimiter=\"_\">\n            <text macro=\"citeKey\"/>\n        </layout>\n    </citation>\n    <bibliography hanging-indent=\"false\">\n        <layout>\n            <group display=\"right-inline\">\n                <text macro=\"zotero2bibtexType\" prefix=\"@\"/>\n                <group prefix=\"{\" suffix=\"&#10;}\" delimiter=\",&#10;\">\n                    <text macro=\"citeKey\"/>\n                    <text macro=\"author\" prefix=\"author = {\" suffix=\"}\"/>\n                    <text macro=\"editor\"/>\n                    <text macro=\"title\" prefix=\"title = {\" suffix=\"}\"/>\n                    <text macro=\"issued-year\" prefix=\"year = {\" suffix=\"}\"/>\n                    <text macro=\"issue-date\" prefix=\"issue_date = {\" suffix=\"}\"/>\n                    <text variable=\"ISBN\" prefix=\"isbn = {\" suffix=\"}\"/>\n                    <text variable=\"publisher\" prefix=\"publisher = {\" suffix=\"}\"/>\n                    <text variable=\"publisher-place\" prefix=\"address = {\" suffix=\"}\"/>\n                    <text variable=\"chapter-number\" prefix=\"chapter = {\" suffix=\"}\"/>\n                    <text macro=\"edition\" prefix=\"edition = {\" suffix=\"}\"/>\n                    <text macro=\"volume\"/>\n                    <text variable=\"issue\" prefix=\"number = {\" suffix=\"}\"/>\n                    <text variable=\"ISSN\" prefix=\"issn = {\" suffix=\"}\"/>\n                    <text variable=\"archive_location\" prefix=\"archiveLocation = {\" suffix=\"}\"/>\n                    <text macro=\"URL\"/>\n                    <text macro=\"DOI\"/>\n                    <text macro=\"abstract\" prefix=\"abstract = {\" suffix=\"}\"/>\n                    <text variable=\"note\" prefix=\"note = {\" suffix=\"}\"/>\n                    <text macro=\"container-title\"/>\n                    <text macro=\"issued-month\" prefix=\"month = {\" suffix=\"}\"/>\n                    <text macro=\"pages\"/>\n                    <text macro=\"keyword\"/>\n                    <text variable=\"event-place\" prefix=\"location = {\" suffix=\"}\"/>\n                    <choose>\n                        <if type=\"PAPER_CONFERENCE\" match=\"any\">\n                            <text variable=\"collection-title\" prefix=\"series = {\" suffix=\"}\"/>\n                        </if>\n                        <else>\n                            <text variable=\"collection-title\" prefix=\"collection = {\" suffix=\"}\"/>\n                        </else>\n                    </choose>\n                </group>\n            </group>\n        </layout>\n    </bibliography>\n</style>\n",
    "suffix": "bib",
    "locale": "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<locale xmlns=\"http://purl.org/net/xbiblio/csl\" version=\"1.0\" xml:lang=\"en-US\">\n  <info>\n    <rights license=\"http://creativecommons.org/licenses/by-sa/3.0/\">This work is licensed under a Creative Commons Attribution-ShareAlike 3.0 License</rights>\n    <updated>2012-07-04T23:31:02+00:00</updated>\n  </info>\n  <style-options punctuation-in-quote=\"true\"\n                 leading-noise-words=\"a,an,the\"\n                 name-as-sort-order=\"ja zh kr my hu vi\"\n                 name-never-short=\"ja zh kr my hu vi\"/>\n  <date form=\"text\">\n    <date-part name=\"month\" suffix=\" \"/>\n    <date-part name=\"day\" suffix=\", \"/>\n    <date-part name=\"year\"/>\n  </date>\n  <date form=\"numeric\">\n    <date-part name=\"month\" form=\"numeric-leading-zeros\" suffix=\"/\"/>\n    <date-part name=\"day\" form=\"numeric-leading-zeros\" suffix=\"/\"/>\n    <date-part name=\"year\"/>\n  </date>\n  <terms>\n    <term name=\"radio-broadcast\">radio broadcast</term>\n    <term name=\"television-broadcast\">television broadcast</term>\n    <term name=\"podcast\">podcast</term>\n    <term name=\"instant-message\">instant message</term>\n    <term name=\"email\">email</term>\n    <term name=\"number-of-volumes\">\n      <single>volume</single>\n      <multiple>volumes</multiple>\n    </term>\n    <term name=\"accessed\">accessed</term>\n    <term name=\"and\">and</term>\n    <term name=\"and\" form=\"symbol\">&amp;</term>\n    <term name=\"and others\">and others</term>\n    <term name=\"anonymous\">anonymous</term>\n    <term name=\"anonymous\" form=\"short\">anon.</term>\n    <term name=\"at\">at</term>\n    <term name=\"available at\">available at</term>\n    <term name=\"by\">by</term>\n    <term name=\"circa\">circa</term>\n    <term name=\"circa\" form=\"short\">c.</term>\n    <term name=\"cited\">cited</term>\n    <term name=\"edition\">\n      <single>edition</single>\n      <multiple>editions</multiple>\n    </term>\n    <term name=\"edition\" form=\"short\">ed.</term>\n    <term name=\"et-al\">et al.</term>\n    <term name=\"forthcoming\">forthcoming</term>\n    <term name=\"from\">from</term>\n    <term name=\"ibid\">ibid.</term>\n    <term name=\"in\">in</term>\n    <term name=\"in press\">in press</term>\n    <term name=\"internet\">internet</term>\n    <term name=\"interview\">interview</term>\n    <term name=\"letter\">letter</term>\n    <term name=\"no date\">no date</term>\n    <term name=\"no date\" form=\"short\">n.d.</term>\n    <term name=\"online\">online</term>\n    <term name=\"presented at\">presented at the</term>\n    <term name=\"reference\">\n      <single>reference</single>\n      <multiple>references</multiple>\n    </term>\n    <term name=\"reference\" form=\"short\">\n      <single>ref.</single>\n      <multiple>refs.</multiple>\n    </term>\n    <term name=\"retrieved\">retrieved</term>\n    <term name=\"scale\">scale</term>\n    <term name=\"version\">version</term>\n\n    <!-- ANNO DOMINI; BEFORE CHRIST -->\n    <term name=\"ad\">AD</term>\n    <term name=\"bc\">BC</term>\n\n    <!-- PUNCTUATION -->\n    <term name=\"open-quote\">“</term>\n    <term name=\"close-quote\">”</term>\n    <term name=\"open-inner-quote\">‘</term>\n    <term name=\"close-inner-quote\">’</term>\n    <term name=\"page-range-delimiter\">–</term>\n\n    <!-- ORDINALS -->\n    <term name=\"ordinal\">th</term>\n    <term name=\"ordinal-01\">st</term>\n    <term name=\"ordinal-02\">nd</term>\n    <term name=\"ordinal-03\">rd</term>\n    <term name=\"ordinal-11\">th</term>\n    <term name=\"ordinal-12\">th</term>\n    <term name=\"ordinal-13\">th</term>\n\n    <!-- LONG ORDINALS -->\n    <term name=\"long-ordinal-01\">first</term>\n    <term name=\"long-ordinal-02\">second</term>\n    <term name=\"long-ordinal-03\">third</term>\n    <term name=\"long-ordinal-04\">fourth</term>\n    <term name=\"long-ordinal-05\">fifth</term>\n    <term name=\"long-ordinal-06\">sixth</term>\n    <term name=\"long-ordinal-07\">seventh</term>\n    <term name=\"long-ordinal-08\">eighth</term>\n    <term name=\"long-ordinal-09\">ninth</term>\n    <term name=\"long-ordinal-10\">tenth</term>\n\n    <!-- LONG LOCATOR FORMS -->\n    <term name=\"book\">\n      <single>book</single>\n      <multiple>books</multiple>\n    </term>\n    <term name=\"chapter\">\n      <single>chapter</single>\n      <multiple>chapters</multiple>\n    </term>\n    <term name=\"column\">\n      <single>column</single>\n      <multiple>columns</multiple>\n    </term>\n    <term name=\"figure\">\n      <single>figure</single>\n      <multiple>figures</multiple>\n    </term>\n    <term name=\"folio\">\n      <single>folio</single>\n      <multiple>folios</multiple>\n    </term>\n    <term name=\"issue\">\n      <single>number</single>\n      <multiple>numbers</multiple>\n    </term>\n    <term name=\"line\">\n      <single>line</single>\n      <multiple>lines</multiple>\n    </term>\n    <term name=\"note\">\n      <single>note</single>\n      <multiple>notes</multiple>\n    </term>\n    <term name=\"opus\">\n      <single>opus</single>\n      <multiple>opera</multiple>\n    </term>\n    <term name=\"page\">\n      <single>page</single>\n      <multiple>pages</multiple>\n    </term>\n    <term name=\"paragraph\">\n      <single>paragraph</single>\n      <multiple>paragraph</multiple>\n    </term>\n    <term name=\"part\">\n      <single>part</single>\n      <multiple>parts</multiple>\n    </term>\n    <term name=\"section\">\n      <single>section</single>\n      <multiple>sections</multiple>\n    </term>\n    <term name=\"sub verbo\">\n      <single>sub verbo</single>\n      <multiple>sub verbis</multiple>\n    </term>\n    <term name=\"verse\">\n      <single>verse</single>\n      <multiple>verses</multiple>\n    </term>\n    <term name=\"volume\">\n      <single>volume</single>\n      <multiple>volumes</multiple>\n    </term>\n\n    <!-- SHORT LOCATOR FORMS -->\n    <term name=\"book\" form=\"short\">bk.</term>\n    <term name=\"chapter\" form=\"short\">chap.</term>\n    <term name=\"column\" form=\"short\">col.</term>\n    <term name=\"figure\" form=\"short\">fig.</term>\n    <term name=\"folio\" form=\"short\">f.</term>\n    <term name=\"issue\" form=\"short\">no.</term>\n    <term name=\"line\" form=\"short\">l.</term>\n    <term name=\"note\" form=\"short\">n.</term>\n    <term name=\"opus\" form=\"short\">op.</term>\n    <term name=\"page\" form=\"short\">\n      <single>p.</single>\n      <multiple>pp.</multiple>\n    </term>\n    <term name=\"paragraph\" form=\"short\">para.</term>\n    <term name=\"part\" form=\"short\">pt.</term>\n    <term name=\"section\" form=\"short\">sec.</term>\n    <term name=\"sub verbo\" form=\"short\">\n      <single>s.v.</single>\n      <multiple>s.vv.</multiple>\n    </term>\n    <term name=\"verse\" form=\"short\">\n      <single>v.</single>\n      <multiple>vv.</multiple>\n    </term>\n    <term name=\"volume\" form=\"short\">\n      <single>vol.</single>\n      <multiple>vols.</multiple>\n    </term>\n\n    <!-- SYMBOL LOCATOR FORMS -->\n    <term name=\"paragraph\" form=\"symbol\">\n      <single>¶</single>\n      <multiple>¶¶</multiple>\n    </term>\n    <term name=\"section\" form=\"symbol\">\n      <single>§</single>\n      <multiple>§§</multiple>\n    </term>\n\n    <!-- LONG ROLE FORMS -->\n    <term name=\"director\">\n      <single>director</single>\n      <multiple>directors</multiple>\n    </term>\n    <term name=\"editor\">\n      <single>editor</single>\n      <multiple>editors</multiple>\n    </term>\n    <term name=\"editorial-director\">\n      <single>editor</single>\n      <multiple>editors</multiple>\n    </term>\n    <term name=\"illustrator\">\n      <single>illustrator</single>\n      <multiple>illustrators</multiple>\n    </term>\n    <term name=\"translator\">\n      <single>translator</single>\n      <multiple>translators</multiple>\n    </term>\n    <term name=\"editortranslator\">\n      <single>editor &amp; translator</single>\n      <multiple>editors &amp; translators</multiple>\n    </term>\n\n    <!-- SHORT ROLE FORMS -->\n    <term name=\"director\" form=\"short\">\n      <single>dir.</single>\n      <multiple>dirs.</multiple>\n    </term>\n    <term name=\"editor\" form=\"short\">\n      <single>ed.</single>\n      <multiple>eds.</multiple>\n    </term>\n    <term name=\"editorial-director\" form=\"short\">\n      <single>ed.</single>\n      <multiple>eds.</multiple>\n    </term>\n    <term name=\"illustrator\" form=\"short\">\n      <single>ill.</single>\n      <multiple>ills.</multiple>\n    </term>\n    <term name=\"translator\" form=\"short\">\n      <single>tran.</single>\n      <multiple>trans.</multiple>\n    </term>\n    <term name=\"editortranslator\" form=\"short\">\n      <single>ed. &amp; tran.</single>\n      <multiple>eds. &amp; trans.</multiple>\n    </term>\n\n    <!-- VERB ROLE FORMS -->\n    <term name=\"director\" form=\"verb\">directed by</term>\n    <term name=\"editor\" form=\"verb\">edited by</term>\n    <term name=\"editorial-director\" form=\"verb\">edited by</term>\n    <term name=\"illustrator\" form=\"verb\">illustrated by</term>\n    <term name=\"interviewer\" form=\"verb\">interview by</term>\n    <term name=\"recipient\" form=\"verb\">to</term>\n    <term name=\"reviewed-author\" form=\"verb\">by</term>\n    <term name=\"translator\" form=\"verb\">translated by</term>\n    <term name=\"editortranslator\" form=\"verb\">edited &amp; translated by</term>\n\n    <!-- SHORT VERB ROLE FORMS -->\n    <term name=\"container-author\" form=\"verb-short\">by</term>\n    <term name=\"director\" form=\"verb-short\">dir.</term>\n    <term name=\"editor\" form=\"verb-short\">ed.</term>\n    <term name=\"editorial-director\" form=\"verb-short\">ed.</term>\n    <term name=\"illustrator\" form=\"verb-short\">illus.</term>\n    <term name=\"translator\" form=\"verb-short\">trans.</term>\n    <term name=\"editortranslator\" form=\"verb-short\">ed. &amp; trans.</term>\n\n    <!-- LONG MONTH FORMS -->\n    <term name=\"month-01\">January</term>\n    <term name=\"month-02\">February</term>\n    <term name=\"month-03\">March</term>\n    <term name=\"month-04\">April</term>\n    <term name=\"month-05\">May</term>\n    <term name=\"month-06\">June</term>\n    <term name=\"month-07\">July</term>\n    <term name=\"month-08\">August</term>\n    <term name=\"month-09\">September</term>\n    <term name=\"month-10\">October</term>\n    <term name=\"month-11\">November</term>\n    <term name=\"month-12\">December</term>\n\n    <!-- SHORT MONTH FORMS -->\n    <term name=\"month-01\" form=\"short\">Jan.</term>\n    <term name=\"month-02\" form=\"short\">Feb.</term>\n    <term name=\"month-03\" form=\"short\">Mar.</term>\n    <term name=\"month-04\" form=\"short\">Apr.</term>\n    <term name=\"month-05\" form=\"short\">May</term>\n    <term name=\"month-06\" form=\"short\">Jun.</term>\n    <term name=\"month-07\" form=\"short\">Jul.</term>\n    <term name=\"month-08\" form=\"short\">Aug.</term>\n    <term name=\"month-09\" form=\"short\">Sep.</term>\n    <term name=\"month-10\" form=\"short\">Oct.</term>\n    <term name=\"month-11\" form=\"short\">Nov.</term>\n    <term name=\"month-12\" form=\"short\">Dec.</term>\n\n    <!-- SEASONS -->\n    <term name=\"season-01\">Spring</term>\n    <term name=\"season-02\">Summer</term>\n    <term name=\"season-03\">Autumn</term>\n    <term name=\"season-04\">Winter</term>\n  </terms>\n</locale>\n",
    "contentType": "Application/x-bibtex",
    "items": [
        {
            "10.5555/2904298.2904304": {
                "id": "10.5555/2904298.2904304",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Mackey",
                        "given": "Andrew L."
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            5,
                            1
                        ]
                    ]
                },
                "abstract": "Description: In this era of mobile devices, cloud computing, social networking and big data, organizations are collecting and generating data in massive quantities at unprecedented rates. The rapid decline in the cost of technology has enabled firms to expand their utilization of data analytics in an attempt to achieve a greater understanding of their business processes. As the tsunami of data continues to increase in volume, velocity and variety, the traditional methods of information management through which relational database systems were primarily, if not singularly, utilized have evolved to incorporate a combination of new hardware and software solutions to address the relatively nascent problems that accompany expanded data collection and utilization.",
                "call-number": "10.5555/2904298.2904304",
                "container-title": "J. Comput. Sci. Coll.",
                "ISSN": "1937-4771",
                "issue": "5",
                "number-of-pages": "2",
                "page": "38–39",
                "publisher": "Consortium for Computing Sciences in Colleges",
                "publisher-place": "Evansville, IN, USA",
                "source": "May 2016",
                "title": "Incorporating big data technology into computing curriculum: conference tutorial",
                "volume": "31"
            }
        },
        {
            "10.1145/3297156.3297249": {
                "id": "10.1145/3297156.3297249",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Samoylov",
                        "given": "Alexey"
                    },
                    {
                        "family": "Sergeev",
                        "given": "Nikolay"
                    },
                    {
                        "family": "Kucherova",
                        "given": "Margarita"
                    },
                    {
                        "family": "Denisov",
                        "given": "Boris"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            12,
                            8
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            12,
                            8
                        ]
                    ]
                },
                "abstract": "The success of data preparation for Big Data analytics directly depends on the quality of data integration from heterogeneous data sources. Extract, Transform and Load (ETL) systems have proved to be an efficient solution for this task. But to the moment, in the stages of data selection, definition of extraction rules and transformation, the decision is usually made exclusively by a data specialist. This, in turn, causes such problems as redundancy and inconsistency of imported data, narrow specialization of rules (up to uniqueness) with a limited number of analytical models and known requirements for the data mart. This paper presents the concept of solving the problem by providing methodological support for Big Data preparation procedure to efficiently collect data from a priory unknown heterogeneous data sources.",
                "call-number": "10.1145/3297156.3297249",
                "collection-title": "CSAI '18",
                "container-title": "Proceedings of the 2018 2nd International Conference on Computer Science and Artificial Intelligence",
                "DOI": "10.1145/3297156.3297249",
                "event-place": "Shenzhen, China",
                "ISBN": "9781450366069",
                "keyword": "data integration, modeling, heterogeneous data sources, knowledge extraction, semantics, Big Data, ETL",
                "number-of-pages": "5",
                "page": "131–135",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Methodology of Big Data Integration from A Priori Unknown Heterogeneous Data Sources",
                "URL": "https://doi.org/10.1145/3297156.3297249"
            }
        },
        {
            "10.1145/3477314.3507692": {
                "id": "10.1145/3477314.3507692",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Franciscatto",
                        "given": "Maria Helena"
                    },
                    {
                        "family": "Fabro",
                        "given": "Marcos Didonet Del"
                    },
                    {
                        "family": "Trois",
                        "given": "Celio"
                    },
                    {
                        "family": "Cabot",
                        "given": "Jordi"
                    },
                    {
                        "family": "Gonçalves",
                        "given": "Leon Augusto Okida"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2022,
                            4,
                            25
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2022,
                            4,
                            25
                        ]
                    ]
                },
                "abstract": "Multidimensional big data organizes information as data cubes, characterized by dimensions and measures. The existing multidimensional approaches usually do not focus on user assistance, meaning that users who are unaware of how the database is structured cannot easily analyze the data. Conversational interfaces, such as chatbots, can minimize these issues, by promoting a comprehensive interaction and facilitating the query formulation. Also, chatbots are able to capture intentions expressed in natural language, which is more user-friendly than other typical multidimensional query formats. This paper presents a chatbot approach for querying multidimensional big data, which captures users intentions through a conversation flow, linking them to multidimensional metadata. Through this process, the bot can set the query parameters and use them to access the data. We implemented the chatbot proposal for accessing an open database containing about 2.5 billions records and over 1700 attributes, including dimensions and metrics. When performing the querying process, the bot presents concise answers to the user.",
                "call-number": "10.1145/3477314.3507692",
                "collection-title": "SAC '22",
                "container-title": "Proceedings of the 37th ACM/SIGAPP Symposium on Applied Computing",
                "DOI": "10.1145/3477314.3507692",
                "event-place": "Virtual Event",
                "ISBN": "9781450387132",
                "keyword": "chatbots, information extraction, multidimensional data",
                "number-of-pages": "4",
                "page": "381–384",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Querying multidimensional big data through a chatbot system",
                "URL": "https://doi.org/10.1145/3477314.3507692"
            }
        },
        {
            "10.1145/3150928.3150943": {
                "id": "10.1145/3150928.3150943",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Llwaah",
                        "given": "Faris"
                    },
                    {
                        "family": "Cała",
                        "given": "Jacek"
                    },
                    {
                        "family": "Thomas",
                        "given": "Nigel"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            12,
                            5
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            12,
                            5
                        ]
                    ]
                },
                "abstract": "Modelling and simulation of Big Data analytics processes running in the cloud is a difficult problem which introduces many challenges. The major one is the collection of training data which is scarce and costly to obtain, due to large scale and long runtime of those processes. In our previous work, we proposed a methodology to model, simulate and predict the runtime of Big Data processes such as complex Next Generation Sequencing (NGS) pipelines. The major contribution of our simulation methodology is that it provides a reasonable prediction of runtime for testing data much larger than the training inputs. To further improve the accuracy of prediction we present now an extension of our previous work that can model cloud data storage. Our simulation framework is based on CloudSim and WorkflowSim, to which we have added a shared storage component. We present the design and implementation of the storage extension together with an evaluation performed on selected scientific workflows: the Pegasus Montage workflow and NGS pipeline implemented in e-Science Central. The evaluation shows that the proposed extension works correctly and can improve prediction accuracy for our largest 390 GB input dataset by about 16% when compared to previous results.",
                "call-number": "10.1145/3150928.3150943",
                "collection-title": "VALUETOOLS 2017",
                "container-title": "Proceedings of the 11th EAI International Conference on Performance Evaluation Methodologies and Tools",
                "DOI": "10.1145/3150928.3150943",
                "event-place": "Venice, Italy",
                "ISBN": "9781450363464",
                "keyword": "WorkflowSim, Big data, Next generation sequencing pipeline, Data-intensive simulation",
                "number-of-pages": "8",
                "page": "74–81",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Runtime Performance Prediction of Big Data Workflows with I/O-aware Simulation",
                "URL": "https://doi.org/10.1145/3150928.3150943"
            }
        },
        {
            "10.1145/3529190.3529222": {
                "id": "10.1145/3529190.3529222",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Pleimling",
                        "given": "Xavier"
                    },
                    {
                        "family": "Shah",
                        "given": "Vedant"
                    },
                    {
                        "family": "Lourentzou",
                        "given": "Ismini"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2022,
                            6,
                            29
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2022,
                            6,
                            29
                        ]
                    ]
                },
                "abstract": "As large-scale machine learning models become more prevalent in assistive and pervasive technologies, the research community has started examining limitations and challenges that arise from training data, e.g., fairness, bias, and interpretability issues. To this end, data-centric approaches are increasingly prevailing over time, showing that high-quality data is a critical component in many applications. Several studies explore methods to define and improve data quality, however, no uniform definition exists. In this work, we present an empirical analysis of the multifaceted problem of evaluating data quality. Our work aims at identifying data quality challenges that are most commonly observed by data users and practitioners. Inspired by the need for generally applicable methods, we select a representative set of quality indicators, that covers a broad spectrum of issues, and investigate the utility of these indicators on a broad range of datasets through inter-annotator agreement analysis. Our work provides insights and presents open challenges in designing improved data life cycles.",
                "call-number": "10.1145/3529190.3529222",
                "collection-title": "PETRA '22",
                "container-title": "Proceedings of the 15th International Conference on PErvasive Technologies Related to Assistive Environments",
                "DOI": "10.1145/3529190.3529222",
                "event-place": "Corfu, Greece",
                "ISBN": "9781450396318",
                "keyword": "data utility, data annotation, incomplete data, incorrect data, datasets, user survey, inconsistent data, data quality, data quality metrics, duplicate data",
                "number-of-pages": "7",
                "page": "118–124",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "[Data] Quality Lies In The Eyes Of The Beholder",
                "URL": "https://doi.org/10.1145/3529190.3529222"
            }
        },
        {
            "10.1145/2666652.2666664": {
                "id": "10.1145/2666652.2666664",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Xiang",
                        "given": "Junlong"
                    },
                    {
                        "family": "Westerlund",
                        "given": "Magnus"
                    },
                    {
                        "family": "Sovilj",
                        "given": "Dušan"
                    },
                    {
                        "family": "Pulkkis",
                        "given": "Göran"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2014,
                            11,
                            7
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2014,
                            11,
                            7
                        ]
                    ]
                },
                "abstract": "Extending state-of-the-art machine learning algorithms to highly scalable (big data) analysis environments is crucial for the handling of authentic datasets in Intrusion Detection Systems (IDS). Traditional supervised learning methods are considered to be too slow for use in these environments. Therefore, we propose the use of Extreme Learning Machine (ELM) for detecting network intrusion attempts. We show they hold great promise for the field by employing a MapReduce based variant evaluated on the open source tool Hadoop.",
                "call-number": "10.1145/2666652.2666664",
                "collection-title": "AISec '14",
                "container-title": "Proceedings of the 2014 Workshop on Artificial Intelligent and Security Workshop",
                "DOI": "10.1145/2666652.2666664",
                "event-place": "Scottsdale, Arizona, USA",
                "ISBN": "9781450331531",
                "keyword": "big data, extreme learning machine, classification, mapreduce, intrusion detection",
                "number-of-pages": "10",
                "page": "73–82",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Using extreme learning machine for intrusion detection in a big data environment",
                "URL": "https://doi.org/10.1145/2666652.2666664"
            }
        },
        {
            "10.1145/2740965": {
                "id": "10.1145/2740965",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Sha",
                        "given": "Kewei"
                    },
                    {
                        "family": "Zeadally",
                        "given": "Sherali"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2015,
                            6,
                            3
                        ]
                    ]
                },
                "call-number": "10.1145/2740965",
                "collection-number": "8",
                "container-title": "J. Data and Information Quality",
                "DOI": "10.1145/2740965",
                "ISSN": "1936-1955",
                "issue": "2-3",
                "keyword": "faculty data detection, cyber physical systems, Data quality",
                "number": "Article 8",
                "number-of-pages": "4",
                "page": "1–4",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "July 2015",
                "title": "Data Quality Challenges in Cyber-Physical Systems",
                "URL": "https://doi.org/10.1145/2740965",
                "volume": "6"
            }
        },
        {
            "10.1145/3148239": {
                "id": "10.1145/3148239",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Bertossi",
                        "given": "Leopoldo"
                    },
                    {
                        "family": "Milani",
                        "given": "Mostafa"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            1,
                            27
                        ]
                    ]
                },
                "abstract": "Data quality assessment and data cleaning are context-dependent activities. Motivated by this observation, we propose the Ontological Multidimensional Data Model (OMD model), which can be used to model and represent contexts as logic-based ontologies. The data under assessment are mapped into the context for additional analysis, processing, and quality data extraction. The resulting contexts allow for the representation of dimensions, and multidimensional data quality assessment becomes possible. At the core of a multidimensional context, we include a generalized multidimensional data model and a Datalog± ontology with provably good properties in terms of query answering. These main components are used to represent dimension hierarchies, dimensional constraints, and dimensional rules and define predicates for quality data specification. Query answering relies on and triggers navigation through dimension hierarchies and becomes the basic tool for the extraction of quality data. The OMD model is interesting per se beyond applications to data quality. It allows for a logic-based and computationally tractable representation of multidimensional data, extending previous multidimensional data models with additional expressive power and functionalities.",
                "call-number": "10.1145/3148239",
                "collection-number": "14",
                "container-title": "J. Data and Information Quality",
                "DOI": "10.1145/3148239",
                "ISSN": "1936-1955",
                "issue": "3",
                "keyword": "weakly-sticky programs, query answering, Datalog±, Ontology-based data access",
                "number": "Article 14",
                "number-of-pages": "36",
                "page": "1–36",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "September 2017",
                "title": "Ontological Multidimensional Data Models and Contextual Data Quality",
                "URL": "https://doi.org/10.1145/3148239",
                "volume": "9"
            }
        },
        {
            "10.1145/2978571": {
                "id": "10.1145/2978571",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Ye",
                        "given": "Conghuan"
                    },
                    {
                        "family": "Ling",
                        "given": "Hefei"
                    },
                    {
                        "family": "Xiong",
                        "given": "Zenggang"
                    },
                    {
                        "family": "Zou",
                        "given": "Fuhao"
                    },
                    {
                        "family": "Liu",
                        "given": "Cong"
                    },
                    {
                        "family": "Xu",
                        "given": "Fang"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            9,
                            15
                        ]
                    ]
                },
                "abstract": "With the advent of social networks and cloud computing, the amount of multimedia data produced and communicated within social networks is rapidly increasing. In the meantime, social networking platforms based on cloud computing have made multimedia big data sharing in social networks easier and more efficient. The growth of social multimedia, as demonstrated by social networking sites such as Facebook and YouTube, combined with advances in multimedia content analysis, underscores potential risks for malicious use, such as illegal copying, piracy, plagiarism, and misappropriation. Therefore, secure multimedia sharing and traitor tracing issues have become critical and urgent in social networks. In this article, a joint fingerprinting and encryption (JFE) scheme based on tree-structured Haar wavelet transform (TSHWT) is proposed with the purpose of protecting media distribution in social network environments. The motivation is to map hierarchical community structure of social networks into a tree structure of Haar wavelet transform for fingerprinting and encryption. First, fingerprint code is produced using social network analysis (SNA). Second, the content is decomposed based on the structure of fingerprint code by the TSHWT. Then, the content is fingerprinted and encrypted in the TSHWT domain. Finally, the encrypted contents are delivered to users via hybrid multicast-unicast. The proposed method, to the best of our knowledge, is the first scalable JFE method for fingerprinting and encryption in the TSHWT domain using SNA. The use of fingerprinting along with encryption using SNA not only provides a double layer of protection for social multimedia sharing in social network environment but also avoids big data superposition effect. Theory analysis and experimental results show the effectiveness of the proposed JFE scheme.",
                "call-number": "10.1145/2978571",
                "collection-number": "61",
                "container-title": "ACM Trans. Multimedia Comput. Commun. Appl.",
                "DOI": "10.1145/2978571",
                "ISSN": "1551-6857",
                "issue": "4s",
                "keyword": "Tree-structured Haar wavelet transform (TSHWT), social network, collusion attack, social multimedia big data sharing, joint fingerprinting and encryption",
                "number": "Article 61",
                "number-of-pages": "23",
                "page": "1–23",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "November 2016",
                "title": "Secure Social Multimedia Big Data Sharing Using Scalable JFE in the TSHWT Domain",
                "URL": "https://doi.org/10.1145/2978571",
                "volume": "12"
            }
        },
        {
            "10.1145/2978575": {
                "id": "10.1145/2978575",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Li",
                        "given": "Yibin"
                    },
                    {
                        "family": "Gai",
                        "given": "Keke"
                    },
                    {
                        "family": "Ming",
                        "given": "Zhong"
                    },
                    {
                        "family": "Zhao",
                        "given": "Hui"
                    },
                    {
                        "family": "Qiu",
                        "given": "Meikang"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            9,
                            15
                        ]
                    ]
                },
                "abstract": "The dramatically growing demand of Cyber Physical and Social Computing (CPSC) has enabled a variety of novel channels to reach services in the financial industry. Combining cloud systems with multimedia big data is a novel approach for Financial Service Institutions (FSIs) to diversify service offerings in an efficient manner. However, the security issue is still a great issue in which the service availability often conflicts with the security constraints when the service media channels are varied. This paper focuses on this problem and proposes a novel approach using the Semantic-Based Access Control (SBAC) techniques for acquiring secure financial services on multimedia big data in cloud computing. The proposed approach is entitled IntercroSsed Secure Big Multimedia Model (2SBM), which is designed to secure accesses between various media through the multiple cloud platforms. The main algorithms supporting the proposed model include the Ontology-Based Access Recognition (OBAR) Algorithm and the Semantic Information Matching (SIM) Algorithm. We implement an experimental evaluation to prove the correctness and adoptability of our proposed scheme.",
                "call-number": "10.1145/2978575",
                "collection-number": "67",
                "container-title": "ACM Trans. Multimedia Comput. Commun. Appl.",
                "DOI": "10.1145/2978575",
                "ISSN": "1551-6857",
                "issue": "4s",
                "keyword": "secure financial services, Intercrossed access control, trust management, multimedia big data, cloud computing",
                "number": "Article 67",
                "number-of-pages": "18",
                "page": "1–18",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "November 2016",
                "title": "Intercrossed Access Controls for Secure Financial Services on Multimedia Big Data in Cloud Systems",
                "URL": "https://doi.org/10.1145/2978575",
                "volume": "12"
            }
        },
        {
            "10.1145/3400903.3400932": {
                "id": "10.1145/3400903.3400932",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Uzunbaz",
                        "given": "Serkan"
                    },
                    {
                        "family": "Aref",
                        "given": "Walid G."
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2020,
                            7,
                            7
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2020,
                            7,
                            7
                        ]
                    ]
                },
                "abstract": "Business Data Analytics require processing of large numbers of data streams and the creation of materialized views in order to provide near real-time answers to user queries. Materializing the view of each query and refreshing it continuously as a separate query execution plan is not efficient and is not scalable. In this paper, we present a global query execution plan to simultaneously support multiple queries, and minimize the number of input scans, operators, and tuples flowing between the operators. We propose shared-execution techniques for creating and maintaining materialized views in support of business data analytics queries. We utilize commonalities in multiple business data analytics queries to support scalable and efficient processing of big data streams. The paper highlights shared execution techniques for select predicates, group, and aggregate calculations. We present how global query execution plans are run in a distributed stream processing system, called INGA which is built on top of Storm. In INGA, we are able to support online view maintenance of 2500 materialized views using 237 queries by utilizing the shared constructs between the queries. We are able to run all 237 queries using a single global query execution plan tree with depth of 21.",
                "call-number": "10.1145/3400903.3400932",
                "collection-number": "25",
                "collection-title": "SSDBM 2020",
                "container-title": "32nd International Conference on Scientific and Statistical Database Management",
                "DOI": "10.1145/3400903.3400932",
                "event-place": "Vienna, Austria",
                "ISBN": "9781450388146",
                "keyword": "stream processing, INGA, online view maintenance, big data, business data analytics",
                "number": "Article 25",
                "number-of-pages": "4",
                "page": "1–4",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Shared Execution Techniques for Business Data Analytics over Big Data Streams",
                "URL": "https://doi.org/10.1145/3400903.3400932"
            }
        },
        {
            "10.1145/3092255.3092272": {
                "id": "10.1145/3092255.3092272",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Bruno",
                        "given": "Rodrigo"
                    },
                    {
                        "family": "Oliveira",
                        "given": "Luís Picciochi"
                    },
                    {
                        "family": "Ferreira",
                        "given": "Paulo"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            6,
                            18
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            6,
                            18
                        ]
                    ]
                },
                "abstract": "Big Data applications suffer from unpredictable and unacceptably high pause times due to Garbage Collection (GC). This is the case in latency-sensitive applications such as on-line credit-card fraud detection, graph-based computing for analysis on social networks, etc. Such pauses compromise latency requirements of the whole application stack and result from applications' aggressive buffering/caching of data, exposing an ill-suited GC design, which assumes that most objects will die young and does not consider that applications hold large amounts of middle-lived data in memory. To avoid such pauses, we propose NG2C, a new GC algorithm that combines pretenuring with user-defined dynamic generations. By being able to allocate objects into different generations, NG2C is able to group objects with similar lifetime profiles in the same generation. By allocating objects with similar lifetime profiles close to each other, i.e. in the same generation, we avoid object promotion (copying between generations) and heap fragmentation (which leads to heap compactions) both responsible for most of the duration of HotSpot GC pause times. NG2C is implemented for the OpenJDK 8 HotSpot Java Virtual Machine, as an extension of the Garbage First GC. We evaluate NG2C using Cassandra, Lucene, and GraphChi with three different GCs: Garbage First (G1), Concurrent Mark Sweep (CMS), and NG2C. Results show that NG2C decreases the worst observable GC pause time by up to 94.8% for Cassandra, 85.0% for Lucene and 96.45% for GraphChi, when compared to current collectors (G1 and CMS). In addition, NG2c has no negative impact on application throughput or memory usage.",
                "call-number": "10.1145/3092255.3092272",
                "collection-title": "ISMM 2017",
                "container-title": "Proceedings of the 2017 ACM SIGPLAN International Symposium on Memory Management",
                "DOI": "10.1145/3092255.3092272",
                "event-place": "Barcelona, Spain",
                "ISBN": "9781450350440",
                "keyword": "Latency, Garbage Collection, Big Data",
                "number-of-pages": "12",
                "page": "2–13",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "NG2C: pretenuring garbage collection with dynamic generations for HotSpot big data applications",
                "URL": "https://doi.org/10.1145/3092255.3092272"
            }
        },
        {
            "10.1145/3156685.3092272": {
                "id": "10.1145/3156685.3092272",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Bruno",
                        "given": "Rodrigo"
                    },
                    {
                        "family": "Oliveira",
                        "given": "Luís Picciochi"
                    },
                    {
                        "family": "Ferreira",
                        "given": "Paulo"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            6,
                            18
                        ]
                    ]
                },
                "abstract": "Big Data applications suffer from unpredictable and unacceptably high pause times due to Garbage Collection (GC). This is the case in latency-sensitive applications such as on-line credit-card fraud detection, graph-based computing for analysis on social networks, etc. Such pauses compromise latency requirements of the whole application stack and result from applications' aggressive buffering/caching of data, exposing an ill-suited GC design, which assumes that most objects will die young and does not consider that applications hold large amounts of middle-lived data in memory. To avoid such pauses, we propose NG2C, a new GC algorithm that combines pretenuring with user-defined dynamic generations. By being able to allocate objects into different generations, NG2C is able to group objects with similar lifetime profiles in the same generation. By allocating objects with similar lifetime profiles close to each other, i.e. in the same generation, we avoid object promotion (copying between generations) and heap fragmentation (which leads to heap compactions) both responsible for most of the duration of HotSpot GC pause times. NG2C is implemented for the OpenJDK 8 HotSpot Java Virtual Machine, as an extension of the Garbage First GC. We evaluate NG2C using Cassandra, Lucene, and GraphChi with three different GCs: Garbage First (G1), Concurrent Mark Sweep (CMS), and NG2C. Results show that NG2C decreases the worst observable GC pause time by up to 94.8% for Cassandra, 85.0% for Lucene and 96.45% for GraphChi, when compared to current collectors (G1 and CMS). In addition, NG2c has no negative impact on application throughput or memory usage.",
                "call-number": "10.1145/3156685.3092272",
                "container-title": "SIGPLAN Not.",
                "DOI": "10.1145/3156685.3092272",
                "ISSN": "0362-1340",
                "issue": "9",
                "keyword": "Garbage Collection, Latency, Big Data",
                "number-of-pages": "12",
                "page": "2–13",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "September 2017",
                "title": "NG2C: pretenuring garbage collection with dynamic generations for HotSpot big data applications",
                "URL": "https://doi.org/10.1145/3156685.3092272",
                "volume": "52"
            }
        },
        {
            "10.1145/2658840": {
                "id": "10.1145/2658840",
                "type": "BOOK",
                "issued": {
                    "date-parts": [
                        [
                            2014
                        ]
                    ]
                },
                "abstract": "The trend of bigger and bigger data---in terms of volume, velocity, and variety---is inevitable. Ultimately, how \"big data\" will impact the broad population of users rests on what value we can bring to them. Historically, the database community has focused primarily on efficient processing of structured queries posed by expert users on preorganized data. But this focus only addresses one of the many different challenges in bringing the value of big data to users. Besides making queries and analysis faster and more scalable, we must address the pain points before and after analytics---i.e., how to put together data from diverse sources and \"wrangle\" it into representations appropriate for analyses, and how to communicate results and insights effectively. To broaden the impact of big data, we must also move beyond our traditional notions of \"users,\" such as programmers and analysts, to a much wider range of new user archetypes, such as nonexpert users who want to \"get something\" from their data, or ordinary citizens who wish to play a more active role in understanding public data.",
                "call-number": "10.1145/2658840",
                "container-title-short": "Data4U '14",
                "event-place": "Hangzhou, China",
                "genre": "proceeding",
                "ISBN": "9781450331869",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Proceedings of the First International Workshop on Bringing the Value of \"Big Data\" to Users (Data4U 2014)"
            }
        },
        {
            "10.1007/s00778-017-0489-y": {
                "id": "10.1007/s00778-017-0489-y",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Sagi",
                        "given": "Tomer"
                    },
                    {
                        "family": "Gal",
                        "given": "Avigdor"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            2,
                            1
                        ]
                    ]
                },
                "abstract": "The evolution of data accumulation, management, analytics, and visualization has led to the coining of the term big data, which challenges the task of data integration. This task, common to any matching problem in computer science involves generating alignments between structured data in an automated fashion. Historically, set-based measures, based upon binary similarity matrices (match/non-match), have dominated evaluation practices of matching tasks. However, in the presence of big data, such measures no longer suffice. In this work, we propose evaluation methods for non-binary matrices as well. Non-binary evaluation is formally defined together with several new, non-binary measures using a vector space representation of matching outcome. We provide empirical analyses of the usefulness of non-binary evaluation and show its superiority over its binary counterparts in several problem domains.",
                "call-number": "10.1007/s00778-017-0489-y",
                "container-title": "The VLDB Journal",
                "DOI": "10.1007/s00778-017-0489-y",
                "ISSN": "1066-8888",
                "issue": "1",
                "keyword": "Matching, Evaluation, Data integration",
                "number-of-pages": "22",
                "page": "105–126",
                "publisher": "Springer-Verlag",
                "publisher-place": "Berlin, Heidelberg",
                "source": "February  2018",
                "title": "Non-binary evaluation measures for big data integration",
                "URL": "https://doi.org/10.1007/s00778-017-0489-y",
                "volume": "27"
            }
        },
        {
            "10.1109/CCGrid.2016.63": {
                "id": "10.1109/CCGrid.2016.63",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Ordonez",
                        "given": "Carlos"
                    },
                    {
                        "family": "García-García",
                        "given": "Javier"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            5,
                            16
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2016,
                            5,
                            16
                        ]
                    ]
                },
                "abstract": "A big data analytics workflow is long and complex, with many programs, tools and scripts interacting together. In general, in modern organizations there is a significant amount of big data analytics processing performed outside a database system, which creates many issues to manage and process big data analytics workflows. In general, data preprocessing is the most time-consuming task in a big data analytics workflow. In this work, we defend the idea of preprocessing, computing models and scoring data sets inside a database system. In addition, we discuss recommendations and experiences to improve big data analytics workflows by pushing data preprocessing (i.e. data cleaning, aggregation and column transformation) into a database system. We present a discussion of practical issues and common solutions when transforming and preparing data sets to improve big data analytics workflows. As a case study validation, based on experience from real-life big data analytics projects, we compare pros and cons between running big data analytics workflows inside and outside the database system. We highlight which tasks in a big data analytics workflow are easier to manage and faster when processed by the database system, compared to external processing.",
                "call-number": "10.1109/CCGrid.2016.63",
                "collection-title": "CCGRID '16",
                "container-title": "Proceedings of the 16th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing",
                "DOI": "10.1109/CCGrid.2016.63",
                "event-place": "Cartagena, Columbia",
                "ISBN": "9781509024520",
                "number-of-pages": "7",
                "page": "649–655",
                "publisher": "IEEE Press",
                "title": "Managing big data analytics workflows with a database system",
                "URL": "https://doi.org/10.1109/CCGrid.2016.63"
            }
        },
        {
            "10.1145/2950290.2983930": {
                "id": "10.1145/2950290.2983930",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Gulzar",
                        "given": "Muhammad Ali"
                    },
                    {
                        "family": "Interlandi",
                        "given": "Matteo"
                    },
                    {
                        "family": "Condie",
                        "given": "Tyson"
                    },
                    {
                        "family": "Kim",
                        "given": "Miryung"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            11,
                            1
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2016,
                            11,
                            1
                        ]
                    ]
                },
                "abstract": "To process massive quantities of data, developers leverage data-intensive scalable computing (DISC) systems in the cloud, such as Google's MapReduce, Apache Hadoop, and Apache Spark. In terms of debugging, DISC systems support post-mortem log analysis but do not provide interactive debugging features in realtime. This tool demonstration paper showcases a set of concrete usecases on how BigDebug can help debug Big Data Applications by providing interactive, realtime debug primitives. To emulate interactive step-wise debugging without reducing throughput, BigDebug provides simulated breakpoints to enable a user to inspect a program without actually pausing the entire computation. To minimize unnecessary communication and data transfer, BigDebug provides on-demand watchpoints that enable a user to retrieve intermediate data using a guard and transfer the selected data on demand. To support systematic and efficient trial-and-error debugging, BigDebug also enables users to change program logic in response to an error at runtime and replay the execution from that step. BigDebug is available for download at http://web.cs.ucla.edu/~miryung/software.html",
                "call-number": "10.1145/2950290.2983930",
                "collection-title": "FSE 2016",
                "container-title": "Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering",
                "DOI": "10.1145/2950290.2983930",
                "event-place": "Seattle, WA, USA",
                "ISBN": "9781450342186",
                "keyword": "interactive tools, Debugging, data-intensive scalable computing (DISC), big data analytics, fault localization and recovery",
                "number-of-pages": "5",
                "page": "1033–1037",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "BigDebug: interactive debugger for big data analytics in Apache Spark",
                "URL": "https://doi.org/10.1145/2950290.2983930"
            }
        },
        {
            "10.1145/3436286.3436294": {
                "id": "10.1145/3436286.3436294",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Wang",
                        "given": "Xiaoze"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2020,
                            4,
                            28
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2020,
                            4,
                            28
                        ]
                    ]
                },
                "abstract": "This essay mainly considers the advantages of cloud accounting over traditional accounting in the context of big data, as well as the difficulties in the initial stage of cloud accounting construction. The article analyzed the widely used model in the market called the Software as a Service (SaaS) online cloud accounting in detail with an example of A company. At last, the essay summarizes the universal challenges faced by the construction of cloud accounting for small and medium-sized enterprises, and puts forward corresponding targeted suggestions, in order to provide beneficial reference for the implementation of cloud accounting for small and medium-sized enterprises (SMEs)",
                "call-number": "10.1145/3436286.3436294",
                "collection-title": "ISBDAI '20",
                "container-title": "Proceedings of the 2020 2nd International Conference on Big Data and Artificial Intelligence",
                "DOI": "10.1145/3436286.3436294",
                "event-place": "Johannesburg, South Africa",
                "ISBN": "9781450376457",
                "keyword": "Traditional accounting, Software as a Service (SaaS), Cloud accounting, Small and medium-sized enterprises (SMEs)",
                "number-of-pages": "6",
                "page": "37–42",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Research on the application of cloud accounting in small and medium-sized enterprises under the background of big data",
                "URL": "https://doi.org/10.1145/3436286.3436294"
            }
        },
        {
            "10.1145/3185768.3186299": {
                "id": "10.1145/3185768.3186299",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Uta",
                        "given": "Alexandru"
                    },
                    {
                        "family": "Obaseki",
                        "given": "Harry"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            4,
                            2
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            4,
                            2
                        ]
                    ]
                },
                "abstract": "Public cloud computing platforms are a cost-effective solution for individuals and organizations to deploy various types of workloads, ranging from scientific applications, business-critical workloads, e-governance to big data applications. Co-locating all such different types of workloads in a single datacenter leads not only to performance degradation, but also to large degrees of performance variability, which is the result of virtualization, resource sharing and congestion. Many studies have already assessed and characterized the degree of resource variability in public clouds. However, we are missing a clear picture on how resource variability impacts big data workloads. In this work, we take a step towards characterizing the behavior of big data workloads under network bandwidth variability. Emulating real-world clouds» bandwidth distribution, we characterize the performance achieved by running real-world big data applications. We find that most big data workloads are slowed down under network variability scenarios, even those that are not network-bound. Moreover, the maximum average slowdown for the cloud setup with highest variability is 1.48 for CPU-bound workloads, and 1.79 for network-bound workloads.",
                "call-number": "10.1145/3185768.3186299",
                "collection-title": "ICPE '18",
                "container-title": "Companion of the 2018 ACM/SPEC International Conference on Performance Engineering",
                "DOI": "10.1145/3185768.3186299",
                "event-place": "Berlin, Germany",
                "ISBN": "9781450356299",
                "keyword": "heterogeneity, spark, network variability, performance variability, big data, performance predictability",
                "number-of-pages": "6",
                "page": "113–118",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "A Performance Study of Big Data Workloads in Cloud Datacenters with Network Variability",
                "URL": "https://doi.org/10.1145/3185768.3186299"
            }
        },
        {
            "10.5555/3192424.3192600": {
                "id": "10.5555/3192424.3192600",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Xylogiannopoulos",
                        "given": "Konstantinos F."
                    },
                    {
                        "family": "Karampelas",
                        "given": "Panagiotis"
                    },
                    {
                        "family": "Alhajj",
                        "given": "Reda"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            8,
                            18
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2016,
                            8,
                            18
                        ]
                    ]
                },
                "abstract": "Big data streaming analysis nowadays has become one of the most important topic in the list of data analysts since enormous amount of data are produced daily by the numerous smart devices. The analysis of such data is very important and the detection of frequent or even non-frequent patterns can be critical for many aspects of our lives. In the current paper, we propose a new methodology based on our previous work regarding the detection of all repeated patterns in a string in order to analyze a very big data stream with 1 Trillion digits, composed from 1 thousand subsequences of 1 billion digits each one. More specifically, using the novel data structure, LERP Reduced Suffix Array, and the innovative ARPaD algorithm which allows the detection of all repeated patterns in a string we managed to analyze each one of the 1 billion data points, using 10 computers with standard hardware configuration, in 33 minutes which outperforms to the best of our knowledge any other existing methodology, which is equivalent to data point generation every 2 microseconds.",
                "call-number": "10.5555/3192424.3192600",
                "collection-title": "ASONAM '16",
                "container-title": "Proceedings of the 2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
                "event-place": "Davis, California",
                "ISBN": "9781509028467",
                "keyword": "big data, ARPaD, data stream, LERP-RSA",
                "number-of-pages": "8",
                "page": "931–938",
                "publisher": "IEEE Press",
                "title": "Frequent and non-frequent pattern detection in big data streams: an experimental simulation in 1 trillion data points"
            }
        },
        {
            "10.5555/3291291.3291307": {
                "id": "10.5555/3291291.3291307",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Lopez",
                        "given": "Eduardo"
                    },
                    {
                        "family": "Sartipi",
                        "given": "Kamran"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            10,
                            29
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            10,
                            29
                        ]
                    ]
                },
                "abstract": "The increasing availability of very large volumes of digital data (i.e. Big Data) enables many interesting research streams on a wide variety of phenomena. However, there has been a paucity of Big Data sets in the area of cybersecurity in information systems, as organizations are reluctant to share data that may provide too much unrestricted visibility into their operations. In this study, we explore the use of a real-life, anonymized, very large dataset containing user behavior - as captured in log files - including both regular usage as well as misuse, typifying the dynamics found in a situation with compromised user credentials. Through the experiment, we validate that the existence of a large user behavior dataset in itself does not necessarily guarantee that abnormal behaviors can be found. It is essential that researchers apply deep domain knowledge, critical thinking and practical focus to ensure the data can produce the knowledge required for the ultimate objective of detecting an insider's threat. In this paper we develop, formulate and calculate the features that best represent user behavior in the underlying information systems, maintaining a parsimonious balance between complexity, resource demands and detection effectiveness. We test the use of a classification model that proves the usefulness and aplicability of the features extracted.",
                "call-number": "10.5555/3291291.3291307",
                "collection-title": "CASCON '18",
                "container-title": "Proceedings of the 28th Annual International Conference on Computer Science and Software Engineering",
                "event-place": "Markham, Ontario, Canada",
                "keyword": "big data security, insider's threat, anomaly detection, feature engineering, predicting misuse",
                "number-of-pages": "12",
                "page": "145–156",
                "publisher": "IBM Corp.",
                "publisher-place": "USA",
                "title": "Feature engineering in big data for detection of information systems misuse"
            }
        },
        {
            "10.1145/3321454.3321461": {
                "id": "10.1145/3321454.3321461",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Zhao",
                        "given": "Haijun"
                    },
                    {
                        "family": "Zhao",
                        "given": "Bang"
                    },
                    {
                        "family": "Cheng",
                        "given": "Susu"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            2,
                            20
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            2,
                            20
                        ]
                    ]
                },
                "abstract": "The confirmation of data property rights is one of the most important functions of big data trading institutions. This paper builds a specialized classifier which aims to the normalization process of data property confirmation in the process of big data transaction and proposes the mechanism of confirming big data property rights based on Smart Contract.",
                "call-number": "10.1145/3321454.3321461",
                "collection-title": "ICIIT '19",
                "container-title": "Proceedings of the 2019 4th International Conference on Intelligent Information Technology",
                "DOI": "10.1145/3321454.3321461",
                "event-place": "Da, Nang, Viet Nam",
                "ISBN": "9781450366335",
                "keyword": "Smart contract, Mechanism, Big Data Exchange, Confirmation of Data Property",
                "number-of-pages": "5",
                "page": "78–82",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "The Mechanism of Confirming Big Data Property Rights Based on Smart Contract",
                "URL": "https://doi.org/10.1145/3321454.3321461"
            }
        },
        {
            "10.1145/3373419.3373440": {
                "id": "10.1145/3373419.3373440",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Zhai",
                        "given": "Chenggong"
                    },
                    {
                        "family": "Zhou",
                        "given": "Shanbo"
                    },
                    {
                        "family": "Zhang",
                        "given": "Gaoyang"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            11,
                            8
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            11,
                            8
                        ]
                    ]
                },
                "abstract": "This paper mainly uses big data technology to analyze and statistics the distribution of conscription number over the years. Based on this result, the budget and preset of conscription material are made. Finally, the classification of conscription number is adjusted according to the characteristics of conscription recruits and the feedback of conscription distribution.",
                "call-number": "10.1145/3373419.3373440",
                "collection-title": "ICAIP 2019",
                "container-title": "Proceedings of the 2019 3rd International Conference on Advances in Image Processing",
                "DOI": "10.1145/3373419.3373440",
                "event-place": "Chengdu, China",
                "ISBN": "9781450376754",
                "keyword": "Big Data, conscription recruits, Cloths",
                "number-of-pages": "5",
                "page": "119–123",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Research on Optimizing the Support of Conscription Recruits Cloths Based on Big Data",
                "URL": "https://doi.org/10.1145/3373419.3373440"
            }
        },
        {
            "10.1145/3481127.3481215": {
                "id": "10.1145/3481127.3481215",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Jiang",
                        "given": "Shudong"
                    },
                    {
                        "family": "Xie",
                        "given": "Jiagui"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            7,
                            17
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2021,
                            7,
                            17
                        ]
                    ]
                },
                "abstract": "The world has entered into the era of Big Data unwittingly. Although no universally accepted definition can be given as of now, the term “Big Data” is commonly referred to as extensive datasets, primarily in the characteristics of 5 Vs: value, variability, variety, velocity, and volume, which require scalable architectures for efficient storage, manipulation, and analysis [1]. Higher education institutions are encouraged to construct a shared E-learning resources database based on Big Data and Cloud because of its convenience and cost-savings to E-learning teachers in terms of data crawling, storage, analysis, as well as data processing, optimization and sharing [2]. This article introduces the conceptual underlying frameworks for developing a shared E-learning resources database based on Big Data and Cloud as well as its prospective benefits to teachers, students, and university-corporation connections. Its prospective application holds a vision to assist associated teachers for improved efficiency and effectiveness of data acquisition, teaching material preparation and processing, user (students) satisfaction and prominent university-corporation connections.",
                "call-number": "10.1145/3481127.3481215",
                "collection-title": "ICEME 2021",
                "container-title": "The 2021 12th International Conference on E-business, Management and Economics",
                "DOI": "10.1145/3481127.3481215",
                "event-place": "Beijing, China",
                "ISBN": "9781450390064",
                "keyword": "Keyword: E-learning,educational resources,Cloud,Big Data,Hadoop",
                "number-of-pages": "6",
                "page": "770–775",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "A Shared E-Learning Resources Database Using Big Data and Cloud Environment",
                "URL": "https://doi.org/10.1145/3481127.3481215"
            }
        },
        {
            "10.1145/2424840.2424842": {
                "id": "10.1145/2424840.2424842",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Wilkes",
                        "given": "Stephany"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2012,
                            6,
                            1
                        ]
                    ]
                },
                "abstract": "Two shifts in the technological landscape -- the era of \"big data\" and the popularity of Agile software development methodologies -- have made users (and specifically data about them) central to the development process and broadened the definition of user-centered design and usability testing. This paper briefly describes the impact of these shifts on the usability practice. Rudimentary data types useful to usability practitioners are introduced, as well as helpful data tools and required skills. The paper concludes with a list of methodological and pedagogical gaps that should be addressed.",
                "call-number": "10.1145/2424840.2424842",
                "container-title": "Commun. Des. Q. Rev",
                "DOI": "10.1145/2424840.2424842",
                "ISSN": "2166-1200",
                "issue": "2",
                "keyword": "ACM proceedings, SIGDOC",
                "number-of-pages": "8",
                "page": "25–32",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "June 2012",
                "title": "Some impacts of \"big data\" on usability practice",
                "URL": "https://doi.org/10.1145/2424840.2424842",
                "volume": "13"
            }
        },
        {
            "10.1145/3386687": {
                "id": "10.1145/3386687",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Bertossi",
                        "given": "Leopoldo"
                    },
                    {
                        "family": "Geerts",
                        "given": "Floris"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2020,
                            4,
                            30
                        ]
                    ]
                },
                "abstract": "In this work, we provide some insights and develop some ideas, with few technical details, about the role of explanations in Data Quality in the context of data-based machine learning models (ML). In this direction, there are, as expected, roles for causality, and explainable artificial intelligence. The latter area not only sheds light on the models, but also on the data that support model construction. There is also room for defining, identifying, and explaining errors in data, in particular, in ML, and also for suggesting repair actions. More generally, explanations can be used as a basis for defining dirty data in the context of ML, and measuring or quantifying them. We think dirtiness as relative to the ML task at hand, e.g., classification.",
                "call-number": "10.1145/3386687",
                "collection-number": "11",
                "container-title": "J. Data and Information Quality",
                "DOI": "10.1145/3386687",
                "ISSN": "1936-1955",
                "issue": "2",
                "keyword": "Machine learning, fairness, causes, bias",
                "number": "Article 11",
                "number-of-pages": "9",
                "page": "1–9",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "June 2020",
                "title": "Data Quality and Explainable AI",
                "URL": "https://doi.org/10.1145/3386687",
                "volume": "12"
            }
        },
        {
            "10.1145/3545897.3545919": {
                "id": "10.1145/3545897.3545919",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Zheng",
                        "given": "Hongting"
                    },
                    {
                        "family": "Chen",
                        "given": "Ying"
                    },
                    {
                        "family": "Yu",
                        "given": "Guochun"
                    },
                    {
                        "family": "Pei",
                        "given": "Gege"
                    },
                    {
                        "family": "Zhang",
                        "given": "Min"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2022,
                            6,
                            15
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2022,
                            6,
                            15
                        ]
                    ]
                },
                "abstract": "China's the new development paradigm featuring dual circulation, in which domestic and overseas markets reinforce each other, with the domestic market as the mainstay has brought challenges and opportunities for the promotion of guochao culture, and the development of internationalization has also made the Chinese people's cultural confidence stronger and stronger. As for the clothing brands, Chinese people no longer blindly pursue international well-known brands, but are more willing to support domestic brands that incorporate Chinese traditional cultural elements. Combined with Python, using regular expressions to extract, process and analyze data the guochao culture promotion platform constructed in the paper explores various technologies including data collection, storage and analysis. The platform captures users’ consumption habits and interests, pushes relevant guochao brands in a personalized way to convey cultural concepts and realize user communication, narrowing the distance between guochao brands and consumers. The combination of cultural and brand confidence provides a certain practical significance for the promotion of guochao culture and the development of real economy.",
                "call-number": "10.1145/3545897.3545919",
                "collection-title": "ICIEB '22",
                "container-title": "Proceedings of the 2022 3rd International Conference on Internet and E-Business",
                "DOI": "10.1145/3545897.3545919",
                "event-place": "Madrid, Spain",
                "ISBN": "9781450397322",
                "keyword": "Data analysis, Big data, Python crawler technology, Guochao culture",
                "number-of-pages": "6",
                "page": "147–152",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Construction of Guochao Culture Promotion Platform in Big Data Environment",
                "URL": "https://doi.org/10.1145/3545897.3545919"
            }
        },
        {
            "10.1145/3281375.3281404": {
                "id": "10.1145/3281375.3281404",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Ishikawa",
                        "given": "Hiroshi"
                    },
                    {
                        "family": "Kato",
                        "given": "Daiju"
                    },
                    {
                        "family": "Masaki",
                        "given": "Endo"
                    },
                    {
                        "family": "Hirota",
                        "given": "Masaharu"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            9,
                            25
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            9,
                            25
                        ]
                    ]
                },
                "abstract": "Recently there is strong demand for analytic methodology as to generation of integrated hypotheses for applications involving different sources of social big data. In this paper, first, we introduce an abstract data model for integrating data management and data mining by using mathematical concepts of families, collections of sets to facilitate reproducibility and accountability required for social big data applications. Next, we propose generalized difference methods as a methodology for integrated analysis based on different sources of data. Finally, we validate our proposal by applying it to three use cases by using our data model as their description.",
                "call-number": "10.1145/3281375.3281404",
                "collection-title": "MEDES '18",
                "container-title": "Proceedings of the 10th International Conference on Management of Digital EcoSystems",
                "DOI": "10.1145/3281375.3281404",
                "event-place": "Tokyo, Japan",
                "ISBN": "9781450356220",
                "keyword": "integrated analysis, hypothesis generation, data mining, social big data, data management, difference method, data model",
                "number-of-pages": "10",
                "page": "13–22",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Generalized difference method for generating integrated hypotheses in social big data",
                "URL": "https://doi.org/10.1145/3281375.3281404"
            }
        },
        {
            "10.1145/3426972": {
                "id": "10.1145/3426972",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Stergiou",
                        "given": "Christos L."
                    },
                    {
                        "family": "Psannis",
                        "given": "Konstantinos E."
                    },
                    {
                        "family": "Gupta",
                        "given": "Brij B."
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            10,
                            22
                        ]
                    ]
                },
                "abstract": "This paper introduces and describes a novel architecture scenario based on Cloud Computing and counts on the innovative model of Federated Learning. The proposed model is named Integrated Federated Model, with the acronym InFeMo. InFeMo incorporates all the existing Cloud models with a federated learning scenario, as well as other related technologies that may have integrated use with each other, offering a novel integrated scenario. In addition to this, the proposed model is motivated to deliver a more energy efficient system architecture and environment for the users, which aims to the scope of data management. Also, by applying the InFeMo the user would have less waiting time in every procedure queue. The proposed system was built on the resources made available by Cloud Service Providers (CSPs) and by using the PaaS (Platform as a Service) model, in order to be able to handle user requests better and faster. This research tries to fill a scientific gap in the field of federated Cloud systems. Thus, taking advantage of the existing scenarios of FedAvg and CO-OP, we were keen to end up with a new federated scenario that merges these two algorithms, and aiming for a more efficient model that is able to select, depending on the occasion, if it “trains” the model locally in client or globally in server.",
                "call-number": "10.1145/3426972",
                "collection-number": "46",
                "container-title": "ACM Trans. Internet Technol.",
                "DOI": "10.1145/3426972",
                "ISSN": "1533-5399",
                "issue": "2",
                "keyword": "secure, federated learning system, Cloud computing, management, big data",
                "number": "Article 46",
                "number-of-pages": "22",
                "page": "1–22",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "May 2022",
                "title": "InFeMo: Flexible Big Data Management Through a Federated Cloud System",
                "URL": "https://doi.org/10.1145/3426972",
                "volume": "22"
            }
        },
        {
            "10.1145/3260383": {
                "id": "10.1145/3260383",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Kumar",
                        "given": "Deepak"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            2,
                            17
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2016,
                            2,
                            17
                        ]
                    ]
                },
                "call-number": "10.1145/3260383",
                "collection-title": "SIGCSE '16",
                "container-title": "Proceedings of the 47th ACM Technical Symposium on Computing Science Education",
                "DOI": "10.1145/3260383",
                "event-place": "Memphis, Tennessee, USA",
                "ISBN": "9781450336857",
                "page": "",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Session details: Paper Session: Big Data",
                "URL": "https://doi.org/10.1145/3260383"
            }
        },
        {
            "10.1145/2339530.2378371": {
                "id": "10.1145/2339530.2378371",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Jordan",
                        "given": "Michael I."
                    },
                    {
                        "family": "Faloutsos",
                        "given": "Christos"
                    },
                    {
                        "family": "Gao",
                        "given": "Wen"
                    },
                    {
                        "family": "Han",
                        "given": "Jiawei"
                    },
                    {
                        "family": "Zheng",
                        "given": "Zijian"
                    },
                    {
                        "family": "Fayyad",
                        "given": "Usuama"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2012,
                            9,
                            24
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2012,
                            8,
                            12
                        ]
                    ]
                },
                "call-number": "10.1145/2339530.2378371",
                "collection-title": "KDD '12",
                "container-title": "Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining",
                "DOI": "10.1145/2339530.2378371",
                "event-place": "Beijing, China",
                "ISBN": "9781450314626",
                "page": "",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Panel on Mining the Big Data",
                "URL": "https://doi.org/10.1145/2339530.2378371"
            }
        },
        {
            "10.1145/3357419.3357441": {
                "id": "10.1145/3357419.3357441",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Chaveesuk",
                        "given": "Singha"
                    },
                    {
                        "family": "Khalid",
                        "given": "Bilal"
                    },
                    {
                        "family": "Chaiyasoonthorn",
                        "given": "Wornchanok"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            8,
                            23
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            8,
                            23
                        ]
                    ]
                },
                "abstract": "The research paper aims to study the factors related to incorporation of the artificial intelligence and big data in the business environment leading to rapid transformation leading to businesses expansion while riding the market trends, taking over of the competitions and other dynamics of the business environment. The need of conducting this research evolved from the recent events which have put many data mining companies under fire with the uncovering of the Big-Data scandals, manipulating and directing consumers' mindsets among masses to affect the favorable impressions. Furthermore, it has been also observed that business tends to seek assistance through its professionals, corporate and social networks to have price parity to achieve its strategic business goals and objectives.",
                "call-number": "10.1145/3357419.3357441",
                "collection-title": "ICICM 2019",
                "container-title": "Proceedings of the 9th International Conference on Information Communication and Management",
                "DOI": "10.1145/3357419.3357441",
                "event-place": "Prague, Czech Republic",
                "ISBN": "9781450371889",
                "keyword": "Collusion, Business Environment, Big-Data, Artificial Intelligence, Anti-Trust Laws, Anti-Free Market Competition",
                "number-of-pages": "5",
                "page": "181–185",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Emergence of New Business Environment with Big Data and Artificial Intelligence",
                "URL": "https://doi.org/10.1145/3357419.3357441"
            }
        },
        {
            "10.1145/3123024.3124439": {
                "id": "10.1145/3123024.3124439",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Ferreira",
                        "given": "Eija"
                    },
                    {
                        "family": "Ferreira",
                        "given": "Denzil"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            9,
                            11
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            9,
                            11
                        ]
                    ]
                },
                "abstract": "High-quality data is a necessity for successful research and development endeavors. In this article, we review relevant literature for data quality (DQ) assessment methods in different domains and discuss the possibilities, challenges and constraints of applying them to mobile sensing. We identify DQ dimensions directly applicable to sensor data: believability (comparison with the correct operating bounds), completeness (missing values), free-of-error (erroneous values), consistency (over time), timeliness (delay), accuracy (deviation from true value) and precision (granularity of readings) are core aspects of high-quality sensor data. We also emphasize that sensor data must be representative of the originating type of sensor. We propose an altruistic approach to DQ assessment for sensor data that facilitates aggregating and sharing of domain knowledge through a community-contributed library of DQ assessment methods organized by sensor type.",
                "call-number": "10.1145/3123024.3124439",
                "collection-title": "UbiComp '17",
                "container-title": "Proceedings of the 2017 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2017 ACM International Symposium on Wearable Computers",
                "DOI": "10.1145/3123024.3124439",
                "event-place": "Maui, Hawaii",
                "ISBN": "9781450351904",
                "keyword": "sensors, mobile instrumentation, metrics, library of domain knowledge, dimensions, data quality",
                "number-of-pages": "6",
                "page": "464–469",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Towards altruistic data quality assessment for mobile sensing",
                "URL": "https://doi.org/10.1145/3123024.3124439"
            }
        },
        {
            "10.1145/3249182": {
                "id": "10.1145/3249182",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Wagner",
                        "given": "Stefan"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2011,
                            9,
                            4
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2011,
                            9,
                            4
                        ]
                    ]
                },
                "call-number": "10.1145/3249182",
                "collection-title": "WoSQ '11",
                "container-title": "Proceedings of the 8th international workshop on Software quality",
                "DOI": "10.1145/3249182",
                "event-place": "Szeged, Hungary",
                "ISBN": "9781450308519",
                "page": "",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Session details: Data quality",
                "URL": "https://doi.org/10.1145/3249182"
            }
        },
        {
            "10.1109/CCGrid.2015.168": {
                "id": "10.1109/CCGrid.2015.168",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Yin",
                        "given": "Jiangling"
                    },
                    {
                        "family": "Wang",
                        "given": "Jun"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2015,
                            5,
                            4
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2015,
                            5,
                            4
                        ]
                    ]
                },
                "abstract": "Recent years the Hadoop Distributed File System(HDFS) has been deployed as the bedrock for many parallel big data processing systems, such as graph processing systems, MPI-based parallel programs and scala/java-based Spark frameworks, which can efficiently support iterative and interactive data analysis in memory. The first part of my dissertation mainly focuses on studying parallel data access on distributed file systems, e.g, HDFS. Since the distributed I/O resources and global data distribution are often not taken into consideration, the data requests from parallel processes/executors will unfortunately be served in a remote or imbalanced fashion on the storage servers. In order to address these problems, we develop I/O middleware systems and matching-based algorithms to map parallel data requests to storage servers such that local and balanced data access can be achieved. The last part of my dissertation presents our plans to improve the performance of interactive data access in big data analysis. Specifically, most interactive analysis programs will scan through the entire data set regardless of which data is actually required. We plan to develop a content-aware method to quickly access required data without this laborious scanning process.",
                "call-number": "10.1109/CCGrid.2015.168",
                "collection-title": "CCGRID '15",
                "container-title": "Proceedings of the 15th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing",
                "DOI": "10.1109/CCGrid.2015.168",
                "event-place": "Shenzhen, China",
                "ISBN": "9781479980062",
                "number-of-pages": "4",
                "page": "721–724",
                "publisher": "IEEE Press",
                "title": "Optimize parallel data access in big data processing",
                "URL": "https://doi.org/10.1109/CCGrid.2015.168"
            }
        },
        {
            "10.1145/2980258.2980301": {
                "id": "10.1145/2980258.2980301",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Chandrasekaran",
                        "given": "Balaji"
                    },
                    {
                        "family": "Balakrishnan",
                        "given": "Ramadoss"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            8,
                            25
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2016,
                            8,
                            25
                        ]
                    ]
                },
                "abstract": "Big data is the next frontier for modernization, rivalry, and profitability. It is the foundation of all the major trends such as social networks, mobile devices, healthcare, stock markets etc. Big data is efficiently stored in the cloud because of its high-volume, high-speed and high-assortment data resources. An unauthorized user access control is the gravest threat of huge information in the cloud environment because of the remote file storage. Attribute Based Encryption (ABE) is an efficient access control procedure to guarantee end-to-end security for huge information in the cloud. Most often existing ABE working principle is based on bilinear pairing. In this paper, we construct a peculiar ABE for big data in the cloud. Our proposed scheme is based on quadratic residue and attribute union which is based on fundamental arithmetic theorem.",
                "call-number": "10.1145/2980258.2980301",
                "collection-number": "19",
                "collection-title": "ICIA-16",
                "container-title": "Proceedings of the International Conference on Informatics and Analytics",
                "DOI": "10.1145/2980258.2980301",
                "event-place": "Pondicherry, India",
                "ISBN": "9781450347563",
                "keyword": "ABE, Quadratic Residue, Bilinear Pairing, Cloud, Big Data",
                "number": "Article 19",
                "number-of-pages": "4",
                "page": "1–4",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Attribute Based Encryption Using Quadratic Residue for the Big Data in Cloud Environment",
                "URL": "https://doi.org/10.1145/2980258.2980301"
            }
        },
        {
            "10.1145/2630729.2630736": {
                "id": "10.1145/2630729.2630736",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Parboteeah",
                        "given": "Paul"
                    },
                    {
                        "family": "Milne",
                        "given": "Alistair"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2014,
                            6,
                            22
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2014,
                            6,
                            22
                        ]
                    ]
                },
                "abstract": "We examine the lessons from the experience of the SNOMED CT ontology in medicine for the monitoring of systemic financial risk. Using big data to create financial stress diagnostics that are informative about the causes of systemic risk, requires ontological understanding of financial data at the granular level. The history of SNOMED indicates that developing such an understanding is a long term project, requiring co-operation of many participants and a well developed structure of governance.",
                "call-number": "10.1145/2630729.2630736",
                "collection-title": "DSMM'14",
                "container-title": "Proceedings of the International Workshop on Data Science for Macro-Modeling",
                "DOI": "10.1145/2630729.2630736",
                "event-place": "Snowbird, UT, USA",
                "ISBN": "9781450330121",
                "keyword": "information governance ontologies, financial stress diagnostics, FIBO, SNOMED CT, Big data, financial stress indicators",
                "number-of-pages": "2",
                "page": "1–2",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Lessons from the Medical Industry for Big Data in Finance: Ontologies and Financial Stress Diagnostics",
                "URL": "https://doi.org/10.1145/2630729.2630736"
            }
        },
        {
            "10.5555/2835377.2835394": {
                "id": "10.5555/2835377.2835394",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Eckroth",
                        "given": "Joshua"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            1,
                            1
                        ]
                    ]
                },
                "abstract": "The increasing awareness of \"big data\" is transforming the academic and business landscape across many disciplines. Yet, big data programming environments are still too complex for non-programmers to utilize. To our knowledge, only computer scientists are ever exposed to big data processing concepts and tools in undergraduate education. Furthermore, non-computer scientists may lack sufficient common ground with computer scientists to explain their specialized big data processing needs. In order to bridge this gap and enhance collaboration among persons with big data processing needs and persons who are trained in programming and system building, we propose the foundations of a cross-disciplinary pedagogy that exposes big data processing paradigms and design decisions at an abstract level. With these tools, students and experts from different disciplines can more effectively collaborate on solving big data problems.",
                "call-number": "10.5555/2835377.2835394",
                "container-title": "J. Comput. Sci. Coll.",
                "ISSN": "1937-4771",
                "issue": "3",
                "number-of-pages": "9",
                "page": "110–118",
                "publisher": "Consortium for Computing Sciences in Colleges",
                "publisher-place": "Evansville, IN, USA",
                "source": "January 2016",
                "title": "Foundations of a cross-disciplinary pedagogy for big data",
                "volume": "31"
            }
        },
        {
            "10.1145/3318464.3389770": {
                "id": "10.1145/3318464.3389770",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Yang",
                        "given": "Zongheng"
                    },
                    {
                        "family": "Chandramouli",
                        "given": "Badrish"
                    },
                    {
                        "family": "Wang",
                        "given": "Chi"
                    },
                    {
                        "family": "Gehrke",
                        "given": "Johannes"
                    },
                    {
                        "family": "Li",
                        "given": "Yinan"
                    },
                    {
                        "family": "Minhas",
                        "given": "Umar Farooq"
                    },
                    {
                        "family": "Larson",
                        "given": "Per-Åke"
                    },
                    {
                        "family": "Kossmann",
                        "given": "Donald"
                    },
                    {
                        "family": "Acharya",
                        "given": "Rajeev"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2020,
                            6,
                            11
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2020,
                            6,
                            11
                        ]
                    ]
                },
                "abstract": "Corporations today collect data at an unprecedented and accelerating scale, making the need to run queries on large datasets increasingly important. Technologies such as columnar block-based data organization and compression have become standard practice in most commercial database systems. However, the problem of best assigning records to data blocks on storage is still open. For example, today's systems usually partition data by arrival time into row groups, or range/hash partition the data based on selected fields. For a given workload, however, such techniques are unable to optimize for the important metric of the number of blocks accessed by a query. This metric directly relates to the I/O cost, and therefore performance, of most analytical queries. Further, they are unable to exploit additional available storage to drive this metric down further. In this paper, we propose a new framework called a query-data routing tree, or qd-tree, to address this problem, and propose two algorithms for their construction based on greedy and deep reinforcement learning techniques. Experiments over benchmark and real workloads show that a qd-tree can provide physical speedups of more than an order of magnitude compared to current blocking schemes, and can reach within 2X of the lower bound for data skipping based on selectivity, while providing complete semantic descriptions of created blocks.",
                "call-number": "10.1145/3318464.3389770",
                "collection-title": "SIGMOD '20",
                "container-title": "Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data",
                "DOI": "10.1145/3318464.3389770",
                "event-place": "Portland, OR, USA",
                "ISBN": "9781450367356",
                "keyword": "OLAP, data layout, deep reinforcement learning, query processing, big data, data partitioning, storage, deep learning, data analytics, indexing",
                "number-of-pages": "16",
                "page": "193–208",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Qd-tree: Learning Data Layouts for Big Data Analytics",
                "URL": "https://doi.org/10.1145/3318464.3389770"
            }
        },
        {
            "10.1145/3110025.3120989": {
                "id": "10.1145/3110025.3120989",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "O'Halloran",
                        "given": "Sharyn"
                    },
                    {
                        "family": "Nowaczyk",
                        "given": "Nikolai"
                    },
                    {
                        "family": "Gallagher",
                        "given": "Donal"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            7,
                            31
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            7,
                            31
                        ]
                    ]
                },
                "abstract": "In this paper, we simulate and analyze the impact of financial regulations concerning the collateralization of derivative trades on systemic risk. We represent a financial system using a weighted directed graph model. We enhance a novel open source risk engine to automatically classify a financial regulation for its impact on systemic risk. The analysis finds that introducing collateralization does reduce the costs of resolving a financial system in crisis. It does not, however, change the distribution of risk in the system. The analysis also highlights the importance of scenario based testing using hands on metrics to quantify the notion of system risk.",
                "call-number": "10.1145/3110025.3120989",
                "collection-title": "ASONAM '17",
                "container-title": "Proceedings of the 2017 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2017",
                "DOI": "10.1145/3110025.3120989",
                "event-place": "Sydney, Australia",
                "ISBN": "9781450349932",
                "keyword": "graph theoretic models, financial risk analytics, collateralizations, big data, Monte Carlo simulation, variation margin, systemic risk, stochastic Linear Gauss-Markov model, initial margin, open source risk engine",
                "number-of-pages": "9",
                "page": "1056–1064",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Big Data and Graph Theoretic Models: Simulating the Impact of Collateralization on a Financial System",
                "URL": "https://doi.org/10.1145/3110025.3120989"
            }
        },
        {
            "10.5555/3007225.3007243": {
                "id": "10.5555/3007225.3007243",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Geise",
                        "given": "Mary Jo"
                    },
                    {
                        "family": "Grant",
                        "given": "Navneet"
                    },
                    {
                        "family": "Miller",
                        "given": "Rick"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            10,
                            1
                        ]
                    ]
                },
                "abstract": "Big data is currently being created at a staggering estimate of 2.5 exabytes per day with data from sources such as sensors, social media sites, mobile devices, and business transaction records. With this quantity of generated and stored data (Volume), the speed at which these data are being generated (Velocity) and the various forms of data (Variety), traditional relational database processing techniques are unable to effectively handle the size and complexity of these data sets. This panel will discuss how businesses are currently dealing with big data, needs businesses have for scientists able to process these data, and how educational institutions can better prepare students for the challenges of big data.",
                "call-number": "10.5555/3007225.3007243",
                "container-title": "J. Comput. Sci. Coll.",
                "ISSN": "1937-4771",
                "issue": "1",
                "number-of-pages": "1",
                "page": "81",
                "publisher": "Consortium for Computing Sciences in Colleges",
                "publisher-place": "Evansville, IN, USA",
                "source": "October 2016",
                "title": "How is big data changing what we do? panel discussion",
                "volume": "32"
            }
        },
        {
            "10.1145/3339186.3339207": {
                "id": "10.1145/3339186.3339207",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Xia",
                        "given": "Qiufen"
                    },
                    {
                        "family": "Bai",
                        "given": "Luyao"
                    },
                    {
                        "family": "Liang",
                        "given": "Weifa"
                    },
                    {
                        "family": "Xu",
                        "given": "Zichuan"
                    },
                    {
                        "family": "Yao",
                        "given": "Lin"
                    },
                    {
                        "family": "Wang",
                        "given": "Lei"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            8,
                            5
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            8,
                            5
                        ]
                    ]
                },
                "abstract": "We are in the era of big data and cloud computing, large quantity of computing resource is desperately needed to detect invaluable information hidden in the coarse big data through query evaluation. Users demand big data analytic services with various Quality of Service (QoS) requirements. However, cloud computing is facing new challenges in meeting stringent QoS requirements of users due to the remoteness from its users. Edge computing has emerged as a new paradigm to address such shortcomings by bringing cloud services to the edge of the operation network in proximity of users for performance improvement. To satisfy the QoS requirements of users for big data analytics in edge computing, the data replication and placement problem must be properly dealt with such that user requests can be efficiently and promptly responded. In this paper, we consider data replication and placement for big data analytic query evaluation. We first cast a novel proactive data replication and placement problem of big data analytics in a two-tier edge cloud environment, we then devise an approximation algorithm with an approximation ratio for it, we finally evaluate the proposed algorithm against existing benchmarks, using both simulation and experiment in a testbed based on real datasets, the evaluation results show that the proposed algorithm is promising.",
                "call-number": "10.1145/3339186.3339207",
                "collection-number": "26",
                "collection-title": "ICPP 2019",
                "container-title": "Proceedings of the 48th International Conference on Parallel Processing: Workshops",
                "DOI": "10.1145/3339186.3339207",
                "event-place": "Kyoto, Japan",
                "ISBN": "9781450371964",
                "keyword": "big data analytics, edge clouds, query evaluation, Data replication and placement",
                "number": "Article 26",
                "number-of-pages": "10",
                "page": "1–10",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "QoS-Aware Proactive Data Replication for Big Data Analytics in Edge Clouds",
                "URL": "https://doi.org/10.1145/3339186.3339207"
            }
        },
        {
            "10.1145/3350546.3352507": {
                "id": "10.1145/3350546.3352507",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Tantalaki",
                        "given": "Nicoleta"
                    },
                    {
                        "family": "Souravlas",
                        "given": "Stavros"
                    },
                    {
                        "family": "Roumeliotis",
                        "given": "Manos"
                    },
                    {
                        "family": "Katsavounis",
                        "given": "Stefanos"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            10,
                            14
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            10,
                            14
                        ]
                    ]
                },
                "abstract": "Nowadays, there is an accelerating need to efficiently and timely handle large amounts of data that arrives continuously. Streams of big data led to the emergence of Distributed Stream Processing Systems (DSPS) that assign processing tasks to the available resources (dynamically or not) and route streaming data between them. Efficient scheduling of processing tasks of data flows can reduce application latencies and eliminate network congestion. In this work, we propose a linear complexity scheme for the task allocation and scheduling problem to improve system’s performance, load balancing and memory efficiency, in applications where there is need for heavy communication (all-to-all) between the tasks assigned to pairs of components.",
                "call-number": "10.1145/3350546.3352507",
                "collection-title": "WI '19",
                "container-title": "IEEE/WIC/ACM International Conference on Web Intelligence",
                "DOI": "10.1145/3350546.3352507",
                "event-place": "Thessaloniki, Greece",
                "ISBN": "9781450369343",
                "keyword": "Stream Processing, Distributed Systems, Apache Storm, Scheduling, Big Data",
                "number-of-pages": "9",
                "page": "107–115",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Linear Scheduling of Big Data Streams on Multiprocessor Sets in the Cloud",
                "URL": "https://doi.org/10.1145/3350546.3352507"
            }
        },
        {
            "10.1145/3378936.3378978": {
                "id": "10.1145/3378936.3378978",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Hukkeri",
                        "given": "Tanmay Sanjay"
                    },
                    {
                        "family": "G",
                        "given": "Shobha"
                    },
                    {
                        "family": "Phal",
                        "given": "Shubham Milind"
                    },
                    {
                        "family": "Shetty",
                        "given": "Jyothi"
                    },
                    {
                        "family": "R",
                        "given": "Yatish H"
                    },
                    {
                        "family": "Mohammed",
                        "given": "Naweed"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2020,
                            1,
                            12
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2020,
                            1,
                            12
                        ]
                    ]
                },
                "abstract": "Today's fast-moving world sees an abundance of image data in everyday life. From messages to insurance claims to even judicial systems, image data plays a pivotal role in facilitating several critical Big Data applications. Some of these applications such as automatic license plate recognition (ALPR) use CCTV cameras to capture snapshots of traffic from real-time video, inadvertently resulting in the generation vast amounts of image data on a daily basis. This brings with it the herculean task of processing these images to extract the essential information as efficiently as possible. The conventional method of processing images in a sequential manner can be very time consuming on account of the vast multitude of images and the intensive computation involved in order to process these. Distributed image processing seeks to provide a solution to this problem by splitting the computations involved across multiple nodes. This paper presents a novel framework to implement distributed image processing via OpenCV on HPCC Systems distributed node architecture*, a set of high-performance computing clusters. The proposed approach when tested on the Indian License Plates Dataset was found to be 85 percent accurate. Additionally, a 30 percent decrease in computation time was observed when executed on a multi-node setup without any impact to accuracy.",
                "call-number": "10.1145/3378936.3378978",
                "collection-title": "ICSIM '20",
                "container-title": "Proceedings of the 3rd International Conference on Software Engineering and Information Management",
                "DOI": "10.1145/3378936.3378978",
                "event-place": "Sydney, NSW, Australia",
                "ISBN": "9781450376907",
                "keyword": "Image Processing, Automatic License Plate Recognition, OpenCV, Distributed Computing, HPCC Systems, Big Data, Machine Learning",
                "number-of-pages": "6",
                "page": "26–31",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Massively Scalable Image Processing on the HPCC Systems Big Data Platform",
                "URL": "https://doi.org/10.1145/3378936.3378978"
            }
        },
        {
            "10.1145/3297156.3297233": {
                "id": "10.1145/3297156.3297233",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Fuzong",
                        "given": "Wang"
                    },
                    {
                        "family": "Helin",
                        "given": "Guo"
                    },
                    {
                        "family": "Jian",
                        "given": "Zhao"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            12,
                            8
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            12,
                            8
                        ]
                    ]
                },
                "abstract": "Data Compression has become a commodity feature for space efficiency and performance by reducing reading and writing traffic and space capacity demand. This technology is particularly valuable for a file system to manage and server the big data processing tasks. However, the fixed data compression scheme cannot fit all the big data workloads and data-set which have complex internal data structure and compressibility. This paper investigates a dynamic and smart data compression algorithm selection scheme for different big data processing cases in the local file system. To this end, we propose a dynamic algorithm selection module in the Linux ZFS which is an open source file system. This module will select a high compression ratio algorithm for high compressibility data, and select a fast compression algorithm for low compressibility data, and skip all data compression process for incompressibility data. The comprehensive evaluations validate that dynamic algorithm selection module can achieve up to 2.69x response time improvement for reading and writing operation in file system and reduce about 32.12% storage space for a large amount data-set.",
                "call-number": "10.1145/3297156.3297233",
                "collection-title": "CSAI '18",
                "container-title": "Proceedings of the 2018 2nd International Conference on Computer Science and Artificial Intelligence",
                "DOI": "10.1145/3297156.3297233",
                "event-place": "Shenzhen, China",
                "ISBN": "9781450366069",
                "keyword": "Compression Algorithm, Big Data, File system, Data Compression",
                "number-of-pages": "5",
                "page": "110–114",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Dynamic Data Compression Algorithm Selection for Big Data Processing on Local File System",
                "URL": "https://doi.org/10.1145/3297156.3297233"
            }
        },
        {
            "10.5555/3382225.3382440": {
                "id": "10.5555/3382225.3382440",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Day",
                        "given": "Min-Yuh"
                    },
                    {
                        "family": "Cheng",
                        "given": "Tun-Kung"
                    },
                    {
                        "family": "Li",
                        "given": "Jheng-Gang"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            8,
                            28
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            8,
                            28
                        ]
                    ]
                },
                "abstract": "Robo-Advisors has been growing attraction from the financial industry for offering financial services by using algorithms and acting as like human advisors to support investors making investment decisions. During the investment planning stage, portfolio optimization plays a crucial role, especially for the medium and long-term investors, in determining the allocation weight of assets to achieve the balance between investors expectation return and risk tolerance. The literature on the topic of portfolio optimization has been offering plenty of theoretical and practical guidance for implementing the theory; however, there is a paucity of studies focusing on the applications which are designed for Robo-Advisors. In this research, we proposed a modular system and focused on integrating big data analysis, deep learning method and the Black-Litterman model to generate asset allocation weight. We developed a portfolio optimization module which takes the information from a variety of sources, such as stocks prices, investor profile and the other alternative data, and used them as input to calculate optimal weights of assets in the portfolio. The module we developed could be used as a sub-system for Robo-Advisors, which offers a customized optimal portfolio based on investors preference.",
                "call-number": "10.5555/3382225.3382440",
                "collection-title": "ASONAM '18",
                "container-title": "Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
                "event-place": "Barcelona, Spain",
                "ISBN": "9781538660515",
                "keyword": "black-litterman, deep learning, portfolio optimization, big data analysis, financial technology, robo-advisors, investment management",
                "number-of-pages": "5",
                "page": "1027–1031",
                "publisher": "IEEE Press",
                "title": "AI robo-advisor with big data analytics for financial services"
            }
        },
        {
            "10.1145/2769458.2769491": {
                "id": "10.1145/2769458.2769491",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Ellul",
                        "given": "Natacha"
                    },
                    {
                        "family": "Capocchi",
                        "given": "Laurent"
                    },
                    {
                        "family": "Santucci",
                        "given": "Jean-Francois"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2015,
                            6,
                            10
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2015,
                            6,
                            10
                        ]
                    ]
                },
                "abstract": "Methods of processing and analyzing traditional data does not answer to the emergence of Big Data stemming from social networks and mobile applications. One of the best ways to bring the perspective of the customers to business decisions is by using data analysis to allow a company to deal with the customer experience for improved management and better profits. The work in progress presented in this paper concerns the development of an approach integrating discrete-event Modeling and Simulation and statistical learning methods in order to perform both customer understanding through data classification and predictive modeling through data prediction. This work involves the integration of statistical learning algorithms in the DEVS formalism.",
                "call-number": "10.1145/2769458.2769491",
                "collection-title": "SIGSIM PADS '15",
                "container-title": "Proceedings of the 3rd ACM SIGSIM Conference on Principles of Advanced Discrete Simulation",
                "DOI": "10.1145/2769458.2769491",
                "event-place": "London, United Kingdom",
                "ISBN": "9781450335836",
                "keyword": "big data, devsimpy, artificial neural network, discrete event modeling, simulation",
                "number-of-pages": "2",
                "page": "257–258",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Big Data Decision Making Based on Predictive Data Analysis Using DEVS Simulations",
                "URL": "https://doi.org/10.1145/2769458.2769491"
            }
        },
        {
            "10.1145/3424978.3424999": {
                "id": "10.1145/3424978.3424999",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Zhang",
                        "given": "Ganghong"
                    },
                    {
                        "family": "Huo",
                        "given": "Chao"
                    },
                    {
                        "family": "He",
                        "given": "Jinhong"
                    },
                    {
                        "family": "Gao",
                        "given": "Jian"
                    },
                    {
                        "family": "Yin",
                        "given": "Zhibin"
                    },
                    {
                        "family": "Luo",
                        "given": "Anqin"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2020,
                            10,
                            20
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2020,
                            10,
                            20
                        ]
                    ]
                },
                "abstract": "Although many methods for collecting electricity data from smart terminals are available, especially for power Internet of things, alternatives have to face to the lack of effective management and analysis methods and hard technical realities for such large data. In order to solve these problems, this paper firstly, summarized the development of power Internet of things and big data, then analyzed the motivation and goals of building power Internet of things using big data technology according to the ubiquitous smart terminals applied in power areas, and illustrated the supporting technologies of big data when effectively serving the power Internet of things. Facing the challenges caused by big data in power of Internet of things, this paper proposed a strategy to handle big data that focused on cloud computing, data mining, and machine learning. In addition, we presented a basic architecture for cloud computing platforms and proposed the establishment of an operating center for power Internet of things. Possible solutions to collect data, data modeling, and data analysis and decision were proposed. We also proposed a typical forecasting system based on big data platform. For power Internet of things, taking advantage of big data and cloud computing technologies would be an effective strategy for improving decision support and analytics applied in power Internet of things. In the near future, this would be a great challenge and opportunity in power Internet of things.",
                "call-number": "10.1145/3424978.3424999",
                "collection-number": "21",
                "collection-title": "CSAE 2020",
                "container-title": "Proceedings of the 4th International Conference on Computer Science and Application Engineering",
                "DOI": "10.1145/3424978.3424999",
                "event-place": "Sanya, China",
                "ISBN": "9781450377720",
                "keyword": "Power internet of things, Electricity data, Smart terminal, Cloud computing, Big data",
                "number": "Article 21",
                "number-of-pages": "5",
                "page": "1–5",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "A Strategy and Architecture Based on Big Data for Power Internet of Things",
                "URL": "https://doi.org/10.1145/3424978.3424999"
            }
        },
        {
            "10.14778/3368289.3368299": {
                "id": "10.14778/3368289.3368299",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Leeka",
                        "given": "Jyoti"
                    },
                    {
                        "family": "Rajan",
                        "given": "Kaushik"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            11,
                            1
                        ]
                    ]
                },
                "abstract": "The cost of big-data analytics is dominated by shuffle operations that induce multiple disk reads, writes and network transfers. This paper proposes a new class of optimization rules that are specifically aimed at eliminating shuffles where possible. The rules substitute multiple shuffle inducing operators (Join, UnionAll, Spool, GroupBy) with a single streaming operator which implements an entire sub-query. We call such operators super-operators.A key challenge with adding new rules that substitute sub-queries with super-operators is that there are many variants of the same sub-query that can be implemented via minor modifications to the same super-operator. Adding each as a separate rule leads to a search space explosion. We propose several extensions to the query optimizer to address this challenge. We propose a new abstract representation for operator trees that captures all possible sub-queries that a super-operator implements. We propose a new rule matching algorithm that can efficiently search for abstract operator trees. Finally we extend the physical operator interface to introduce new parametric super-operators.We implement our changes in SCOPE, a state-of-the-art production big-data optimizer used extensively at Microsoft. We demonstrate that the proposed optimizations provide significant reduction in both resource cost (average 1.7x) and latency (average 1.5x) on several production queries, and do so without increasing optimization time.",
                "call-number": "10.14778/3368289.3368299",
                "container-title": "Proc. VLDB Endow.",
                "DOI": "10.14778/3368289.3368299",
                "ISSN": "2150-8097",
                "issue": "3",
                "number-of-pages": "14",
                "page": "348–361",
                "publisher": "VLDB Endowment",
                "source": "November 2019",
                "title": "Incorporating super-operators in big-data query optimizers",
                "URL": "https://doi.org/10.14778/3368289.3368299",
                "volume": "13"
            }
        },
        {
            "10.1145/3354153.3354156": {
                "id": "10.1145/3354153.3354156",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Suwansrikham",
                        "given": "Parinya"
                    },
                    {
                        "family": "She",
                        "given": "Kun"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            6,
                            15
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            6,
                            15
                        ]
                    ]
                },
                "abstract": "Big data is the name that defines data which has enormous size and unstructured. Due to the file size is pretty huge. It is impracticable to store a large file in one storage volume. However, cloud computing is a solution to this impossible. Data owner can store the file in a cloud storage provider (CSP). Nevertheless, the new dilemma has arisen. Relying on single cloud storage may generate trouble for the customer. A CSP may stop its service anytime. Moreover, the CSP is the third party that user have to trust without verification. In that case, the privacy or unauthorized accessing of data may be violated without notice. To overcome this risk, we propose secure data storage scheme for big data storing on multiple CSPs. The one big data file is split into chunks and distributed to multiple cloud storage provider. After splitting the file, metadata is generated. Metadata is a place to keep chunks information, includes; chunk locations, access paths, username and password of the data owner, methods to connect each CSP. The metadata is encrypted and transferred to the user who requests to access the file. The user utilizes the metadata and chunks of the file to compose the original file. This method will minimize the risk of privacy. The goal of this paper is to provide the method to protect the privacy of data stored on multiple cloud storage providers. Furthermore, we discuss and analyze how this data storage scheme promote the protection of big data privacy.",
                "call-number": "10.1145/3354153.3354156",
                "collection-title": "DSDE 2019",
                "container-title": "Proceedings of the 2019 2nd International Conference on Data Storage and Data Engineering",
                "DOI": "10.1145/3354153.3354156",
                "event-place": "Jeju, Republic of Korea",
                "ISBN": "9781450372169",
                "keyword": "Data Security and Privacy, Cloud Storage, Big Data",
                "number-of-pages": "7",
                "page": "47–53",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Protection of Big Data Privacy on Multiple Cloud Providers by Asymmetric Security Scheme",
                "URL": "https://doi.org/10.1145/3354153.3354156"
            }
        },
        {
            "10.1109/TCBB.2015.2454551": {
                "id": "10.1109/TCBB.2015.2454551",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Janga",
                        "given": "Sarath Chandra"
                    },
                    {
                        "family": "Zhu",
                        "given": "Dongxiao"
                    },
                    {
                        "family": "Chen",
                        "given": "Jake Y."
                    },
                    {
                        "family": "Zaki",
                        "given": "Mohammed J."
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2015,
                            7,
                            1
                        ]
                    ]
                },
                "abstract": "The 13th International Workshop on Data Mining in Bioinformatics (BIOKDD'14) was organized in conjunction with the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining on August 24, 2014 in New York, USA. It brought together international researchers in the interacting disciplines of data mining, systems biology, and bioinformatics at the Bloomberg Headquarters venue. The goal of this workshop is to encourage Knowledge Discovery and Data mining (KDD) researchers to take on the numerous challenges that Bioinformatics offers. This year, the workshop featured the theme of \"Knowledge discovery using big data in biological/biomedical systems\".",
                "call-number": "10.1109/TCBB.2015.2454551",
                "container-title": "IEEE/ACM Trans. Comput. Biol. Bioinformatics",
                "DOI": "10.1109/TCBB.2015.2454551",
                "ISSN": "1545-5963",
                "issue": "4",
                "number-of-pages": "3",
                "page": "726–728",
                "publisher": "IEEE Computer Society Press",
                "publisher-place": "Washington, DC, USA",
                "source": "July/August 2015",
                "title": "Knowledge discovery using big data in biomedical systems",
                "URL": "https://doi.org/10.1109/TCBB.2015.2454551",
                "volume": "12"
            }
        },
        {
            "10.1145/3175628.3175653": {
                "id": "10.1145/3175628.3175653",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Abdelhakim",
                        "given": "Boudhir Anouar"
                    },
                    {
                        "family": "Mohamed",
                        "given": "Ben Ahmed"
                    },
                    {
                        "family": "Fellaji",
                        "given": "Soumaya"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            10,
                            25
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            10,
                            25
                        ]
                    ]
                },
                "abstract": "The work on health research in general, and cancer in particular, is dual-purpose; on the one hand because it is a great subject and of essential importance given the difficulty of reaching a miraculous and rapid solution. On the other hand, because this subject is a humanitarian field and his success is of immense social size. This pulse pushed us a lot to get involved in this research in order to contribute massively to research collaboration in this area by creating multidisciplinary groups to make technology available to the medical sciences and fight against pathological scourge. In this paper, we present a new architecture to serve doctors and nurses in the assignment of adequate treatment for patients based on common digital repository of decision support. This repository is a result of given treatment by doctors dispatched in time and place. The System of proposed architecture will be able to analyze treatments, symptoms, historical medical history of patients, and results on order to help other doctors to prescribe the correct prescription according to similarity of cases and successful results.",
                "call-number": "10.1145/3175628.3175653",
                "collection-number": "5",
                "collection-title": "SCAMS '17",
                "container-title": "Proceedings of the Mediterranean Symposium on Smart City Application",
                "DOI": "10.1145/3175628.3175653",
                "event-place": "Tangier, Morocco",
                "ISBN": "9781450352116",
                "keyword": "healthcare, big data, hadoop, analytics",
                "number": "Article 5",
                "number-of-pages": "4",
                "page": "1–4",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Big data architecture for decision making in protocols and medications assignment",
                "URL": "https://doi.org/10.1145/3175628.3175653"
            }
        },
        {
            "10.1145/3436369.3437426": {
                "id": "10.1145/3436369.3437426",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Li",
                        "given": "Weiwei"
                    },
                    {
                        "family": "Xu",
                        "given": "Huarong"
                    },
                    {
                        "family": "Chen",
                        "given": "Guanhua"
                    },
                    {
                        "family": "Qian",
                        "given": "Jianhong"
                    },
                    {
                        "family": "Wen",
                        "given": "Xinping"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2020,
                            10,
                            30
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2020,
                            10,
                            30
                        ]
                    ]
                },
                "abstract": "In order to solve the problem of urban traffic congestion, and according to the current status and operation of bus dispatching vehicles of public transportation enterprises, taking into account the interests of passengers and public transportation enterprises, an intelligent scheduling scheme based on big data was studied. By analyzing bus card swiping data, mining and processing the data, we get the number of passengers on the bus, the passenger flow data of the bus line is used to realize the optimization of the bus dispatch table..Based on the research and analysis of genetic algorithm and tabu search algorithm, the optimal scheduling schedule can be obtained by combining the two hybrid algorithms under certain constraints and aiming at minimizing the number of trips and the cost of passengers. Experiments show that the hybrid genetic algorithm can accelerate the convergence speed and get the optimal departure interval and the minimum number of buses in different time periods of a day.",
                "call-number": "10.1145/3436369.3437426",
                "collection-title": "ICCPR 2020",
                "container-title": "Proceedings of the 2020 9th International Conference on Computing and Pattern Recognition",
                "DOI": "10.1145/3436369.3437426",
                "event-place": "Xiamen, China",
                "ISBN": "9781450387835",
                "keyword": "Big data, intelligent scheduling, hybrid algorithm, genetic algorithm",
                "number-of-pages": "5",
                "page": "467–471",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Research on Optimization of Intelligent Public Transportation Scheduling Based on Big Data",
                "URL": "https://doi.org/10.1145/3436369.3437426"
            }
        },
        {
            "10.1145/3361785.3361786": {
                "id": "10.1145/3361785.3361786",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Alberic",
                        "given": "Minno Dekassan"
                    },
                    {
                        "family": "YeZheng",
                        "given": "Liu"
                    },
                    {
                        "family": "Rodrigue",
                        "given": "Dibonji Ndjansse Stephane"
                    },
                    {
                        "family": "Vellem",
                        "given": "Vuyolwethu"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            9,
                            12
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            9,
                            12
                        ]
                    ]
                },
                "abstract": "With the rapid development of the economy and the continuous deepening of the application of Internet technology, today's social economy has gradually entered the era of big data, which provides a good social environment for the development of e-commerce. This paper first introduces the background and significance of big data, and analyzes the connotation and advantages of e-commerce in the era of big data. On the basis of analyzing the development status of e-commerce industry under the background of big data, it summarizes the opportunities and challenges faced by e-commerce enterprises in the context of big data, and discusses the e-commerce service model under the era of big data. The main purpose of this research is to contribute to the improvement of e-commerce service level in the era of big data and promote the rapid development of e-commerce.",
                "call-number": "10.1145/3361785.3361786",
                "collection-title": "ICBIM '19",
                "container-title": "Proceedings of the 3rd International Conference on Business and Information Management",
                "DOI": "10.1145/3361785.3361786",
                "event-place": "Paris, France",
                "ISBN": "9781450372329",
                "keyword": "cross-border e-commerce, big data, business model",
                "number-of-pages": "4",
                "page": "1–4",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Research on innovation of cross-border e-commerce business model based on big data",
                "URL": "https://doi.org/10.1145/3361785.3361786"
            }
        },
        {
            "10.1145/3414274.3414280": {
                "id": "10.1145/3414274.3414280",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Heng",
                        "given": "Li"
                    },
                    {
                        "family": "Longfu",
                        "given": "Zhou"
                    },
                    {
                        "family": "Kaiyou",
                        "given": "Yuan"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2020,
                            7,
                            24
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2020,
                            7,
                            24
                        ]
                    ]
                },
                "abstract": "The big data technology which taking data analysis and mining as core can efficiently collect, store and process data of filling station safety accident cases, and further realize the statistical analysis, knowledge mining, principle summary and early warning forecast of filling station safety accidents. The article collected 461 cases of filling station safety accidents from 1981 to 2019 domestically and internationally. Given that those electronic text data are not highly standardized and cannot be directly analyzed and utilized, keyword extraction and structured storage were adopted by this article, in order to implement structured processing of electronic text data. Through the multi-dimensional statistical analysis of the structured data, high-quality, reliable, and practical analytical results were achieved. The results demonstrated that big data technology would become the main development stream and one of the most vital weapons of statistical analysis in various industries in the future.",
                "call-number": "10.1145/3414274.3414280",
                "collection-title": "DSIT 2020",
                "container-title": "Proceedings of the 3rd International Conference on Data Science and Information Technology",
                "DOI": "10.1145/3414274.3414280",
                "event-place": "Xiamen, China",
                "ISBN": "9781450376044",
                "keyword": "Keyword extraction, Filling station, Statistic analysis, Big data, Safety accident case, Structured storage",
                "number-of-pages": "6",
                "page": "36–41",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Statistic Analysis of Safety Accidents in Filling Stations Based on Big Data",
                "URL": "https://doi.org/10.1145/3414274.3414280"
            }
        },
        {
            "10.1145/3543106.3543115": {
                "id": "10.1145/3543106.3543115",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Li",
                        "given": "Kai"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2022,
                            5,
                            13
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2022,
                            5,
                            13
                        ]
                    ]
                },
                "abstract": "With the rapid development of China's economy, the degree of integration of economics and management is deepening. Based on the statistical analysis method of big data, the trend and law of economic development can be obtained. Big data statistical methods are widely used in the field of economics and improve work efficiency. Effective statistical analysis of data can not only reflect the operation of the product in time, but also reflect the market demand for the product. Therefore, this paper studies the role of big data statistical analysis methods in the field of economic management. This paper first classifies and sorts out the representative quantitative research methods and models in the era of big data, and then based on the BP neural network model and combines 36 indicator data to make multivariate forecasts for China's consumer price index (CPI). The research results show that the prediction results of the BP neural network are good.",
                "call-number": "10.1145/3543106.3543115",
                "collection-title": "ICEMC '22",
                "container-title": "Proceedings of the 2022 International Conference on E-business and Mobile Commerce",
                "DOI": "10.1145/3543106.3543115",
                "event-place": "Seoul, Republic of Korea",
                "ISBN": "9781450397162",
                "keyword": "BP neural network, Quantitative economy, CPI, Big data",
                "number-of-pages": "5",
                "page": "54–58",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "A Study on the Economic Model of Volume in the Age of Big Data",
                "URL": "https://doi.org/10.1145/3543106.3543115"
            }
        },
        {
            "10.1145/2905055.2905124": {
                "id": "10.1145/2905055.2905124",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Khodke",
                        "given": "Priti"
                    },
                    {
                        "family": "Lawange",
                        "given": "Saurabh"
                    },
                    {
                        "family": "Bhagat",
                        "given": "Amol"
                    },
                    {
                        "family": "Dongre",
                        "given": "Kiran"
                    },
                    {
                        "family": "Ingole",
                        "given": "Chetan"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            3,
                            4
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2016,
                            3,
                            4
                        ]
                    ]
                },
                "abstract": "Internet search is done by exploring the link graph and keyword frequency. In 2012, Google released \"Knowledge Graph\" --Semantic Web. The human reasoning can be enhanced by the use semantic web an emerging area. Most of the current applications link open data views due to which there is huge flow of data in semantic web, particularly Resource Description Framework (RDF) data. In the semantic web research community this leads to design and development of scalable data processing techniques for RDF data. The aim of semantic web is to make available semantically connected data across the globe. This is a review paper giving analysis of techniques implemented to achieve the aim of semantic web, various approaches to processes RDF data. Within the semantic web community, RDF is a common acronym because it forms one of the basic building blocks for forming the web of semantic data, called a \"graph database\". This paper compares various methodologies followed by different researchers along with the results analysis of implemented techniques over different datasets.",
                "call-number": "10.1145/2905055.2905124",
                "collection-number": "66",
                "collection-title": "ICTCS '16",
                "container-title": "Proceedings of the Second International Conference on Information and Communication Technology for Competitive Strategies",
                "DOI": "10.1145/2905055.2905124",
                "event-place": "Udaipur, India",
                "ISBN": "9781450339629",
                "keyword": "RDF Graph, Hadoop, SPARQL, MapReduce, Semantic Web, Big Data",
                "number": "Article 66",
                "number-of-pages": "6",
                "page": "1–6",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Query Processing over Large RDF using SPARQL in Big Data",
                "URL": "https://doi.org/10.1145/2905055.2905124"
            }
        },
        {
            "10.1145/3563042": {
                "id": "10.1145/3563042",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Liu",
                        "given": "Yisheng"
                    },
                    {
                        "family": "Tang",
                        "given": "Anying"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2022,
                            8,
                            24
                        ]
                    ]
                },
                "abstract": "In order to improve the forecasting accuracy of economic situation, a government economic situation forecasting method based on big data analysis is proposed. According to the hardware structure of the system, STC12C5608AD is used as the data acquisition terminal chip to simplify the circuit. The proposed forecasting method can give real-time early warning to the government's economic situation. The software part screens the influencing factors of government economic development, constructs a government economic development index system, collects government economic index data, cleans, clusters, classifies and standardizes the government economic index data, and extracts the preprocessed government economic index data from the preprocessed government economic index data through data mining. The economic development features are extracted and then input into the neural network. After training and learning, the predicted value of the economic situation is output, and the economic situation level is classified. The experimental results show that the proposed method reduces the error rate of economic situation forecast, shortens the forecast time, improves the forecast accuracy and efficiency, with the peak error ratio not exceeding 15%.",
                "call-number": "10.1145/3563042",
                "container-title": "Digit. Gov.: Res. Pract.",
                "DOI": "10.1145/3563042",
                "ISSN": "2691-199X",
                "keyword": "Government economy, Situation forecast, Economic situation, Big data analysis",
                "note": "Just Accepted",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Prediction Method of Government Economic Situation Based on Big Data Analysis",
                "URL": "https://doi.org/10.1145/3563042"
            }
        },
        {
            "10.1145/3274005.3274021": {
                "id": "10.1145/3274005.3274021",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Fotache",
                        "given": "Marin"
                    },
                    {
                        "family": "Greavu-Şerban",
                        "given": "Valerică"
                    },
                    {
                        "family": "Hrubaru",
                        "given": "Ionuţ"
                    },
                    {
                        "family": "Tică",
                        "given": "Alexandru"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            9,
                            13
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            9,
                            13
                        ]
                    ]
                },
                "abstract": "Big Data technologies brought the idea of parallel processing on cheaper commodity servers. When dealing with huge amount of data, instead of migrating to more performant and costly hardware platforms, or buying resources in cloud, it is more affordable to add a number of cheaper servers as nodes for data processing and/or storage. NoSQL data stores, Hadoop ecosystems, NewSQL platforms have proved viable for Big Data storage and processing. In this paper we were concerned with setting up a platform for big data processing using commodity workstations. Many small and medium sized companies have limited resources and their workstations remain unused for more than 12 hours a day. Here Beowulf Cluster Computing could prove useful. Apache Impala was installed as part of a Hadoop distribution on a 9-node cluster. Three TPC-H database schema were loaded for the scale factors of 1, 2 and 10GB. A series of 100 SQL queries were randomly generated and executed for each scale factor. Results were collected and analyzed for determining if the cluster can provide a decent level of data processing performance.",
                "call-number": "10.1145/3274005.3274021",
                "collection-title": "CompSysTech'18",
                "container-title": "Proceedings of the 19th International Conference on Computer Systems and Technologies",
                "DOI": "10.1145/3274005.3274021",
                "event-place": "Ruse, Bulgaria",
                "ISBN": "9781450364256",
                "keyword": "Query performance, Beowulf clustering, Distributed computing, Impala, Hadoop",
                "number-of-pages": "6",
                "page": "110–115",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Big Data Technologies on Commodity Workstations: A Basic Setup for Apache Impala",
                "URL": "https://doi.org/10.1145/3274005.3274021"
            }
        },
        {
            "10.1145/3465375": {
                "id": "10.1145/3465375",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Song",
                        "given": "Jie"
                    },
                    {
                        "family": "He",
                        "given": "Qiang"
                    },
                    {
                        "family": "Chen",
                        "given": "Feifei"
                    },
                    {
                        "family": "Yuan",
                        "given": "Ye"
                    },
                    {
                        "family": "Yu",
                        "given": "Ge"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            8,
                            21
                        ]
                    ]
                },
                "abstract": "In big data query processing, there is a trade-off between query accuracy and query efficiency, for example, sampling query approaches trade-off query completeness for efficiency. In this article, we argue that query performance can be significantly improved by slightly losing the possibility of query completeness, that is, the chance that a query is complete. To quantify the possibility, we define a new concept, Probability of query Completeness (hereinafter referred to as PC). For example, If a query is executed 100 times, PC = 0.95 guarantees that there are no more than 5 incomplete results among 100 results. Leveraging the probabilistic data placement and scanning, we trade off PC for query performance. In the article, we propose PoBery (POssibly-complete Big data quERY), a method that supports neither complete queries nor incomplete queries, but possibly-complete queries. The experimental results conducted on HiBench prove that PoBery can significantly accelerate queries while ensuring the PC. Specifically, it is guaranteed that the percentage of complete queries is larger than the given PC confidence. Through comparison with state-of-the-art key-value stores, we show that while Drill-based PoBery performs as fast as Drill on complete queries, it is 1.7 ×, 1.1 ×, and 1.5 × faster on average than Drill, Impala, and Hive, respectively, on possibly-complete queries.",
                "call-number": "10.1145/3465375",
                "collection-number": "23",
                "container-title": "ACM/IMS Trans. Data Sci.",
                "DOI": "10.1145/3465375",
                "ISSN": "2691-1922",
                "issue": "3",
                "keyword": "key-value stores, data query, query completeness, probability, scanning, data placement, Big data",
                "number": "Article 23",
                "number-of-pages": "28",
                "page": "1–28",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "August 2021",
                "title": "PoBery: Possibly-complete Big Data Queries with Probabilistic Data Placement and Scanning",
                "URL": "https://doi.org/10.1145/3465375",
                "volume": "2"
            }
        },
        {
            "10.5555/3291291.3291361": {
                "id": "10.5555/3291291.3291361",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Arruda",
                        "given": "Darlan"
                    },
                    {
                        "family": "Madhavji",
                        "given": "Nazim H."
                    },
                    {
                        "family": "Taylor",
                        "given": "Colin"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            10,
                            29
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            10,
                            29
                        ]
                    ]
                },
                "abstract": "Research from Gartner (2015) indicates that, in 2017, 60% of Big Data projects failed or did not provide the expected benefits [1]. However, in November 2017, Nick Heudecker, a Gartner analyst, posted in his twitter account that they were too conservative. The Big Data project failure rate is now close to 85%. The reasons are not only related to technology itself [2]. It is a mix of environmental, technological and managerial problems. Some of the reasons for Big Data projects failure are: At the project level [3], [4]: missing link to business objectives, lacking big data skills, relying too much on the data, failing to convince executives, and poor planning; At the technical level [5]: Rapid technology changes, difficulty in selecting Big Data technologies to address the systems and project requirements, complex integration between new and old systems, computation of intensive analytics, and the necessity of high scalability, availability and reliability, to name a few. Further, a previous study [6] has shown that there is approximately a 80:20 split in the industry focus in favor of algorithms for analytics and infrastructure, thereby shortchanging the aspects of creating and evolving applications and services concerned with Big Data.",
                "call-number": "10.5555/3291291.3291361",
                "collection-title": "CASCON '18",
                "container-title": "Proceedings of the 28th Annual International Conference on Computer Science and Software Engineering",
                "event-place": "Markham, Ontario, Canada",
                "number-of-pages": "3",
                "page": "407–409",
                "publisher": "IBM Corp.",
                "publisher-place": "USA",
                "title": "CASCON workshop on developing big data applications and services"
            }
        },
        {
            "10.1145/3281375.3281393": {
                "id": "10.1145/3281375.3281393",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Alouneh",
                        "given": "Sahel"
                    },
                    {
                        "family": "Hababeh",
                        "given": "Ismail"
                    },
                    {
                        "family": "Alajrami",
                        "given": "Tamer"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            9,
                            25
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            9,
                            25
                        ]
                    ]
                },
                "abstract": "In recent years, big data and cloud computing are considered key trends of modern computer technology. Extracting valuable information is the key purpose of analyzing big data that needs to be secured in order to avoid any potential risks. Most cloud systems applications contain sensitive data, such as; financial, legal and private information. Therefore, threats on such data may put cloud systems holding this data at high risk. The demand on securing cloud systems applications has been increasing rapidly; however, big data protection is still a challenge. This paper proposes a new methodology to protect big data during analysis by classifying data before any action such as moving, copying or processing take place. Big data files are classified according to the criticality level of their contents into three categories from the most to the least sensitive: restricted, confidential and public. Based on big data classification, the encryption algorithm AES 128 is applied on confidential big data, while the encryption algorithm AES 256 is applied on the restricted big data files. The experimental results show that our method enhances the performance of big data analysis systems and outperforms other approaches in the literature.",
                "call-number": "10.1145/3281375.3281393",
                "collection-title": "MEDES '18",
                "container-title": "Proceedings of the 10th International Conference on Management of Digital EcoSystems",
                "DOI": "10.1145/3281375.3281393",
                "event-place": "Tokyo, Japan",
                "ISBN": "9781450356220",
                "keyword": "threats, data classification, encryption",
                "number-of-pages": "4",
                "page": "106–109",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Toward big data analysis to improve enterprise information security",
                "URL": "https://doi.org/10.1145/3281375.3281393"
            }
        },
        {
            "10.1145/2945408.2945419": {
                "id": "10.1145/2945408.2945419",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Bersani",
                        "given": "Marcello M."
                    },
                    {
                        "family": "Marconi",
                        "given": "Francesco"
                    },
                    {
                        "family": "Rossi",
                        "given": "Matteo"
                    },
                    {
                        "family": "Erascu",
                        "given": "Madalina"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            7,
                            21
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2016,
                            7,
                            21
                        ]
                    ]
                },
                "abstract": "Quality-driven frameworks for developing data-intensive applications are becoming more and more popular, following the remarkable popularity of Big Data approaches. The DICE framework, designed within the DICE project (www.dice-h2020.eu), has the goal of offering a novel profile and tools for data-aware quality-driven development. One of its tools is the DICE Verification Tool (D-VerT), which allows designers to evaluate their design against safety properties, such as reachability of undesired configurations of the system. This paper describes the first version of D-VerT, available open source at github.com/dice-project/DICE-Verification.",
                "call-number": "10.1145/2945408.2945419",
                "collection-title": "QUDOS 2016",
                "container-title": "Proceedings of the 2nd International Workshop on Quality-Aware DevOps",
                "DOI": "10.1145/2945408.2945419",
                "event-place": "Saarbrücken, Germany",
                "ISBN": "9781450344111",
                "keyword": "Formal verification, temporal logic",
                "number-of-pages": "2",
                "page": "44–45",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "A tool for verification of big-data applications",
                "URL": "https://doi.org/10.1145/2945408.2945419"
            }
        },
        {
            "10.1145/3149572.3149575": {
                "id": "10.1145/3149572.3149575",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Francisco",
                        "given": "Maritza M. C."
                    },
                    {
                        "family": "Alves-Souza",
                        "given": "Solange N."
                    },
                    {
                        "family": "Campos",
                        "given": "Edit G. L."
                    },
                    {
                        "family": "De Souza",
                        "given": "Luiz S."
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            10,
                            9
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            10,
                            9
                        ]
                    ]
                },
                "abstract": "Data quality (DQ) is an important issue for modern organizations, mainly for decision-making based on information, using solutions such as CRM, Business Analytics, and Big Data. In order to obtain quality data, it is necessary to implement methods, processes, and specific techniques that handle information as a product, with well established, controlled, and managed production processes. The literature provides several types of quality data management methodologies that treat structured data, and few treating semi- and non-structured data. Choosing the methodology to be adopted is one the major issues faced by organizations, when challenged to treat the data quality in a systematic manner. This paper makes a comparative analysis between TDQM -- Total Data Quality Management and TIQM -- Total Information Quality Management approaches, focusing on data quality problems in the context of a CRM -- Costumer Relationship Management application. Such analysis identifies the strengths and weaknesses of each methodology and suggests the most suitable for the CRM scenario.",
                "call-number": "10.1145/3149572.3149575",
                "collection-title": "ICIME 2017",
                "container-title": "Proceedings of the 9th International Conference on Information Management and Engineering",
                "DOI": "10.1145/3149572.3149575",
                "event-place": "Barcelona, Spain",
                "ISBN": "9781450353373",
                "keyword": "data quality methodology, Data quality, data quality dimensions, data quality problems, data quality management",
                "number-of-pages": "6",
                "page": "40–45",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Total Data Quality Management and Total Information Quality Management Applied to Costumer Relationship Management",
                "URL": "https://doi.org/10.1145/3149572.3149575"
            }
        },
        {
            "10.1145/2998476.2998498": {
                "id": "10.1145/2998476.2998498",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Agarwal",
                        "given": "Bhoomika"
                    },
                    {
                        "family": "Ravikumar",
                        "given": "Abhiram"
                    },
                    {
                        "family": "Saha",
                        "given": "Snehanshu"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            10,
                            21
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2016,
                            10,
                            21
                        ]
                    ]
                },
                "abstract": "In today's world data is being generated at a tremendous pace and there have to be enough measures in place to verify the nature of big data. Analysis performed on 'dirty' data may lead to erroneous insights and thereby shaping decisions poorly. The aspect of big data that deals with its correctness is known as big data veracity. Trusting the data acquired goes a long way in implementing decisions from an automated decision-making system and veracity helps to validate the data acquired. In this paper, we present our solution to the big data veracity problem using crowdsourcing techniques. Our solution involves the use of sentiment analysis, which deals with identifying the sentiment expressed in a piece of text. As a proof of concept, we have developed an app that requires users to tag tweets as per the sentiment it evokes in them. Each tweet would therefore get ratified by hundreds of our participants and the sentiment associated to the tweet gets tagged. The tagged emotion was then evaluated against the verified emotion as compared to a verified data set. This analysis was then plotted on a ROC curve and also evaluated against verified data using a Bayesian predictor trained with a trinomial function. As can be seen, an accuracy of 81% was obtained as displayed by the ROC curve and 89% through the Bayesian predictor. Also, a MAP analysis of the Bayesian predictor yields neutral sentiment as the most probable hypothesis. By doing this, we have proven that crowdsourcing of sentiment analysis is a viable solution to the problem of big data veracity and therefore an aid in making better decisions.",
                "call-number": "10.1145/2998476.2998498",
                "collection-title": "COMPUTE '16",
                "container-title": "Proceedings of the 9th Annual ACM India Conference",
                "DOI": "10.1145/2998476.2998498",
                "event-place": "Gandhinagar, India",
                "ISBN": "9781450348089",
                "keyword": "Tweet Mining, Bayesian Predictor, Sentiment Analysis, Big Data, Crowdsourcing, Machine Learning",
                "number-of-pages": "8",
                "page": "153–160",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "A Novel Approach to Big Data Veracity using Crowdsourcing Techniques and Bayesian Predictors",
                "URL": "https://doi.org/10.1145/2998476.2998498"
            }
        },
        {
            "10.1145/2649387.2660825": {
                "id": "10.1145/2649387.2660825",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "McDermott",
                        "given": "Jason"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2014,
                            9,
                            20
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2014,
                            9,
                            20
                        ]
                    ]
                },
                "abstract": "The advent of multiple new technologies for measuring many components in biological systems offers a huge opportunity and challenge for researchers. An important question is how to make sense of the mountains of data that describe different aspects of the same, or similar, biological systems. We are taking several approaches to this problem in terms of statistical methods and data mining, network and pathway analysis, and generation of testable biological hypotheses. We discuss applications of these approaches to study host-pathogen interactions and cancer, and talk about future opportunities and challenges in this area.",
                "call-number": "10.1145/2649387.2660825",
                "collection-title": "BCB '14",
                "container-title": "Proceedings of the 5th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics",
                "DOI": "10.1145/2649387.2660825",
                "event-place": "Newport Beach, California",
                "ISBN": "9781450328944",
                "keyword": "modeling, data integration, proteomics, cancer, systems biology, host-pathogen interactions, omics, network biology, big data",
                "number-of-pages": "1",
                "page": "680",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Promises and challenges in analysis of biological big data",
                "URL": "https://doi.org/10.1145/2649387.2660825"
            }
        },
        {
            "10.1145/2972958.2972967": {
                "id": "10.1145/2972958.2972967",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Liebchen",
                        "given": "Gernot"
                    },
                    {
                        "family": "Shepperd",
                        "given": "Martin"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            9,
                            9
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2016,
                            9,
                            9
                        ]
                    ]
                },
                "abstract": "Context: We revisit our review of data quality within the context of empirical software engineering eight years on from our PROMISE 2008 article.Objective: To assess the extent and types of techniques used to manage quality within data sets. We consider this a particularly interesting question in the context of initiatives to promote sharing and secondary analysis of data sets.Method: We update the 2008 mapping study through four subsequently published reviews and a snowballing exercise.Results: The original study located only 23 articles explicitly considering data quality. This picture has changed substantially as our updated review now finds 283 articles, however, our estimate is that this still represents perhaps 1% of the total empirical software engineering literature.Conclusions: It appears the community is now taking the issue of data quality more seriously and there is more work exploring techniques to automatically detect (and sometimes repair) noise problems. However, there is still little systematic work to evaluate the various data sets that are widely used for secondary analysis; addressing this would be of considerable benefit. It should also be a priority to work collab-oratively with practitioners to add new, higher quality data to the existing corpora.",
                "call-number": "10.1145/2972958.2972967",
                "collection-number": "7",
                "collection-title": "PROMISE 2016",
                "container-title": "Proceedings of the The 12th International Conference on Predictive Models and Data Analytics in Software Engineering",
                "DOI": "10.1145/2972958.2972967",
                "event-place": "Ciudad Real, Spain",
                "ISBN": "9781450347723",
                "keyword": "empirical software engineering, mapping study, data quality",
                "number": "Article 7",
                "number-of-pages": "4",
                "page": "1–4",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Data Sets and Data Quality in Software Engineering: Eight Years On",
                "URL": "https://doi.org/10.1145/2972958.2972967"
            }
        },
        {
            "10.1145/3109453.3109468": {
                "id": "10.1145/3109453.3109468",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Felicetti",
                        "given": "Luca"
                    },
                    {
                        "family": "Femminella",
                        "given": "Mauro"
                    },
                    {
                        "family": "Ivanov",
                        "given": "Todor"
                    },
                    {
                        "family": "Lio'",
                        "given": "Pietro"
                    },
                    {
                        "family": "Reali",
                        "given": "Gianluca"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            9,
                            27
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            9,
                            27
                        ]
                    ]
                },
                "abstract": "We present a novel architecture for analyzing molecular communications systems in blood vessels for drug delivery and monitoring. This architecture leverages a big data platform for simultaneously using data produced by the existing simulation platforms, health records, and medical data acquisition systems. An included machine learning engine may provide useful insight for medical purposes.",
                "call-number": "10.1145/3109453.3109468",
                "collection-number": "14",
                "collection-title": "NanoCom '17",
                "container-title": "Proceedings of the 4th ACM International Conference on Nanoscale Computing and Communication",
                "DOI": "10.1145/3109453.3109468",
                "event-place": "Washington, D.C.",
                "ISBN": "9781450349314",
                "keyword": "molecular communications, drug delivery, blood vessels, big data",
                "number": "Article 14",
                "number-of-pages": "2",
                "page": "1–2",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "A big-data layered architecture for analyzing molecular communications systems in blood vessels",
                "URL": "https://doi.org/10.1145/3109453.3109468"
            }
        },
        {
            "10.1145/3327964.3328493": {
                "id": "10.1145/3327964.3328493",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "To",
                        "given": "Alex"
                    },
                    {
                        "family": "Meymandpour",
                        "given": "Rouzbeh"
                    },
                    {
                        "family": "Davis",
                        "given": "Joseph G."
                    },
                    {
                        "family": "Jourjon",
                        "given": "Guillaume"
                    },
                    {
                        "family": "Chan",
                        "given": "Jonathan"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            6,
                            30
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            6,
                            30
                        ]
                    ]
                },
                "abstract": "For network analysts, understanding how traffic flows through a network is crucial to network management and forensics such as network monitoring, vulnerability assessment and defence. In order to understand how traffic flows through a network, network analysts typically access multiple, disparate data sources and mentally fuse this information. Providing some sort of automated support is crucial for network management. However, information about the quality of the network data sources is essential in order to build analyst's trust in automated tools. This paper presents SydNet, a novel Linked Data quality assessment framework which allows analysts to define quality dimensions and metrics which provide an accurate reflection of the quality of the data sources. The SydNet architecture also provides a number of novel fusion heuristics which can be used to fuse data from various network data sources. We demonstrate the utility of the SydNet architecture using CAIDA longitudinal topological data from a recent 24 months period and we demonstrate that our approach was able to detect dataset quality anomalies that would require further investigation.",
                "call-number": "10.1145/3327964.3328493",
                "collection-number": "4",
                "collection-title": "GRADES-NDA'19",
                "container-title": "Proceedings of the 2nd Joint International Workshop on Graph Data Management Experiences & Systems (GRADES) and Network Data Analytics (NDA)",
                "DOI": "10.1145/3327964.3328493",
                "event-place": "Amsterdam, Netherlands",
                "ISBN": "9781450367899",
                "keyword": "Data quality, Network, CAIDA",
                "number": "Article 4",
                "number-of-pages": "8",
                "page": "1–8",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "A Linked Data Quality Assessment Framework for Network Data",
                "URL": "https://doi.org/10.1145/3327964.3328493"
            }
        },
        {
            "10.1145/2783258.2790458": {
                "id": "10.1145/2783258.2790458",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "John",
                        "given": "George"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2015,
                            8,
                            10
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2015,
                            8,
                            10
                        ]
                    ]
                },
                "abstract": "In 2008, Rocket Fuel's founders saw a gap in the digital advertising market. None of the existing players were building autonomous systems based on big data and artificial intelligence, but instead they were offering fairly simple technology and relying on human campaign managers to drive success. Five years later in 2013, Rocket Fuel had the best technology IPO of the year on NASDAQ, reported $240 million in revenue, and was ranked by accounting firm Deloitte as the #1 fastest-growing technology company in North America. Along the way we learned that it's okay to be bold in our expectations of what is possible with fully autonomous systems, we learned that mainstream customers will buy advanced technology if it's delivered in a familiar way, and we also learned that it's incredibly difficult to debug the complex \"robot psychology\" when a number of complex autonomous systems interact. We also had excellent luck and timing: as we were building the company, real-time ad impression-level auctions with machine-to-machine buying and selling became commonplace, and marketers became increasingly focused on delivering better results for their company and delivering better personalized and relevant digital experiences for their customers. The case study presentation will present a fast-paced overview of the business and technology context for Rocket Fuel at inception and at present, key learnings and decisions, and the road ahead.",
                "call-number": "10.1145/2783258.2790458",
                "collection-title": "KDD '15",
                "container-title": "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
                "DOI": "10.1145/2783258.2790458",
                "event-place": "Sydney, NSW, Australia",
                "ISBN": "9781450336642",
                "keyword": "computational advertising, big data analytics, advertising, real-time bidding, artificial intelligence",
                "number-of-pages": "1",
                "page": "1629",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "How Artificial Intelligence and Big Data Created Rocket Fuel: A Case Study",
                "URL": "https://doi.org/10.1145/2783258.2790458"
            }
        },
        {
            "10.1145/3456887.3456908": {
                "id": "10.1145/3456887.3456908",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Le",
                        "given": "Fei"
                    },
                    {
                        "family": "Tan",
                        "given": "Wenqian"
                    },
                    {
                        "family": "Le",
                        "given": "Yimin"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            5,
                            25
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2021,
                            5,
                            25
                        ]
                    ]
                },
                "abstract": "Economic management courses have the characteristics of specialty and systematization, which require higher comprehensive ability of students. In addition to mastering rich professional knowledge, they also need to use the second language for communication. Therefore, the construction of bilingual teaching mode has laid a foundation for the cultivation of modern comprehensive talents, which is conducive to promoting the overall development of students and improving the teaching level of economic management courses. Especially in the era of big data, we must change the traditional teaching ideas and methods to adapt to the requirements and characteristics of education reform. This paper will analyze the concept and level of bilingual teaching, put forward the necessity and existing problems of bilingual teaching of economic and management courses, and explore the construction strategy of bilingual teaching mode of economic management courses in the era of big data.",
                "call-number": "10.1145/3456887.3456908",
                "collection-title": "CIPAE 2021",
                "container-title": "2021 2nd International Conference on Computers, Information Processing and Advanced Education",
                "DOI": "10.1145/3456887.3456908",
                "event-place": "Ottawa, ON, Canada",
                "ISBN": "9781450389969",
                "keyword": "Big Data, Bilingual Teaching, Economic Management Courses, Teaching Mode",
                "number-of-pages": "4",
                "page": "99–102",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Bilingual Teaching Mode of Economic Management Courses in the Era of Big Data",
                "URL": "https://doi.org/10.1145/3456887.3456908"
            }
        },
        {
            "10.1145/3089251.3089252": {
                "id": "10.1145/3089251.3089252",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Xue",
                        "given": "Bing"
                    },
                    {
                        "family": "Zhang",
                        "given": "Mengjie"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            5,
                            2
                        ]
                    ]
                },
                "abstract": "Known as the GIGO (Garbage In, Garbage Out) principle, the quality of the input data highly influences or even determines the quality of the output of any machine learning, big data and data mining algorithm. The input data which is often represented by a set of features may suffer from many issues. Feature manipulation is an effective means to improve the feature set quality, but it is a challenging task. Evolutionary computation (EC) techniques have shown advantages and achieved good performance in feature manipulation. This paper reviews recent advances on EC based feature manipulation methods in classifcation, clustering, regression, incomplete data, and image analysis, to provide the community the state-of-the-art work in the field.",
                "call-number": "10.1145/3089251.3089252",
                "container-title": "SIGEVOlution",
                "DOI": "10.1145/3089251.3089252",
                "issue": "1",
                "number-of-pages": "8",
                "page": "4–11",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "March 2017",
                "title": "Evolutionary feature manipulation in data mining/big data",
                "URL": "https://doi.org/10.1145/3089251.3089252",
                "volume": "10"
            }
        },
        {
            "10.1145/3545897.3545903": {
                "id": "10.1145/3545897.3545903",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Li",
                        "given": "Ying"
                    },
                    {
                        "family": "Wang",
                        "given": "Jinliang"
                    },
                    {
                        "family": "Lu",
                        "given": "Kunxiu"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2022,
                            6,
                            15
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2022,
                            6,
                            15
                        ]
                    ]
                },
                "abstract": "As fresh e-commerce is believed to be a blue ocean in e-commerce sector, investors contribute to it successively, which results in the rapid development of fresh e-commerce. However, with the diversification of consumers’ demands and the unprofitable circumstance of fresh e-commerce, fresh e-commerce faces great crisis currently. Its fast growth provides a wider platform for reasonable application of big data technology. Meanwhile, big data technology also brings new optimization ideas for operation of fresh e-commerce. This paper mainly started from the characteristics of fresh e-commerce and big data technology to analyze the status quo and problems of development of fresh e-commerce, so as to study the application status of big data technology to fresh e-commerce, with a focus on analyzing the application status of big data technology to logistics and target customers management in fresh e-commerce.",
                "call-number": "10.1145/3545897.3545903",
                "collection-title": "ICIEB '22",
                "container-title": "Proceedings of the 2022 3rd International Conference on Internet and E-Business",
                "DOI": "10.1145/3545897.3545903",
                "event-place": "Madrid, Spain",
                "ISBN": "9781450397322",
                "keyword": "Fresh e-commerce, Operation, Big data technology",
                "number-of-pages": "6",
                "page": "38–43",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Study on Application of Big Data Technology to Operation of Fresh E-commerce",
                "URL": "https://doi.org/10.1145/3545897.3545903"
            }
        },
        {
            "10.1145/3372938.3372962": {
                "id": "10.1145/3372938.3372962",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Hibti",
                        "given": "Meryem"
                    },
                    {
                        "family": "Baïna",
                        "given": "Karim"
                    },
                    {
                        "family": "Benatallah",
                        "given": "Boualem"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            10,
                            23
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            10,
                            23
                        ]
                    ]
                },
                "abstract": "The Internet of Things (IoT) is exploding. It is made up of billions of smart devices -from minuscule chips to mammoth machines - that use wireless technology to talk to each other (and to us). IoT infrastructures can vary from instrumented connected devices providing data externally to smart, and autonomous systems. To accompany data explosion resulting, among others, from IoT, Big data analytics processes examine large data sets to uncover hidden patterns, unknown correlations between collected events, either at a very technical level (incident/anomaly detection, predictive maintenance) or at business level (customer preferences, market trends, revenue opportunities) to provide improved operational efficiency, better customer service, competitive advantages over rival organizations, etc. In order to capitalize business value of the data generated by IoT sensors, IoT, Big Data Analytics/IA need to meet in the middle. One critical use case for IoT is to warn organizations when a product or service is at risk. The aim of this paper is to present a first proposal of IoT-Big Data-IA architectural patterns catalogues with a Blockchain implementation perspective in seek of design methodologies artifacts.",
                "call-number": "10.1145/3372938.3372962",
                "collection-number": "24",
                "collection-title": "BDIoT'19",
                "container-title": "Proceedings of the 4th International Conference on Big Data and Internet of Things",
                "DOI": "10.1145/3372938.3372962",
                "event-place": "Rabat, Morocco",
                "ISBN": "9781450372404",
                "keyword": "AI, Big Data analytics, decision making, IoT, patterns, swarm intelligence",
                "number": "Article 24",
                "number-of-pages": "8",
                "page": "1–8",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Towards Swarm Intelligence Architectural Patterns: an IoT-Big Data-AI-Blockchain convergence perspective",
                "URL": "https://doi.org/10.1145/3372938.3372962"
            }
        },
        {
            "10.1145/2834118": {
                "id": "10.1145/2834118",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Ahmad",
                        "given": "Awais"
                    },
                    {
                        "family": "Paul",
                        "given": "Anand"
                    },
                    {
                        "family": "Rathore",
                        "given": "Mazhar"
                    },
                    {
                        "family": "Chang",
                        "given": "Hangbae"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            6,
                            7
                        ]
                    ]
                },
                "abstract": "Machine-to-Machine communication (M2M) is nowadays increasingly becoming a world-wide network of interconnected devices uniquely addressable, via standard communication protocols. The prevalence of M2M is bound to generate a massive volume of heterogeneous, multisource, dynamic, and sparse data, which leads a system towards major computational challenges, such as, analysis, aggregation, and storage. Moreover, a critical problem arises to extract the useful information in an efficient manner from the massive volume of data. Hence, to govern an adequate quality of the analysis, diverse and capacious data needs to be aggregated and fused. Therefore, it is imperative to enhance the computational efficiency for fusing and analyzing the massive volume of data. Therefore, to address these issues, this article proposes an efficient, multidimensional, big data analytical architecture based on the fusion model. The basic concept implicates the division of magnitudes (attributes), i.e., big datasets with complex magnitudes can be altered into smaller data subsets using five levels of the fusion model that can be easily processed by the Hadoop Processing Server, resulting in formalizing the problem of feature extraction applications using earth observatory system, social networking, or networking applications. Moreover, a four-layered network architecture is also proposed that fulfills the basic requirements of the analytical architecture. The feasibility and efficiency of the proposed algorithms used in the fusion model are implemented on Hadoop single-node setup on UBUNTU 14.04 LTS core i5 machine with 3.2GHz processor and 4GB memory. The results show that the proposed system architecture efficiently extracts various features (such as land and sea) from the massive volume of satellite data.",
                "call-number": "10.1145/2834118",
                "collection-number": "39",
                "container-title": "ACM Trans. Embed. Comput. Syst.",
                "DOI": "10.1145/2834118",
                "ISSN": "1539-9087",
                "issue": "2",
                "keyword": "Hadoop processing server, Big Data, data fusion, M2M",
                "number": "Article 39",
                "number-of-pages": "25",
                "page": "1–25",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "May 2016",
                "title": "An Efficient Multidimensional Big Data Fusion Approach in Machine-to-Machine Communication",
                "URL": "https://doi.org/10.1145/2834118",
                "volume": "15"
            }
        },
        {
            "10.1145/3236024.3264586": {
                "id": "10.1145/3236024.3264586",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Gulzar",
                        "given": "Muhammad Ali"
                    },
                    {
                        "family": "Wang",
                        "given": "Siman"
                    },
                    {
                        "family": "Kim",
                        "given": "Miryung"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            10,
                            26
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2018,
                            10,
                            26
                        ]
                    ]
                },
                "abstract": "Developing Big Data Analytics often involves trial and error debugging, due to the unclean nature of datasets or wrong assumptions made about data. When errors (e.g. program crash, outlier results, etc.) arise, developers are often interested in pinpointing the root cause of errors. To address this problem, BigSift takes an Apache Spark program, a user-defined test oracle function, and a dataset as input and outputs a minimum set of input records that reproduces the same test failure by combining the insights from delta debugging with data provenance. The technical contribution of BigSift is the design of systems optimizations that bring automated debugging closer to a reality for data intensive scalable computing. BigSift exposes an interactive web interface where a user can monitor a big data analytics job running remotely on the cloud, write a user-defined test oracle function, and then trigger the automated debugging process. BigSift also provides a set of predefined test oracle functions, which can be used for explaining common types of anomalies in big data analytics--for example, finding the origin of the output value that is more than k standard deviations away from the median. The demonstration video is available at https://youtu.be/jdBsCd61a1Q.",
                "call-number": "10.1145/3236024.3264586",
                "collection-title": "ESEC/FSE 2018",
                "container-title": "Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
                "DOI": "10.1145/3236024.3264586",
                "event-place": "Lake Buena Vista, FL, USA",
                "ISBN": "9781450355735",
                "keyword": "data provenance, Automated debugging, fault localization, big data, data-intensive scalable computing (DISC), and data cleaning",
                "number-of-pages": "4",
                "page": "863–866",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "BigSift: automated debugging of big data analytics in data-intensive scalable computing",
                "URL": "https://doi.org/10.1145/3236024.3264586"
            }
        },
        {
            "10.1145/3460537.3460561": {
                "id": "10.1145/3460537.3460561",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Liu",
                        "given": "Yu"
                    },
                    {
                        "family": "Zeng",
                        "given": "Junfang"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2021,
                            3,
                            26
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2021,
                            3,
                            26
                        ]
                    ]
                },
                "abstract": "Blockchain technology, with its distributed ledger, decentralization, high security, no tampering and other features, helps to solve the problems of data source confirmation, traceability and authorized data sharing. The effective utilization of data to achieve automatic governance and trusted decision-making of the city is known as \"City Brain\". The city brain is the key in smart city development, while the key to build a city brain is data resources. Aiming at the challenges existing in the construction of big data platform, this paper proposes a new solution based on blockchain, establishes the entity model and data model, analyzes the business model, designs the blockchain data platform framework and the cloud-blockchain integrated operation mode, and at last discusses the issues concerned in the application.",
                "call-number": "10.1145/3460537.3460561",
                "collection-title": "ICBCT '21",
                "container-title": "2021 The 3rd International Conference on Blockchain Technology",
                "DOI": "10.1145/3460537.3460561",
                "event-place": "Shanghai, China",
                "ISBN": "9781450389624",
                "keyword": "Smart contract, City brain, Index on blockchain, Cloud-blockchain integrated",
                "number-of-pages": "8",
                "page": "82–89",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Blockchain based Big Data Platform of City Brain",
                "URL": "https://doi.org/10.1145/3460537.3460561"
            }
        },
        {
            "10.1007/s00778-011-0219-9": {
                "id": "10.1007/s00778-011-0219-9",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Denev",
                        "given": "Dimitar"
                    },
                    {
                        "family": "Mazeika",
                        "given": "Arturas"
                    },
                    {
                        "family": "Spaniol",
                        "given": "Marc"
                    },
                    {
                        "family": "Weikum",
                        "given": "Gerhard"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2011,
                            4,
                            1
                        ]
                    ]
                },
                "abstract": "Web archives preserve the history of born-digital content and offer great potential for sociologists, business analysts, and legal experts on intellectual property and compliance issues. Data quality is crucial for these purposes. Ideally, crawlers should gather coherent captures of entire Web sites, but the politeness etiquette and completeness requirement mandate very slow, long-duration crawling while Web sites undergo changes. This paper presents the SHARC framework for assessing the data quality in Web archives and for tuning capturing strategies toward better quality with given resources. We define data quality measures, characterize their properties, and develop a suite of quality-conscious scheduling strategies for archive crawling. Our framework includes single-visit and visit---revisit crawls. Single-visit crawls download every page of a site exactly once in an order that aims to minimize the \"blur\" in capturing the site. Visit---revisit strategies revisit pages after their initial downloads to check for intermediate changes. The revisiting order aims to maximize the \"coherence\" of the site capture(number pages that did not change during the capture). The quality notions of blur and coherence are formalized in the paper. Blur is a stochastic notion that reflects the expected number of page changes that a time-travel access to a site capture would accidentally see, instead of the ideal view of a instantaneously captured, \"sharp\" site. Coherence is a deterministic quality measure that counts the number of unchanged and thus coherently captured pages in a site snapshot. Strategies that aim to either minimize blur or maximize coherence are based on prior knowledge of or predictions for the change rates of individual pages. Our framework includes fairly accurate classifiers for change predictions. All strategies are fully implemented in a testbed and shown to be effective by experiments with both synthetically generated sites and a periodic crawl series for different Web sites.",
                "call-number": "10.1007/s00778-011-0219-9",
                "container-title": "The VLDB Journal",
                "DOI": "10.1007/s00778-011-0219-9",
                "ISSN": "1066-8888",
                "issue": "2",
                "keyword": "Blur, Coherence, Crawls strategies, Data quality, Web archiving",
                "number-of-pages": "25",
                "page": "183–207",
                "publisher": "Springer-Verlag",
                "publisher-place": "Berlin, Heidelberg",
                "source": "April     2011",
                "title": "The SHARC framework for data quality in Web archiving",
                "URL": "https://doi.org/10.1007/s00778-011-0219-9",
                "volume": "20"
            }
        },
        {
            "10.1145/2670386.2670388": {
                "id": "10.1145/2670386.2670388",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Goergen",
                        "given": "David"
                    },
                    {
                        "family": "Mendiratta",
                        "given": "Veena"
                    },
                    {
                        "family": "State",
                        "given": "Radu"
                    },
                    {
                        "family": "Engel",
                        "given": "Thomas"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2014,
                            10,
                            1
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2014,
                            10,
                            1
                        ]
                    ]
                },
                "abstract": "Mobile communication flows describe the on-going traffic on the network and are therefore a good indication of what is happening. Analysing these flows can improve the overall quality offered to the users and it can enable operators to detect abnormal patterns and react.This paper will focus on the analysis of cellular communications records. By using the collected call and message exchanges we present a method based on the PageRank algorithm that detects abnormal communications events. Taking the number of calls and the total call duration as parameters we use a weighted version of the PageRank algorithm to further investigate the influence of these parameters on the connected network graph. We proceed by correlating the results obtained with events happening in the respective region and at that time.",
                "call-number": "10.1145/2670386.2670388",
                "collection-number": "8",
                "collection-title": "IPTComm '14",
                "container-title": "Proceedings of the Conference on Principles, Systems and Applications of IP Telecommunications",
                "DOI": "10.1145/2670386.2670388",
                "event-place": "Chicago, Illinois",
                "ISBN": "9781450321242",
                "keyword": "big data processing, page rank, call pattern detection, call data record analysis",
                "number": "Article 8",
                "number-of-pages": "6",
                "page": "1–6",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Analysis of large call data records with big data",
                "URL": "https://doi.org/10.1145/2670386.2670388"
            }
        },
        {
            "10.5555/3026877.3026922": {
                "id": "10.5555/3026877.3026922",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Papadimitriou",
                        "given": "Antonis"
                    },
                    {
                        "family": "Bhagwan",
                        "given": "Ranjita"
                    },
                    {
                        "family": "Chandran",
                        "given": "Nishanth"
                    },
                    {
                        "family": "Ramjee",
                        "given": "Ramachandran"
                    },
                    {
                        "family": "Haeberlen",
                        "given": "Andreas"
                    },
                    {
                        "family": "Singh",
                        "given": "Harmeet"
                    },
                    {
                        "family": "Modi",
                        "given": "Abhishek"
                    },
                    {
                        "family": "Badrinarayanan",
                        "given": "Saikrishna"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            11,
                            2
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2016,
                            11,
                            2
                        ]
                    ]
                },
                "abstract": "Today, enterprises collect large amounts of data and leverage the cloud to perform analytics over this data. Since the data is often sensitive, enterprises would prefer to keep it confidential and to hide it even from the cloud operator. Systems such as CryptDB and Monomi can accomplish this by operating mostly on encrypted data; however, these systems rely on expensive cryptographic techniques that limit performance in true \"big data\" scenarios that involve terabytes of data or more.This paper presents Seabed, a system that enables efficient analytics over large encrypted datasets. In contrast to previous systems, which rely on asymmetric encryption schemes, Seabed uses a novel, additively symmetric homomorphic encryption scheme (ASHE) to perform large-scale aggregations efficiently. Additionally, Seabed introduces a novel randomized encryption scheme called Splayed ASHE, or SPLASHE, that can, in certain cases, prevent frequency attacks based on auxiliary data.",
                "call-number": "10.5555/3026877.3026922",
                "collection-title": "OSDI'16",
                "container-title": "Proceedings of the 12th USENIX conference on Operating Systems Design and Implementation",
                "event-place": "Savannah, GA, USA",
                "ISBN": "9781931971331",
                "number-of-pages": "16",
                "page": "587–602",
                "publisher": "USENIX Association",
                "publisher-place": "USA",
                "title": "Big data analytics over encrypted datasets with seabed"
            }
        },
        {
            "10.1145/2743065.2743097": {
                "id": "10.1145/2743065.2743097",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Karthikeyan",
                        "given": "P."
                    },
                    {
                        "family": "Amudhavel",
                        "given": "J."
                    },
                    {
                        "family": "Abraham",
                        "given": "A."
                    },
                    {
                        "family": "Sathian",
                        "given": "D."
                    },
                    {
                        "family": "Raghav",
                        "given": "R. S."
                    },
                    {
                        "family": "Dhavachelvan",
                        "given": "P."
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2015,
                            3,
                            6
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2015,
                            3,
                            6
                        ]
                    ]
                },
                "abstract": "As technology grows very fast with trendy outcome applications like Social networking, web analysis, bio-informatics network analysis, product analysis, etc., a huge amount of heterogeneous data is delivered in a wide range. Effective management of this huge data is interesting but faces many challenges in accuracy and processing. When a term huge data arrives then a recent and growing field namely BIG DATA comes into the act as it becomes a mass attracter of industry, academia and government for efficient processing of variety of huge data. This paper surveys a various technologies and the different areas where big data is implemented currently with a help of cloud environment [1] and its complete architecture [13]. Following it also explains about the different map reduce techniques and the framework that is being implanted for processing such huge data. Finally we discuss the future on big data processing with the cloud environment and the challenges [28] faced at these areas.",
                "call-number": "10.1145/2743065.2743097",
                "collection-number": "32",
                "collection-title": "ICARCSET '15",
                "container-title": "Proceedings of the 2015 International Conference on Advanced Research in Computer Science Engineering & Technology (ICARCSET 2015)",
                "DOI": "10.1145/2743065.2743097",
                "event-place": "Unnao, India",
                "ISBN": "9781450334419",
                "keyword": "Cloud computing, Security, Hadoop, Big data",
                "number": "Article 32",
                "number-of-pages": "5",
                "page": "1–5",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "A Comprehensive Survey on Variants And Its Extensions Of Big Data In Cloud Environment",
                "URL": "https://doi.org/10.1145/2743065.2743097"
            }
        },
        {
            "10.5555/3204979.3205017": {
                "id": "10.5555/3204979.3205017",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Singh",
                        "given": "Anshuman"
                    },
                    {
                        "family": "Singh",
                        "given": "Sumi"
                    },
                    {
                        "family": "Yousef",
                        "given": "Mahmoud"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2018,
                            5,
                            1
                        ]
                    ]
                },
                "abstract": "Big data is a fast changing area, and it is challenging to design a college course in big data that is up to date and is grounded in the fundamentals of the discipline. Some themes have emerged in the last few years that can be considered candidates for fundamentals of big data. In this paper, we identify and organize these fundamental concepts into a framework that can be used to design a big data course. We also present our course designs based on the framework and student feedback from our offerings in the last two years.",
                "call-number": "10.5555/3204979.3205017",
                "container-title": "J. Comput. Sci. Coll.",
                "ISSN": "1937-4771",
                "issue": "5",
                "number-of-pages": "7",
                "page": "192–198",
                "publisher": "Consortium for Computing Sciences in Colleges",
                "publisher-place": "Evansville, IN, USA",
                "source": "May 2018",
                "title": "A conceptual framework for designing a big data course",
                "volume": "33"
            }
        },
        {
            "10.1145/872757.872875": {
                "id": "10.1145/872757.872875",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Johnson",
                        "given": "Theodore"
                    },
                    {
                        "family": "Dasu",
                        "given": "Tamraparni"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2003,
                            6,
                            9
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2003,
                            6,
                            9
                        ]
                    ]
                },
                "abstract": "Data quality is a serious concern in any data-driven enterprise, often creating misleading findings during data mining, and causing process disruptions in operational databases. The manifestations of data quality problems can be very expensive- \"losing\" customers, \"misplacing\" billions of dollars worth of equipment, misallocated resources due to glitched forecasts, and so on. Solving data quality problems typically requires a very large investment of time and energy -- often 80% to 90% of a data analysis project is spent in making the data reliable enough that the results can be trusted.In this tutorial, we present a multi disciplinary approach to data quality problems. We start by discussing the meaning of data quality and the sources of data quality problems. We show how these problems can be addressed by a multidisciplinary approach, combining techniques from management science, statistics, database research, and metadata management. Next, we present an updated definition of data quality metrics, and illustrate their application with a case study. We conclude with a survey of recent database research that is relevant to data quality problems, and suggest directions for future research.",
                "call-number": "10.1145/872757.872875",
                "collection-title": "SIGMOD '03",
                "container-title": "Proceedings of the 2003 ACM SIGMOD international conference on Management of data",
                "DOI": "10.1145/872757.872875",
                "event-place": "San Diego, California",
                "ISBN": "158113634X",
                "number-of-pages": "1",
                "page": "681",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Data quality and data cleaning: an overview",
                "URL": "https://doi.org/10.1145/872757.872875"
            }
        },
        {
            "10.1145/3341161.3343518": {
                "id": "10.1145/3341161.3343518",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Xylogiannopoulos",
                        "given": "Konstantinos"
                    },
                    {
                        "family": "Karampelas",
                        "given": "Panagiotis"
                    },
                    {
                        "family": "Alhajj",
                        "given": "Reda"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            8,
                            27
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            8,
                            27
                        ]
                    ]
                },
                "abstract": "In recent years, there are very frequent reports of disasters attributed to the climate change and there are several reports that these extreme phenomena will further affect people not only as weather disasters but also indirectly with the shortage of natural resources such as water or food due to the climate change. Towards this direction, there is an on-going research that studies weather phenomena by collecting data not only in the surface of the globe but also at the different levels of the atmosphere. Having such a large volume of data, traditional numerical weather prediction models may not be able to assimilate those data and extract knowledge useful for the prediction of extreme phenomena. Thus, analysis of weather data has been transformed into a big data analytics problem which may enable weather scientists to better understand the interrelations of the weather variables and use the knowledge discovered to improve their prediction models. In this context, the current paper proposes a big data analytics methodology that is able to detect all common patterns between different weather variables in neighboring or distant points in a specific time window revealing useful associations between weather variables which is not possible to detect otherwise with the traditional numerical methods. The proposed methodology is based on a data structure that is able to store the magnitude of the weather data in different dimensions and a pattern detection algorithm which is able to detect all common patterns. The experimental results using weather data from the National Oceanic and Atmospheric Administration (NOAA) revealed interesting otherwise unknown patterns in two weather variables for two specific locations that were studied.",
                "call-number": "10.1145/3341161.3343518",
                "collection-title": "ASONAM '19",
                "container-title": "Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining",
                "DOI": "10.1145/3341161.3343518",
                "event-place": "Vancouver, British Columbia, Canada",
                "ISBN": "9781450368681",
                "keyword": "ARPaD, LERP-RSA, data mining, weather analysis",
                "number-of-pages": "8",
                "page": "749–756",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Multivariate motif detection in local weather big data",
                "URL": "https://doi.org/10.1145/3341161.3343518"
            }
        },
        {
            "10.1145/2723372.2742784": {
                "id": "10.1145/2723372.2742784",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "G.C.",
                        "given": "Paul Suganthan"
                    },
                    {
                        "family": "Sun",
                        "given": "Chong"
                    },
                    {
                        "family": "K.",
                        "given": "Krishna Gayatri"
                    },
                    {
                        "family": "Zhang",
                        "given": "Haojun"
                    },
                    {
                        "family": "Yang",
                        "given": "Frank"
                    },
                    {
                        "family": "Rampalli",
                        "given": "Narasimhan"
                    },
                    {
                        "family": "Prasad",
                        "given": "Shishir"
                    },
                    {
                        "family": "Arcaute",
                        "given": "Esteban"
                    },
                    {
                        "family": "Krishnan",
                        "given": "Ganesh"
                    },
                    {
                        "family": "Deep",
                        "given": "Rohit"
                    },
                    {
                        "family": "Raghavendra",
                        "given": "Vijay"
                    },
                    {
                        "family": "Doan",
                        "given": "AnHai"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2015,
                            5,
                            27
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2015,
                            5,
                            27
                        ]
                    ]
                },
                "abstract": "Big Data industrial systems that address problems such as classification, information extraction, and entity matching very commonly use hand-crafted rules. Today, however, little is understood about the usage of such rules. In this paper we explore this issue. We discuss how these systems differ from those considered in academia. We describe default solutions, their limitations, and reasons for using rules. We show examples of extensive rule usage in industry. Contrary to popular perceptions, we show that there is a rich set of research challenges in rule generation, evaluation, execution, optimization, and maintenance. We discuss ongoing work at WalmartLabs and UW-Madison that illustrate these challenges. Our main conclusions are (1) using rules (together with techniques such as learning and crowdsourcing) is fundamental to building semantics-intensive Big Data systems, and (2) it is increasingly critical to address rule management, given the tens of thousands of rules industrial systems often manage today in an ad-hoc fashion.",
                "call-number": "10.1145/2723372.2742784",
                "collection-title": "SIGMOD '15",
                "container-title": "Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data",
                "DOI": "10.1145/2723372.2742784",
                "event-place": "Melbourne, Victoria, Australia",
                "ISBN": "9781450327589",
                "keyword": "rule management, big data, classification",
                "number-of-pages": "12",
                "page": "265–276",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Why Big Data Industrial Systems Need Rules and What We Can Do About It",
                "URL": "https://doi.org/10.1145/2723372.2742784"
            }
        },
        {
            "10.1145/1951365.1951432": {
                "id": "10.1145/1951365.1951432",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Agrawal",
                        "given": "Divyakant"
                    },
                    {
                        "family": "Das",
                        "given": "Sudipto"
                    },
                    {
                        "family": "El Abbadi",
                        "given": "Amr"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2011,
                            3,
                            21
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2011,
                            3,
                            21
                        ]
                    ]
                },
                "abstract": "Scalable database management systems (DBMS)---both for update intensive application workloads as well as decision support systems for descriptive and deep analytics---are a critical part of the cloud infrastructure and play an important role in ensuring the smooth transition of applications from the traditional enterprise infrastructures to next generation cloud infrastructures. Though scalable data management has been a vision for more than three decades and much research has focussed on large scale data management in traditional enterprise setting, cloud computing brings its own set of novel challenges that must be addressed to ensure the success of data management solutions in the cloud environment. This tutorial presents an organized picture of the challenges faced by application developers and DBMS designers in developing and deploying internet scale applications. Our background study encompasses both classes of systems: (i) for supporting update heavy applications, and (ii) for ad-hoc analytics and decision support. We then focus on providing an in-depth analysis of systems for supporting update intensive web-applications and provide a survey of the state-of-the-art in this domain. We crystallize the design choices made by some successful systems large scale database management systems, analyze the application demands and access patterns, and enumerate the desiderata for a cloud-bound DBMS.",
                "call-number": "10.1145/1951365.1951432",
                "collection-title": "EDBT/ICDT '11",
                "container-title": "Proceedings of the 14th International Conference on Extending Database Technology",
                "DOI": "10.1145/1951365.1951432",
                "event-place": "Uppsala, Sweden",
                "ISBN": "9781450305280",
                "number-of-pages": "4",
                "page": "530–533",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Big data and cloud computing: current state and future opportunities",
                "URL": "https://doi.org/10.1145/1951365.1951432"
            }
        },
        {
            "10.1145/3349341.3349516": {
                "id": "10.1145/3349341.3349516",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Qi",
                        "given": "Baohua"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            7,
                            12
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            7,
                            12
                        ]
                    ]
                },
                "abstract": "The wide application of big data technology is not only an important revolution in the field of information technology, but also a sharp tool to accelerate enterprise innovation worldwide. As an important main force of innovation in China, high-tech enterprises have the characteristics of knowledge-intensive and talent-intensive. Big data has brought about changes in thinking and technology for enterprise knowledge management. Under the background of big data era, the renewal of knowledge management concepts, the establishment of knowledge discovery system, knowledge integration and accumulation system, knowledge learning and application system and knowledge innovation system based on big data are effective measures to promote knowledge interaction and knowledge transformation of enterprises and enhance the efficiency of knowledge innovation.",
                "call-number": "10.1145/3349341.3349516",
                "collection-title": "AICS 2019",
                "container-title": "Proceedings of the 2019 International Conference on Artificial Intelligence and Computer Science",
                "DOI": "10.1145/3349341.3349516",
                "event-place": "Wuhan, Hubei, China",
                "ISBN": "9781450371506",
                "keyword": "Knowledge management system, High-tech enterprises, Big data",
                "number-of-pages": "5",
                "page": "803–807",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Research on Knowledge Management System Construction of High-tech Enterprises Based on Big Data",
                "URL": "https://doi.org/10.1145/3349341.3349516"
            }
        },
        {
            "10.1145/3511707": {
                "id": "10.1145/3511707",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Goel",
                        "given": "Kanika"
                    },
                    {
                        "family": "Leemans",
                        "given": "Sander J. J."
                    },
                    {
                        "family": "Martin",
                        "given": "Niels"
                    },
                    {
                        "family": "Wynn",
                        "given": "Moe T."
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2022,
                            4,
                            5
                        ]
                    ]
                },
                "abstract": "Real-life event logs, reflecting the actual executions of complex business processes, are faced with numerous data quality issues. Extensive data sanity checks and pre-processing are usually needed before historical data can be used as input to obtain reliable data-driven insights. However, most of the existing algorithms in process mining, a field focusing on data-driven process analysis, do not take any data quality issues or the potential effects of data pre-processing into account explicitly. This can result in erroneous process mining results, leading to inaccurate, or misleading conclusions about the process under investigation. To address this gap, we propose data quality annotations for event logs, which can be used by process mining algorithms to generate quality-informed insights. Using a design science approach, requirements are formulated, which are leveraged to propose data quality annotations. Moreover, we present the “Quality-Informed visual Miner” plug-in to demonstrate the potential utility and impact of data quality annotations. Our experimental results, utilising both synthetic and real-life event logs, show how the use of data quality annotations by process mining techniques can assist in increasing the reliability of performance analysis results.",
                "call-number": "10.1145/3511707",
                "collection-number": "97",
                "container-title": "ACM Trans. Knowl. Discov. Data",
                "DOI": "10.1145/3511707",
                "ISSN": "1556-4681",
                "issue": "5",
                "keyword": "Process mining, quality-informed conformance checking, metadata, data quality, quality-informed performance analysis, annotations",
                "number": "Article 97",
                "number-of-pages": "47",
                "page": "1–47",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "October 2022",
                "title": "Quality-Informed Process Mining: A Case for Standardised Data Quality Annotations",
                "URL": "https://doi.org/10.1145/3511707",
                "volume": "16"
            }
        },
        {
            "10.1145/2791347.2791377": {
                "id": "10.1145/2791347.2791377",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Cuzzocrea",
                        "given": "Alfredo"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2015,
                            6,
                            29
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2015,
                            6,
                            29
                        ]
                    ]
                },
                "abstract": "Aggregation and multidimensional analysis are well-known powerful tools for extracting useful knowledge, shaped in a summarized manner, which are being successfully applied to the annoying problem of managing and mining big data produced by large-scale scientific applications. Indeed, in the context of big data analytics, aggregation approaches allow us to provide meaningful descriptions of these data, otherwise impossible for alternative data-intensive analysis tools. On the other hand, multidimensional analysis methodologies introduce fortunate metaphors that significantly empathize the knowledge discovery phase from such huge amounts of data. Following this main trend, several big data aggregation and multidimensional analysis approaches have been proposed recently. The goal of this paper is to (i) provide a comprehensive overview of state-of-the-art techniques and (ii) depict open research challenges and future directions adhering to the reference scientific field.",
                "call-number": "10.1145/2791347.2791377",
                "collection-number": "23",
                "collection-title": "SSDBM '15",
                "container-title": "Proceedings of the 27th International Conference on Scientific and Statistical Database Management",
                "DOI": "10.1145/2791347.2791377",
                "event-place": "La Jolla, California",
                "ISBN": "9781450337090",
                "keyword": "large-scale scientific applications, big data aggregation, multidimensional analysis of big data, big data analytics",
                "number": "Article 23",
                "number-of-pages": "6",
                "page": "1–6",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Aggregation and multidimensional analysis of big data for large-scale scientific applications: models, issues, analytics, and beyond",
                "URL": "https://doi.org/10.1145/2791347.2791377"
            }
        },
        {
            "10.1145/3176653.3176657": {
                "id": "10.1145/3176653.3176657",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Sahoh",
                        "given": "Bukhoree"
                    },
                    {
                        "family": "Choksuriwong",
                        "given": "Anant"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            12,
                            27
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            12,
                            27
                        ]
                    ]
                },
                "abstract": "Emergency management needs to be self-operating and utilize an autonomous processing framework to discover knowledge from various data sources. The aim of Smart Emergency Management (SEM) is to help Executive Management Teams (EMTs) make better decisions and deal with drastic events effectively; SEM needs reliable, accurate, and timely information and knowledge. The exponential growth of social networks, cloud computing, and the Internet of Things means that during emergency events human can share, reuse, and generate vast amounts of data through the application of Social Big Data (SBD). Currently, research tends to focus on the usage of SBD as a knowledge source in SEM, in particular, as a real-time data source for tracking disaster situations in order to manage them more effectively. This paper discusses potential research areas for state-of-the-art SBD analytics in support of SEM, examining fundamental frameworks, methodologies, and technologies for SEM; future trends for SEM using SBD analytics are also considered.",
                "call-number": "10.1145/3176653.3176657",
                "collection-title": "ICIT 2017",
                "container-title": "Proceedings of the 2017 International Conference on Information Technology",
                "DOI": "10.1145/3176653.3176657",
                "event-place": "Singapore, Singapore",
                "ISBN": "9781450363518",
                "keyword": "Machine Learning, Social Big Data Analytics, Smart Emergency Management, Bayesian Deep learning",
                "number-of-pages": "6",
                "page": "1–6",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Smart Emergency Management Based on Social Big Data Analytics: Research Trends and Future Directions",
                "URL": "https://doi.org/10.1145/3176653.3176657"
            }
        },
        {
            "10.1145/3523286.3524685": {
                "id": "10.1145/3523286.3524685",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Zeng",
                        "given": "Yan"
                    },
                    {
                        "family": "Li",
                        "given": "Jun"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2022,
                            1,
                            21
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2022,
                            1,
                            21
                        ]
                    ]
                },
                "abstract": "Abstract: Frequent itemsets mining is the core of association rule mining data. However, with the continuous increase of data, the traditional Apriori algorithm cannot meet people's daily needs, and the algorithm efficiency is low. This paper proposes the Eclat algorithm based on the Spark framework. In view of the shortcomings of serial algorithm in processing big data, it is modified. Using the vertical structure to avoid repetitive traversal of large amounts of data, while computing based on memory can greatly reduce I/O load and reduce computing time. Combined with the pruning strategy, the calculation of irrelevant itemsets is reduced, and the parallel computing capability of the algorithm is improved. The experimental results show that the efficiency of the Eclat algorithm based on the Spark framework is far better than that of the Eclat algorithm, and it has high efficiency and good scalability when processing massive data.",
                "call-number": "10.1145/3523286.3524685",
                "collection-title": "BIC 2022",
                "container-title": "2022 2nd International Conference on Bioinformatics and Intelligent Computing",
                "DOI": "10.1145/3523286.3524685",
                "event-place": "Harbin, China",
                "ISBN": "9781450395755",
                "keyword": "Spark framework, big data, Eclat, association rule mining",
                "number-of-pages": "6",
                "page": "333–338",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Optimization of Big Data Mining Algorithm Based on Spark Framework: Preparation of Camera-Ready Contributions to SCITEPRESS Proceedings",
                "URL": "https://doi.org/10.1145/3523286.3524685"
            }
        },
        {
            "10.1145/2661829.2661837": {
                "id": "10.1145/2661829.2661837",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Wang",
                        "given": "Hongzhi"
                    },
                    {
                        "family": "Li",
                        "given": "Mingda"
                    },
                    {
                        "family": "Bu",
                        "given": "Yingyi"
                    },
                    {
                        "family": "Li",
                        "given": "Jianzhong"
                    },
                    {
                        "family": "Gao",
                        "given": "Hong"
                    },
                    {
                        "family": "Zhang",
                        "given": "Jiacheng"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2014,
                            11,
                            3
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2014,
                            11,
                            3
                        ]
                    ]
                },
                "abstract": "In this demo, we present Cleanix, a prototype system for cleaning relational Big Data. Cleanix takes data integrated from multiple data sources and cleans them on a shared-nothing machine cluster. The backend system is built on-top-of an extensible and flexible data-parallel substrate - the Hyracks framework. Cleanix supports various data cleaning tasks such as abnormal value detection and correction, incomplete data filling, de-duplication, and conflict resolution. We demonstrate that Cleanix is a practical tool that supports effective and efficient data cleaning at the large scale.",
                "call-number": "10.1145/2661829.2661837",
                "collection-title": "CIKM '14",
                "container-title": "Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management",
                "DOI": "10.1145/2661829.2661837",
                "event-place": "Shanghai, China",
                "ISBN": "9781450325981",
                "keyword": "data cleaning, big data, data quality",
                "number-of-pages": "3",
                "page": "2024–2026",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Cleanix: A Big Data Cleaning Parfait",
                "URL": "https://doi.org/10.1145/2661829.2661837"
            }
        },
        {
            "10.1145/3342827.3342843": {
                "id": "10.1145/3342827.3342843",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Shen",
                        "given": "Zhengru"
                    },
                    {
                        "family": "Wang",
                        "given": "Xi"
                    },
                    {
                        "family": "Spruit",
                        "given": "Marco"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            6,
                            28
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            6,
                            28
                        ]
                    ]
                },
                "abstract": "The massive size of available biomedical literature requires researchers to utilize novel big data technologies in data storage and analysis. Among them is cloud computing which has become the most popular solution for big data applications in industry. However, many bioinformaticians still rely on expensive and inefficient in-house infrastructure to discover knowledge from biomedical literature. Although some cloud-based solutions were constructed recently, they failed to sufficiently address a few key issues including scalability, flexibility, and reusability. Moreover, no study has taken computational cost into consideration. To fill the gap, we proposed a cloud-based big data framework that enables researchers to perform reproducible and scalable large-scale biomedical literature mining in an efficient and cost-effective way. Additionally, a cloud agnostic platform was constructed and then evaluated on two open access corpora with millions of full-text biomedical articles. The results indicate that our framework supports scalable and efficient large-scale biomedical literature mining.",
                "call-number": "10.1145/3342827.3342843",
                "collection-title": "NLPIR 2019",
                "container-title": "Proceedings of the 2019 3rd International Conference on Natural Language Processing and Information Retrieval",
                "DOI": "10.1145/3342827.3342843",
                "event-place": "Tokushima, Japan",
                "ISBN": "9781450362795",
                "keyword": "cloud computing, document classification, topic modeling, biomedical literature, big data, text mining",
                "number-of-pages": "7",
                "page": "80–86",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Big Data Framework for Scalable and Efficient Biomedical Literature Mining in the Cloud",
                "URL": "https://doi.org/10.1145/3342827.3342843"
            }
        },
        {
            "10.1145/3361785.3361803": {
                "id": "10.1145/3361785.3361803",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Zareravasan",
                        "given": "Ahad"
                    },
                    {
                        "family": "Ashrafi",
                        "given": "Amir"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            9,
                            12
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            9,
                            12
                        ]
                    ]
                },
                "abstract": "Nowadays, big data analytics (BDA) have widely used in our business environment as an undeniable function for firms to not only survive in turbulence but also have the opportunity to be ahead of their major competitors. One of the promising aspects of BDA relates to its influence on innovation performance. In line, the present study proposed a conceptual model in order to investigate the relationship between BDA use and innovation performance by considering the role of dynamic capability (DC) theory. In this research, we consider firm agility in terms of DC theory and decompose it into three main factors contacting sensing agility, decision making agility, and acting agility. The research model and required data were analyzed using Partial Least Squares (PLS)/Structured Equation Modelling (SEM). The outcome of this study indicates that firms would be able to increase their innovation performance from a DC theory. This study also shows that BDA use has a positive influence on sensing agility of firms.",
                "call-number": "10.1145/3361785.3361803",
                "collection-title": "ICBIM '19",
                "container-title": "Proceedings of the 3rd International Conference on Business and Information Management",
                "DOI": "10.1145/3361785.3361803",
                "event-place": "Paris, France",
                "ISBN": "9781450372329",
                "keyword": "big data analytics (BDA), agility, innovation performance, dynamic capabilities (DC)",
                "number-of-pages": "5",
                "page": "97–101",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "An empirical investigation on big data analytics (BDA) and innovation performance",
                "URL": "https://doi.org/10.1145/3361785.3361803"
            }
        },
        {
            "10.1145/3511716.3511733": {
                "id": "10.1145/3511716.3511733",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Hu",
                        "given": "Zhifeng"
                    },
                    {
                        "family": "Zhao",
                        "given": "Feng"
                    },
                    {
                        "family": "Zhao",
                        "given": "Xiaona"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2022,
                            12,
                            29
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2021,
                            12,
                            29
                        ]
                    ]
                },
                "abstract": "Abstract: Big data technology is an important force to promote the reform of teaching. Smart classroom is the inevitable result of education modernization condensed in classroom teaching, education informatization converges on teacher-student interaction, and education intelligence converges on students’ smart thinking in the background of big data era. The hotspot of education intelligence research. The rapid development of artificial intelligence has made sufficient preparations for its involvement in school education and realization of smart classrooms. Based on the intelligent architecture of artificial intelligence, it will redefine classroom teaching with its closed-loop architecture formed by intelligent interaction and intelligent deep learning. To further strengthen the ideological education of college students is necessary to conform to the development situation of the big data era. Therefore, it is necessary to actively explore ideological education strategies for college students based on the era of big data by building network positions, applying social media, developing application software, relying on communication tools, and building smart classrooms. This article is based on the smart classroom information technology platform to design teaching goals and plans, implement interactive teaching, and innovate the teaching design and implementation strategies of smart classrooms.",
                "call-number": "10.1145/3511716.3511733",
                "collection-title": "EBIMCS 2021",
                "container-title": "2021 4th International Conference on E-Business, Information Management and Computer Science",
                "DOI": "10.1145/3511716.3511733",
                "event-place": "Hong Kong, China",
                "ISBN": "9781450395687",
                "keyword": "Artificial intelligence, Big data, Smart thinking",
                "number-of-pages": "9",
                "page": "102–110",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Research on the Design of Smart Classroom Mode from the Perspective of Big Data",
                "URL": "https://doi.org/10.1145/3511716.3511733"
            }
        },
        {
            "10.4108/icst.valuetools.2013.254398": {
                "id": "10.4108/icst.valuetools.2013.254398",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Barbierato",
                        "given": "Enrico"
                    },
                    {
                        "family": "Gribaudo",
                        "given": "Marco"
                    },
                    {
                        "family": "Iacono",
                        "given": "Mauro"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2013,
                            12,
                            10
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2013,
                            12,
                            10
                        ]
                    ]
                },
                "abstract": "Performance prediction for Big Data applications is a powerful tool supporting designers and administrators in achieving a better exploitation of their computing resources. Big Data architectures are complex, continuously evolving and adaptive, thus a rapid design and verification modeling approach can be fit to the needs. As a result, a minimal semantic gap between models and applications would enable a wider number of designers to directly benefit from the results. The paper presents a multiformalism modeling approach based on a one-to-one mapping of Apache Hive querying primitives to modeling primitives. This approach exploits a combination of proper Big Data specific submodels and Petri nets to enable modeling of conventional application logic.",
                "call-number": "10.4108/icst.valuetools.2013.254398",
                "collection-title": "ValueTools '13",
                "container-title": "Proceedings of the 7th International Conference on Performance Evaluation Methodologies and Tools",
                "DOI": "10.4108/icst.valuetools.2013.254398",
                "event-place": "Torino, Italy",
                "ISBN": "9781936968480",
                "number-of-pages": "9",
                "page": "30–38",
                "publisher": "ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)",
                "publisher-place": "Brussels, BEL",
                "title": "Modeling apache hive based applications in big data architectures",
                "URL": "https://doi.org/10.4108/icst.valuetools.2013.254398"
            }
        },
        {
            "10.1145/2905055.2905326": {
                "id": "10.1145/2905055.2905326",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Shekhar",
                        "given": "Nikkita"
                    },
                    {
                        "family": "Pawar",
                        "given": "Ambika V."
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            3,
                            4
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2016,
                            3,
                            4
                        ]
                    ]
                },
                "abstract": "As the capacity of main memory is growing, in-memory based big data analytics is becoming more popular. In-memory technologies support interactive analysis by providing high I/O throughput. On traditional high performance computing (HPC), big data processing needs data-intensive as well as computation-intensive systems for large data storage and high speed processing respectively. Currently, there are many such tools and technologies available which supports memory centric data processing to perform analysis on them. Taking advantage of in-memory on a HPC platform can result in a high speed, more reliable and fault tolerant data analysis. In this paper, we survey the existing storage and computation engines to perform big data analysis, and their performance while integrating together. Also, we discuss the contribution of such infrastructures in solving many I/O intensive analytical issues.",
                "call-number": "10.1145/2905055.2905326",
                "collection-number": "110",
                "collection-title": "ICTCS '16",
                "container-title": "Proceedings of the Second International Conference on Information and Communication Technology for Competitive Strategies",
                "DOI": "10.1145/2905055.2905326",
                "event-place": "Udaipur, India",
                "ISBN": "9781450339629",
                "keyword": "Tachyon, Hadoop, Analytics, High performance Computing, In-memory, Spark, I/O throughput, Big-data",
                "number": "Article 110",
                "number-of-pages": "5",
                "page": "1–5",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Big Data Analytics Based on In-Memory Infrastructure On Traditional HPC: A Survey",
                "URL": "https://doi.org/10.1145/2905055.2905326"
            }
        },
        {
            "10.1145/3152723.3152735": {
                "id": "10.1145/3152723.3152735",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Masood",
                        "given": "Mona"
                    },
                    {
                        "family": "Mokmin",
                        "given": "Nur Azlina Mohamed"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            10,
                            22
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            10,
                            22
                        ]
                    ]
                },
                "abstract": "The Big Data technology has paved a new way to provide inputs for the educational activities. The Case-based Reasoning (CBR) is an Artificial Intelligence (AI) algorithmthat is widely used for Big Data application. CBR has the ability to give a solution based on previous experiences like a human in making a decision. Thus, this study has designed and developed an Intelligent Tutoring System (ITS) that utilized the CBR to suggest the most suitable learning material to the students based on the real-time information of the students' profiles. The application has been tested in actual setting and the results show that the students that have been presented with personalized learning materials performed significantly better than the students that were presented with non-personalized lessons. The application also has the ability to calculate the similarity between cases accurately and presented the students with the most suitable learning materials.",
                "call-number": "10.1145/3152723.3152735",
                "collection-title": "ICBDR 2017",
                "container-title": "Proceedings of the 2017 International Conference on Big Data Research",
                "DOI": "10.1145/3152723.3152735",
                "event-place": "Osaka, Japan",
                "ISBN": "9781450353564",
                "keyword": "Mastery, Instructional, Understanding, Internet of Things, Artificial Intelligence, Self-Expressive, Learning Style, Intelligent Tutoring System, Case-based Reasoning, Mathematics, Interpersonal, Algebra, Big Data",
                "number-of-pages": "5",
                "page": "28–32",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Case-based Reasoning Intelligent Tutoring System: An Application of Big Data and IoT",
                "URL": "https://doi.org/10.1145/3152723.3152735"
            }
        },
        {
            "10.1145/2963143": {
                "id": "10.1145/2963143",
                "type": "ARTICLE",
                "author": [
                    {
                        "family": "Khalifa",
                        "given": "Shadi"
                    },
                    {
                        "family": "Elshater",
                        "given": "Yehia"
                    },
                    {
                        "family": "Sundaravarathan",
                        "given": "Kiran"
                    },
                    {
                        "family": "Bhat",
                        "given": "Aparna"
                    },
                    {
                        "family": "Martin",
                        "given": "Patrick"
                    },
                    {
                        "family": "Imam",
                        "given": "Fahim"
                    },
                    {
                        "family": "Rope",
                        "given": "Dan"
                    },
                    {
                        "family": "Mcroberts",
                        "given": "Mike"
                    },
                    {
                        "family": "Statchuk",
                        "given": "Craig"
                    }
                ],
                "issued": {
                    "date-parts": [
                        [
                            2016,
                            8,
                            2
                        ]
                    ]
                },
                "abstract": "With almost everything now online, organizations look at the Big Data collected to gain insights for improving their services. In the analytics process, derivation of such insights requires experimenting-with and integrating different analytics techniques, while handling the Big Data high arrival velocity and large volumes. Existing solutions cover bits-and-pieces of the analytics process, leaving it to organizations to assemble their own ecosystem or buy an off-the-shelf ecosystem that can have unnecessary components to them. We build on this point by dividing the Big Data Analytics problem into six main pillars. We characterize and show examples of solutions designed for each of these pillars. We then integrate these six pillars into a taxonomy to provide an overview of the possible state-of-the-art analytics ecosystems. In the process, we highlight a number of ecosystems to meet organizations different needs. Finally, we identify possible areas of research for building future Big Data Analytics Ecosystems.",
                "call-number": "10.1145/2963143",
                "collection-number": "33",
                "container-title": "ACM Comput. Surv.",
                "DOI": "10.1145/2963143",
                "ISSN": "0360-0300",
                "issue": "2",
                "keyword": "analytics talent gap, consumable analytics, Orchestration",
                "number": "Article 33",
                "number-of-pages": "36",
                "page": "1–36",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "source": "June 2017",
                "title": "The Six Pillars for Building Big Data Analytics Ecosystems",
                "URL": "https://doi.org/10.1145/2963143",
                "volume": "49"
            }
        },
        {
            "10.1145/3156346.3156347": {
                "id": "10.1145/3156346.3156347",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Kwoh",
                        "given": "Chee Keong"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2017,
                            12,
                            7
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2017,
                            12,
                            7
                        ]
                    ]
                },
                "abstract": "With the technological advances that allow for high throughput profiling of biological systems at a low cost. The low cost of data generation is leading us to the \"big data\" era. The availability of big data provides unprecedented opportunities but also raises new challenges for data mining and analysis. In this talk, I will start will the concepts in the analysis of big data, specifically the AI algorithms.My group has in The Biomedical Informatics Lab (BIL) is a research Centre is the focus of the education, research and development, and human-resource training in heath informatics and bioinformatics at NTU. The mission of BIL is to provide the interdisciplinary environment and training for students and researchers to engage in leading and cutting edge research in bioinformatics, and thereby become a part of the life sciences workforce in Singapore and elsewhere.This talk, by presenting selected research activities, will provide an overview of some of the innovative and creative approaches with the application of AI in big data analytics to address the challenges and solutions in both health and bioinformatics.",
                "call-number": "10.1145/3156346.3156347",
                "collection-title": "CSBio '17",
                "container-title": "Proceedings of the 8th International Conference on Computational Systems-Biology and Bioinformatics",
                "DOI": "10.1145/3156346.3156347",
                "event-place": "Nha Trang City, Viet Nam",
                "ISBN": "9781450353502",
                "number-of-pages": "1",
                "page": "1",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "AI and Big Data Analytics for Health and Bioinformatics",
                "URL": "https://doi.org/10.1145/3156346.3156347"
            }
        },
        {
            "10.1145/3357223.3366029": {
                "id": "10.1145/3357223.3366029",
                "type": "PAPER_CONFERENCE",
                "author": [
                    {
                        "family": "Patel",
                        "given": "Hiren"
                    },
                    {
                        "family": "Jindal",
                        "given": "Alekh"
                    },
                    {
                        "family": "Szyperski",
                        "given": "Clemens"
                    }
                ],
                "accessed": {
                    "date-parts": [
                        [
                            2022,
                            10,
                            6
                        ]
                    ]
                },
                "issued": {
                    "date-parts": [
                        [
                            2019,
                            11,
                            20
                        ]
                    ]
                },
                "original-date": {
                    "date-parts": [
                        [
                            2019,
                            11,
                            20
                        ]
                    ]
                },
                "abstract": "The past decade has seen a tremendous interest in large-scale data processing at Microsoft. Typical scenarios include building business-critical pipelines such as advertiser feedback loop, index builder, and relevance/ranking algorithms for Bing; analyzing user experience telemetry for Office, Windows or Xbox; and gathering recommendations for products like Windows and Xbox. To address these needs a first-party big data analytics platform, referred to as Cosmos, was developed in the early 2010s at Microsoft. Cosmos makes it possible to store data at exabyte scale and process in a serverless form factor, with SCOPE [4] being the query processing workhorse. Over time, however, several newer challenges have emerged, requiring major technical innovations in Cosmos to meet these newer demands. In this abstract, we describe three such challenges from the query processing viewpoint, and our approaches to handling them.Hyper Scale. Cosmos has witnessed a significant growth in usage from its early days, from the number of customers (starting from Bing to almost every single business unit at Microsoft today), to the volume of data processed (from petabytes to exabytes today), to the amount of processing done (from tens of thousands of SCOPE jobs to hundreds of thousands of jobs today, across hundreds of thousands of machines). Even a single job can consume tens of petabytes of data and produce similar volumes of data by running millions of tasks in parallel. Our approach to handle this unprecedented scale is two fold. First, we decoupled and disaggregated the query processor from storage and resource management components, thereby allowing different components in the Cosmos stack to scale independently. Second, we scaled the data movement in the SCOPE query processor with quasilinear complexity [2]. This is crucial since data movement is often the most expensive step, and hence the bottleneck, in massive-scale data processing.Massive Complexity. Cosmos workloads are also highly complex. Thanks to adoption across the whole of Microsoft, Cosmos needs to support workloads that are representative of multiple industry segments, including search engine (Bing), operating system (Windows), workplace productivity (Office), personal computing (Surface), gaming (XBox), etc. To handle such diverse workloads, our approach has been to provide a one-size-fits-all experience. First of all, to make it easy for the customers to express their computations, SCOPE supports different types of queries, from batch to interactive to streaming and machine learning. Second, SCOPE supports both structured and unstructured data processing. Likewise, multiple data formats, including both propriety and open source source such as Parquet, are supported. Third, users can write business logic using a mix of declarative and imperative languages, over even different imperative languages such as C# and Python, in the same job. Furthermore, users can express all of the above in simple data flow style computation for better readability and maintainability. Finally, considering the diverse workload mix inside Microsoft, we have come to realization that it is not possible to fits all scenarios using SCOPE. Therefore, we also support the popular Spark query processing engine. Overall, the one-size-fits-all query processing experience in Cosmos covers very diverse workloads, including data formats, programming languages, and the backend engines.Minimal Cost. While scale and complexity are hard by themselves, the biggest challenge is to achieve all of that at minimal cost. In fact, there is a pressing need to improve Cosmos efficiency and reduce operational costs. This is challenging due to several reasons. First, optimizing a SCOPE job is hard considering that the SCOPE DAGs are super large (up to 1000s of operators in single job!), and the optimization estimates (cardinality, cost, etc.) are often way off from the actuals. Second, SCOPE optimizes a given query, while the operational costs depend on the overall workload. Therefore workload optimization becomes very important. And finally, SCOPE jobs are typically interlinked in data pipelines, i.e., the output of one job is consumed by other jobs. This means that workload optimization needs to be aware of these dependencies. Our approach is to develop a feedback loop to learn from past workloads in order to optimize the future ones. Specifically, we leverage machine learning to learn models for optimizing individual jobs [3], apply multi-query optimizations to optimize the costs of overall workload [1], and build dependency graphs to identify and optimize for the data pipelines.",
                "call-number": "10.1145/3357223.3366029",
                "collection-title": "SoCC '19",
                "container-title": "Proceedings of the ACM Symposium on Cloud Computing",
                "DOI": "10.1145/3357223.3366029",
                "event-place": "Santa Cruz, CA, USA",
                "ISBN": "9781450369732",
                "number-of-pages": "1",
                "page": "490",
                "publisher": "Association for Computing Machinery",
                "publisher-place": "New York, NY, USA",
                "title": "Big Data Processing at Microsoft: Hyper Scale, Massive Complexity, and Minimal Cost",
                "URL": "https://doi.org/10.1145/3357223.3366029"
            }
        }
    ]
}