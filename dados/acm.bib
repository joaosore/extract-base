@inproceedings{10.1145/3341620.3341629,
author = {El Alaoui, Imane and Gahi, Youssef and Messoussi, Rochdi},
title = {Big Data Quality Metrics for Sentiment Analysis Approaches},
year = {2019},
isbn = {9781450360913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341620.3341629},
doi = {10.1145/3341620.3341629},
abstract = {In a world increasingly connected, and in which information flows quickly and affects a very large number of people, sentiment analysis has seen a spectacular development over the past ten years. This is due to the fact that the explosion of social networks has allowed anyone with internet access to publicly express his opinion. Moreover, the emergence of big data has brought enormous opportunities and powerful storage and analytics tools to the field of sentiment analysis. However, big data introduces new variables and constraints that could radically affect the traditional models of sentiment analysis. Therefore, new concerns, such as big data quality, have to be addressed to get the most out of big data. To the best of our knowledge, no contributions have been published so far which address big data quality in SA throughout its different processes. In this paper, we first highlight the most important big data quality metrics to consider in any big data project. Then, we show how these metrics could be specifically considered in SA approaches and this for each phase in the big data value chain.},
booktitle = {Proceedings of the 2019 International Conference on Big Data Engineering},
pages = {36–43},
numpages = {8},
keywords = {Big data, Sentiment analysis, Big data quality metrics},
location = {Hong Kong, Hong Kong},
series = {BDE 2019}
}

@inproceedings{10.1145/3419604.3419803,
author = {Reda, Oumaima and Sassi, Imad and Zellou, Ahmed and Anter, Samir},
title = {Towards a Data Quality Assessment in Big Data},
year = {2020},
isbn = {9781450377331},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3419604.3419803},
doi = {10.1145/3419604.3419803},
abstract = {In recent years, as more and more data sources have become available and the volumes of data potentially accessible have increased, the assessment of data quality has taken a central role whether at the academic, professional or any other sector. Given that users are often concerned with the need to filter a large amount of data to better satisfy their requirements and needs, and that data analysis can be based on inaccurate, incomplete, ambiguous, duplicated and of poor quality, it makes everyone wonder what the results of these analyses will really be like. However, there is a very complex process involved in the identification of new, valid, potentially useful and meaningful data from a large data collection and various information systems, and is critically dependent on a number of measures to be developed to ensure data quality. To this end, the main objective of this paper is to introduce a general study on data quality related with big data, by providing what other researchers came up with on that subject. The paper will be finalized by a comparative study between the different existing data quality models.},
booktitle = {Proceedings of the 13th International Conference on Intelligent Systems: Theories and Applications},
articleno = {16},
numpages = {6},
keywords = {Data Quality, Big Data, Quality Models, Data Quality evaluation},
location = {Rabat, Morocco},
series = {SITA'20}
}

@inproceedings{10.1145/3010089.3010090,
author = {Emmanuel, Isitor and Stanier, Clare},
title = {Defining Big Data},
year = {2016},
isbn = {9781450347792},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3010089.3010090},
doi = {10.1145/3010089.3010090},
abstract = {As Big Data becomes better understood, there is a need for a comprehensive definition of Big Data to support work in fields such as data quality for Big Data. Existing definitions of Big Data define Big Data by comparison with existing, usually relational, definitions, or define Big Data in terms of data characteristics or use an approach which combines data characteristics with the Big Data environment. In this paper we examine existing definitions of Big Data and discuss the strengths and limitations of the different approaches, with particular reference to issues related to data quality in Big Data. We identify the issues presented by incomplete or inconsistent definitions. We propose an alternative definition and relate this definition to our work on quality in Big Data.},
booktitle = {Proceedings of the International Conference on Big Data and Advanced Wireless Technologies},
articleno = {5},
numpages = {6},
keywords = {Big Data, Data Quality Dimensions, Big Data characteristics, Data Quality},
location = {Blagoevgrad, Bulgaria},
series = {BDAW '16}
}

@inproceedings{10.1145/3281022.3281026,
author = {Baldassarre, Maria Teresa and Caballero, Ismael and Caivano, Danilo and Rivas Garcia, Bibiano and Piattini, Mario},
title = {From Big Data to Smart Data: A Data Quality Perspective},
year = {2018},
isbn = {9781450360548},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281022.3281026},
doi = {10.1145/3281022.3281026},
abstract = {Big Data (BD) solutions are designed to better support decision-making processes in order to optimize organizational performance. These BD solutions use company’s core business data, using typically large datasets. However, data that doesn’t meet adequate quality levels will lead to BD solutions that will not produce useful results, and consequently may not be used to make adequate business decisions. For a long time, companies have collected and stored large amounts of data without being able to exploit the advantage of exploring it. Nowadays, and thanks to the Big Data explosion, organizations have begun to recognize the need for estimating the value of their data and, vice-versa, managing data accordingly to their value. This need of managing the Value of data has led to the concept of Smart Data. It not only involves the datasets, but also the set of technologies, tools, processes and methodologies that enable all the Values from the data to the End-users (Business, data scientist, BI…). Consequently, Smart data is data actionable. We discovered that data quality is one of the most important issues when it comes to “smartizing” data. In this paper, we introduce a methodology to make data smarter, taking as a reference point, the quality level of the data itself.},
booktitle = {Proceedings of the 1st ACM SIGSOFT International Workshop on Ensemble-Based Software Engineering},
pages = {19–24},
numpages = {6},
keywords = {Big Data, Smart Data, Data Quality},
location = {Lake Buena Vista, FL, USA},
series = {EnSEmble 2018}
}

@inproceedings{10.1145/2513591.2527071,
author = {Cuzzocrea, Alfredo and Sacc\`{a}, Domenico and Ullman, Jeffrey D.},
title = {Big Data: A Research Agenda},
year = {2013},
isbn = {9781450320252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2513591.2527071},
doi = {10.1145/2513591.2527071},
abstract = {Recently, a great deal of interest for Big Data has risen, mainly driven from a widespread number of research problems strongly related to real-life applications and systems, such as representing, modeling, processing, querying and mining massive, distributed, large-scale repositories (mostly being of unstructured nature). Inspired by this main trend, in this paper we discuss three important aspects of Big Data research, namely OLAP over Big Data, Big Data Posting, and Privacy of Big Data. We also depict future research directions, hence implicitly defining a research agenda aiming at leading future challenges in this research field.},
booktitle = {Proceedings of the 17th International Database Engineering &amp; Applications Symposium},
pages = {198–203},
numpages = {6},
keywords = {big data, OLAP over big data, privacy of big data, big data posting},
location = {Barcelona, Spain},
series = {IDEAS '13}
}

@inproceedings{10.1145/3141128.3141139,
author = {Pti\v{c}ek, Marina and Vrdoljak, Boris},
title = {Big Data and New Data Warehousing Approaches},
year = {2017},
isbn = {9781450353434},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3141128.3141139},
doi = {10.1145/3141128.3141139},
abstract = {Big data are a data trend present around us mainly through Internet -- social networks and smart devices and meters -- mostly without us being aware of them. Also they are a fact that both industry and scientific research needs to deal with. They are interesting from analytical point of view, for they contain knowledge that cannot be ignored and left unused. Traditional system that supports the advanced analytics and knowledge extraction -- data warehouse -- is not able to cope with large amounts of fast incoming various and unstructured data, and may be facing a paradigm shift in terms of utilized concepts, technologies and methodologies, which have become a very active research area in the last few years. This paper provides an overview of research trends important for the big data warehousing, concepts and technologies used for data storage and (ETL) processing, and research approaches done in attempts to empower traditional data warehouses for handling big data.},
booktitle = {Proceedings of the 2017 International Conference on Cloud and Big Data Computing},
pages = {6–10},
numpages = {5},
keywords = {MapReduce, data warehouse, NewSQL, databases, big data, NoSQL},
location = {London, United Kingdom},
series = {ICCBDC 2017}
}

@inproceedings{10.1145/2658840.2658845,
author = {Neamtu, Rodica and Ahsan, Ramoza and Stokes, Jeff and Hoxha, Armend and Bao, Jialiang and Gvozdenovic, Stefan and Meyer, Ted and Patel, Nilesh and Rangan, Raghu and Wang, Yumou and Zhang, Dongyun and Rundensteiner, Elke A.},
title = {Taming Big Data: Integrating Diverse Public Data Sources for Economic Competitiveness Analytics},
year = {2014},
isbn = {9781450331869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658840.2658845},
doi = {10.1145/2658840.2658845},
abstract = {In an era where Big Data can greatly impact a broad population, many novel opportunities arise, chief among them the ability to integrate data from diverse sources and "wrangle" it to extract novel insights. Conceived as a tool that can help both expert and non-expert users better understand public data, MATTERS was collaboratively developed by the Massachusetts High Tech Council, WPI and other institutions as an analytic platform offering dynamic modeling capabilities. MATTERS is an integrative data source on high fidelity cost and talent competitiveness metrics. Its goal is to extract, integrate and model rich economic, financial, educational and technological information from renowned heterogeneous web data sources ranging from The US Census Bureau, The Bureau of Labor Statistics to the Institute of Education Sciences, all known to be critical factors influencing economic competitiveness of states. This demonstration of MATTERS illustrates how we tackle challenges of data acquisition, cleaning, integration and wrangling into appropriate representations, visualization and story-telling with data in the context of state competitiveness in the high-tech sector.},
booktitle = {Proceedings of the First International Workshop on Bringing the Value of "Big Data" to Users (Data4U 2014)},
pages = {25–28},
numpages = {4},
keywords = {data integration, diverse data sources, Big data},
location = {Hangzhou, China},
series = {Data4U '14}
}

@inproceedings{10.1145/3538950.3538951,
author = {Zhang, Lin and Jiang, Rong and Wang, Meng and Yang, Yue and Wang, Chenguang},
title = {A Drug Safety Traceability Model Based on Big Data},
year = {2022},
isbn = {9781450395632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3538950.3538951},
doi = {10.1145/3538950.3538951},
abstract = {In order to solve the storage and management problems of heterogeneous drug data, this paper uses big data technology to complete the cleaning and distributed storage of drug data, and improve the function of data sharing and traceability. At the same time, in order to improve the drug traceability function, ensure the reliable storage of traceability information, and make the traceability process more reliable. This paper will put forward a drug traceability system model based on big data on the basis of existing research. Secondly, an evidence chain framework is proposed to verify evidence files. At last, the simulation experiment is carried out to test and illustrate the credibility of the traceability verification model.},
booktitle = {Proceedings of the 4th International Conference on Big Data Engineering},
pages = {1–7},
numpages = {7},
keywords = {Big Data, Drug Traceability, Traceability Verification},
location = {Beijing, China},
series = {BDE '22}
}

@inproceedings{10.1145/3206157.3206166,
author = {Khan, Nawsher and Alsaqer, Mohammed and Shah, Habib and Badsha, Gran and Abbasi, Aftab Ahmad and Salehian, Soulmaz},
title = {The 10 Vs, Issues and Challenges of Big Data},
year = {2018},
isbn = {9781450363587},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206157.3206166},
doi = {10.1145/3206157.3206166},
abstract = {In this emerging computing and digital globe, information and Knowledge are created and then collected with a rapid approach by wide range of applications through scientific computing and commercial workloads. Over 3.8 billion people out of 7.6 billion population of the world are connected to the internet. Out of 13.4 billion devices, 8.06 billion devices have a mobile connection. In 2020, 38.5 billion devices will be connected and globally internet traffic will be 92 times greater than it was in 2005. The use of such devices and internet not only increase the data volume but the velocity of market brings in fast-track and accelerates as information is transferred and shared with light speed on optic fiber and wireless networks. This fast generation of huge data creates numerous challenges. The existing approaches addressing issues such as, Volume, Variety, Velocity and Value in big data research perspective. The objectives of the paper are to investigate and analyze the current status of Big Data and furthermore a comprehensive overview of various aspects has discussed, and additionally has been described all 10 Vs' (Issues) of Big Data.},
booktitle = {Proceedings of the 2018 International Conference on Big Data and Education},
pages = {52–56},
numpages = {5},
keywords = {Big Data, Data Management},
location = {Honolulu, HI, USA},
series = {ICBDE '18}
}

@inproceedings{10.1145/2640087.2644168,
author = {Cho, Wonhee and Lim, Yoojin and Lee, Hwangro and Varma, Mohan Krishna and Lee, Moonsoo and Choi, Eunmi},
title = {Big Data Analysis with Interactive Visualization Using R Packages},
year = {2014},
isbn = {9781450328913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2640087.2644168},
doi = {10.1145/2640087.2644168},
abstract = {Compared to the traditional data storing, processing, analyzing and visualization which have been performed, Big data requires evolutionary technologies of massive data processing on distributed and parallel systems, such as Hadoop system. Big data analytic systems, thus, have been popular to derive important decision making in various areas. However, visualization on analytic system faces various limitation due to the huge amount of data. This brings the necessity of interactive visualization techniques beyond the traditional static visualization. R has been used and improved for a big data analysis and mining tool. Also, R is supported with various and abundant packages for different targets with visualization. However interactive visualization packages are not easily found in the market. This paper compares and analyzes interactive web packages with visualization packages for R. This paper also proposes interactive web visualized analysis environment for big data with a combination of interactive web packages and visualization packages. In particular, Big data analysis techniques with sensed data are presented as the result by reflecting the decision view on sensing field.},
booktitle = {Proceedings of the 2014 International Conference on Big Data Science and Computing},
articleno = {18},
numpages = {6},
keywords = {Visualization, Big data, Hadoop, R, Mining},
location = {Beijing, China},
series = {BigDataScience '14}
}

@inproceedings{10.1145/3404687.3404694,
author = {Raza, Muhammad Umair and XuJian, Zhao},
title = {A Comprehensive Overview of BIG DATA Technologies: A Survey},
year = {2020},
isbn = {9781450375474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404687.3404694},
doi = {10.1145/3404687.3404694},
abstract = {In as much as the approaches of the new revolution, machines including transmission media like social media sites, nowadays quantity of data swell hastily. So, size is the core and only facet that leaps the mention of BIG DATA. In this article, an effort to touch a comprehensive view of big data technologies, because of the swift evolution of data by an industry trying the academic press to catch up. This paper also offers a unified explanation of big data as well as the analytics methods. A practical discriminate characteristic of this paper is core analytics associated with unstructured data which is more than 90% of big data. To deal with complicated Big Data problems, great work has been done. This paper analyzes contemporary Big Data technologies. Therein article further strengthens the necessity to formulate new tools for analytics. It bestows not sole an intercontinental overview of big data techniques even though the valuation according to big data Hadoop Ecosystem. It classifies and debates the main technologies feature, challenges, and usage as well.},
booktitle = {Proceedings of the 5th International Conference on Big Data and Computing},
pages = {23–31},
numpages = {9},
keywords = {Big Data Technology, MapReduce, Apache Hadoop, HDFS},
location = {Chengdu, China},
series = {ICBDC '20}
}

@inproceedings{10.1145/3456389.3456390,
author = {Cao, Shuangshuang},
title = {Opportunities and Challenges of Marketing in the Context of Big Data},
year = {2021},
isbn = {9781450389945},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3456389.3456390},
doi = {10.1145/3456389.3456390},
abstract = {In the era of big data, under the conditions of rapid economic development in our country, various enterprises have also vigorously carried out marketing. In the context of big data, marketing research should be strengthened to effectively improve market. Market issues ensure that marketing has improved its status in the era of big data. This article has conducted a research and analysis on marketing in the context of big data. And then the opportunities and challenges of marketing in the context of big data has been explained, which gradually optimize the marketing implementation effect. The challenges faced by marketing has been understood which ensures that the big data model plays its best role in it.},
booktitle = {2021 Workshop on Algorithm and Big Data},
pages = {79–82},
numpages = {4},
keywords = {Big data, Personalized service, Marketing},
location = {Fuzhou, China},
series = {WABD 2021}
}

@inproceedings{10.1145/2896825.2896837,
author = {Chen, Hong-Mei and Kazman, Rick and Garbajosa, Juan and Gonzalez, Eloy},
title = {Toward Big Data Value Engineering for Innovation},
year = {2016},
isbn = {9781450341523},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896825.2896837},
doi = {10.1145/2896825.2896837},
abstract = {This article articulates the requirements for an effective big data value engineering method. It then presents a value discovery method, called Eco-ARCH (Eco-ARCHitecture), tightly integrated with the BDD (Big Data Design) method for addressing these requirements, filling a methodological void. Eco-ARCH promotes a fundamental shift in design thinking for big data system design -- from "bounded rationality" for problem solving to "expandable rationality" for design for innovation. The Eco-ARCH approach is most suitable for big data value engineering when system boundaries are fluid, requirements are ill-defined, many stakeholders are unknown and design goals are not provided, where no architecture pre-exists, where system behavior is non-deterministic and continuously evolving, and where co-creation with consumers and prosumers is essential to achieving innovation goals. The method was augmented and empirically validated in collaboration with an IT service company in the energy industry to generate a new business model that we call "eBay in the Grid".},
booktitle = {Proceedings of the 2nd International Workshop on BIG Data Software Engineering},
pages = {44–50},
numpages = {7},
keywords = {energy industry, innovation, big data, value discovery, ecosystem, architecture landscape, value engineering},
location = {Austin, Texas},
series = {BIGDSE '16}
}

@inproceedings{10.5555/2819289.2819302,
author = {Chen, Hong-Mei and Kazman, Rick and Haziyev, Serge and Hrytsay, Olha},
title = {Big Data System Development: An Embedded Case Study with a Global Outsourcing Firm},
year = {2015},
publisher = {IEEE Press},
abstract = {Big data system development is dramatically different from small (traditional, structured) data system development. At the end of 2014, big data deployment is still scarce and failures abound. Outsourcing has become a main strategy for many enterprises. We therefore selected an outsourcing company who has successfully deployed big data projects for our study. Our research results from analyzing 10 outsourced big data projects provide a glimpse into early adopters of big data, illuminates the challenges for system development that stem from the 5Vs of big data and crystallizes the importance of architecture design choices and technology selection. We followed a collaborative practice research (CPR) method to develop and validate a new method, called BDD. BDD is the first attempt to systematically combine architecture design with data modeling approaches to address big data system development challenges. The use of reference architectures and a technology catalog are advancements to architecture design methods and are proving to be well-suited for big data system architecture design and system development.},
booktitle = {Proceedings of the First International Workshop on BIG Data Software Engineering},
pages = {44–50},
numpages = {7},
keywords = {big data, system engineering, software architecture, embedded case study methodology, data system design methods, collaborative practice research},
location = {Florence, Italy},
series = {BIGDSE '15}
}

@inproceedings{10.1145/3335484.3335545,
author = {Zhi-peng, Sun and Gui-ming, Chen and Hui, Zhang},
title = {Evaluation of Large-Scale Complex Systems Effectiveness Based on Big Data},
year = {2019},
isbn = {9781450362788},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3335484.3335545},
doi = {10.1145/3335484.3335545},
abstract = {With the advent of the information age, big data technology came into being. The wide application of big data brings new opportunities and challenges to the construction of national defense and military information. Under the background of information-based joint operations characterized by large complex systems, how to scientifically and rationally plan the construction of large complex systems, and maximize the effectiveness of the complex system has become a key concern for system construction decision makers and researchers. This paper combines the application of big data in the construction of large complex systems, and focuses on the evaluation of the effectiveness of large complex systems based on big data, which can be used for reference by relevant researchers.},
booktitle = {Proceedings of the 4th International Conference on Big Data and Computing},
pages = {72–76},
numpages = {5},
keywords = {large-scale complex systems, evaluation, effectiveness, big data},
location = {Guangzhou, China},
series = {ICBDC '19}
}

@article{10.1145/3408314,
author = {Davoudian, Ali and Liu, Mengchi},
title = {Big Data Systems: A Software Engineering Perspective},
year = {2020},
issue_date = {September 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3408314},
doi = {10.1145/3408314},
abstract = {Big Data Systems (BDSs) are an emerging class of scalable software technologies whereby massive amounts of heterogeneous data are gathered from multiple sources, managed, analyzed (in batch, stream or hybrid fashion), and served to end-users and external applications. Such systems pose specific challenges in all phases of software development lifecycle and might become very complex by evolving data, technologies, and target value over time. Consequently, many organizations and enterprises have found it difficult to adopt BDSs. In this article, we provide insight into three major activities of software engineering in the context of BDSs as well as the choices made to tackle them regarding state-of-the-art research and industry efforts. These activities include the engineering of requirements, designing and constructing software to meet the specified requirements, and software/data quality assurance. We also disclose some open challenges of developing effective BDSs, which need attention from both researchers and practitioners.},
journal = {ACM Comput. Surv.},
month = {sep},
articleno = {110},
numpages = {39},
keywords = {software engineering, software reference architecture, quality assurance, requirements engineering, Big Data, Big Data systems}
}

@inproceedings{10.1145/2811222.2811235,
author = {Abell\'{o}, Alberto},
title = {Big Data Design},
year = {2015},
isbn = {9781450337854},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2811222.2811235},
doi = {10.1145/2811222.2811235},
abstract = {It is widely accepted today that Relational databases are not appropriate in highly distributed shared-nothing architectures of commodity hardware, that need to handle poorly structured heterogeneous data. This has brought the blooming of NoSQL systems with the purpose of mitigating such problem, specially in the presence of analytical workloads. Thus, the change in the data model and the new analytical needs beyond OLAP take us to rethink methods and models to design and manage these newborn repositories. In this paper, we will analyze state of the art and future research directions.},
booktitle = {Proceedings of the ACM Eighteenth International Workshop on Data Warehousing and OLAP},
pages = {35–38},
numpages = {4},
keywords = {nosql, big data, database design},
location = {Melbourne, Australia},
series = {DOLAP '15}
}

@inproceedings{10.1145/3366030.3366121,
author = {Safhi, Hicham Moad and Frikh, Bouchra and Ouhbi, Brahim},
title = {Data Source Selection in Big Data Context},
year = {2019},
isbn = {9781450371797},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366030.3366121},
doi = {10.1145/3366030.3366121},
abstract = {Big Data presents promising technological and economical opportunities. In fact, it has become the raw material of production for many organizations. Data is available in large quantities, and it continues generating abundantly. However, not all the data will have valuable knowledge. Unreliable sources provide misleading and biased information, and even reliable sources could suffer from low data quality.In this paper, we propose a novel methodology for the selectability of data sources, by both considering the presence and the absence of users' preferences. The proposed model integrates multiple factors that affect the reliability of data sources, including their quality, gain, cost and coverage. Experimental results on real world data-sets, show its capability to find the subset of relevant and reliable sources with the lowest cost.},
booktitle = {Proceedings of the 21st International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {611–616},
numpages = {6},
keywords = {Big Data Source Selection, Source reliability, Big Data integration, Data quality},
location = {Munich, Germany},
series = {iiWAS2019}
}

@inproceedings{10.1145/2656346.2656358,
author = {Mohammad, Atif and Mcheick, Hamid and Grant, Emanuel},
title = {Big Data Architecture Evolution: 2014 and Beyond},
year = {2014},
isbn = {9781450330282},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2656346.2656358},
doi = {10.1145/2656346.2656358},
abstract = {This paper aims at developing the Big Data Architecture, and its relation with Analytics, Cloud Services as well as Business Intelligence. The chief aim from all mentioned is to enable the Enterprise Architecture and the Vision of an Organizational target to utilize all the data they are ingesting and regressing data for their short-term or long-terms analytical needs, while making sure that they are addressing during the design phase of such data architecture for both directly and indirectly related stakeholder. Since all stakeholders have their relative interests to utilize the transformed data-sets. This paper also identifies most of the Big Data Architecture, threat analysis within a Big Data System and Big Data Analytic Roadmaps, in terms of smaller components by conducting a gap-analysis that has significant importance as Baseline Big Data Architecture, targeting the end resultant Architectures, once the distillation process of main Big Data Architecture is completed by the Data Architects.},
booktitle = {Proceedings of the Fourth ACM International Symposium on Development and Analysis of Intelligent Vehicular Networks and Applications},
pages = {139–144},
numpages = {6},
keywords = {cloud computing, big data},
location = {Montreal, QC, Canada},
series = {DIVANet '14}
}

@article{10.1145/2331042.2331058,
author = {Heer, Jeffrey and Kandel, Sean},
title = {Interactive Analysis of Big Data},
year = {2012},
issue_date = {Fall 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {1},
issn = {1528-4972},
url = {https://doi.org/10.1145/2331042.2331058},
doi = {10.1145/2331042.2331058},
abstract = {New user interfaces can transform how we work with big data, and raise exciting research problems that span human-computer interaction, machine learning, and distributed systems.},
journal = {XRDS},
month = {sep},
pages = {50–54},
numpages = {5}
}

